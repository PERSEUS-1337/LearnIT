{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions. NPs efficiently fit data with linear complexity and can model a wide range of conditional distributions. However, they suffer from underfitting issues. Attention mechanisms are incorporated into NPs to improve prediction accuracy, speed up training, and expand the range of functions that can be modeled. In regression, Neural Processes (NPs) model a distribution over functions mapping inputs to outputs, allowing for reasoning about multiple functions consistent with the data. NPs offer an efficient method for predicting the distribution of target outputs based on context input-output pairs, with flexibility to model data generated from a stochastic process. Neural Processes (NPs) can model data from a stochastic process and offer efficient prediction based on context input-output pairs. However, NPs may underfit the context set, leading to inaccurate predictions. The encoder aggregates context to a fixed-length latent summary, affecting the model's reconstruction accuracy. The model's reconstruction of the top-half is imperfect due to underfitting behavior caused by the mean-aggregation step in the encoder acting as a bottleneck. Increasing dimensionality alone is not sufficient to address this issue. Drawing inspiration from Gaussian Processes (GPs), which define conditional distributions for regression, can help mitigate underfitting by considering the similarity among context points. The Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by using differentiable attention to attend to relevant contexts, preserving permutation invariance. ANPs show enhanced expressiveness, better reconstruction of contexts, and faster training. NPs are models for regression functions that map inputs to outputs, with conditional distributions that are invariant to ordering of contexts and targets. Inspired by Gaussian Processes, ANPs mitigate underfitting by considering similarity among context points. The model for Neural Processes (NPs) is defined to be invariant to the ordering of contexts and targets. It uses a deterministic function to aggregate context pairs and model the likelihood of targets with a Gaussian distribution. A latent variable version of the NP model includes a global latent variable to address uncertainty in predictions. The Neural Processes model incorporates a latent path with a factorised Gaussian for z, modelled by s. The encoder and decoder parameters are learned by maximizing the ELBO for a subset of contexts and targets. The global latent variable in the model allows for different realisations of the data generating process. The Neural Processes model uses a reparametrisation trick to reconstruct targets from contexts, regularised by a KL term. It learns a wide family of conditional distributions with scalability, flexibility, and permutation invariance properties. The Neural Processes model utilizes a reparametrisation trick for reconstructing targets from contexts, with scalability, flexibility, and permutation invariance properties. Attention mechanisms compute weights for key-value pairs based on a query, allowing for aggregation of values. This permutation invariance property is crucial for Neural Processes applications. Attention mechanisms play a key role in Neural Processes by computing weights for key-value pairs based on a query. Differentiable addressing mechanisms have been successfully applied in various areas of Deep Learning. Examples include self-attention for expressive sequence-to-sequence mappings in natural language processing and image modeling. Simple forms of attention, such as dot-product attention, allow for similarity measurement between queries and keys to weight the keys accordingly. The use of dot-product attention in multihead architecture allows for efficient computation of query values with matrix multiplications and softmax. This architecture enables the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. Self-attention is applied to context points to compute representations of each (x, y) pair, with the target input attending to these context representations for predicting the target output. The self-attention mechanism models interactions between context points to obtain richer representations that encode relations. In the deterministic path, mean-aggregation is replaced by cross-attention, allowing each query to attend closely to relevant context points for prediction. The latent path preserves global latent dependencies between target predictions. The latent path in the model preserves global latent dependencies between target predictions, with z giving rise to correlations in the marginal distribution of the predictions. The decoder remains the same, but with a query-specific representation. The NP with attention increases expressivity and accuracy but also raises computational complexity. The computational complexity of (A)NPs is raised to O(n(n + m)) due to self-attention across contexts. Despite this, training time remains comparable to NPs, with ANPs learning significantly faster. The (A)NP learns a stochastic process and should be trained on multiple functions. The decoder architecture is consistent across experiments, with 8 heads for multihead. The (A)NPs are trained on data generated from a Gaussian Process with varying kernel hyperparameters. The number of contexts and targets are randomly chosen, and cross-attention is used in the deterministic path. ANP shows faster learning compared to NP when trained on a GP with random kernel hyperparameters. The ANP demonstrates faster learning and lower reconstruction error compared to NP when trained on a Gaussian Process with random kernel hyperparameters. Different attention mechanisms show varying performance, with multihead ANP outperforming NP in terms of convergence time and reconstruction quality. Increasing the bottleneck size in NPs improves reconstructions up to a limit, beyond which learning becomes slow, highlighting the benefits of using ANPs over solely increasing bottleneck size in NPs. In a comparison between attention mechanisms in the context of Gaussian Process training, it is observed that dot-product attention outperforms Laplace attention due to the learned representation space for similarities. The predictive means of NPs underfit the context, while dot-product attention accurately predicts most context points. The multiple heads in multihead attention help smooth out interpolations, improving context reconstruction and target prediction. The (A)NP is more expressive than the NP, learning a wider range of functions. Trained (A)NPs are used in a toy Bayesian Optimization problem, showcasing the ability to sample entire functions. The task involves finding the minimum of test functions from a GP prior. The experiment demonstrates the ability to sample entire functions from the (A)NP and achieve accurate context reconstructions. Image data regression is performed on MNIST and CelebA datasets using self-attentional layers in the encoder. Three models are compared: NP, Multihead ANP, and ANP with multihead attention in the deterministic path. See Appendix C and D for details and results. The experiment compares three models for image data regression on MNIST and CelebA datasets: NP, Multihead ANP, and Stacked Multihead ANP. Results show that Stacked Multihead ANP provides accurate reconstructions of the full image with the use of attention, enhancing its ability to model less smooth 2D functions compared to NP. The diversity in faces and digits obtained with different z values supports the claim that z can model global features. The experiment compares three models for image data regression on MNIST and CelebA datasets: NP, Multihead ANP, and Stacked Multihead ANP. Results show that Stacked Multihead ANP provides accurate reconstructions of the full image with the use of attention, enhancing its ability to model less smooth 2D functions compared to NP. The diversity in faces and digits obtained with different z values supports the claim that z can model global features. Multihead ANP shows improved context reconstruction error compared to NP, with noticeable gains in crispness and global coherence when using stacked self-attention. The Multihead ANP for CelebA demonstrates different attention focuses for each head, with varying roles such as looking at nearby pixels, specific columns, or exploiting symmetry. Additionally, ANPs trained on images can map images from one resolution to another by predicting pixel intensities in a continuous space. The Multihead ANP for CelebA shows diverse attention focuses for each head, mapping images from one resolution to another by predicting pixel intensities in a continuous space. The reconstructions of ANPs can accurately map low resolutions to higher resolutions, with some diversity for different values of z. The model can even map 32 \u00d7 32 images to 256 \u00d7 256 resolutions. The Multihead ANP can map images from low resolutions to higher resolutions, such as from 32 \u00d7 32 to 256 \u00d7 256. The model learns realistic high-resolution images with sharper edges and internal representations of features like faces. It is not meant to replace state-of-the-art algorithms for image inpainting or super-resolution. The ANP is not a replacement for state-of-the-art algorithms in image inpainting or super-resolution. Attention in NPs is related to GP kernels, measuring similarity between points in the same domain. The training regimes of GPs and NPs differ, making direct comparison challenging. The training regimes of Gaussian Processes (GPs) and Neural Processes (NPs) differ, making direct comparison challenging. GPs depend heavily on kernel choice for predictive uncertainties, while NPs learn uncertainties directly from data. GPs have the benefit of being consistent stochastic processes with exact closed-form expressions for predictions, a feature NPs lack. Variational Implicit Processes (VIP) are related to NPs, approximating processes and posteriors with GPs. The process and its posterior are approximated by a GP in the same decoder setup with a finite dimensional z. Meta-Learning (A)NPs focus on few-shot learning, with works using attention for tasks in Meta-RL. Few-shot density estimation using attention has also been explored extensively in numerous works. ANPs have a similar permutation invariant encoder as the Neural Statistician and the Variational Homoencoder, but use local latents on top of a global latent. The Variational Homoencoder BID12 and ANPs explore regression settings, with ANPs addressing underfitting by augmenting NPs with attention. Generative Query Networks BID16 focus on spatial prediction, with a model similar to NPs but with attention applied differently. ANPs improve prediction accuracy by augmenting NPs with attention, addressing underfitting. Future work includes incorporating cross-attention in model architecture and training ANPs on text data. ANPs have connections with Image Transformer in terms of self-attention for predicting target pixels. The ANP with self-attention in the decoder closely resembles an Image Transformer defined on arbitrary pixel orderings. Targets in this setup affect each other's predictions, making target ordering and grouping important. Architectural details of the NP and Multihead ANP models are shown for 1D and 2D regression experiments. The curr_chunk discusses parameterizing the regression model, using multihead cross-attention in 1D and 2D regression experiments, and the architecture of self-attention modules. It also mentions the hyperparameters used for the data generating GP and the likelihood noise. In the fixed kernel hyperparameter experiments, 16 curves are drawn from a GP with specific hyperparameters, while in the random kernel hyperparameter case, 16 random values of hyperparameters are sampled. The Adam Optimiser BID14 with a fixed learning rate is used, and one sample of q(z|s C ) is used to estimate the loss. The Multihead ANP model is closer to the oracle GP predictions but still underestimates predictive variance, possibly due to variational inference. Investigating how to address this issue would be interesting. The conditional distributions for fixed and random GP kernel hyperparameters show non-smooth behavior in dot-product attention. Dot-product attention can collapse to a local minimum, leading to good reconstructions but poor interpolations between context points. The KL term in NP loss differs between training on fixed and random kernel hyperparameter GP data, with the model deeming the deterministic path sufficient for accurate predictions in the fixed hyperparameter case. In the random hyperparameter case, the attention gives a non-zero KL and uses the latents to model the uncertainty in the realisation of the stochastic process given some context points. ANPs can be used for Bayesian optimization by considering all previous function evaluations as context points, obtaining an informed surrogate of the target function. Thompson sampling is used to act according to the minimal predicted value. Thompson sampling is used with ANPs for Bayesian optimization, showing consistently small simple regret for NP with multihead attention. The cumulative regret decreases rapidly for multihead, utilizing previous function evaluations effectively. The lower cumulative regret initially compared to the oracle GP is due to under-exploration. Random pixels of images are taken as targets and contexts for training, with rescaled x and y values. A batch size of 16 is used for both MNIST and CelebA datasets. The rescaled x and y values are used for training with a batch size of 16 for both MNIST and CelebA datasets. The stacked self-attention architecture is employed without Dropout or positional embeddings of pixels. Little tuning has been done regarding the architectural hyperparameters. The NP overestimates predictive variance, but with attention, uncertainty is reduced as the number of contexts increases. The Stacked Multihead ANP improves results significantly over Multihead ANP, providing sharper images with better global coherence even when the face isn't axis-aligned. In the visualization, each head of multihead attention in the NP attends to different pixels, with all heads becoming useful for target prediction when the context is disjoint from the target."
}