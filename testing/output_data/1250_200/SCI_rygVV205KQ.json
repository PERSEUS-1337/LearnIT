{
    "title": "rygVV205KQ",
    "content": "High-dimensional sparse reward tasks in reinforcement learning are challenging. This work uses imitation learning to address these challenges, focusing on learning a representation of the world from pixels and exploring efficiently with rare reward signals. Adversarial imitation can work well in high-dimensional observation spaces, with a tiny adversary acting as the reward function. The proposed agent can solve a robot manipulation task of block stacking using only video demonstrations and sparse rewards, outperforming non-imitating agents and competing approaches. In this work, the authors demonstrate the effectiveness of using imitation learning with a tiny adversary to handle high-dimensional pixel observations in reinforcement learning tasks. They show that by providing the right features, the Generative Adversarial Imitation Learning (GAIL) approach can efficiently learn from sparse rewards and outperform standard baselines. Additionally, they introduce a new adversarial goal recognizer that enables the agent to learn stacking without task rewards purely through imitation. The authors demonstrate the effectiveness of using imitation learning with a tiny adversary to handle high-dimensional pixel observations in reinforcement learning tasks. They show that by providing the right features, the Generative Adversarial Imitation Learning (GAIL) approach can efficiently learn from sparse rewards and outperform standard baselines. Additionally, they introduce a new adversarial goal recognizer that enables the agent to learn stacking without task rewards purely through imitation. The proposed approach successfully solves a challenging simulated robotic block stacking task using only demonstrations and a sparse binary reward. The paper introduces a 6-DoF Jaco robot arm agent that learns block stacking from demonstration videos and sparse reward, achieving a 94% success rate. It also presents an adversary-based early termination method for actor processes to improve task performance and learning speed. Additionally, an agent learns with no task reward using an auxiliary goal recognizer adversary, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark are conducted to understand the reasons for improvement in the agent's performance. The DDPG algorithm involves training an agent to maximize expected rewards by using actor-critic neural networks. Transitions are added to a replay buffer for training, with the action-value function trained to match 1-step returns. Target networks are updated periodically for stability, and the policy network is trained via gradient descent. The D4PG agent builds on the DDPG algorithm by incorporating improvements such as GAIL, which involves training a discriminator network to learn a reward function. The D4PG agent also utilizes off-policy training with experience replay to maximize training data utilization. The reward function in the D4PG agent interpolates imitation reward and sparse task reward, using a modified equation. The discriminator distinguishes expert transitions from previous agents without needing importance sampling. The reward function is bounded between 0 and 1, allowing for intuitive early termination of episodes. Multiple CPU actor processes run in parallel with a single GPU learner process. In practice, the D4PG agent uses multiple CPU actor processes and a single GPU learner process. The actor processes receive updated network parameters every 100 steps and terminate episodes early when the discriminator score falls below a threshold. The type of network used in the discriminator is a critical design choice, as it affects the agent's ability to fool the discriminator. Expert demonstrations provide valuable data for feature learning due to their coverage of state space regions necessary for task-solving. Without access to expert actions, behavior cloning is not an option. High-resolution images rule out learning features in pixel space, as pixel prediction may not capture long-term data structure crucial for imitation learning. Contrastive predictive coding (CPC) emerges as a promising representation learning technique for mapping observations into a latent space conducive to autoregressive modeling. Contrastive Predictive Coding (CPC) is a representation learning technique that maps observations into a latent space for autoregressive modeling. It uses a probabilistic contrastive loss with negative sampling, allowing joint training of the encoder and autoregressive model without the need for a decoder. To address issues with sparse rewards, a neural network goal recognizer can replace task rewards, but a discriminator may be needed to detect if the agent reaches a goal state to prevent exploitation. To address sparse rewards, a secondary goal discriminator network can replace task rewards by detecting if an agent reaches a goal state in the latter portion of expert demonstrations. This allows the agent to surpass the demonstrator by learning to reach the goal faster, as observed in combined imitation and sparse task reward training. The environments used for training agents include a Kinova Jaco arm with two blocks on a tabletop and a 2D walker from the DeepMind control suite. Demonstrations were collected using a SpaceNavigator 3D motion controller, with 500 episodes gathered for each task. Additionally, a dataset of 30 \"non-expert\" trajectories was collected for diagnostic purposes. The second environment is a 2D walker from the DeepMind control suite BID33. Demonstrations were collected using a D4PG agent from proprioceptive states to match a target velocity. The imitation method is compared to a D4PG agent and GAIL agents with discriminator networks. The proposed method using a tiny adversary shows favorable results. The CPC model predicts future observations well for expert sequences but not for non-expert sequences. Conditioning on k-step predictions improves the performance on stacking tasks when the discriminator uses CPC embeddings. The performance of the method on stacking tasks improves when the discriminator uses CPC embeddings. D4PG with sparse rewards struggles due to exploration complexity, while dense rewards lead to slow learning. Imitation methods excel with sparse rewards, outperforming conventional GAIL from pixels. GAIL with tiny adversaries on random projections has limited success. The discriminator network has 128 parameters with CPC features and 2048 with value network features. Norm clipping in the critic optimizer may explain why GAIL value features work better than pixel features. The use of CPC embeddings in the discriminator improves performance on stacking tasks. Adding layers to the discriminator network does not enhance performance, and early termination hurts performance. Learning a discriminator directly on pixels results in poor performance. In ablation experiments, it was found that a small discriminator on meaningful representations outperformed deeper networks. Early termination during training improved learning speed by stopping episodes with low discriminator scores. The discriminator initially struggles to distinguish expert from agent trajectories, leading to longer episodes, but improves with more training episodes. In ablation experiments, a small discriminator on meaningful representations outperformed deeper networks. The agent improves at imitating the expert over 6000 episodes. Data efficiency is shown with 60, 120, 240, and 500 demonstrations. Results on the planar walker indicate success with the proposed method using value network features and random projections. Videos of the trained agent are available in the supplementary materials. Agents trained without rewards using expert states as positive examples for D goal are also discussed. The study discusses training agents without rewards by using expert states as positive examples. Results show that two out of five runs were successful, with the best agent achieving a 55% success rate. The agent learns to stack more efficiently than a human demonstrator, completing the task in under 2 seconds. Additionally, the study highlights the use of expert demonstrations to enhance agent performance in robotics. Imitation learning in robotics involves using expert demonstrations to enhance agent performance. Different approaches, such as supervised imitation and one-shot imitation, have been explored to replicate desired behaviors from single demonstrations. These methods aim to extend the success of deep learning beyond discriminative tasks in computer vision to interacting with environments. The approach in the curr_chunk focuses on using a gradient-based meta learning approach for one-shot learning of observed behaviors in robotics. It contrasts with behavior cloning by emphasizing learning through interaction with the environment rather than supervised learning. Additionally, it introduces inverse reinforcement learning to optimize a learned reward function from demonstrations. The curr_chunk discusses various methods such as DQfD, DPGfD, and GAIL for imitation learning in robotics, with a focus on optimizing learned rewards and addressing sparse reward challenges. These methods involve using expert trajectories, replay buffers, and adversarial learning to train agents without direct access to expert actions. Our major contribution is using minimal adversaries to solve sparse reward tasks with high-dimensional input spaces. We learn compact representations for imitation learning from expert observations and utilize self-supervised features for training block stacking agents from sparse rewards on pixels. The study demonstrates successful training of block stacking agents from sparse rewards on pixels using behavior cloning and contrastive predictive coding (CPC) models. The behavior cloning model includes a residual network pixel encoder architecture followed by an LSTM and linear layer, achieving a stacking accuracy of approximately 15%. The CPC model consists of an encoder and autoregressive model optimizing the same loss function with negative samples. The study demonstrates successful training of block stacking agents using behavior cloning and contrastive predictive coding (CPC) models. The CPC model optimizes a loss function with negative samples to maximize mutual information between variables. This approach is useful for extracting slow features and does not require predicting in pixel space. The study presents a method for training block stacking agents using behavior cloning and contrastive predictive coding models. The reward system is defined with different stages and rewards for specific actions. Actor and critic networks share a residual network with convolutional layers and fully connected layers. Distributional Q functions are used instead of a scalar state-action value function. In this paper, a categorical representation of Z is adopted for computing bootstrap targets with N-step returns. The bootstrap target Z is constructed using a categorical projection \u03a6 and the loss function for training distributional value functions involves cross entropy. Distributed prioritized experience replay is utilized to enhance stability and learning efficiency."
}