{
    "title": "rkxJus0cFX",
    "content": "Data parallelism is a popular method for scaling DNN training across multiple nodes. Compressing communication traffic to alleviate synchronization bottlenecks in distributed training has gained attention. Residual Gradient Compression (RGC) is a successful approach that significantly compresses transmitting message size while preserving accuracy. A new RGC method called RedSync improves training time in real-world multi-GPU systems by optimizing communication bandwidth with limited overhead. RedSync shows performance improvement for DNNs with high communication to computation ratio, especially in training large-scale deep neural networks on multiple computing nodes. Poor scalability in data parallelism for training large-scale deep neural networks on multiple computing nodes due to communication bandwidth bottleneck. Models of DNNs are growing bigger, increasing the challenge of communicating model parameter updates. Synchronization overhead has become the bottleneck in data parallelism on distributed systems using new computing hardware. Recent studies focus on reducing communication cost by quantizing gradients. Recent research is focused on reducing communication costs between nodes by quantizing gradients and sparsifying communication gradients. Residual Gradient Compression (RGC) is a promising pruning method that transmits only a small subset of gradients while maintaining the rest locally as residuals. RGC improves robustness by selecting the top 1% gradients to communicate based on their magnitude. Recent research has focused on reducing communication costs between nodes by quantizing and sparsifying gradients. Residual Gradient Compression (RGC) selects the top 1% gradients to communicate based on their magnitude, improving robustness. The latest RGC variants achieve a 0.1% compression ratio on local gradients with minimal loss of model accuracy. Challenges arise when applying RGC to distributed GPU systems due to the lack of efficient compression algorithms. RedSync is a highly-efficient RGC implementation for multi-GPU systems. It combines pruning and quantization techniques to compress transmitting gradients and uses MPI for sparse synchronization. Top-0.1% selection methods are designed for pruning operations inside GPU memory, significantly faster than existing methods. A cost model analyzes communication and calculation overhead for potential performance gains. RedSync is a highly-efficient RGC implementation for multi-GPU systems, combining pruning and quantization techniques to compress gradients. A cost model analyzes communication and calculation overhead for performance gains. RedSync ensures almost no accuracy loss when training DNNs and provides significant performance improvements for communication-intensive networks like VGG and AlexNet. RedSync is a highly-efficient RGC implementation for multi-GPU systems, combining pruning and quantization techniques to compress gradients. Synchronous SGD method is adopted in RedSync, where nodes compute gradients using local data and synchronize important elements among all nodes using allreduce operations. Residuals are accumulated and compressed into sparse data structures for communication efficiency. The efficiency of communication-set selection method is crucial for the overall performance of the RGC system. Recent work suggests selecting the top 0.1% elements from residuals of each layer as the communication-set. Implementing this on GPU can be challenging, but a top-k selection method based on the radixSelect algorithm can be efficient. The efficiency of communication-set selection is crucial for RGC system performance. Implementing top 0.1% element selection on GPU can be challenging. Two efficient GPU algorithms proposed: trimmed top-k selection and threshold binary search selection. Trimmed top-k selection uses statistical features to limit radixSelect operation on a small subset. Operation count nonzero determines elements above a chosen threshold. The communication-set selection in RGC system performance is crucial. Two efficient GPU algorithms proposed: trimmed top-k selection and threshold binary search selection. The threshold binary search selection method dynamically adjusts the threshold to find the number of parameters above a certain value. To avoid time-consuming operations, a method to select approximate top-0.1% elements as communication-set is proposed. The threshold binary search selection method dynamically adjusts the threshold to find the number of parameters above a certain value. A method to select approximate top-0.1% elements as communication-set is proposed using a binary search algorithm. The threshold binary search selection method optimizes the selection of parameters above a certain value, reducing count nonzero operations for efficiency. This method significantly reduces selection time for large sizes compared to radixSelect. In practice, compressed residuals include k indices and values, with the possibility of quantizing these values. By setting values of elements with the same sign to their mean, communication bandwidth requirements can be almost eliminated. Different compression strategies are dynamically chosen based on parameter sizes, with trimmed top-k and sampled threshold binary search selection methods significantly faster than radixSelect for large sizes. By setting elements of the same sign in the communication-set to their mean, the communication bandwidth requirement for transmitting value information can be reduced. The select method is modified to ensure all elements in the communication-set have the same sign by choosing the largest and smallest k elements alternately. Quantization compression is facilitated, but sampled threshold binary search selection cannot be used. The output layer of the DNN is not quantified to preserve correct classification information. Synchronization of dense gradient structures in distributed DNN systems can be achieved with an allreduce operation, but designing a sparse allreduce in a distributed setting is more complex. In a distributed setting, designing a sparse allreduce operation is complex due to different non-zero indices in compressed residuals. Utilizing the allgather operation, we gather data from each node to implement sparse allreduce. Messages include indices and values of elements in the communication-set, with varying lengths due to threshold binary search selection. Packaging indices and values into a single message avoids the need for separate allgather operations. RedSync implements algorithm improvement techniques for sparse synchronization, including momentum correction, momentum factor masking, and warmup training modifications. It adds compressed residuals to weights in the local model after scaling with the learning rate. The performance gain of sparse synchronization is analyzed using a communication cost model. The time taken to send a message between nodes is modeled as \u03b1 + n\u03b2, where \u03b1 is latency, \u03b2 is transfer time per byte, and n is bytes transferred. Network interface is single ported. M is number of elements in residuals, D is compression ratio. \u03b3 2 is cost for reduction operation, \u03b3 1 is cost to decompress sparse message. Compression ratio varies for each node. Recursive doubling and Rabenseifners algorithm used for communication. Cost of quantized sparse and dense synchronization illustrated. Compression rate implications in performance model. The compression rate implications in the performance model are illustrated by sparse and dense synchronization in Equations 1 and 2. The communication bandwidth for sparse synchronization is proportional to the number of nodes, leading to potential bottlenecks when scaling RedSync. Reduction overhead increases linearly with the number of nodes in Equation 1, but remains almost constant in Equation 2. Testing was conducted on two multi-GPU systems, including a server with eight GPUs and a GPU supercomputer. Piz Daint is a GPU supercomputer with 5320 nodes connected by Aries interconnect. Muradin is a server with eight GPUs and an Intel Xeon CPU. Pytorch v4.0 and Horovod were used for DNN training operations. Performance was tested on Image Classification tasks with various CNNs and Language Modeling tasks with the Penn Treebank corpus dataset. For Language Modeling tasks, two datasets were evaluated: the Penn Treebank corpus with 923,000 training words and the WikiText dataset with over 100 million tokens. A 2-layer LSTM model with 1500 hidden units per layer was used for evaluation. RedSync convergence was examined on these datasets. Additionally, ResNet44 and VGG16 were tested on the Cifar10 dataset, while AlexNet, ResNet50, and VGG16 were tested on the ImageNet dataset. The validation error of RGC and quantized RGC provided by RedSync on three test cases is shown in FIG1. The perplexity of the 2-layer LSTM was examined on the PTB and Wiki2 datasets. Validation error of RGC and quantized RGC by RedSync on three test cases was compared with original SGD. Results in Table 1 show no loss of accuracy when increasing batch size to 2048. Scalability of RedSync on Piz Daint and Muradin with multiple GPUs was tested. Comparison with Quantized-RedSync and horovod was done using trimmed top-k algorithm for CNNs and threshold binary search algorithm for LSTM. Cost analysis of RedSync with 128 GPUs on Piz Daint is illustrated in Fig. 6. The RedSync algorithm is effective for accelerating data parallel training on DNNs with high communication to computation ratio. While it may not perform as well as the baseline version on a single GPU due to compression and decompression overhead, significant speedup can be achieved with more than 2 GPUs. However, ResNet50 shows no performance gain with RedSync due to its high computation to communication ratio. The use of parallel-friendly selection methods for compression is crucial for overall system performance. The RedSync algorithm accelerates data parallel training on DNNs with high communication to computation ratio. ResNet50 has the highest computation to communication ratio among the DNNs investigated. RedSync's scalability curve on Piz Daint is concave, showing better speedup on 32 GPUs than 128 GPUs for AlexNet. Quantized-RedSync outperforms RedSync for CNNs but performs worse for LSTM training on a small scale. CNN uses trimmed top-k for communication-set selection, with quantization reducing communication costs. The RedSync algorithm accelerates data parallel training on DNNs with high communication to computation ratio. It reduces communication costs through quantization, improving overall performance. RedSync outperforms Quantized-RedSync for small-scale LSTM training due to faster selection. The paper proposes RedSync for DNN training using Residual Gradient Compression. Performance tests on two GPU platforms show significant speedup for AlexNet, VGG16, and LSTM models. The RedSync algorithm accelerates DNN training by reducing communication costs through quantization. Performance tests on two GPU platforms show significant speedup for AlexNet, VGG16, and LSTM models. The sparse allgather method is illustrated in FIG3, with nodes exchanging compressed residuals in steps based on their distance. The Rabenseifners algorithm is used for allreduce operation on messages, doing a reduce-scatter followed by an allgather. It employs a recursive halving algorithm where each node exchanges data with a node that is a distance p/2 away, performing reduction operations on received data. The Rabenseifners algorithm performs reduce-scatter and allgather operations recursively, halving the data communicated at each step for a total of lgp steps. It aims to improve data parallel efficiency by overlapping communication with computation through pipelining. Gradient clipping is used to avoid gradient explosion before updating aggregated gradients. The Rabenseifners algorithm aims to improve data parallel efficiency by overlapping communication with computation through pipelining. Gradient clipping is used to avoid gradient explosion before updating aggregated gradients. The clipping technique BID9 is adopted to perform gradient clipping by a new threshold locally before adding the current gradients to previous residuals. Traditional data parallel does clipping after communication of all layers are completed, while the RGC algorithm needs to do clipping before communication. Local clipping introduces synchronization between computing and communication, eliminating the possibility of Communication hiding. Gradient clipping is abandoned for CNNs, which seldom have gradient exploration problems for deep networks. For RNNs, gradients are achieved after backpropagation of all time steps using Back Propagation Through Time (BPTT). Communication time can only overlap with compression calculation. The RedSync algorithm integrates momentum masking and correction schemes for momentum SGD and Nesterov momentum SGD optimizers. A warm-up training approach is used to accelerate convergence by decreasing the compression ratio of residuals in the communication-set in the first few epochs. However, this method may be inefficient for large-scale models due to high bandwidth requirements. The RedSync algorithm integrates momentum masking and correction schemes for momentum SGD and Nesterov momentum SGD optimizers. Instead of using high-compression-ratio RGC method for warm-up training, original SGD optimizer synchronized by allreduce is utilized in the first few epochs if needed, even with a compression ratio as low as 1.5625%."
}