{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires expensive per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing to new domains without annotations, a domain adaptation method is proposed. This method involves learning discriminative feature representations of patches based on label histograms in the source domain and using an adversarial learning scheme to align feature distributions between source and target patches. The framework achieves state-of-the-art performance on semantic segmentation and is validated through experiments on various benchmark datasets. Recent deep learning methods have shown progress in vision tasks like object recognition and semantic segmentation, but models often struggle to generalize to new domains. Domain adaptation methods aim to bridge this gap between annotated training data and unlabeled test domains. While image classification methods have been developed, there is still room for improvement in domain adaptation for pixel-level prediction tasks like semantic segmentation. This is crucial as annotating ground truth data is costly, especially for tasks like road-scene images with varying appearance distributions. Existing methods align image distributions between different cities or within the same city using adversarial learning. Global distribution alignment may not be effective due to differences in camera pose or field of view. Instead of aligning global statistics, the focus is on matching patches shared across domains. Aligning patch distributions through adversarial learning is challenging due to high variation among patches. Motivated by recent advances, the goal is to learn disentangled representations for better adaptation. The approach aims to align patch distributions between source and target domains by learning discriminative representations to address the high-variation problem among patches. Two adversarial modules are utilized to align both global and patch-level distributions, with a focus on matching patches shared across domains. The proposed approach aims to align patch distributions between source and target domains by learning discriminative representations. It utilizes pixel-level annotations to extract label histograms and applies K-means clustering to group patch representations. An adversarial loss is used to push feature representations of target patches closer to the distribution of source patches. This representation learning is guided by the label histogram and differs from existing methods. The approach is evaluated in pixel-level road-scene image segmentation experiments. The proposed framework utilizes global and patch-level alignments for domain adaptation in structured output prediction. Extensive experiments are conducted in synthetic-to-real and cross-city scenarios, showing superior performance compared to state-of-the-art methods. The framework is general and can be applied to other structured outputs like depth. Key contributions include the use of adversarial learning modules, discriminative representations guided by label histograms, and patch-level alignment. The proposed method utilizes discriminative representations guided by label histograms for patch-level alignment in domain adaptation. It outperforms various baselines and state-of-the-art methods in semantic segmentation tasks. Domain adaptation techniques for image classification and pixel-level prediction are discussed, including the use of deep architectures to learn domain-invariant features. Adversarial learning schemes and Maximum Mean Discrepancy are commonly employed to minimize domain discrepancies. Variants with different classifiers and loss functions have been developed to enhance feature alignment. The CDA method applies SVM classifier to capture label distributions on superpixels for domain adaptation in structured pixel-level predictions, outperforming baselines in semantic segmentation tasks. Our proposed method focuses on learning discriminative representations for patches to aid in patch-level alignment during domain adaptation. Unlike existing methods that use global distribution alignment and class-specific priors, our approach preserves structured information at the patch level. Additionally, our framework does not require additional priors or annotations, allowing for end-to-end training of the entire network. Learning a latent disentangled space aids in tasks like facial recognition, image generation, and view synthesis. Various approaches use pre-defined factors to learn interpretable representations of images, such as graphic codes for 3D rendering and generative adversarial networks with auxiliary classifiers. While these methods focus on single-domain data, we propose learning discriminative representations for patches to assist in domain adaptation without the need for additional priors or annotations. In this line of research, the proposed framework aims to learn discriminative representations for patches to aid in domain adaptation without pre-defined factors. The framework utilizes label distributions and adversarial learning to align distributions across domains, incorporating a classification loss to learn patch-level discriminative representations. The proposed framework aims to learn patch-level discriminative representations in a clustered space for domain adaptation. It utilizes adversarial loss functions to align distributions between source and target data, with a focus on pushing the target distribution closer to the source distribution. The framework includes supervised loss functions for structured prediction and discriminative representation on the source data, as well as global and patch-level adversarial loss functions for alignment. The method involves a baseline model with supervised cross-entropy loss and global alignment using an output space adaptation module. It includes optimizing a fully-convolutional network for structured output prediction and adversarial loss for GAN training. Patch-level alignment with discriminative representations is emphasized for transferable structured output representations shared across source and target images. The proposed method involves patch-level domain alignment to find transferable structured output representations shared across source and target images. Clustering is used to construct prototypical patch patterns from the source domain, guiding patches from the target domain to adapt to this space via adversarial objective. Learning discriminative representations is emphasized for the disentangled space. The proposed method involves patch-level domain alignment to find transferable structured output representations shared across source and target images. Learning discriminative representations is emphasized for the disentangled space, utilizing per-pixel annotations in the source domain to construct semantically disentangled patch representations through label histograms. Randomly sampled patches are used to extract spatial label histograms, which are then clustered using K-means to assign labels to patches based on the closest cluster center distance. The method involves patch-level domain alignment for transferable structured output representations shared between source and target images. Discriminative representations are learned by constructing label histograms from per-pixel annotations in the source domain. K-means clustering assigns labels to patches based on histogram distances. A classification module is added to the network to simulate label histogram construction and learn a discriminative representation. Adversarial alignment aligns target patches to the clustered space in the source domain using cross-entropy loss. The method involves patch-level domain alignment for transferable structured output representations shared between source and target images. The goal is to align patches regardless of their location in the image by reshaping the feature representation. The adversarial objective is formulated using a discriminator to classify the feature representation. The network optimization involves updating the discriminator Dg to distinguish between source and target output distributions. The optimization process alternates between updating the discriminators and updating the network while fixing the discriminators. The method involves updating the discriminator Dg to distinguish between the source and target output distributions. The goal is to push the target distribution closer to the source distribution while maintaining good performance on the main tasks using G and H. The minimization problem combines supervised and adversarial loss functions, with the feature representations enhanced in G. Only G is required during the testing phase, ensuring runtime is unaffected compared to the baseline approach. The discriminator Dg is updated to distinguish between source and target output distributions, with the goal of aligning them while maintaining performance using G and H. The architecture of Dg includes 5 convolution layers with leaky ReLU activation, while Dl utilizes 3 fully-connected layers. The generator consists of network G with a categorization module H, following the DeepLab-v2 framework with ResNet-101 architecture pre-trained on ImageNet. The proposed architecture involves generating a spatial map with desired receptive fields, followed by two convolution layers to produce a feature map F. Implementation details include using PyTorch on a Titan X GPU, Adam optimizer for discriminators, and Stochastic Gradient Descent for the generator. Ablation study on GTA5-to-Cityscapes using ResNet-101 network is conducted with specific loss functions and parameters. The proposed framework for domain adaptation in semantic segmentation involves training with specific hyperparameters and conducting experiments on synthetic-to-real scenarios. The method is evaluated against state-of-the-art approaches on benchmark datasets, showing favorable performance. Experiments include adapting datasets like GTA5 BID27 and SYNTHIA BID28 to Cityscapes BID5, with training and test sets split according to BID14. The study involves adapting Cityscapes images to the Oxford RobotCar BID23 dataset with rainy scenes. 10 sequences are manually selected, with 7 for training and 3 for testing. 895 training images and 271 annotated test images are used for evaluation. The IoU ratio is used as the evaluation metric. An ablation study on the GTA5-to-Cityscapes scenario is conducted to analyze the impact of different loss functions and design choices in the proposed framework. The proposed method involves disentanglement, global alignment, and patch-level alignment to improve performance. Adding disentanglement alone enhances feature representation, while combining global and patch-level alignments achieves the highest IoU at 43.2%. The effectiveness of patch-level alignment is validated by the necessity of losses Ld and Ll adv, with a performance loss if either is removed. The clustered space constructed by Ld enables effective patch-level alignment by Ll adv. In the proposed method, patch-level alignment is crucial for performance improvement, as shown by a 2.4% drop in IoU without reshaping features. Visualization of feature representations in the clustered space demonstrates effective alignment. Experimental results comparing the method with state-of-the-art algorithms in various scenarios are presented. In the synthetic-to-real and cross-city cases, experimental results show favorable performance in adapting GTA5 to Cityscapes using VGG-16 and ResNet-101 architectures. The proposed method improves IoU by 1.8% and achieves the best IoU in 14 out of 19 categories. Adapting SYNTHIA to Cityscapes also shows improvements compared to state-of-the-art methods. In the cross-city case, adapting Cityscapes to Oxford RobotCar in different weather conditions demonstrates the effectiveness of the proposed method. The paper presents a domain adaptation method for structured output, combining global and patch-level alignments to improve segmentation results. The method achieves a mean IoU of 63.6%, outperforming previous approaches in adapting GTA5 to Cityscapes. Extensive experiments validate the effectiveness of the proposed framework. The proposed method utilizes an adversarial learning scheme to align target patch distributions with source ones. Extensive experiments validate its effectiveness in semantic segmentation under various challenges, including synthetic-to-real and cross-city scenarios. The model is trained in an end-to-end manner by sampling one image from each domain per training iteration. An entropy loss is used as regularization to push target feature representation to source clusters, achieving favorable results. The proposed method uses entropy regularization to align target patches with source distribution, achieving an IoU of 41.9%. Unlike entropy minimization, the model learns discriminative representations by pushing target patches closer to the source distribution in a clustered space guided by label histogram. Visual comparisons and results for adapting Cityscapes to Oxford RobotCar are provided. The proposed method utilizes entropy regularization to align target patches with the source distribution, achieving an IoU of 41.9%. Visual comparisons for adapting Cityscapes to Oxford RobotCar are also presented, along with qualitative results for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes scenarios. The approach often produces better segmentation outputs with more details and less noisy regions. The proposed method achieves an IoU of 41.9% by aligning target patches with the source distribution using entropy regularization. Visual comparisons for different adaptation scenarios, such as Cityscapes to Oxford RobotCar, GTA5-to-Cityscapes, and SYNTHIA-to-Cityscapes, show improved segmentation results with more details and less noise. Example results of adapted segmentation for SYNTHIA-to-Cityscapes setting are demonstrated, comparing results before adaptation, output space adaptation BID31, and the proposed method."
}