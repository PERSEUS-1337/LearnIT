{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified by showing that its negation is a descent direction for minimizing the population loss in a two-linear-layer network with binarized ReLU activation and Gaussian input data. Deep neural networks (DNN) have achieved remarkable success in various machine learning applications. Recent efforts have focused on training coarsely quantized DNN to achieve memory savings and energy efficiency at inference time. A poor choice of straight-through estimator (STE) can lead to instability in the training algorithm near certain local minima, as verified in CIFAR-10 experiments. Efforts have been made to train coarsely quantized DNN while maintaining performance. Training fully quantized DNN involves minimizing a nonconvex empirical risk function subject to discrete set-constraints. Weight quantization of DNN has been extensively studied. Gradient in training activation quantized DNN is almost everywhere zero, requiring a non-trivial search direction by modifying the chain rule. The straight-through estimator (STE) is a proxy derivative used in the backward pass for quantized activation functions in neural networks. It originates from the perceptron algorithm and allows for training networks with binary activations. Additionally, feasible target propagation algorithms have been proposed for learning hard-threshold networks via convex optimization. The straight-through estimator (STE) is a proxy derivative used in the backward pass for quantized activation functions in neural networks. It allows for training networks with binary activations and has been extended to multi-layer networks with various activation functions. Despite empirical success, there is limited theoretical understanding of STE in training networks with stair-case activations. Goel et al. (2018) showed convergence of the Convertron algorithm using the identity STE for leaky ReLU activation in a one-hidden-layer network. In a recent study, leaky ReLU activation was considered for a one-hidden-layer network, showing convergence of the Convertron algorithm using the identity STE in the backward pass. Other research has highlighted scenarios where certain layers are not ideal for back-propagation, proposing alternative approaches to improve DNN generalization accuracy and break adversarial defenses. The concept of \"coarse gradient\" is introduced, representing the gradient of the loss function with respect to weight variables modified by STE. The concept of \"coarse gradient\" is introduced to represent the gradient of the loss function with respect to weight variables modified by different choices of Straight-Through Estimators (STE). The study explores the optimization perspective of STE in training quantized ReLU nets and considers three representative STEs for learning a two-linear-layer network with binary activation and Gaussian data. It is proven that proper choices of STE give rise to training algorithms that are descent, with negative expected coarse gradients based on STEs of vanilla and clipped ReLUs serving as provably descent directions for minimizing population loss. The study explores the optimization perspective of Straight-Through Estimators (STE) in training quantized ReLU nets. It is proven that proper choices of STE result in descent algorithms, with negative expected coarse gradients of vanilla and clipped ReLUs serving as descent directions for minimizing population loss. Empirical performances show that clipped ReLU STE is the best for deeper networks like VGG-11 and ResNet-20, while identity or ReLU STE can lead to instability at good minima in CIFAR experiments. The study discusses the optimization perspective of Straight-Through Estimators (STE) in training quantized ReLU nets. It highlights the importance of proper choices of STE for descent algorithms and mentions the convergence guarantees of perceptron and Convertron algorithms for the identity STE. However, these results do not apply to networks with two trainable layers. The study also emphasizes the role of quantized activation functions in coarse gradient descent and the significance of choosing the right STE, such as clipped ReLU, to avoid instability issues. The study discusses the energy landscape of a two-linear-layer network with binary activation and Gaussian data. It compares the empirical performances of different Straight-Through Estimators (STE) in 2-bit and 4-bit activation quantization, highlighting instability phenomena observed in CIFAR experiments. Technical proofs and figures are deferred to the appendix. The first and second linear layers are described, with Z representing row vectors and \u03c3 as the activation function. The first layer acts as a convolutional layer, while the second layer functions as a classifier. The label generation and loss function are discussed, with a binary activation function used instead of ReLU. The parameters v* and w* are assumed to be non-zero, and the learning task is framed as a population loss minimization problem. The Gaussian distribution of Z is assumed, allowing for analytic expressions of f(v, w) and its gradient to be found. Loss (v, w; Z) is calculated with a Gaussian assumption on Z, allowing for analytic expressions of f(v, w) and its gradient. The gradient of the objective function is not directly available for network training, only the expected sample gradient. The idea of Straight-Through Estimator (STE) is to replace the zero component \u03c3 with a related non-trivial function \u00b5, resulting in a coarse gradient descent for training a two-linear-layer CNN with binary activation. The activation leads to coarse gradient descent for learning a two-linear-layer CNN with STE. Preliminaries about the population loss function are discussed, including expressions for f(v, w) and \u2207f(v, w). The population loss function and partial gradients are detailed for specific cases of w. Possible minimizers of the model are identified as stationary points and non-differentiable points. Saddle points are shown to be the only potential stationary points. The text discusses the behavior of coarse gradient descent in learning a two-linear-layer CNN with STE. It identifies stationary points and spurious local minimizers, proving that the model has no saddle points or spurious local minimizers in certain cases. The text also shows that using the derivative of vanilla or clipped ReLU in Algorithm 1 leads to convergence to a critical point, while using the identity function does not. The text proves that Algorithm 1 using the derivative of vanilla or clipped ReLU converges to a critical point, while using the identity function does not. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. The empirical loss descends along the negative coarse gradient direction, gaining monotonicity and smoothness with increasing sample size. This explains why proper STE works well with large amounts of data in deep learning. The results hold even if the Gaussian assumption on input data is weakened to follow a rotation-invariant distribution. The mathematical analysis for the main results is outlined in this section. The key observation in Lemma 5 is that the coarse partial gradient has non-negative correlation with the population partial gradient, forming a descent direction for minimizing the population loss. If certain conditions are met, the inner product between expected coarse and population gradients is positive. The significance of this estimate guarantees the descent property of Algorithm 1, leading to monotonically decreasing energy until convergence. When Algorithm 1 converges, both the partial gradients vanish. When Algorithm 1 using ReLU STE converges, it can only converge to a critical point of the population loss function. The coarse gradient using clipped ReLU STE generally correlates positively with the true partial gradient of the population loss and vanishes at critical points. If certain conditions are met, there exists a constant A crelu > 0 ensuring convergence. Lemma 8 states that Algorithm 1 converges when the coarse gradient and clipped ReLU function vanish simultaneously at saddle points. However, Lemma 9 and Lemma 10 show that the coarse gradient may not converge at local minima, especially near spurious minimizers. This implies that Algorithm 1 may never converge in such cases. The curr_chunk discusses the performance of identity, ReLU, and clipped ReLU Straight-Through Estimators (STEs) on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations. It highlights the instability issue that arises when using an improper STE. The resolution \u03b1 for quantized ReLU needs careful selection to maintain stability. The training algorithm focuses on selecting the proper resolution \u03b1 for quantized ReLU activations to maintain stability. A modified batch normalization layer is used without scale and shift, and \u03b1 is pre-computed using Lloyd's algorithm. The optimizer is stochastic gradient descent with momentum, and different epochs are used for training LeNet-5 and VGG-11 on MNIST. The training algorithm uses stochastic gradient descent with momentum for all experiments, training 50 epochs for LeNet-5 on MNIST and 200 epochs for VGG-11 and ResNet-20 on CIFAR-10. Parameters are initialized from pre-trained full-precision counterparts. Results show that the derivative of clipped ReLU performs best, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on the shallow LeNet-5 network. The use of identity function leads to instability in ResNet-20 with 4-bit activations. The training algorithm uses stochastic gradient descent with momentum for all experiments, training 50 epochs for LeNet-5 on MNIST and 200 epochs for VGG-11 and ResNet-20 on CIFAR-10. Results show that the derivative of clipped ReLU performs best, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on the shallow LeNet-5 network. The use of identity function leads to instability in ResNet-20 with 4-bit activations. When using the identity STE, the coarse gradient descent algorithms converge to neighborhoods of minima with different validation accuracies. The training using the identity STE ends up with a much worse minimum due to the coarse gradient not vanishing at the good minima. The coarse gradient with identity STE does not vanish at good minima, leading to poor performance. ReLU STE on 2-bit activated ResNet-20 is also unstable at good minima. Coarse gradient descent using identity STE is repelled from good minima in ResNet-20 with 4-bit activations. Theoretical justification for STE as a descent training algorithm is provided, considering derivatives of identity function, vanilla ReLU, and clipped ReLU. Negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing population loss. Identity STE generates incompatible coarse gradients with energy. The coarse gradient descent using ReLU STE with a 10^-5 learning rate is unstable on ResNet-20 with 2-bit activations, leading to increasing classification and training errors. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. The third identity was proven in a previous study, while the first identity is derived assuming specific vector configurations. The last identity is proven using polar coordinates. Lemma 12 discusses Gaussian random vectors and angles between nonzero vectors. It proves identities using polar coordinates and the rearrangement inequality. Lemma 13 and Lemma 14 provide further proofs using Cauchy-Schwarz inequality. The text discusses the projection of vectors, population loss, and partial gradients. It utilizes various inequalities and identities to prove different lemmas related to angles between vectors and their gradients. The text discusses the local optimality of stationary points in a mathematical context, proving that the stationary points are saddle points. It also explores the perturbed objective value and conditions for minimization of a quadratic function. The text discusses the local optimality of stationary points in a mathematical context, proving that the stationary points are saddle points. It also explores the perturbed objective value and conditions for minimization of a quadratic function. The proof involves various Lemmas and conditions related to Lipschitz constants and expected gradients. The text discusses the local optimality of stationary points in a mathematical context, proving that the stationary points are saddle points. It also explores conditions for minimization of a quadratic function using Lemmas and expected gradients. The inner product between expected coarse and true gradients w.r.t. w is analyzed under certain constraints. The text discusses the local optimality of stationary points in a mathematical context, proving that the stationary points are saddle points. It also explores conditions for minimization of a quadratic function using Lemmas and expected gradients. The inner product between expected coarse and true gradients w.r.t. w is analyzed under certain constraints. If w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0), then DISPLAYFORM3 where DISPLAYFORM4 * same as in Lemma 5, and DISPLAYFORM5 2 )dr. The inner product between the expected coarse and true gradients w.r.t. w DISPLAYFORM6 Moreover, if further v \u2264 C v and w \u2265 c w , there exists a constant A crelu > 0 depending on C v and c w , such that DISPLAYFORM7 Proof of Lemma 7. Denote \u03b8 := \u03b8(w, w * ). We first compute E Z g crelu (v, w; Z) . By (5), DISPLAYFORM8 Since \u00b5 = 1 {0<x<1} and \u03c3 = 1 {x>0} , we have In the last equality above, we called Lemma 12. DISPLAYFORM9 Notice that I n \u2212 ww w 2 w = 0 n and w * = 1. If \u03b8(w, w * ) = 0, \u03c0, then the inner product between E Z g crelu (v, w; Z) and Combining the above estimate together with FORMULA2 , FORMULA2 and FORMULA2 , and using Cauchy-Schwarz inequality, we have DISPLAYFORM10 where p(0, w) and q(\u03b8, w) are uniformly bounded. This completes the proof. Proof of Lemma 8. The proof of Lemma 8 is similar to that of Lemma 6, and we omit it here. The core part is that q(\u03b8, w) defined in Lemma 12 is non-negative and equals 0 only at \u03b8 = 0, \u03c0, as well as p(0, w) \u2265 p(\u03b8, w) \u2265 p(\u03c0, w) = 0.Lemma 9. Let \u00b5(x) = x in (5). Then the expected coarse partial gradient w.r.t. w is Proof of Lemma 9. By (5), DISPLAYFORM11. Lemma 9 states that the expected coarse partial gradient with respect to w is calculated when \u00b5(x) = x. The proof involves using certain identities and equations. Lemma 10 discusses the inner product between expected coarse and true gradients under specific conditions."
}