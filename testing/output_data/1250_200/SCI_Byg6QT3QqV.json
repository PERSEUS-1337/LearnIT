{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the focus is on generating online explanations during execution to reduce human mental workload. The challenge lies in the interdependence of different parts of an explanation. Three different implementations of online explanation generation are presented, based on a model reconciliation setting. Evaluation is done with human subjects in a planning competition domain and in simulation with various problems across two domains. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective, but a good explanation should consider the discrepancies between the human and robot models. Model reconciliation is key, where the robot's model (M R) and the human's model (M H) are used to generate lucid explanations. The model reconciliation setting BID6 involves the robot's and human's models of expectation. When differences arise, the robot should explain to reconcile the models. An issue is the human's mental workload in understanding explanations. This work suggests providing explanations online during plan execution to facilitate comprehension. Online explanations during plan execution can reduce mental workload by spreading out information smoothly. This approach ensures that different parts of the explanation do not cause cognitive dissonance. For example, in a scenario between friends Mark and Emma, differences in study session preferences highlight the importance of clear online explanations. Mark gradually reveals his plan to Emma during their study session, explaining the need for a lunch break to maintain energy. He refrains from disclosing his additional need for a walk until after lunch to avoid conflicting preferences. This example emphasizes the importance of providing online explanations during plan execution to ensure understanding and acceptance, especially when dealing with different values and preferences. In this paper, a new method for online explanation generation is developed, intertwining explanation with plan execution to reduce mental workload. Three approaches are implemented, focusing on matching plan prefix, making the next action understandable, and matching the robot's plan with optimal human actions. The focus is on making the next action understandable to the human teammate by matching the robot's plan with optimal human actions. Explainable AI is essential for human-AI collaboration to improve trust and maintain shared situation awareness. Explainable AI is crucial for human-AI collaboration, improving trust and maintaining shared situation awareness. The effectiveness of explainable agency is evaluated based on accurately modeling the human's perception of the AI agent. This model allows the AI agent to generate legible motions, explicable plans, or assistive actions, while also signaling its intentions before execution and explaining its behavior through generated explanations. The model allows the AI agent to explain its behavior by generating explanations, focusing on providing understandable explanations in an online fashion, especially for complex scenarios. The online explanation generation aims to provide concise information intertwined with plan execution. The problem is closely related to planning problems, defined by a tuple (F, A, I, G) in PDDL. The robot's plan to be explained must be optimal according to M R. Explanation generation in a model reconciliation setting aims to bring two models, M H and M R, close enough by updating M H so that the robot's plan, \u03c0 * I,G, becomes fully explainable and optimal in the human's model. A mapping function converts a planning problem into a set of features that specify the problem. Explanation generation in a model reconciliation setting aims to bring two models, M H and M R, close enough by updating M H so that the robot's plan, \u03c0 * I,G, becomes fully explainable and optimal in the human's model. The explanation generation problem involves a set of unit feature changes to reconcile the models, ensuring the robot's plan is optimal in the human's model after the changes. A minimal complete explanation (MCE) contains the minimum number of unit feature changes. The introduction of online explanation generation addresses the mental workload requirement of the human for understanding the explanation. Online explanation generation is introduced to address the mental workload requirement of humans for understanding explanations in a model reconciliation setting. It involves providing minimal information during plan execution to explain the part of the plan that is of interest and not explainable. The key is to split explanations into sub-explanations made in an online fashion as the plan is executed. Three different approaches are discussed, each focusing on a different aspect of explanation generation intertwined with plan execution. Online explanation generation aims to reduce the mental workload for humans by providing minimal information during plan execution. Three approaches are discussed, each focusing on a different aspect of explanation generation intertwined with plan execution. To generate sub-explanations for an online explanation, the planning process must consider how model changes affect human expectations. The challenge lies in ensuring that future model changes do not render a mismatch in previously reconciled plan prefixes. This is addressed by searching for the largest set of model changes that would not alter the plan prefixes in the future. The search process illustrated in FIG1 aims to find the largest set of model changes that ensure the plan prefix remains unchanged after further sub-explanations. The process involves recursively searching for sub-explanations starting from the robot model and stopping when the plan prefixes for the updated human model and robot model match. This approach is more akin to MME BID6 but focuses on matching prefixes rather than the entire plan at once. Our research process involves recursively searching for sub-explanations starting from the human model. By focusing on matching prefixes rather than the whole plan at once, our approach runs this process multiple times, allowing us to beat both MCE and MME in terms of computation. The dotted line represents the border of the maximum state space model modification in the robot model, reconciling the two models up to the current plan execution. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. We use a recursive model reconciliation procedure on the model space to find the largest set of model changes that can satisfy constraints. The goal is to ensure that the robot and human plan have the same prefix at any step of plan execution. The goal of explanation generation is to ensure that the robot and human plan have the same prefix at any step of plan execution. To achieve this, the plan prefix condition is relaxed so that the robot only needs to reconcile between M R and M H to match the very next action in the plan. This approach is motivated by the human's limited cognitive memory span and focuses on explaining the next action that is different between the most recent human plan and the robot's plan. The search procedure is performed from M H \\M H for computational efficiency. The OEG approach combines search from M H and M R for better performance in reconciling human and robot plans. The search process is similar to minimally monotonically explanation (MME) in BID6 but must be executed multiple times in an online fashion. The OEG-PP approach assumes the robot has only one right plan but relaxes this assumption by considering a set of optimal plans. The OEG approach combines search from human and robot models to reconcile plans. OEG-AP aims to find a human optimal plan that matches the robot's plan prefix efficiently. A compilation approach is used to check if the robot's plan prefix is also a prefix in the human's model. The key is to ensure that a plan prefix is always satisfied in the compiled model by adding predicates as effects and prerequisites for consecutive actions. A recursive model reconciliation process is used to search for differences and identify new sub-explanations when a human optimal plan matching the robot's plan prefix does not exist. The robot's plan is checked using a compilation approach to identify new sub-explanations if an optimal human plan matching the robot's plan does not exist. The approach was evaluated for online explanation generation in both human subjects and simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. The evaluation was done on ten different problems in the rover and barman domains, with differences between human and simulation evaluations made by randomly removing preconditions from model features. The human subject study aimed to confirm the benefits of online explanation generation. The study evaluated online explanation generation benefits with human subjects in a rover domain. The rover's tasks include exploring space, taking samples, and communicating results. The robot also serves drinks in a barman role. The study compared explanation generation approaches in rover and barman domains. Results showed differences in the number of model features shared over time between MCE and OEG approaches. The study compared explanation generation approaches in rover and barman domains, showing differences in the amount of information shared over time. OEG-NA shares more information than MCE, focusing on generating minimal information at each time step. OEG-PP and OEG-NA share more total information compared to MCE due to the dependence between features and planner behavior. OEG-AP considers all optimal plans, highlighting the distance between robot and human plans in terms of action distance. The OEG approaches intertwine execution and explanation. In our implementation, model updates are sorted based on feature size, with backtracking performed if consistency check fails. A human study compared three approaches for online explanation generation, showing a smoother adjustment for human workload. The search process takes advantage of information independence between sub-explanations. In a human study, different approaches for online explanation generation were compared to assess their impact on mental workload. The experiment involved subjects acting as rover commanders in a 3D simulated scenario on Mars, where they had to determine the validity of the rover's actions with explanations provided in plain English and GIF images. Each subject had a 30-minute time limit to complete the task. The study compared online explanation generation approaches in a human study on mental workload. Subjects acted as rover commanders in a Mars simulation, with added spatial puzzles to increase cognitive demand. Hidden information differences between settings led to scenarios where explanations were necessary. Robots used various online explanation generation approaches in different settings. The study compared online explanation generation approaches in a human study on mental workload during a Mars simulation. Different approaches were used by the robot to provide explanations to subjects at various steps. Subjects evaluated the efficiency of these approaches using the NASA Task Load Index questionnaire. The study used subjective measurements to capture mental workload using the BID24 scale, which calculates an overall score based on mental demand, temporal demand, performance, effort, and frustration. The experiment recruited 150 human subjects on MTurk and analyzed their responses to understand how well they understood the robot's plan in different settings. The study analyzed human subjects' understanding of the robot's plan with different explanations, comparing distances across five settings. The distance metric calculated the ratio of questionable actions to total actions, showing closer alignment between human and robot plans. Overall, OEG approaches reduced mental workload better than MCE approaches, as seen in NASA TLX measures. OEG approaches created more temporal demand due to intertwining explanation with plan execution. FIG6 displays objective performance measures and subjective results across 5 TLX categories. The study compared OEG approaches to MCEs in terms of trust towards robots, accuracy of identifying actions, mental workload, and task completion time. OEG approaches had lower questionable actions, higher accuracy, and lower mental workload compared to MCEs. Time analysis showed OEG-NA had the shortest task completion time. In this study, OEG-NA had the shortest task completion time compared to other approaches. The novel approach introduced aims to reduce mental workload during human-robot interaction by breaking down complex explanations into smaller parts. Three different approaches were evaluated, showing improved task performance and reduced mental workload."
}