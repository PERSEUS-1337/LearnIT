{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning. A new technique, Quantized Bottleneck Insertion, helps learn finite representations of RNN memory vectors and observation features. This allows for better understanding of memory use and behavior. Results on synthetic environments and Atari games show small finite representations can lead to improved interpretability in policies learned through deep reinforcement and imitation learning. In this paper, the focus is on understanding and explaining RNN policies by learning more compact memory representations. The challenge lies in interpreting the high-dimensional continuous memory vectors used in RNN policies, which are updated through complex gating networks. The goal is to quantize the memory and observation representation to capture discrete concepts, which could enhance explainability and aid in understanding the decision-making process of RNN policies. The paper focuses on transforming RNN policies into a Moore Machine Network (MMN) with quantized memory and observations using Quantized Bottleneck Network (QBN) insertion. This approach aims to capture powerful memory usage in a more understandable and explainable way. The study demonstrates that transforming RNN policies into MMNs with quantized memory using QBN insertion is effective. The MMNs can be fine-tuned to improve accuracy and provide insights into memory usage. Experiments on synthetic domains and Atari games show near-equivalent MMNs can be extracted, revealing new insights into memory usage not apparent from observing RNN policies. Our work focuses on learning finite-memory representations of continuous RNN policies, which is a novel approach compared to prior work on extracting Finite State Machines from recurrent networks. Previous methods involved discretizing memory space or using query-based learning algorithms, but our method offers new insights into memory usage. Our approach involves directly inserting discrete elements into the RNN to preserve its behavior and allow for a finite state characterization. This method allows for fine-tuning and visualization using standard learning frameworks, unlike previous approaches that produced FSM approximations separate from the RNN. Our work extends prior methods by learning Moore Machines with the introduction of QBN insertion as a way to learn from a continuous RNN. Our work extends prior methods by introducing QBN insertion to learn Moore Machines from a continuous RNN. This allows for discrete representations of memory and observations while maintaining arbitrary activations and weights in the network. The focus is on interpretability rather than efficiency in representing policies that require internal memory in reinforcement learning. The text discusses how an RNN maintains a continuous-valued hidden state that influences action choice by extracting observation features and transitioning to a new state. The goal is to extract compact quantized representations of the hidden state and observation features using Moore Machines and their deep network counterparts. A Moore Machine is a finite state machine where states are labeled by output values corresponding to actions. It is described by hidden states, observations, actions, a transition function, and a policy mapping hidden states to actions. A Moore Machine Network (MMN) is a Moore Machine where the transition function and policy are represented by deep networks. MMNs also provide a mapping from continuous observations to a finite discrete observation space. In this work, a new approach for learning Moore Machine Networks (MMNs) is introduced. MMNs are represented by deep networks and provide a mapping from continuous observations to a finite discrete observation space. The memory in MMNs is composed of k-level activation units, and environmental observations are transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for non-trivial problems, such as training high-performing MMNs for Atari games. The new approach for learning Moore Machine Networks (MMNs) involves leveraging the ability to learn RNNs. By first learning quantized bottleneck networks (QBNs) for embedding continuous features and hidden states, and then inserting them into the original recurrent net, a network that consumes quantized features and maintains quantized state, effectively creating an MMN. QBNs are autoencoders with a k-level quantized representation, aimed at discretizing a continuous space. QBNs aim to discretize a continuous space by quantizing the activations of units in the encoding layer. The QBN output is achieved through 3-level quantization using the quantize function. To support 3-valued quantization, a specific activation function is used. However, the introduction of the quantize function makes the network non-differentiable, posing challenges for backpropagation. Various methods can be employed to address this issue. The straight-through estimator is effective for dealing with the non-differentiability of the quantize function in QBNs. Training involves using L2 reconstruction error and running a recurrent policy in the target environment to generate training sequences. Two QBNs, b_f and b_h, are trained on observed features and states, aiming for low reconstruction error to identify latent bottlenecks. The approach involves training two QBNs, b_f and b_h, on features and states to achieve low reconstruction error. The bottlenecks of the QBNs serve as high-quality encodings of the original hidden states and features. These bottlenecks are inserted into the original RNN, creating an MMN. Despite imperfect reconstruction, the resulting MMN closely resembles the original RNN in performance. The resulting MMN closely resembles the original RNN in performance, and fine-tuning can be done by training on the original rollout data of the RNN to match the softmax distribution over actions. Visualization and analysis tools can be used to investigate the memory and its feature bits for a semantic understanding. Another approach is to use the MMN to create an equivalent Moore Machine over atomic state and observation spaces for further analysis. The Moore Machine is created by running the learned MMN to produce a dataset of quantized states, features, and actions. The state-space corresponds to distinct quantized states, while the observation-space consists of unique quantized feature vectors. The transition function is constructed from the data, and minimization techniques are applied to reduce the number of states and observations in the resulting Moore Machine. The Moore Machine is minimized to create the minimal 2 equivalent Moore Machine BID19, reducing the number of states and observations. Experiments aim to extract MMNs from RNNs without performance loss, determine the number of states and observations in minimal machines, and assess interpretability of recurrent policies. Two known domains are considered: Mode Counter synthetic environment and benchmark grammar learning problems. Mode Counter Environments vary memory requirements for policies and implement internal counters, resembling Partially Observable Markov Decision Processes. Memory is crucial in Mode Counter Environments (MCEs) as agents transition between modes based on a distribution. Different parameterizations test the need for memory to infer modes and achieve optimal performance. Three MCE instances, including Amnesia and Blind, test memory and observation use variations. In Mode Counter Environments (MCEs), memory is essential for optimal performance. Three MCE instances - Amnesia, Blind, and Tracker - test different memory and observation use variations. The recurrent architecture includes a feed-forward layer, GRU, and softmax layer for action selection. Imitation learning is used for training, resulting in 100% accuracy on the imitation dataset. The observation and hidden-state QBNs have varying bottleneck sizes in MCE environments. Training QBNs is faster than RNN training as they do not learn temporal dependencies. QBNs with bottleneck sizes of 4 and 8 were embedded into the RNN to create a discrete MMN, with performance measured before and after fine-tuning. The average test score over 50 episodes indicates optimal performance. After embedding QBNs with bottleneck sizes of 4 and 8 into the RNN to create a discrete MMN, the performance was measured before and after fine-tuning. Most cases did not require fine-tuning due to low reconstruction error, resulting in perfect performance. However, in one exception, fine-tuning improved MMN performance to 98% accuracy. The number of states and observations in the MMNs decreased significantly after minimization, indicating successful learning. After embedding QBNs with bottleneck sizes of 4 and 8 into the RNN to create a discrete MMN, the number of states and observations in the MMNs decreased significantly after minimization, indicating successful learning. The MMNs learned via QBN insertions were equivalent to the true minimal machines in most cases, with exceptions where perfect accuracy was not achieved. The ground truth minimal machines found were shown in the Appendix, demonstrating the optimal performance of the MMNs. The machines for different domains revealed insights into memory use and action choices based on observations. In policy learning problems, the RNNs are trained using imitation learning with a one-layer GRU and fully connected softmax layer. The training dataset consists of accept/reject strings with lengths between 1 and 50. Test results show the accuracy of the trained RNNs on a test set of 100 strings. The trained RNNs achieved 100% accuracy on a test set of 100 strings. Bottleneck encoders were not needed due to observations from a finite alphabet. MMNs with bottlenecks were created and fine-tuned, showing minor improvements in accuracy. MM extraction and minimization resulted in reduced state-space while maintaining performance. After training RNNs on Atari games, the authors applied their technique to extract finite state representations for Atari policies. Unlike previous experiments with known ground truth minimal machines, the complexity of Atari inputs made it unclear what to expect in terms of machine minimization. The recurrent architecture of all Atari agents was the same. The authors applied their technique to extract finite state representations for Atari policies using RNN training. The Atari agents have a recurrent architecture with specific preprocessing steps and network layers. They used the A3C RL algorithm for training and reported the performance on six games. The authors applied their technique to extract finite state representations for Atari policies using RNN training. They used the same general architecture for the QBN as for the MCE experiments, adjusting input and output sizes to match observation features. Training data for bottlenecks was generated using noisy rollouts to increase diversity. Bottlenecks were trained for B_h \u2208 {64, 128} and B_f \u2208 {100, 400} for Atari, with larger values due to complexity. Each bottleneck was trained to saturation of training loss. The authors applied their technique to extract finite state representations for Atari policies using RNN training. Each bottleneck was trained to saturation of training loss and then inserted into the RNN to give an MMN for each Atari game. MMN Performance. TAB2 gives the performance of the trained MMNs before and after finetuning for different combinations of B h and B f. Results show that for some games, fine-tuning was required to match the RNN performance, while for others, the MMNs achieved lower scores due to poor reconstruction on rare parts of the game. After investigating a drop in performance due to poor reconstruction in rare game states, the authors explored more intelligent approaches for training QBNs. Minimization of MMNs resulted in a significant reduction in the number of states and observations, making them easier to analyze. Understanding the meaning of observations and states in moderately complex policies remains a non-trivial task. The analysis of moderately complex policies involves understanding the meaning of observations and states. In Atari games, different types of memory use were observed. Pong's policy maps observations to actions without the need for memory, while Bowling and Freeway policies are open-loop controllers that ignore input images. Freeway's policy always takes the Up action at each time step. The MM extraction approach provides insight into the policy structures of Atari games like Bowling, Breakout, Space Invaders, and Boxing. The approach reveals open-loop policies and the use of memory and observations in RNN policies. Further analysis is needed to fully understand the discrete observations and states in these policies. This work enables future research in visualizing and interacting with memory use in RNN policies. Our approach involves extracting finite state Moore Machines from RNN policies by training Quantized Bottleneck Networks to produce binary encodings of memory and input features. This allows us to extract discrete Moore machines for analysis and usage. Results show accurate extraction of ground truth in known environments and maintained performance in Atari games. The extracted machines provide insights into memory usage, revealing surprisingly small memory states and observations required. The study focuses on extracting finite state Moore Machines from RNN policies to analyze memory usage. It reveals insights into policies' reliance on memory and observations in complex domains like Atari games. Future work includes developing tools for interpreting discrete observations and analyzing formal properties of policies. The hidden state in the system is represented by a tuple (m t , c t ), where m t is the current mode and c t is the count of consecutive time-steps in that mode. The mode changes based on a lifespan condition and transition distribution. The agent receives continuous-valued observations o t at each step, based on the current state. The agent must remember the mode and use memory to track how long it has been active for optimal performance. The experiments involve three different MCE instances: Amnesia, Blind, and Tracker. Amnesia does not use memory to track past information, Blind requires memory to keep track of a deterministic mode sequence, and Tracker requires paying attention to observations and using memory. The experiments involve three different MCE instances: Amnesia, Blind, and Tracker. Amnesia does not use memory to track past information, Blind requires memory to keep track of a deterministic mode sequence, and Tracker requires paying attention to observations and using memory. The \u2206(m) values can be larger than 1 in Tracker, requiring an optimal policy to pay attention to observations when c t = 0 and use memory to keep track of the current mode and mode count. This is the most general instance of the environment and can result in difficult problems when the number of modes and their life-spans grow. In all instances, M = 4 is used, with 'A' and 'R' denoting accept and reject states, respectively. Other than Grammar 6, all machines are 100% accurate."
}