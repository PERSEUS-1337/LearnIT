{
    "title": "SJFM0ZWCb",
    "content": "Unsupervised learning of timeseries data is a challenging problem in machine learning. The proposed Deep Temporal Clustering (DTC) algorithm integrates dimensionality reduction and temporal clustering in an unsupervised manner. It utilizes an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment. The algorithm optimizes both clustering and dimensionality reduction objectives, with customizable temporal similarity metrics. A visualization method is used to analyze learned features, showing superior performance compared to traditional methods across various domains. The performance of Deep Temporal Clustering (DTC) algorithm is attributed to integrated temporal dimensionality reduction and clustering criterion. While deep learning is dominant in supervised learning, unsupervised learning techniques are crucial for drawing inferences from unlabeled data. Clustering approaches have been successful in organizing similar objects into clusters, but their extension to time series data remains a challenge, especially for accurate unsupervised learning in areas like financial trading and medical monitoring. The Deep Temporal Clustering (DTC) algorithm addresses challenges in unsupervised time series clustering by transforming data into a low dimensional latent space using a deep autoencoder network. This novel approach is designed to overcome limitations of standard clustering techniques on time series data, which often exhibit variations in properties, temporal scales, and dimensionality. The Deep Temporal Clustering (DTC) algorithm utilizes a three-level approach to disentangle data manifolds in time series data. It includes a CNN to learn short-time-scale waveforms, a BI-LSTM to learn temporal connections across all time scales, and non-parametric clustering to find spatio-temporal dimensions for data separation. This approach achieves high performance on various datasets without parameter adjustment. The Deep Temporal Clustering (DTC) algorithm achieves high performance on real-life and benchmark datasets without parameter adjustment. It includes a unique algorithm for visualizing cluster-assignment activations over time, allowing for event localization in unlabeled time series data. This is the first work on applying deep learning to temporal clustering, with a focus on formulating an end-to-end algorithm for meaningful clustering. The objective formulation encompasses effective latent representation and a similarity metric for superior clustering accuracy. The Deep Temporal Clustering (DTC) algorithm optimizes network for reconstruction and clustering loss, outperforming other methods on real-world datasets. Existing research focuses on dimensionality reduction and similarity metrics, with drawbacks including loss of long-range correlations and hand-crafted transformations. Other limitations of existing solutions for clustering time series data include the hand-crafted nature of transformations and the need for domain knowledge. Different similarity measures have been explored, such as complexity, correlation, and time warping, but the choice of measure significantly impacts clustering results. While a good similarity measure is important, without proper dimensionality reduction, optimal clustering results may not be achieved due to the high-dimensional nature of time series data. Approaches that transform time series data into a low-dimensional latent space are effective for temporal clustering, but there is a lack of a general methodology for selecting an effective latent space. Recent research has focused on clustering methods for time series data, aiming to optimize latent space selection and similarity metrics for meaningful results. A proposed Deep Temporal Clustering (DTC) approach utilizes a convolutional autoencoder and BI-LSTM for dimensionality reduction and clustering of temporal sequences. This method addresses the limitations of existing solutions by providing a systematic approach to unsupervised clustering of time series data. The BI-LSTM acts as a temporal autoencoder (TAE) and feeds its latent representation to a temporal clustering layer for cluster assignments. The network architecture includes a 1D convolution layer for extracting short-term features, followed by a max pooling layer and Bidirectional LSTM for learning temporal changes. This process reduces dimensionality and improves clustering performance. The BI-LSTM acts as a temporal autoencoder (TAE) and feeds its latent representation to a temporal clustering layer for cluster assignments. The network architecture includes a 1D convolution layer for extracting short-term features, followed by a max pooling layer and Bidirectional LSTM for learning temporal changes. This allows collapsing input sequences in all dimensions except temporal, casting them into a smaller latent space, and optimizing high-level features for clustering. The clustering metric optimization in the BI-LSTM and CNN modifies weights to separate sequences into clusters, disentangling spatio-temporal manifolds of dynamics. End-to-end optimization efficiently extracts features for categorization, unlike traditional approaches focusing on reconstruction or clustering alone. Direct comparison shows improved unsupervised categorization with end-to-end optimization. Our approach emphasizes effective end-to-end optimization by utilizing the temporal continuity of spatio-temporal data x to extract informative features in the latent representation of the BI-LSTM. The temporal clustering layer initializes cluster centroids using latent signals z i obtained from the initialized TAE, followed by hierarchical clustering to obtain clusters and estimate initial centroids w j. Training of the temporal clustering layer is done in an unsupervised manner after obtaining initial centroid estimates. The temporal clustering layer initializes cluster centroids using latent signals from the initialized TAE, followed by hierarchical clustering to obtain clusters and estimate initial centroids. Training is done in an unsupervised manner, alternating between computing assignment probabilities and updating centroids using a loss function to maximize high confidence assignments. Distance from centroids is computed using a similarity metric, normalized into probability assignments using a Student's t distribution kernel. The parameter \u03b1 represents the degrees of freedom of the Students t distribution. In an unsupervised setting, \u03b1 can be set to 1. The siml() metric calculates the distance between the encoded signal z and centroid w. Different similarity metrics are experimented with, such as Complexity Invariant Similarity (CID) which uses the euclidean distance corrected by complexity estimation. The core idea of CID is to account for complexity differences. The Complexity Invariant Similarity (CID) metric calculates distance between time series based on complexity estimates. CID considers complexity differences between series, with the distance increasing as complexity differences increase. If both sequences have the same complexity, the distance is simply the euclidean distance. Other similarity metrics include Correlation based Similarity (COR) using pearson's correlation, and Auto Correlation based Similarity (ACF) using autocorrelation coefficients and weighted euclidean distance. The objective is to minimize KL divergence loss between q ij and target distribution p ij in training the temporal clustering layer. The choice of target distribution is crucial to strengthen high confidence predictions and normalize losses. Empirical properties of the distribution are discussed in BID9 and BID19. The KL divergence loss is computed using the target distribution, with joint optimization of clustering and autoencoder by minimizing KL divergence and mean squared error losses. Effective initialization of cluster centroids is important as they reflect the latent representation of the data. Pretraining parameters of the autoencoder ensures meaningful latent representation before optimizing the cluster centers. The initial centroids are pre-trained to represent the data well, followed by hierarchical clustering to initialize cluster centers. Autoencoder weights and cluster centers are updated using backpropagation mini-batch SGD. Target distribution is also updated during each SGD update. This approach helps prevent problematic solutions and ensures convergence of the latent representation to minimize clustering and MSE loss. A heatmap-generating network is used to identify main data features for classification, following a similar approach used in BID10 for localizing tumors in medical images. The final classification involves creating a heatmap-generating network to localize tumors in medical images using cluster labels from a DTC network. The network is trained to classify inputs and generate heatmaps showing relevant parts of the inputs. Implementation was done using Python, TensorFlow, and Keras on Nvidia GTX 1080Ti. The performance of the DTC algorithm is evaluated on various real-world datasets. The DTC algorithm performance is evaluated on real-world datasets including UCR Time series Classification Archive datasets and spacecraft magnetometer data from NASA's MMS Mission for automated detection of flux transfer events (FTEs). The B N data consists of 104 time series with 1440 time steps each. Results are compared against hierarchical clustering and k-Shape methods. The DTC algorithm is compared to hierarchical clustering and k-Shape methods using four similarity metrics. Expert labels are used for evaluation, but the training pipeline is unsupervised. ROC and AUC are used as evaluation metrics, with bootstrap sampling and averaging over 5 trials. Parameter optimization is avoided, with a convolution layer of 50 filters and two Bi-LSTM's. The DTC algorithm uses commonly used parameters, including a convolution layer with 50 filters, two Bi-LSTM's, and a pooling size chosen to keep the latent representation size < 100. The deconvolutional layer has a kernel size of 10, weights are initialized with a zero-mean Gaussian distribution, and the autoencoder network is pre-trained using the Adam optimizer over 10 epochs. The temporal clustering layer centroids are initialized using hierarchical clustering, and the entire deep architecture is jointly trained for clustering and autoencoder loss. Mini-batch size is set to 64, starting learning rate is 0.1, and these parameters are held constant across all datasets. The DTC algorithm uses common parameters and baseline algorithms that are parameter free. Results from the MMS dataset show activation maps correlating well with event locations. Joint training of reconstruction and clustering loss leads to superior performance compared to disjoint training. The DTC algorithm, which involves joint training of reconstruction and clustering loss, outperforms disjoint training on the MMS dataset with an average AUC of 0.93 compared to 0.88. Results also show DTC improving baseline performance across various datasets and metrics, with superior performance over existing techniques. In this work, the focus is on unsupervised learning of patterns in temporal sequences, event detection, and clustering. The results show a high agreement between unsupervised clustering and human-labeled categories, indicating effective dimensionality reduction. The approach is promising for real-world applications, with potential for generalization to multichannel spatio-temporal input."
}