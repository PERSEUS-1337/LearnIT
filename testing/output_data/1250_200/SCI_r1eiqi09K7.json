{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization is shown to yield faster convergence and lower train loss values compared to standard algorithms in experiments involving embedding the WordNet taxonomy in the Poincare ball. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when optimizing a large number of parameters. Recent advancements have led to successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD. While these algorithms are designed for parameters in a Euclidean space, there is a growing interest in optimizing parameters on a Riemannian manifold, allowing for non-Euclidean geometries. This approach has shown promising applications, including solving Lyapunov equations. In a more general setting, algorithms are being developed for non-Euclidean geometries on a Riemannian manifold. These algorithms have various applications, such as solving Lyapunov equations and matrix factorization. While first-order stochastic methods have been adapted to this setting, there is a need for Riemannian counterparts for adaptive algorithms. The challenge lies in generalizing adaptive schemes to the agnostic Riemannian setting in an intrinsic manner. In this work, the authors propose generalizations of adaptive algorithms for Riemannian settings, specifically focusing on a product of manifolds. They provide convergence analysis and empirical support for their claims using hyperbolic taxonomy embedding as a realistic task. Their motivation stems from the need to optimize symbolic embeddings in non-Euclidean spaces, highlighting the importance of Riemannian adaptive algorithms for competitive optimization-based methods in hyperbolic spaces. The recent rise of embedding methods in hyperbolic spaces could benefit from developments in competitive optimization-based Riemannian embedding methods. A manifold M of dimension n can be locally approximated by a Euclidean space R^n and is a generalization to higher dimensions of the notion of surface. Riemannian metric \u03c1 defines the geometry locally on M with inner-products varying smoothly with x. A Riemannian manifold is defined by a Riemannian metric \u03c1, which induces a global distance function on M. Riemannian SGD updates are performed using the exponential map to update along the shortest path in the relevant direction while staying within the manifold. When exp x (v) is unknown, a retraction map R x (v) = x + v is commonly used as a first-order approximation. Algorithms like ADAGRAD and ADAM offer different update rules for rescaling gradients and incorporating momentum and adaptivity terms. The RMSPROP BID24 method is a variation of ADAGRAD that uses an exponential moving average instead of a sum to forget past gradients over time. The momentum term introduced by ADAM has shown significant improvements. BID18 identified a mistake in the convergence proof of ADAM and proposed AMSGRAD as a fix. Intrinsic updates on a Riemannian manifold require a coordinate system choice. A small epsilon is often added for numerical stability in the square-root. The formalism for intrinsic updates on a Riemannian manifold involves working with local coordinate systems called charts. Quantities defined using a chart are intrinsic if their definition does not depend on the chart used. The RSGD update is intrinsic as it only involves objects intrinsic to the manifold. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free or intrinsic manner. The update on a Riemannian manifold involves parallel-transporting a coordinate system along the optimization trajectory. In a general Riemannian manifold, parallel transport depends on the path and curvature, breaking the sparsity of gradients and adaptivity benefits. The interpretation of adaptivity as optimizing different features at different speeds is lost as the coordinate system for gradients depends on the optimization path. Techniques used to prove theorems may not apply to updates defined in a similar vein. The update on a Riemannian manifold involves parallel-transporting a coordinate system along the optimization trajectory. In a general Riemannian manifold, parallel transport depends on the path and curvature, breaking the sparsity of gradients and adaptivity benefits. Techniques used to prove theorems may not apply to updates defined in a similar vein. In the context of Riemannian ADAGRAD, adapting schemes for each coordinate is proposed, with the components seen as \"coordinates\" for a simple adaptation of the optimization equation. _The general form of the Riemannian gradient is given by the following equation:_\n\n\\begin{equation}\n\\begin{aligned}\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^{n} \\left( \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta_{i}} \\right) \\frac{\\partial}{\\partial \\theta_{i}}\n\\end{aligned}\n\\end{equation}\n\n_The Riemannian gradient is a generalization of the Euclidean gradient to Riemannian manifolds. It is used in optimization problems where the objective function is defined on a Riemannian manifold. The Riemannian gradient is a vector field that points in the direction of the steepest ascent of the objective function. It is defined in terms of the partial derivatives of the objective function with respect to the coordinates of the manifold. The Riemannian gradient is a key concept in Riemannian optimization, which is a field of optimization that deals with optimization problems on Riemannian manifolds._\n\n\n## Riemannian Hessian\n\n_The Riemannian Hessian is a generalization of the Euclidean Hessian to Riemannian manifolds. It is used in optimization problems where the objective function is defined on a Riemannian manifold. The Riemannian Hessian is a matrix field that describes the curvature of the objective function on the manifold. It is defined in terms of the second partial derivatives of the objective function with respect to the coordinates of the manifold. The Riemannian Hessian is a key concept in Riemannian optimization, which is a field of optimization that deals with optimization problems on Riemannian manifolds._\n\n\n## Riemannian Gradient Descent\n\n_Riemannian Gradient Descent is an optimization algorithm that is used to find the minimum of an objective function defined on a Riemannian manifold. It is a generalization of the Euclidean Gradient Descent algorithm to Riemannian manifolds. The algorithm works by iteratively moving in the direction of the negative Riemannian gradient of the objective function. Riemannian Gradient Descent is a key algorithm in Riemannian optimization, which is a field of optimization that deals with optimization problems on Riemannian manifolds._\n\n\n## Riemannian Newton's Method\n\n_Riemannian Newton's Method is an optimization algorithm that is used to find the minimum of an objective function defined on a Riemannian manifold. It is a generalization of the Euclidean Newton's Method to Riemannian manifolds. The algorithm works by iteratively moving in the direction of the negative inverse Riemannian Hessian of the objective function. Riemannian Newton's Method is a key algorithm in Riemannian optimization, which is a field of optimization that deals with optimization problems on Riemannian manifolds._\n\n\n## Riemannian Trust-Region Methods\n\n_Riemannian Trust-Region Methods are optimization algorithms that are used to find the minimum of an objective function defined on a Riemannian manifold. They are generalizations of the Euclidean Trust-Region Methods to Riemannian manifolds. The algorithms work by iteratively moving in the direction of the negative Riemannian gradient of the objective function within a trust region. Riemannian Trust-Region Methods are key algorithms in Riemannian optimization, which is a field of optimization that deals with optimization problems on Riemannian manifolds._\n\n\n## Riemannian Conjugate Gradient Method\n\n_The Riemannian Conjugate Gradient Method is an optimization algorithm that is used to find the minimum of an objective function defined on a Riemannian manifold. It is a generalization of the Euclidean Conjugate Gradient Method to Riemannian manifolds. The algorithm works by iteratively moving in the direction of the negative Riemannian gradient of the objective function while maintaining conjugacy with previous directions. The Riemannian Conjugate Gradient Method is a key algorithm in Riemannian optimization, which is a field of optimization that deals with optimization problems on Riemannian manifolds._\n\n\n## Riemannian Stochastic Gradient Descent\n\n_Riemannian Stochastic Gradient Descent is an optimization algorithm that is used to find the minimum of an objective function defined on  The Riemannian AMSGRAD algorithm is presented in FIG1, along with the standard AMSGRAD algorithm for comparison. The convergence guarantee for RAMSGRAD is obtained by removing the max operations to derive RADAM and ADAM algorithms. The convergence guarantee for RAMSGRAD is presented in Theorem 1, where the quantity \u03b6 is defined. When (Mi, \u03c1i) = R for all i, convergence guarantees between RAMSGRAD and AMSGRAD coincide. The regret bound worsens at a speed of approximately 1 + D\u221e|\u03ba|/6 when the curvature is small but non-zero. The convergence guarantee for RADAMNC is shown in Theorem 2, with \u03b21 := 0 yielding a convergence proof for RADAGRAD. The role of convexity in the proofs is crucial. Regret bounds for convex objectives are typically obtained by bounding the difference between the objective function values at each iteration and the optimal value. In the Riemannian case, this term becomes \u03c1 xt (g t , \u2212 log xt (x * )). The cosine law is used to obtain a bound on this difference, particularly in the case of an SGD update. The cosine law is utilized to bound the difference in the Riemannian case, specifically for an SGD update. The bound is improved for sparse gradients, where only a few words are updated at a time on a manifold. The choice of \u03d5 i does not need to be specified for convergence theorems, suggesting potential improvements in regret bounds. The regret bounds could potentially be enhanced by utilizing momentum/acceleration in the proofs for a specific choice of \u03d5 i. Empirical assessment of RADAM, RAMSGRAD, and RADAGRAD algorithms compared to non-adaptive RSGD method is conducted by embedding the WordNet noun hierarchy in the Poincar\u00e9 model of hyperbolic geometry. The optimization tools could benefit algorithms proposed in previous studies. The Poincar\u00e9 model is chosen due to closed form expressions accessibility for all quantities used in the algorithm. The curr_chunk discusses the use of conformal factors, rescaled gradients, distance functions, geodesics, exponential and logarithmic maps, and parallel transport in a mathematical model. It also mentions the dataset and model used for embedding WordNet nouns in hyperbolic geometry, minimizing loss functions, and reporting metrics like loss value and mean average precision. The curr_chunk focuses on training details and optimization methods for embedding WordNet nouns in hyperbolic spaces. It discusses the use of a \"burn-in phase\" for training, sampling negative words based on graph degree, and the preference for RADAM over RAMS-GRAD in optimization. The curr_chunk discusses the experimental results favoring RADAM over RAMS-GRAD in optimization methods for embedding WordNet nouns in hyperbolic spaces. It also highlights the use of retraction methods and their impact on convergence to lower loss values. The results of different learning rates for RSGD, RADAM, and RAMSGRAD are presented in FIG2. In optimization methods for embedding WordNet nouns in hyperbolic spaces, RADAM consistently achieves the lowest training loss compared to RAMSGRAD. The use of retraction methods impacts convergence to lower loss values. Different learning rates for RSGD, RADAM, and RAMSGRAD are compared in the experimental results. In the context of optimizing WordNet nouns in hyperbolic spaces, new methods like accelerated gradient descent and stochastic gradient Langevin dynamics are introduced for convergence analysis. Riemannian counterparts of SGD with momentum and RMSprop are proposed, but lack convergence guarantees. Another version of Riemannian ADAM for the Grassmann manifold is discussed, focusing on adaptivity across manifolds and convergence analysis. The discussion focuses on generalizing adaptive optimization tools to Cartesian products of Riemannian manifolds in a principled manner. Convergence rates similar to Euclidean models are derived, showing superiority over non-adaptive methods in hyperbolic word taxonomy embedding tasks. Mathematical formulas and inequalities are used to support the analysis. The lemma presented discusses inequalities and proofs related to convergence of gradient-based optimization algorithms in Alexandrov spaces. It includes a cosine inequality and an analogue of Cauchy-Schwarz, which are used in convergence proofs for ADAMNC. Lemma 8 (BID0) states that for any non-negative real numbers y1, ..., yt, a certain inequality holds, which is crucial for convergence proofs in optimization algorithms in Alexandrov spaces."
}