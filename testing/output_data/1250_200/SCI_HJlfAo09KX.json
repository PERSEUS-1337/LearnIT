{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness near the ground truth, allowing gradient descent to converge linearly to a critical point close to the ground truth. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, with near-optimal sample and computational complexity. Neural networks have gained research interest for their success in practical domains like computer vision and artificial intelligence. Efforts are being made to understand the theoretical underpinnings behind their success, including model recovery to generalize well. Studies focus on recovering the underlying model parameter W from training samples generated from a neural network model. The curr_chunk discusses two types of data generations in neural network models: regression and classification problems. Previous studies have focused on model recovery using gradient descent over squared loss for both settings. In this study, the focus is on developing a strong statistical guarantee for the loss function in eq. (2) for classification problems, which is more challenging but practical than the squared loss. The goal is to achieve guaranteed linear convergence without the need for per-iteration resampling, similar to what has been shown for the squared loss function. This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function. For a multi-neuron classification problem with sigmoid activations, the empirical risk function based on the cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth. Gradient descent converges linearly to a critical point with a near-optimal sample complexity. The recovery of W is only up to certain statistical accuracy, and W converges at a specific rate. The study provides a performance guarantee for recovering one-hidden-layer neural networks using the cross-entropy loss function. The convergence rate of W is O(dK 9/2 log n/n) in the Frobenius norm, and a computational complexity of O(ndK 2 log(1/ ) is required for -accuracy. The tensor method in BID38 offers an initialization near the ground truth, with new techniques developed to analyze the cross-entropy loss function and ensure uniform concentrations. The study focuses on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization. It discusses parameter recovery in non-convex learning for signal processing problems and the statistical model for data generation. The landscape analysis and model recovery of one-hidden-layer network models are also explored. In landscape analysis, it is noted that with a large network size compared to data input, there are no spurious local minima in the optimization landscape. However, spurious bad local minima can exist in the presence of multiple neurons in the under-parameterized setting. In model recovery, when the number of neurons is smaller than the input dimension, gradient descent converges linearly with ReLU activation under Gaussian input. The study analyzes the cross entropy loss function in the model recovery classification problem under the multi-neuron case, which is a novel approach. Previous research focused on different structures of one-hidden-layer or two-layer neural networks with Gaussian input. The paper discusses the comparison of different neural network structures and loss functions. It is organized into sections covering problem formulation, local geometry, gradient descent convergence, initialization method, numerical examples, and conclusions. Mathematical notations and definitions are provided throughout the paper. The paper describes the generative model for training data and the gradient descent algorithm for learning network weights. It focuses on estimating W by minimizing the empirical risk function using cross entropy loss. The classification setting involves mapping y to a discrete label using a one-hidden layer neural network model with a sigmoid activation function. The paper introduces a gradient descent algorithm for estimating W by minimizing the empirical risk function using cross entropy loss. The algorithm includes a well-designed initialization scheme to avoid local minima. The algorithm uses the same set of training samples throughout execution, unlike other methods that resample at each iteration. Additionally, an important quantity regarding \u03c6(z) is introduced to capture the geometric properties of the loss function. The paper introduces a gradient descent algorithm for estimating W by minimizing the empirical risk function using cross entropy loss. An important quantity regarding \u03c6(z) is introduced to capture the geometric properties of the loss function, distilled in BID38. The local strong convexity of f n (W) in a neighborhood of the ground truth W is characterized. The Hessian of the empirical risk function f n (W) in the local neighborhood of W is guaranteed to be positive definite with high probability for the classification model with sigmoid activation function. The Hessian of the empirical cross-entropy loss function is guaranteed to be positive definite in a neighborhood of the ground truth W for the classification model with sigmoid activation function. The bounds depend on network dimension parameters and activation function, ensuring the Hessian's positivity as long as certain conditions are met. The sample complexity for the classification problem is near-optimal in d up to polynomial factors of K and log d. Theorem 2 guarantees the existence of a unique critical point W n in a local neighborhood of the ground truth W, where gradient descent converges linearly to W n under certain conditions. Theorem 2 ensures the existence of a critical point W n in a local neighborhood of W, where gradient descent converges linearly to W n. The proof is in Appendix B and guarantees recovery of W at a rate of O(K 9/4 d log n/n) as n approaches infinity. Initialization with the tensor method from BID38 achieves -accuracy with a computational complexity of O(ndK 2 log(1/ )). The tensor method introduced in BID38 defines products \u2297 for vectors and identity matrices. It includes an initialization algorithm in Algorithm 2 with two major steps for estimating directions and magnitudes of vectors. Assumptions are made for the classification problem, similar to those in (Zhong et al., 2017b). The tensor method introduced in BID38 includes an initialization algorithm in Algorithm 2 for estimating directions and magnitudes of vectors. Assumptions are made for the classification problem, similar to those in (Zhong et al., 2017b). The activation function \u03c6(z) must satisfy certain conditions, and a performance guarantee for the initialization algorithm is provided in Theorem 3. Theorem 3 states that for a classification model under certain assumptions, if the sample size is sufficient, the output of Algorithm 2 satisfies a certain condition with high probability. The proof involves accurate estimation of the direction and norm of W. Gradient descent is used to show strong convexity of the empirical risk function. Multiple random initializations are used to verify convergence to the same critical point. Variance of the output is calculated for different initializations. The variance of the output of gradient descent is quantified under different initializations with the same training samples. Successful experiments have a standard deviation \u2264 10^-2. Gradient descent converges to the same local minima with high probability with a large enough sample complexity. Statistical accuracy of the local minimizer is shown when initialized close to the ground truth. Average estimation error is calculated through Monte Carlo simulations. The average estimation error decreases as sample size increases, matching theoretical predictions. Gradient descent with cross entropy loss outperforms squared loss in a classification problem. Study focuses on model recovery of a neural network using cross entropy loss, characterizing sample complexity for convergence to ground truth. The analysis in the current paper focuses on extending the classification model to different activation functions and network structures. The population loss function is denoted as DISPLAYFORM0, and the proof of Theorem 1 involves showing the smoothness and convexity properties of the Hessian of the population loss function. The Hessian of the empirical loss function is also analyzed to establish its properties in a neighborhood of W. The analysis extends the classification model to various activation functions and network structures. The Hessian of the population loss function is shown to be smooth and convex. The Hessian of the empirical loss function is also examined in the vicinity of W. Lemmas are provided to establish the smoothness and convexity properties of the Hessian of the population loss function. Lemma 3 states that for sigmoid activations, there exists a constant C such that under certain conditions, a specific property holds. The proof can be found in Appendix D.4. Combining Lemma 3 and Lemma 1 leads to the proof of Theorem 1. The proof of Theorem 2 involves showing the concentration of gradients and the convergence of gradient descent to a critical point. The lemma establishes that \u2207f n (W ) concentrates around \u2207f (W ) for sigmoid activation function. With Lemma 3 and Lemma 4, (Mei et al., 2016, Theorem 2) guarantees the existence of a critical point W n \u2208 B (W * , r). Local linear convergence of gradient descent is then established. The proof shows local linear convergence of gradient descent towards the local minimizer Wn, with two parts: (a) accurate estimation of the direction of W, and (b) proof based on a mild condition in Assumption 2. A tensor operation is defined for matrices A, B, and C. The tensor operation T(A, B, C) estimates the direction of each wi for regression and classification problems by applying Bernstein inequality. For regression, the estimation error of P2 and R3 is bounded individually for each neuron, while for classification, it is done collectively for all neurons. The proof details can be found in BID38. The proof for estimating wi in regression and classification problems involves using a different approach from BID38, which relaxes the conditions on the activation function. A quantity Q1 is defined to estimate wi, and an optimization problem is solved to obtain the solution. The estimation of wi is further refined using specific equations and substitutions, leading to accurate estimation of si. The text discusses estimating si in regression and classification problems by using specific equations and substitutions. The sub-gaussian and sub-exponential norms of random variables are defined to aid in the proofs. The sub-gaussian and sub-exponential norms of random variables are defined to aid in the proofs of estimating si in regression and classification problems. The calculations of the gradient and Hessian of E are provided, along with the evaluation of \u2206 j,l. The upper bound of E T 2 j,l,k is discussed using specific equations and substitutions. The upper bound of E T 2 j,l,k is discussed using specific equations and substitutions, with a focus on bounding the denominator and applying Lemma 5 for the sigmoid activation function. The Hessian of the population risk at ground truth is analyzed, leading to a uniform bound in the neighborhood of W. The text discusses inequalities and bounds related to the population risk at ground truth in the neighborhood of W. Lemmas and analysis from previous studies are referenced to derive conclusions about the upper and lower bounds of certain functions. The proof of Lemma 3 is outlined using a covering number approach. Events A t, B t, and C t are defined and their probabilities are bounded in the subsequent analysis. The text discusses bounding the probabilities of events A t, B t, and C t separately. A technical lemma is introduced to aid in the proof, stating an upper bound for G i \u03c81."
}