{
    "title": "BJg1fgBYwH",
    "content": "The proposed SAFE-DNN enhances classification robustness by incorporating unsupervised learning of low-level features using SNN with STDP. Experimental results show improved noise robustness for various DNN architectures without compromising accuracy on clean images. This is crucial for deploying DNNs in autonomous systems like autonomous vehicles and robotics, where reliable classifications are needed even with noisy data. Approaches for improving robustness of a DNN to pixel perturbation can be divided into two categories: image de-noising networks can preprocess images before classification, but may reduce accuracy for clean images. Advanced de-noising networks can generalize to different noise types but are complex and less suitable for real-time applications. The paper proposes a new class of DNN architecture that is inherently resilient to input perturbations without requiring training on noisy data, making it computationally efficient. The paper introduces a new DNN architecture that combines features from unsupervised neuro-inspired learning with supervised training to enhance robustness to input perturbations. The integration of features from a spiking neural network with spike-timing-dependent plasticity (STDP) and a DNN improves classification accuracy for complex datasets. This approach leverages stochastic gradient descent (SGD) for global learning and STDP for local feature extraction. The SAFE-DNN architecture combines STDP-based local feature learning with SGD-based supervised training to improve image classification accuracy under noisy input. It integrates a spiking convolutional module within a DNN pipeline and introduces a frequency-dependent stochastic STDP learning rule for robust learning. The proposed SAFE-DNN architecture integrates a spiking convolutional module with a DNN pipeline, using a frequency-dependent stochastic STDP learning rule for local competitive learning of low-level features. A methodology is developed to transform the spiking convolution to an equivalent CNN using a special neuron activation unit (SAU). The supervised training is performed in the deep network after freezing the STDP-learnt weights in the spiking CNN module. Experimentation is conducted on CIFAR10 and ImageNet subsets with different types of noise. The SAFE-DNN architecture demonstrates robust classification under various types of noise without prior knowledge of perturbations. It complements de-noising networks for input pre-processing with minimal computation and memory overhead, making it suitable for resource-constrained platforms. Unlike deep SNNs, SAFE-DNN does not convert pre-trained DNNs and functions independently during inference without supervision or back-propagation. The SAFE-DNN hybridizes STDP and SGD during learning to create a single hybrid network operating as a DNN during inference. Spiking neural networks use biologically plausible neuron and synapse models to exploit temporal relationships between spiking events. The Leaky Integrate Fire (LIF) model is used to capture the firing pattern of real biological neurons in SNNs. Conductance of synapses determines the strength of connections between neurons, with learning achieved through spike-timing-dependent plasticity (STDP). The conductance in spiking neural networks is modulated through spike-timing-dependent plasticity (STDP), which includes long-term potentiation (LTP) and long-term depression (LTD) operations. The magnitude of modulation is determined by specific parameters, and the weight update process in a deep neural network (DNN) involves gradient descent. The weight update process in a DNN involves computing the new weight as W = W \u2212 \u03b7\u2207L, where the gradient of the loss function L is taken with respect to the weight. This optimization process considers cross entropy loss and involves back-propagation from the output layer to the target layer using the chain rule. The gradient of the loss with respect to a parameter is affected by all pixels in the entire input image, making the weight update sensitive to non-neighboring pixels and facilitating global learning. The weight update process in a DNN involves back-propagation from the output layer to the target layer, making weight updates sensitive to non-neighboring pixels for global learning. However, this global learning makes it challenging to impose local constraints during training, leading to difficulty in ignoring local perturbations. To improve robustness to input noise, low-level feature extractors must learn local spatial correlation to effectively filter out noisy pixels and prevent noise propagation in the network. The SAFE-DNN approach enables local learning in a network by utilizing STDP in SNN to modulate conductance based on input spike timing differences. This allows for weight modulation within a local receptive field, improving the network's ability to learn spatial correlations and filter out noisy pixels effectively. The SAFE-DNN approach utilizes STDP in SNN to enable local learning and improve the network's ability to filter out noisy pixels and learn spatial correlations effectively. The network architecture includes spiking layers for robust extraction of low-level features and conventional CNN layers for global learning. The SAFE-DNN approach combines spiking layers for low-level feature extraction with conventional CNN layers for global learning. An auxiliary CNN module is used in parallel with the spiking convolution module to integrate global and local learning. The main CNN module, based on existing deep learning models like MobileNetV2, ResNet101, or DenseNet121, is responsible for higher-level feature detection and final classification. The network architecture drops the first convolution layer and one block from the original network, maintaining the same height and width of output feature maps for concatenation. In this work, three configurations are tested with the main CNN module based on MobileNetV2, ResNet101, and DenseNet121. The storage and computational complexity of the networks are compared, showing that SAFE-DNN implementations do not add significant overhead. In the dynamical system of SNN, neurons transmit information through spikes, requiring input signal intensity to be converted to spike trains. The native SNN model cannot be used in the spiking convolution module of SAFE-DNN, leading to the adaptation of the module to a single-time-step response system to avoid slowing down training and inference. The training process for SAFE-DNN involves two stages: first, the spiking convolution module learns images without supervision using a frequency-dependent STDP method. In the second stage, network parameters are migrated to the spiking convolution module, which undergoes a conversion process and includes batch normalization and a special activation unit to preserve non-linear properties. The entire SAFE-DNN is then trained using statistical methods while keeping weights in the spiking convolution module fixed. After the migration process, the SAFE-DNN is fully trained using statistical methods while preserving features learned by the spiking convolution module. Network inference utilizes the SAU for modeling neurons instead of the baseline LIF. A frequency-dependent stochastic STDP is proposed to address the associative potentiation issue in STDP, dynamically adjusting the probability of LTP/LTD based on input signal frequency. The proposed frequency-dependent stochastic STDP dynamically adjusts the probability of LTP/LTD based on input signal frequency. Parameters like \u03c4 d, \u03c4 p, \u2206t, \u03b3 p, and \u03b3 d control the probabilities, with f max and f min defining input spike frequency limits. Weak inputs require closer pre-synaptic spikes for inducing LTP, showing better learning capability than conventional STDP. The architecture of the spiking convolutional module in FD stochastic STDP exhibits better learning capability than conventional STDP. Input images are converted to spike trains with frequency range. Neurons in the convolution layer connect with plastic synapses following STDP rule. Cross-depth inhibition prevents neurons from learning the same feature, achieving robust low-level features crucial for SAFE-DNN implementation. The spiking neuron network achieves competitive local learning behavior for robust low-level features crucial to SAFE-DNN. To enable deep learning in multiple-layer networks, a layer-by-layer learning procedure is used, adjusting spiking thresholds to facilitate signal propagation. The spike conversion process converts input values to spike frequencies within a specified range. The spike conversion process of SNN involves converting input values to spike frequencies within a specified range, leading to the total received spikes for the recipient. Input perturbations do not cause extra spikes in the receiving neuron due to its unique non-linearity. The Special Activation Unit (SAU) is designed as a step function to enhance baseline networks like MobileNetV2, ResNet101, and DenseNet121, compared to SAFE-DNN. Training with noisy input and using an average filter for image pre-processing are studied as enhancement methods. The spike conversion process in SNN involves converting input values to spike frequencies within a specified range, leading to total received spikes for the recipient. Input perturbations do not cause extra spikes in the receiving neuron due to its unique non-linearity. The Special Activation Unit (SAU) is designed as a step function to enhance baseline networks like MobileNetV2, ResNet101, and DenseNet121, compared to SAFE-DNN. Two SAFEMobileNetV2 models are trained with FD stochastic STDP and deterministic STDP, respectively, and tested on noisy input with AWGN noise. Visualization of the embedding space shows improved local feature extraction of FD stochastic STDP in clustering noisy input. The SAFE-MobileNetV2 with features extracted via FD stochastic STDP provides better clustering of different classes and achieves higher accuracy compared to alternative designs. The MobileNetV2 models, including MobileNetV2-\u00b5 and MobileNetV2-\u03bb, are compared with SAFE-MobileNetV2 to assess the benefits of architectural modifications and SAU activation function. All networks are trained on CIFAR10 dataset and visualized in embedding space with clean and noisy images. While baseline MobileNetV2 clusters overlap with noisy images, SAFE-MobileNetV2 maintains good separation between classes up to 25 dB SNR. This demonstrates the effectiveness of SAFE-MobileNetV2 in preserving feature mappings under noisy conditions. Table 2 displays the accuracy of different network variants on CIFAR-10. Noise in images significantly reduces classification accuracy for baseline DNNs. Networks trained with noise show higher robustness, especially when inference noise levels match training noise. Average filtering improves accuracy in highly noisy conditions but results in performance drop under mild to no noise. SAFE-DNN implementation enhances performance in noisy conditions across all three DNN architectures. Results show that SAFE-DNN outperforms the original network in noisy conditions, with significant gains in accuracy at 20 dB SNR. The advantage of SAFE-DNN increases at higher noise levels compared to networks trained with noise. In a test on a subset of ImageNet related to traffic, all networks achieve around 70% accuracy on clean images, with noise training improving robustness but still impacting performance. In Table 3, all networks achieve approximately 70% accuracy on clean images. Noise training improves robustness but affects clean image accuracy. DensNet121 shows more noise robustness than MobileNetV2 and ResNet101. SAFE-DNN implementations exhibit better robustness over all noise levels without affecting clean image accuracy. SAFE-MobileNetV2 maintains above 80% accuracy even at 5 dB SNR, outperforming baselines. Testing SAFE-DNN in different noise structures shows promising results. Noise-trained DNNs for different distributions (Wald, Poisson, S&P) show improved noise robustness compared to baseline and average filtering. Performance drops when inference noise is not aligned with training noise, especially for mis-aligned noise types. MobileNetV2 networks tested on ImageNet subset also exhibit robustness to different noise structures without specific training. SAFE-MobileNetV2 demonstrates robustness to various noise structures, including adversarial perturbations. The integration of SAFE-DNN with adversarial training methods is suggested for future research. In this paper, SAFE-DNN is introduced as a deep learning architecture that combines spiking convolutional networks with STDP based learning in a conventional DNN for improved low-level feature extraction. The experimental results demonstrate that SAFE-DNN enhances robustness to various input perturbations without prior knowledge of noise during training/inference. It is compatible with different DNN designs and has minimal computation/memory overhead, making it suitable for real-time autonomous systems in noisy environments."
}