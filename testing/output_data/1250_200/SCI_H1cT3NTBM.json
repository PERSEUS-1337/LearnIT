{
    "title": "H1cT3NTBM",
    "content": "In this paper, the use of neural networks for music information retrieval tasks is explored. The study focuses on improving convolutional neural networks (CNNs) on spectral audio features for singer classification and singing performance embedding. Three aspects of CNN design are investigated: network depth, residual blocks with grouped convolution, and global time aggregation. Results indicate that global time aggregation significantly enhances CNN performance. Additionally, a singing recording dataset is released for training and evaluation. The study aims to leverage recent advancements in deep learning to enhance the capabilities of neural networks for music analysis. Experiments explore the use of ResNet and ResNeXt variants with more than 5 convolution layers on audio time-frequency representations, showing effectiveness in music analysis. In this paper, a deeper architecture with more than 5 convolution layers is proposed for audio time-frequency representations. Convolution layers learn local patterns in input matrices, while attention mechanisms from natural language processing are used to model temporal relations. The attention mechanism is seen as a global aggregation operation with learnable parameters along the time axis, compared to typical aggregation operations like average or max. The effects of global aggregation using different methods are experimentally investigated in two specific applications. The paper investigates global aggregation effects using average, max, or attention mechanism along the time axis in two specific applications: singer classification and singing performance embedding. Singer classification aims to predict the singer's identity from audio recordings, while singing performance embedding creates an embedding space to group singers with similar styles. The challenge lies in isolating the \"singer effect\" from the \"song effect\" in the analysis. The paper discusses the challenge of isolating the \"singer effect\" from the \"song effect\" in audio tasks like singer classification and singing performance embedding. It emphasizes the need to learn representations that highlight singer similarity while reducing the impact of song similarity, similar to face identification in computer vision. In this approach, an embedded space is learned for singing voice audio recordings to identify \"singing style\" or \"singing characteristics\" by clustering recordings of the same identity together and pushing those of different identities apart. The architecture employs CNNs to extract features and a global aggregation layer followed by fully connected dense layers for both singer identity classification and singing performance embedding. The architecture uses CNNs to extract features and a global aggregation layer followed by fully connected dense layers for singer identity classification and singing performance embedding. The output layer differs between the two tasks, with a softmax layer for classification and a linear layer for embedding. The model enables fast similarity comparison of spectrogram sequences by calculating Euclidean distance between fixed length embedding vectors. A new set of \"balanced\" singing recordings has been gathered and released for unbiased evaluation of the embedding model. The paper introduces a new dataset of balanced singing recordings for unbiased evaluation. It describes the neural network architecture used in experiments, involving convolutional layers and global aggregation for feature extraction. The model enables singer identity classification and singing performance embedding. The experiments involve using a vanilla convolution layer with shared weights and tied biases, as well as a ResNet design with a bottleneck block. The ResNet variant is extended with a grouped convolutional block from ResNeXt. Different convolution building blocks are depicted in FIG0, with the ResNeXt configuration being a special case of ResNet. Max pooling layers with specific sizes and strides are placed between convolutional blocks in a specific manner. The neural network architecture used in this paper involves max pooling layers placed between convolutional blocks, with batch normalizations applied after each non-linearity activation. The 3-D feature map is reshaped before feeding it to the global time-wise aggregation layer. The 3-D feature map is reshaped into 2-D matrices for the attention mechanism in a feed-forward version, allowing weighted access to information from input hidden sequences. The feed-forward attention calculates weight vector \u03c3 over time-steps using a non-linear function, resulting in an outputX as a weighted average of X with learnable parameters w and b. The feed-forward attention layer calculates the outputX as a weighted average of X using learnable parameters w and b. This operation aggregates information over the time-axis, similar to max or average pooling. The network architecture includes convolutional and global aggregation parts, with singer identity classification and singing performance embedding as tasks. Singer classification provides clear evaluation criteria for model performance. Different hyperparameters and network architectures are experimented with for this task. The singer classification problem offers clear evaluation criteria for model performance with various hyperparameters and network architectures. The embedding task explores spatial relationships between samples. Evaluation metrics and plots are provided for both tasks using the DAMP dataset, which contains unbalanced solo singing recordings by multiple singers. The DAMP-balanced dataset was created to address bias in singer-specific song collections for predicting singer identity. It consists of 24874 singing recordings by 5429 singers, with 14 songs in the collection. The dataset is structured with the last 4 songs designated as the test set, and the first 10 songs for training and validation splits. This dataset is suitable for singing performance embedding tasks, while the original DAMP dataset can be used for singer identity classification algorithms. The input to neural networks for singing performance embedding tasks is time-frequency representations extracted from raw audio signals. These representations are typically 2-D matrices with one axis for time and the other for frequencies. The Mel-scaled magnitude spectrogram (Mel-spectrogram) is commonly used as the input feature, while the constant-Q transformed spectrogram (CQT) is also used in music information retrieval tasks for preserving octave relationships between frequency bins. The constant-Q transformed spectrogram (CQT) is used in music information retrieval tasks for preserving octave relationships between frequency bins. However, neural network configurations using CQT perform worse than their Mel-spectrogram versions due to the linear relationships not applying to the distances between different harmonics of one pitch. The audio recordings are resampled to 22050Hz and transformed into Mel-scaled magnitude spectrograms using FFT with specific parameters. The Mel-spectrogram is obtained from audio recordings with specific parameters. The spectrogram is squared to get the power spectrogram, then transformed into decibels. Values below -60dB are clipped to zero and an offset is added for values between 0 and 60. Each recording is chopped into overlapping matrices of 6 seconds with a 20% hop size. Gradient descent is optimized with ADAM, a learning rate of 0.0001, and a batch size of 32. Dropout of 10% is applied, L2 weight regularization with weight 1e-6 is used, and hyperparameters are chosen by Bayesian optimization. Early stopping is applied every 50 epochs for singer identity classification and every 1000 epochs for singing performance embedding. The singer identity classification uses a patience of 300 epochs with at least 99.5% improvement, while the singing performance embedding task has a patience of 1000 epochs. The non-linear activation function used is the rectified linear unit. Convolutional filter sizes are (10, 10) for the first layer and (5, 5) for subsequent layers. Three fully connected layers with 1024 hidden units each are used before the output layer. A subset of 46 singers from the DAMP dataset is selected for classification, with a 10-fold cross validation approach. Different neural network configurations, including vanilla CNN and ResNeXt building blocks, are explored for the classification task. In this task, various neural network configurations are explored for singer identity classification. Different types of aggregation methods and number of layers are investigated, with baseline SVM classifier included. Experimental results show that neural network models outperform the baseline method by at least 35%, with global aggregation methods improving performance by 5% to 10%. Global aggregation with average or feed-forward attention shows slightly better performance than max aggregation. For singer identity classification, neural network models outperform baseline methods by at least 35%. Global aggregation methods improve performance by 5% to 10%, with average or feed-forward attention showing slightly better results than max aggregation. A siamese neural network architecture is used for embedding space learning, with a focus on placing recordings by the same singer closer together and pushing recordings by different singers apart. The siamese network in this experiment optimizes the squared euclidean distance for pairs of samples with binary labels. The contrastive loss function is used as the optimization goal, with a target margin of 1. Training involves randomly sampling pairs of samples from the same or different singers. Results show contrastive losses for different network configurations and training/validation errors over epochs. Feed-forward attention and average aggregation tend to overfit the data more than max and no aggregation methods. Results from the experiment show that feed-forward attention and average aggregation methods tend to overfit the data more than max and no aggregation methods. Shallow architectures perform slightly better than deeper ones with similar parameters. Qualitative characteristics of the embeddings are illustrated in Figure 4, comparing embeddings from different architectures for singer classification. The handcrafted audio features capture the \"song\" effect, while learned embeddings show distinct patterns in a 2-D space. The experiment results show that shallow architectures outperform deeper ones in singer classification. Handcrafted audio features capture the \"song\" effect, while learned embeddings group performances by singers. Classification accuracies using k-nearest neighbor method are obtained for different network configurations. The embeddings are quantitatively assessed through leave-one-out k-nearest neighbor classifications. The handcrafted features in Figure 5 demonstrate the \"song effect\" in k-nearest neighbor classifications on performed songs. The study shows that singing performance embedding learning can dilute the \"song\" effect and enhance \"singer style\" through global aggregation. The experiment results highlight the effectiveness of global aggregation in improving performance, with similar outcomes for max, average, and feed-forward attention strategies. The balanced dataset enables k-nearest neighbor classification on performed songs, showcasing the benefits of recent developments in deep learning for singer identification and embedding problems. The study demonstrates that time significantly enhances performance. The three aggregation strategies (max, average, feed-forward attention) show similar performances. Feed-forward attention accelerates learning by creating \"frequency templates\" for each convolutional channel. Training deep neural networks with over 15 convolutional layers on time-frequency input is feasible with global time aggregation. A dataset of 20000 single singing voice recordings, DAMP-balanced, is released. The study introduces a dataset of over 20000 single singing voice recordings called DAMP-balanced. Future experiments will involve replacing max-pooling with striding in convolutional layers and considering temporal order during global-aggregation. The dataset is collected from the Sing! Karaoke app and is separate from the original DAMP dataset. The DAMP-balanced dataset differs from the original DAMP in how audio recordings and metadata are collected. Queries for DAMP-balanced specifically request users who sang specific collections of songs once, with only one performance per song and user. Popular songs from the past year were selected for queries, with each query returning a set of users with one performance of each song. The DAMP-balanced dataset selects popular songs from the past year and queries for users who sang specific collections of songs once, with only one performance per song and user. Different 6/4 splits result in varying numbers of singers in each set, affecting the total number of performances for different songs. The \"balanced\" structure allows for train/validation rotation within the first 10 songs and leaves the last 4 songs as a test set, providing more balanced test sets for models training on other datasets."
}