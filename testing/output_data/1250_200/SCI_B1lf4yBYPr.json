{
    "title": "B1lf4yBYPr",
    "content": "Existing deep learning approaches for visual feature learning extract more information than necessary, compromising privacy. This research introduces a novel trust score metric for deep learning models, a model-agnostic framework to improve trust scores by suppressing unwanted tasks, and a benchmark dataset called PreserveTask. Experiments on popular models show Inception-v1 has the lowest trust score. The framework is also applied to the color-MNIST dataset and face attribute preservation. The primary objective of artificial intelligence is to imitate human intelligence, especially with the advent of deep learning models striving to learn complex relationships in data. There is a focus on privacy and security of models, with governance frameworks controlling data sharing. Model privacy involves protecting content from adversarial attacks, but models themselves could learn private information. The strive for model generalization encourages learning generic features from data for multiple tasks, leading to models becoming more intelligent than intended. In privacy-preserving applications, data and visual attributes must be kept private from the model. For example, a deep learning model trained to predict gender from a face image learns generic features beyond its specific task. The research focuses on ensuring that a learning model is restricted to perform only specific tasks, such as predicting gender from a face image, without learning other irrelevant features like age or identity. The goal is to remove biases in learning tasks and ensure the model only learns selected tasks while unlearning others. This is crucial for privacy-preserving applications where data and visual attributes need to be kept private from the model. The research aims to ensure a learning model only learns specific tasks, not irrelevant ones. Challenges include the lack of a balanced dataset for multiple tasks on the same image and the need for complete knowledge of tasks. The proposed framework introduces a trust score metric for DL models and a solution to enhance trust scores during training. Key contributions include a class-balanced multi-task dataset called PreserveTask and a novel trustworthiness score metric for DL models. The research introduces a trust score metric for DL models and a solution to enhance trust scores during training. Experimental analysis shows improvement in trust scores, particularly for the Inception-v1 model. Practical applications are demonstrated using colored MNIST dataset and face attribute preservation with two datasets. In the context of enhancing trust scores for DL models, previous research has focused on k-anonymity preservation and attribute suppression techniques. Studies have explored preserving anonymity in face images by masking sensitive information and de-identifying faces. Other research has focused on suppressing attributes by perturbing input data to the model. Attribute suppression techniques aim to explicitly suppress certain attributes in input data to models. Studies have tested the robustness of models against adversarial attacks by perturbing input face images. Using constrained generative adversarial networks (GANs), researchers have successfully generated attribute-free face images. Previous work has shown that specific attributes in images can be preserved while suppressing others, and there is ongoing research in bias mitigation during model learning. The primary aim is to debias the model learning from correlated attributes, different from improving the model's trust. Existing research gaps include a focus on data perturbation, lack of model perturbation techniques, limited datasets with binary attributes, and a need for a benchmark dataset to evaluate privacy preservation in deep learning models. The objective is to untangle shared tasks in images for deep learning models to perform specific tasks. The research aims to enable deep learning models to perform specific tasks by untangling shared tasks in images. The dataset for evaluating this framework should have multiple tasks on the same image with varying numbers of classes to study classification task complexity. It should be noise-free, class-balanced, and designed so that certain tasks share common attributes while others are independent. Some similar publicly available datasets include LFW, CelebA, and IMDB-Wiki. The PreserveTask dataset is a new multi-task dataset designed for benchmarking models against preserving task privacy. It aims to provide easy-to-perform tasks with high individual task performance. The dataset includes five different tasks and is inspired by the CLEVR dataset. The PreserveTask dataset consists of five classification tasks: Shape, Color, Size, Location, and Background Color. Each task has specific categories, and the images are 256x256 colored images. There are 1260 variations with 50 training and 10 testing images for each, totaling 63,000 training and 12,600 testing images. The dataset ensures perfect class balance across all tasks. The PreserveTask dataset includes five classification tasks with specific categories for shape, color, size, location, and background color. It ensures perfect class balance and serves as a benchmark for different frameworks in shared task learning. The dataset may be extended in the future with more real-world objects. Techniques exist to control the information contained in deep learning models for task privacy preservation. In the context of task privacy preservation, techniques exist to suppress model learning of specific attributes or tasks. This can be achieved by applying negative loss or gradient to prevent certain features from carrying information while retaining others. The proposed framework overcomes the need for task information during training and offers a model-agnostic approach. The proposed framework aims to preserve task privacy by using a custom loss function with gradient reversal branches to suppress specific classification tasks in a deep learning model. This approach is model-agnostic and allows for the suppression of multiple tasks by adjusting the weights of the minimization and maximization losses. The proposed framework is model-agnostic and loss function agnostic, aiming to preserve task privacy by using a custom loss function. The trust score of a trained DL model is evaluated against different tasks in the PreserveTask dataset, with an ideal model achieving 100% accuracy on trained tasks and random accuracy on others. The performance matrix T should closely resemble the ideal matrix M for higher trust. The trust score of a trained DL model is computed based on its deviation from the ideal matrix M, with a higher score indicating higher trustworthiness. The metric considers the performance of diagonal and non-diagonal elements to assess model performance and privacy preservation. Trust scores above 0.9 are considered highly trustworthy, and using random labels for unknown tasks can improve trustworthiness. Inception-v1 techniques involve using random labels to enhance trustworthiness. A trust score above 0.9 is ideal, 0.8-0.9 is acceptable, and below 0.8 is poor. The trust score of the ideal matrix is 1, while a score of 0.6259 indicates all task classification performance is 100%. Sensitivity analysis shows a small trust score reduction corresponds to an additional unwanted task learned. Experimental results demonstrate task suppression in various settings using the PreserveTask dataset. For detailed comparisons, refer to the appendix. The Inception-v1 model was trained on the PreserveTask dataset for shape classification, achieving 99.98% accuracy. The model's features were used to predict size, color, location, and background color with accuracies of 97.29%, 51.25%, 99.98%, and 92.05% respectively. The experiment showed high performance in size, location, and background prediction, while color prediction was low due to the independence of shape and color tasks. The color prediction performance is very low in the Inception-v1 model, with a trust score of 0.7530. Similar experiments with other deep learning models show varying trust scores, with MobileNet having the highest and Inception-v1 and DenseNet having the lowest. The Inception-v1 model has a poor trust score for privacy preservation. Experiments are conducted to suppress tasks during training, using the gradient reversal layer to unlearn the suppressed task while learning the preserved task. A customized negative loss function is also used to compare performance. The features extracted from the flatten layer must perform well on the preserved task but poorly on the suppressed task. Color prediction performance is reduced when using random labels, regardless of the preserved task being shape or size prediction. Suppressing known tasks with the GR layer improves the trust of the baseline model. Experimental settings involve using randomly generated class labels instead of ground truth labels for training the Inception-v1 model. Using random class labels during mini-batch training ensures the model learns to suppress all possible n-class classification tasks. Results show that using random labels can reduce prediction performance in certain settings, such as preserving shape features while suppressing background color prediction. The proposed framework of using random labels significantly reduces the performance of color prediction while preserving shape and size predictions. The framework of using random labels reduces color prediction performance from 51.25% to 26.83% with actual labels and 17.94% with random labels, while preserving shape and size predictions. Trust scores improved from 0.756 to 0.824 using the framework on MobileNet model. In the DiF dataset, task suppression with gradient reversal training increased the trust score of the Inception-v1 model from 0.7497 to 0.8606 with known class labels and to 0.9069 with random unknown class labels. In the IMDB-Wiki dataset, a DenseNet model's trust score increased from 0.7846 to 0.7883 with known class labels and to 0.7860 with random unknown class labels. The framework showcased in the research can be used to measure and improve the trustworthiness of deep learning models for applications like face recognition and gender classification. The proposed framework aims to improve model trustworthiness in privacy preservation without needing task labels during training. A new benchmark dataset, PreserveTask, evaluates DL models' ability in shared task learning suppression. Popular DL models like VGG16, VGG19, Inception-v1, DenseNet, and MobileNet show poor trust scores and exhibit higher intelligence than trained for. A case study on face attribute classification using DiF and IMDB-Wiki datasets is presented, with plans to explore multi-label classification effects. Detailed hyper-parameters for model training are provided for result reproducibility. The material contains detailed hyper-parameters for different deep learning models used in experiments, including Inception-v1, VGG16, VGG19, DenseNet, and MobileNet. The models were trained with z-normalized data and a two hidden layer neural network classifier. Standard architectures were borrowed from the Keras library, and training utilized categorical cross-entropy and Adam optimizer with specific parameters. The architecture used for experiments includes Dense (512) \u2192 Dropout (0.5) \u2192 Dense (100) \u2192 Dropout (0.3) \u2192 Dense (num of classes) with ReLU activation. Categorical cross-entropy is the loss function with Adam optimizer (learning rate = 0.0001). Validation data is 20%, trained for 100 epochs with early stopping. Batch size of 32 was used for faster computation on 1 \u00d7 K80 GPU. Additional analysis, visualizations, and charts are included for comparison. Trust scores for DL models show MobileNet with highest trust and Inception-v1 and DenseNet with least trust. Trust scores after suppression techniques are also presented. Trust scores for DL models show MobileNet with the highest trust score, while Inception-v1 has the least trust score. Various suppression techniques were applied to improve trustworthiness, as shown in performance heat-maps. Even using random labels for unknown tasks, trust scores were enhanced. The Diversity in Faces (DiF) dataset also displayed improved trust scores after suppression techniques. Even with random labels for unknown tasks, the trustworthiness of the Inception-v1 model improved in the Diversity in Faces (DiF) dataset after applying various suppression techniques."
}