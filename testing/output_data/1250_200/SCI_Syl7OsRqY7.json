{
    "title": "Syl7OsRqY7",
    "content": "End-to-end neural models have advanced question answering, but recent studies reveal a limitation in assuming answers and evidence are close in a single document. The Coarse-grain Fine-grain Coattention Network (CFC) is a new model that combines evidence from multiple documents. It includes a coarse-grain module for interpreting documents with respect to the query and a fine-grain module for scoring candidate answers by comparing occurrences across all documents. Using hierarchies of coattention and self-attention, the CFC achieved a state-of-the-art result of 70.6% on the Qangaroo WikiHop task, outperforming previous models by 3% accuracy. Scalable question answering systems must reason over multiple documents, and the CFC demonstrates the ability to combine information effectively. The Coarse-grain Fine-grain Coattention Network (CFC) is a model that focuses on multi-evidence question answering by aggregating information from multiple documents. It combines coarse-grain reasoning, where a summary of support documents is created based on the query, and fine-grain reasoning, which matches specific contexts for scoring candidate answers. The CFC achieved a state-of-the-art result on the Qangaroo WikiHop task, showcasing its effectiveness in combining information from various sources. The Coarse-grain Fine-grain Coattention Network (CFC) utilizes hierarchical attention to combine information from support documents and candidates. It achieved state-of-the-art results on the Qangaroo WikiHop test set and improved accuracy on the TriviaQA task compared to traditional models. The Coarse-grain Fine-grain Coattention Network (CFC) improves accuracy by 3.1% in exact match and 3.0% in F1. The attention hierarchies of the modules focus on distinct parts of the input, enabling better representation of long documents. Common errors in CFC include difficulty in aggregating references, noise in supervision, and challenging relation types. The coarse-grain module summarizes support documents independently of candidates, while the fine-grain module retrieves specific contexts for each candidate. This division of labor enhances performance. The Coarse-grain Fine-grain Coattention Network (CFC) improves accuracy by focusing on distinct parts of the input, allowing better representation of long documents. It uses bidirectional GRUs to encode sequences and builds codependent representations between mentions and the query. This division of labor enhances performance by enabling the model to effectively represent a large number of support documents. The Coarse-grain Fine-grain Coattention Network (CFC) utilizes parameters q and b q for query projection, with d hid representing the bidirectional GRU size. Coattention is employed to create codependent representations of support documents and queries, crucial for question answering models. Affinity matrices are computed between documents and queries, with summary vectors defined using softmax normalization. The coattention context is formed by concatenating document context and summary vectors. This process enhances accuracy by focusing on distinct parts of input for better representation of long documents. The Coarse-grain Fine-grain Coattention Network (CFC) utilizes parameters for query projection and bidirectional GRU size. It employs coattention to create codependent representations of support documents and queries. Affinity matrices are computed between documents and queries to form a coattention context, enhancing accuracy by focusing on distinct parts of input for better representation of long documents. The coattention context is summarized using hierarchical self-attention, creating fixed-length summary vectors through a multi-layer perceptron (MLP) scorer. The Coarse-grain Fine-grain Coattention Network (CFC) utilizes self-attention layers to summarize support documents and candidate answers. It then computes coarse-grain scores by multiplying these summaries. The fine-grain module uses coreference resolution to find specific context in supporting documents, summarizing mentions with self-attention for scoring candidates. This approach enhances accuracy by focusing on distinct parts of input for better representation. The Coarse-grain Fine-grain Coattention Network (CFC) utilizes self-attention layers to summarize support documents and candidate answers. It computes coarse-grain scores by multiplying these summaries. The fine-grain module uses coreference resolution to find specific context in supporting documents, summarizing mentions with self-attention for scoring candidates. This approach enhances accuracy by focusing on distinct parts of input for better representation. The mentions of the candidate are encoded via self-attention to produce fine-grain summaries for scoring. Each mention representation is extracted using self-attention to create a sequence of mention representations. A linear layer determines the fine-grain score of the candidate, which is combined with the coarse-grain score to calculate the final score. The Coarse-grain Fine-grain Coattention Network (CFC) combines coarse-grain and fine-grain scores to evaluate candidate answers. It achieves state-of-the-art results in multi-evidence question answering tasks like WikiHop and TriviaQA. WikiHop dataset links entities in documents with a knowledge base, while TriviaQA is framed as a span reranking task. The CFC significantly improves performance when reranking outputs of a span-extraction model. The Qangaroo WikiHop task involves selecting the correct candidate answer from a set of plausible options based on a query. Support documents are used to determine the answer, with the unmasked version representing candidate answers with original text. Evaluation is done using the unmasked version, and tokenization is performed using Stanford CoreNLP. GloVe embeddings and character ngram embeddings are utilized for the task. The CFC achieves state-of-the-art results on both masked and unmasked versions of WikiHop, with a new best accuracy of 70.6% on the blind test set. It outperforms previous results by 3% without using pretrained contextual encoders. The division of labor between coarse-grain and fine-grain modules allows for effective attention hierarchies, enabling better modeling of input data. The CFC model achieves state-of-the-art results on WikiHop by utilizing attention hierarchies from coarse-grain and fine-grain modules. It also performs well on TriviaQA by decomposing tasks into proposing candidate answers and reranking them. Ablation study on WikiHop shows the importance of different modules in the model's performance. The CFC model achieves state-of-the-art results on WikiHop by utilizing attention hierarchies from coarse-grain and fine-grain modules. Reranking using the CFC provides consistent performance gains over only using the span extraction question answering model, improving performance regardless of correct answers in the candidate set. The CFC can refine outputs produced by span extraction question answering models, with a gain of 3.1% EM and 3.0% F1 on the TriviaQA dev set. The coarse-grain and fine-grain modules significantly contribute to model performance, with alternative model decisions showing less performance degradation. Replacing selfattention layers with mean-pooling and bidirectional GRUs with unidirectional GRUs results in less performance degradation. However, replacing the encoder with a projection over word embeddings leads to a significant performance drop, highlighting the importance of contextual encodings capturing positional information. The fine-grain-only model consistently under-performs the coarse-grain-only model, particularly in coreference resolution due to exact lexical matching. Yet, the fine-grain-only model excels in examples with multiple or lengthy support documents, showcasing better intra-document and inter-document dependency capture compared to hierarchical attention. The entity-matching coreference resolution captures dependencies more precisely than hierarchical attention. Coattention layers focus on similar phrases between the document and query, while self-attention layers capture phrases describing the entity. Fine-grain coattention aligns the query relation to the text context, with mentions related to the query located in Richmond upon Thames. The model excels in capturing intra-document and inter-document dependencies. The third describes Richmond upon Thames itself. Coarse-grain summary self-attention scores for the query country of origin the troll, for which the answer is \"United Kingdom\". The top three support documents 2, 4, 5 respectively present information about the literary work The Troll, its author Julia Donaldson, and Old Norse. Coarse-grain summary self-attention tends to focus on documents relevant to the subject in the query. The self-attention focuses on documents relevant to the literary work \"The Troll\", namely those about The Troll, its author Julia Donaldson, and Old Norse. Fine-grain coattention over mention representations tends to focus on the relation part of the query. Coattention focuses on the relationship between mentions and the phrase \"located in the administrative territorial entity\". Errors in the CFC are categorized into four types, with examples provided in the Appendix. The first type (42% of errors) occurs when the model aggregates the wrong reference. The second type (28% of errors) results from unanswerable questions. The third type (22% of errors) is caused by... The support documents do not provide the narrative location of the play \"The Beloved Vagabond\" for the query \"narrative location the beloved vagabond\". Errors in the CFC are categorized into four types, with examples provided in the Appendix. The second and third types of errors underscore the difficulty of using distant supervision to create large-scale datasets such as WikiHop. The fourth type (8% of errors) results from complex relation types such as parent taxon which are difficult to interpret using pretrained word embeddings. One method to alleviate this type of errors is to embed relations using tunable symbolic embeddings as well as fixed word embeddings. The Qangaroo WikiHop dataset and query-focused multi-document summarization task require reasoning over multiple pieces of evidence across documents. Various question answering models have been developed, including document attention models, multi-hop memory networks, and cross-sequence attention models for span-extraction QA. These models aim to aggregate information from multiple documents to answer questions effectively. Recent advances in attention models for span-extraction QA include variations such as match-LSTM, coattention, bidirectional attention, and query-context attention. These models utilize reinforcement learning, convolutions, self-attention, and reranking to improve span-extraction output. Attention has been successfully applied in various tasks like machine translation, relation extraction, summarization, and semantic parsing. Coattention is used to encode codependent representations. In the context of attention models for span-extraction QA, coattention has been successfully applied in various tasks like relation extraction, summarization, and semantic parsing. The combination of self-attention and coattention in a hierarchical manner is proposed in the CFC for effective representation of long documents. Hierarchical coarse-to-fine modeling is highlighted as an effective technique for modeling long documents. The CFC model demonstrates effectiveness in parsing, speech recognition, and machine translation. It also excels in question answering and semantic parsing tasks. The CFC utilizes complementary coarse and fine-grain modules for multi-evidence question answering, achieving 70.6% test accuracy on the WikiHop task. The model focuses on different aspects of input and represents large document collections efficiently. Future work may explore integrating full-scale coreference resolution systems. The CFC model shows effectiveness in various tasks like parsing, speech recognition, and question answering. It utilizes lexical matching for coreference resolution and trains using Adam with specific parameters. The model's performance is evaluated on a development set, and the best model is tested on a held-out test set. The CFC model demonstrates effectiveness in tasks such as parsing, speech recognition, and question answering. It uses lexical matching for coreference resolution and is trained using specific parameters with Adam. The model's performance is evaluated on a development set, and the best model is tested on a held-out test set. The convergence plot in FIG5 shows the model's performance using fixed embeddings and GRUs with specific hidden sizes. Regularization techniques such as dropout are applied at various stages in the model. Attention maps produced by the CFC on the development split of WikiHop are included, showcasing fine-grain mention self-attention and coattention, coarse-grain summary self-attention, and document self-attention and coattention for top scoring supporting documents. The query is visible in the coattention maps, with the answer serving as the title of the subsection. The section includes identifiers and examples of unanswerable questions found during error analysis on the development set of WikiHop. It highlights errors made by the CFC model, with details about Glasgow and Edinburgh in Scotland. Edinburgh is Scotland's second most populous city and the seventh most populous in the UK. It is the capital of Scotland, home to the Scottish Parliament, and various national institutions. The River Clyde in Scotland is the eighth-longest river in the UK and flows through Glasgow, historically important for shipbuilding and trade. The River Clyde in Scotland is historically important for shipbuilding and trade, flowing through Glasgow. The Avon Water, a tributary of the River Clyde, is located in Lanarkshire, a historic county in the central Lowlands of Scotland. The North Sea is a marginal sea of the Atlantic Ocean located between Great Britain, Scandinavia, Germany, the Netherlands, Belgium, and France. It connects to the ocean through the English Channel in the south and the Norwegian Sea in the north. Worms is a city in Rhineland-Palatinate, Germany, situated on the Upper Rhine. William George \"Will\" Barker was a British film producer who revolutionized filmmaking in Britain. Ealing is a suburban district of west London, England. Ealing, a suburban district of west London, historically shifted to market garden supply and suburban development due to improved communications with London. Paris is the capital of France with a population of 2,229,621 in 2013. Bordeaux is a port city in southwestern France. The Mediterranean Sea is connected to the Atlantic Ocean and surrounded by land. The Mediterranean Sea is a sea connected to the Atlantic Ocean, surrounded by land. Maurice Auguste Chevalier was a French actor and entertainer known for his songs and films. Nice is the fifth most populous city in France, located on the French Riviera. Nice is the second-largest French city on the Mediterranean coast, located in the French Riviera. It is about 13 kilometers from Monaco and serves as a gateway to the principality. Ealing Studios in London is the oldest continuously working film production facility in the world, known for classic films produced post-WWII. Europe is a continent bordered by the Arctic Ocean, Atlantic Ocean, and Mediterranean Sea. Europe is a continent bordered by the Arctic Ocean, Atlantic Ocean, and Mediterranean Sea. France, officially the French Republic, is a country in western Europe with overseas territories. Overseas France includes French Guiana on the South American continent and various island territories in the Atlantic, Pacific, and Indian oceans. France is a unitary semi-presidential republic with a population of almost 67 million people. The capital is Paris, the largest city and main cultural center. Other major urban centers include Marseille, Lyon, Lille, Nice, Toulouse, and Bordeaux. The BBC is a British public service broadcaster headquartered in London, the world's oldest national broadcasting organization with over 20,950 employees. The Rhine is a European river that begins in the Swiss Alps and empties into the North Sea in the Netherlands. The Rhine River flows from Swiss-German border to the North Sea in the Netherlands. The largest city on the river is Cologne, Germany. The Beloved Vagabond is a 1936 British film set in 19th century France. The Atlantic Ocean is the second largest ocean, separating the \"Old World\" from the \"New World\". Claude Austin Trevor was a Northern Irish actor. The English Channel separates countries. The English Channel separates southern England from northern France and joins the North Sea to the Atlantic Ocean. North America is a continent within the Western Hemisphere, bordered by the Arctic Ocean to the north and the Atlantic Ocean to the east. North America is a northern subcontinent bordered by the Arctic Ocean to the north, the Atlantic Ocean to the east, the Pacific Ocean to the west and south, and South America and the Caribbean Sea to the southeast. Inuit are indigenous peoples living in the Arctic regions of Greenland, Canada, and Alaska. Inuit Sign Language is spoken in Nunavut. Qilakitsoq in Greenland is known for the discovery of mummified bodies. Norway is a monarchy in Scandinavia with territories including Jan Mayen and Svalbard. Norway is a monarchy in Scandinavia with territories including Jan Mayen, Svalbard, and claims to Queen Maud Land in Antarctica. The Kingdom historically included Faroe Islands, Greenland, Iceland, Shetland, Orkney, and provinces now in Sweden. The Arctic region consists of the Arctic Ocean, adjacent seas, and parts of Alaska, Canada, Finland, Greenland, Iceland, Norway, Russia, and Sweden, characterized by seasonal snow and ice cover. Archaeology is the study of human activity through material culture analysis. Archaeology is the study of human activity through material culture analysis. It involves the recovery and analysis of artifacts, architecture, biofacts, and cultural landscapes. Archaeological sites preserve evidence of past activities and can range from barely visible remains to still-in-use structures. The Nuussuaq Peninsula in Greenland is a large peninsula with fjords created by glacial erosion. Fjords are long, narrow inlets with steep cliffs created by glacial erosion. They can be found in various locations such as Alaska, Greenland, Norway, and Scotland. The archaeological record is the physical evidence of the past, essential for understanding human cultures. Human activities like agriculture and land development can damage or destroy archaeological sites, impacting the archaeological record. The archaeological record is crucial for understanding human cultures and history. It can be impacted by natural phenomena and scavenging, leading archaeologists to limit excavation to preserve resources. Greenland, part of the Danish Realm, has a majority Inuit population with roots dating back to the 13th century. Uummannaq is a town in northwestern Greenland. Uummannaq, a town in northwestern Greenland, was founded in 1763 as a hunting and fishing base. It is home to the country's most northerly ferry terminal and has a canning factory and marble quarry. Iceland, a Nordic island country in the North Atlantic Ocean, has a population of and is known for its volcanically and geologically active landscape. Iceland is warmed by the Gulf Stream and has a temperate climate, despite its high latitude just outside the Arctic Circle. The Canadian Arctic Archipelago is a group of islands north of the Canadian mainland. Uummannaq Fjord is a large fjord system in western Greenland, emptying into Baffin Bay in the northwest. A honey bee is a bee known for producing honey and constructing colonial nests from wax. Honey bees are known for producing honey and constructing colonial nests from wax. There are seven recognized species of honey bees with 44 subspecies. The Western honey bee is the most well-known and has been domesticated for honey production and crop pollination. Honey bees are just a small fraction of the 20,000 known bee species. The study of bees, including honey bees, is called melittology. Honey is produced and stored by certain social hymenopteran insects through regurgitation, enzymatic activity, and water evaporation. Honey is produced from sugary secretions of plants or insects, such as floral nectar or aphid honeydew, through regurgitation, enzymatic activity, and water evaporation. It is well-known for its sweetness from fructose and glucose, similar to granulated sugar. Honey has unique flavor and baking properties, does not spoil due to lack of microorganism growth, but may contain dangerous bacteria like Clostridium botulinum. It has potential medical uses, but evidence is inconclusive. Honey provides 64 calories per tablespoon serving. Honey, with 64 calories per tablespoon serving, has no significant nutritional value and its medical use is inconclusive. It is generally safe but may have adverse effects with excessive consumption or existing health conditions. Honey production has a long history dating back at least 8,000 years. Australia is the world's sixth-largest country with Canberra as its capital and Sydney as its largest urban area. Bees play a crucial role in pollination, with the European honey bee being the most well-known species. Bees are flying insects related to wasps and ants, known for pollination and producing honey. There are nearly 20,000 species of bees found on every continent except Antarctica. Solomon Islands is a sovereign country in Oceania with its capital in Honiara. The Colletidae family of bees is often referred to collectively. Indonesia is a unitary sovereign state located in Southeast Asia with over 17,000 islands. The Colletidae family of bees, known as plasterer bees, have unique nesting habits and over 2000 species. Some subfamilies lack the typical pollen-carrying apparatus and instead carry pollen in their crops. Indonesia is the world's largest island country with over 17,000 islands and a population of over 260 million people. It is the world's 14th-largest country by land area and the 7th-largest by combined sea and land area. Java, the most populous island, holds more than half of the country's population. Ants, part of the family Formicidae, evolved from wasp-like ancestors about 99 million years ago. Tasmania is an island state of Australia located south of the mainland, separated by Bass Strait. New Zealand is an island nation in the southwestern Pacific Ocean, situated east of Australia across the Tasman Sea. It consists of two main islands, the North Island and the South Island, along with numerous smaller islands. Due to its remoteness, New Zealand was one of the last lands to be settled by humans, leading to a unique biodiversity of animal, fungal, and plant life. The country's varied topography, including sharp mountain peaks like the Southern Alps, is a result of tectonic uplift and volcanic activity. New Zealand's varied topography, including sharp mountain peaks like the Southern Alps, is a result of tectonic uplift and volcanic activity. The country's capital is Wellington, and its most populous city is Auckland. Angiosperms are the most diverse group of land plants, distinguished by characteristics such as flowers, endosperm within seeds, and fruit production. Pollination is the process by which pollen is transferred for fertilization in seed plants. The female reproductive organs of seed plants enable fertilization for genetic information transfer to the next generation. Insects, the most diverse group of animals, play a crucial role in pollination. The Stenotritidae bee family, with 21 species in Australia, is considered a sister taxon of the Colletidae family. The Stenotritidae bee family is considered a sister taxon of the Colletidae family. Stenotritids have unmodified mouthparts and make burrows in the ground for their provision masses. They are large, fast-flying bees that do not spin cocoons. Fossil brood cells of a stenotritid bee have been found in South Australia. Wasps, belonging to the order Hymenoptera, are neither bees nor ants and form a clade with a common evolutionary ancestor."
}