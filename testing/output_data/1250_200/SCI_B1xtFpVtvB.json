{
    "title": "B1xtFpVtvB",
    "content": "In this paper, the authors investigate the overfitting of reinforcement learning agents in visual navigation tasks. They propose a regularization method called invariance regularization, which combines RL with supervised learning to improve generalization of policies to unseen environments. Learning control policies from high-dimensional sensory input has been gaining traction with deep reinforcement learning (DRL), enabling simultaneous learning of perception and control modules. Evaluating learned policies in different environments is crucial for assessing generalization abilities, as variations in visual aspects, physical structures, and agent's goals can impact performance. In deep reinforcement learning (DRL), agents need to generalize to new environments with different levels and control models. Overfitting to training environments is a common issue, leading to varied performance on testing environments. While supervised learning algorithms offer some generalization guarantees, reinforcement learning algorithms struggle due to non-i.i.d. data sources. To effectively apply DRL progress, policies must be robust to changes in sensory inputs, environment structure, and task aspects. This paper explores generalization in visual navigation control policies. In this paper, the study focuses on generalization in visual navigation control policies learned with DRL. It includes an investigation of policy generalization to changes in the underlying system and proposes a training method combining DRL with supervised learning. Experiments are conducted using the VizDoom platform, showcasing how environments can differ visually. Visual navigation for mobile robots involves combining vision and control to find safe paths between starting and goal states. The study focuses on using reinforcement learning and deep learning to navigate an agent towards a goal object with limited visual observations. The approach aims to solve the task without providing an explicit map of the environment to the agent, treating it as a partially observed Markov decision process (POMDP). In reinforcement learning, a partially observed Markov decision process (POMDP) is defined by sets of states, actions, and observations, with transition and observation probability functions. Deep reinforcement learning (DRL) involves a parameterized policy to adjust parameters for high value outcomes. The objective in reinforcement learning is to adjust parameters to maximize the discounted reward. Various methods, such as policy gradient methods and proximal policy optimization (PPO) algorithm, are used to approximate optimal policies. The goal is to learn from a training set and generalize well to unseen examples. Multiple environments or tasks are represented by a distribution over partially observed Markov decision processes (POMDPs), from which a sample of POMDPs is drawn for training. In reinforcement learning, the objective is to maximize the discounted reward by adjusting parameters. Policy gradient methods and PPO algorithm are used for this purpose. A distribution over POMDPs is used to represent multiple environments or tasks, from which a sample is drawn for training a policy \u03c0 \u03b8. The policy generalizes well if it attains a high value for the discounted generalization reward over the full distribution of POMDPs. The problem of generalization in POMDPs involves choosing a policy \u03c0 \u03b8 that maximizes the discounted reward over a distribution D of POMDPs. This involves obtaining a high value for the discounted generalization reward by adjusting parameters. The state space in the larger POMDP P D is the Cartesian product of original states and unique identifiers, with common action and observation spaces. Training in synthetic environments allows for the efficient simulation of experience to train reinforcement learning agents. However, there is often a gap between synthetic and real-world environments, leading to the need for transfer learning techniques like domain randomization to bridge this reality gap. Domain randomization has been successful in transferring grasping policies from simulation to the real world. Previous studies have shown success in transferring perception modules but not controllers. Cobbe et al. (2018) conducted a study on generalization using a new environment called CoinRun, finding that RL agents tend to overfit even with large training sets. Zhang et al. (2018a) also found that agents tend to memorize training set levels in grid-world environments. They suggest methods like sticky actions and random initializations to prevent memorization. In our work, we focus on generalization in navigating under partial observability, unlike fully observable environments like CoinRun or grid-world. Domain adaptation methods have been used for simulated to real transfer, with success in transferring grasping policies. Generalization also involves transferring learned skills to solve different tasks by maximizing different reward functions. Schaul et al. (2015) consider working with value functions containing the goal as part of the agent's strategy. In navigating under partial observability, generalization involves transferring learned skills to maximize different reward functions. Schaul et al. (2015) introduce universal value functions that include the goal in the agent's state, leading to reward functions based on state-action-goal tuples. Universal value function approximators (UVFA) aim to learn V \u03b8 (s, g) and generalize for unseen state-goal pairs in grid-world setups. Deep reinforcement learning is used for training control policies, mapping pixels to motor control commands from raw camera images. DRL algorithms have been applied to goal-conditioned and mapless navigation tasks, but policies learned from visual input lack robustness in novel situations. A proposed regularization term aims to enhance the robustness of control policies. Our main contribution is a regularization term proposed to improve the robustness of learned policies to variations in observations. Domain randomization is explored as a current practice to train policies that can generalize well by exposing them to variations and noise in observations. The motivation behind domain randomization is to provide policies that are invariant to changes in observations. The problem of navigating an agent towards a goal object with random noise added to observations is also examined. The agent's ability to perform tasks in different environments defined by POMDPs is studied, focusing on features that are invariant across variations. The study explores domain randomization in RL training to improve policy generalization, addressing the issue of overfitting to training environments. It is noted that training on multiple environments may not guarantee policies that generalize well, contrary to assumptions in supervised learning. This finding aligns with previous research and highlights the need for careful consideration in reinforcement learning techniques. In reinforcement learning, it is important to include a term in the training objective that encourages policy generalization. This involves adding a regularization penalty term to the RL objective to ensure the policy is invariant to changes in observations. The penalty term is added to the PPO objective and includes parameters defining the policy, a distance function, and a weighting coefficient. This approach helps the policy generalize well across different environments. In reinforcement learning, a regularization penalty term is added to the PPO objective to ensure policy invariance to observation changes. This penalty term encourages policy generalization and is similar to trust region policy optimization. The method is called invariance regularization (IR) and aims to maintain policy invariance to observations. The text discusses invariance regularization (IR) in reinforcement learning, proposing two methods to solve the RL problem. The first method involves adding a penalty to the original PPO loss, while the second method combines reinforcement learning with supervised learning. The effectiveness of domain randomization in reducing overfitting in DRL agents is also explored through experiments. The text explores the use of invariance regularization (IR) in reinforcement learning to improve generalization to unseen environments. Experiments with domain randomization show a significant improvement in success rate. Customizable VizDoom maps are used to train an actor-critic style agent to reach objects for rewards. The network architecture includes convolutional and fully connected layers for policy and value function estimation. The network architecture consists of convolutional and fully connected layers for policy and value function estimation. ReLUs are used as non-linear operations in all layers. The PPO objective is optimized with a binary reward function and a discount factor. Variations in the training environment are generated by changing textures on surfaces. Agents are trained on a subset of rooms and tested on a hold-out set. Different types of visual input are experimented with, including RGB, RGB-D, and Grayscale. The experimental setup includes testing different types of visual input (RGB, RGB-D, Grayscale) with a fixed number of training iterations. Depth plays a significant role in generalization, with agents using depth information (RGB-D) performing better. The success rate of PPO models is shown in Table 1, indicating better generalization with depth information. The success rate of PPO models is compared based on the number of training environments and input type (RGB, RGB-D). Results show that agents with depth perform better than those without depth. RGB agents struggle to generalize well, while RGB-D agents perform well even with limited training environments. Agents trained on 100 and 500 environments tend to overfit, despite maximizing rewards in training. The 100 and 500 experiments show higher variance in success rates compared to those trained on 10 and 50 environments. The 100 and 500 experiments outperform the rest but have higher variance in success rates. Some seeds achieve near-perfect scores while others fail, indicating a lack of generalization in RL agents. Domain randomization alone may not effectively adapt policies to observation variations in RGB input. Training with randomized environments showed success in supervised learning but not in RL policies. The randomized environment improved supervised learning but not RL policies. Adding random variations and relying on RL objective alone is insufficient for generalization. Adapting supervised learning techniques to regularize models trained with DRL is proposed. Two methods using IR penalty are discussed: adding to PPO objective or splitting into RL and supervised learning steps. The value of \u03bb in Equation 1 used in all IR experiments is 1.0. Transformation T of observations tested with randomly textured environments from VizDoom. KL divergence returned best results for distance penalty term. Results show combining PPO with IR penalty improves stability and generalization. Full objective training outperformed vanilla PPO with domain randomization and split version of IR algorithm. Training on full objective returned best results. Training on the full objective outperformed vanilla PPO with domain randomization and the split version of the IR algorithm. Results show stable performance across different inputs and seeds. Models trained on the full objective achieve high test success rates with only 10 training environments, similar to those trained on 50, 100, and 500 environments. This suggests that training with the full objective does not require a large number of environments to learn invariant features. RGB-D testing results also show near-perfect scores, attributed to the availability of an invariant feature map in the input. Regularization in supervised learning models has been shown to improve generalization. However, it is not commonly used in DRL setups. Comparisons with other regularization techniques such as dropout, batchnorm, and L2 show that some methods achieve similar results. The lower success rates of other regularization methods suggest randomness in learned policies due to poor testing and training practices. Regularization techniques such as dropout, batchnorm, and L2 were compared in a DRL setup. The dropout method steadily improved with more environments, while batchnorm performed the worst. Dropout and L2 achieved similar success rates to a split version of the proposed method. However, policies with dropout and L2 had higher entropy, suggesting they could generalize better by acting randomly in some instances. The study explores the generalization capabilities of visual navigation agents trained with deep reinforcement learning algorithms. It is found that policies with higher entropy, such as those with dropout and L2 regularization, are able to act randomly in some instances, making them more robust in certain situations. However, these policies are shown to be inefficient in terms of success weighted shortest path length compared to policies with lower entropy. The research highlights the challenge of overfitting in RL agents even when exposed to large training sets, and emphasizes the importance of adding invariant features to improve generalization. In the study, it is shown that domain randomization with RL alone is insufficient for generalization. A method called Invariance Regularization (IR) is proposed to improve generalization by regularizing the RL model with a supervised learning loss. Future work includes exploring different architectural designs of the environment and investigating appropriate transformation functions for observations. The first part involves training RL on original observations, while the second part focuses on supervised learning on transformed observations. The training process involves two stages: training RL on one environment and then using the actions for supervised learning on textured environments. The model is trained with one iteration of the algorithm on a subset of rooms with varied textures. Multiple agents run in parallel to collect data quickly, allowing for testing on unseen rooms with different textures. The training process involves running multiple agents on different environments due to hardware limitations. Each agent samples one environment and runs on it for a set number of episodes before sampling another. Success Performance Level (SPL) measures agents' success rates considering the time taken to succeed, using a formula that includes the number of runs, success indicators, shortest path length, and path taken length."
}