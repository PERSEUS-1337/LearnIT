{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning. It extends existing probabilistic interpretations of meta-learning and introduces \\Versa{}, which uses a flexible amortization network for few-shot learning. This framework replaces optimization at test time with forward passes through inference networks, reducing the need for second derivatives during training. The paper presents ML-PIP, a framework for meta-learning approximate probabilistic inference for prediction. It introduces \\Versa{}, which achieves state-of-the-art results on benchmark datasets for few-shot learning tasks. The approach can handle varying numbers of shots and classes at train and test time, demonstrating its flexibility and efficiency in adapting to new datasets. The framework ML-PIP introduces a new method called VERSA for meta-learning approximate probabilistic inference. It extends existing interpretations of meta-learning and incorporates shared statistical structure between tasks, information sharing between tasks, and fast learning through amortization. VERSA substitutes optimization procedures with forward passes through inference networks at test time. The VERSA method proposes amortizing the cost of inference by using forward passes through inference networks at test time. It employs a flexible amortization network for few-shot learning datasets, achieving faster test-time performance without the need for second derivatives during training. VERSA sets new state-of-the-art results on standard benchmarks and handles varying shot and way conditions, including a challenging one-shot view reconstruction task. The standard multi-task directed graphical model employs shared parameters \u03b8 for all tasks and task-specific parameters {\u03c8. The goal is to meta-learn fast and accurate approximations to the posterior predictive distribution for unseen tasks. Point estimates are used for shared parameters \u03b8, while distributional estimates are used for task-specific parameters. The model employs shared parameters \u03b8 for all tasks and task-specific parameters {\u03c8. Distributional estimates are used for task-specific parameters. An inference network with parameters \u03c6 approximates the posterior predictive distribution for unseen tasks by taking training data and test inputs as inputs. The model uses shared parameters \u03b8 for all tasks and task-specific parameters {\u03c8. An inference network with parameters \u03c6 approximates the posterior predictive distribution for unseen tasks by taking training data and test inputs as inputs. Amortization is used to construct the predictive distribution, enabling fast predictions at test time. The training method focuses on meta-learning the approximate posterior predictive distribution to minimize the KL-divergence between true and approximate distributions. The goal is to return parameters \u03c6 that best approximate the posterior predictive distribution in an average KL sense. The training procedure aims to optimize parameters \u03c6 to approximate the posterior predictive distribution, supporting accurate prediction through meta-learning. By selecting tasks randomly, sampling training data, forming posterior predictive distributions, and computing log-densities, the procedure evaluates the objective for optimization. This approach scores the approximate inference by simulating Bayesian held-out log-likelihood evaluation. The training procedure involves optimizing parameters to approximate the posterior predictive distribution for accurate prediction through meta-learning. It focuses on minimizing KL divergence between the posterior predictive distribution and the inference network, using end-to-end stochastic training with shared parameters. This approach utilizes episodic train/test splits at meta-train time. The approach developed is Meta-Learning Probabilistic Inference for Prediction (ML-PIP), which uses episodic train/test splits at meta-train time and approximates the integral over \u03c8 using Monte Carlo samples. The learning objective implicitly learns the prior distribution over parameters through q \u03c6 (\u03c8|D, \u03b8). This framework unifies existing approaches and supports versatile learning for rapid and flexible inference. The system supports various tasks without retraining, using a deep neural network for rapid inference. Design choices enable flexibility in processing sets of variable size for few-shot image classification. The model is inspired by early work from BID18 and recent extensions to deep learning. It uses a feature extractor neural network shared across tasks, with task-specific linear classifiers. A novel amortization approach is proposed to model the distribution over weight matrices in a context-independent manner, allowing for flexibility in few-shot image classification. The model uses a feature extractor neural network shared across tasks with task-specific linear classifiers. A novel amortization approach is proposed to model the distribution over weight matrices in a context-independent manner for few-shot image classification. The amortization network operates directly on extracted features to reduce the number of learned parameters. The assumption of context-independent inference is supported theoretically and empirically. The context-independent approximation in Density Ratio Estimation addresses limitations of naive amortization by reducing the number of parameters needed for inference. VERSA tackles few-shot image reconstruction by inferring object views from a small set of observed views through multi-output regression. The generative model involves a latent vector as input for image generation. Our generative model involves a latent vector as input for image generation, with specified orientations. The generator network parameters are treated as global parameters, while the latent inputs are task-specific parameters. An amortization network processes image representations of objects, concatenates them with view orientations, and produces a distribution over vectors. This process is illustrated in FIG5. In this section, ML-PIP unifies various meta-learning approaches, including gradient and metric-based variants, amortized MAP inference, and conditional modeling. The task-specific parameters are estimated using point estimates. Comparisons are made with previous approaches to VERSA.Gradient-Based Meta-Learning, highlighting the use of semi-amortized inference. The perspective of semi-amortized ML-PIP complements Model-agnostic meta-learning, providing a new insight into the one-step optimization process. In this perspective, Eq. (6) presents Model-agnostic meta-learning as semi-amortized ML-PIP, contrasting with BID16's justification of one-step gradient updates in MAML. The update choice is viewed as amortization trained using predictive KL, recovering test-train splits. VERSA, unlike other methods, is distributional over \u03c8, eliminating the need for back-propagation during training and simplifying inference by treating both local and global parameters. The shared parameters in \u03c8 (t) are the lower layer weights. Amortized point estimates for these parameters are constructed by averaging top-layer activations for each class, with \u00b5 determining the predictive distribution. VERSA, a distributional method, uses a flexible amortization function beyond simple averaging. BID43 proposed predicting weights of classes from activations for online learning and transfer tasks, utilizing hyper-networks to amortize learning about weights. VERSA goes beyond point estimates, employing end-to-end training and supporting full inference. VERS goes beyond point estimates by employing end-to-end training and supporting full multi-task learning through sharing information between tasks. The amortization network computes \u03c8 * (D, \u03b8) and can be seen as part of the model specification rather than the inference scheme. This approach establishes a strong connection to neural processes and differs from standard amortized VI for \u03c8 in the multi-task discriminative model. In addition to the conceptual difference from ML-PIP, VERSA differs from ML-PIP objective by not using meta train/test splits and including KL for regularization. VERSA significantly improves few-shot classification and is compared to recent VI/meta-learning hybrids. It is evaluated on various few-shot learning tasks, including toy experiments on amortized posterior inference, few-shot classification on Omniglot and miniImageNet datasets, and performance on a one-shot view reconstruction task with ShapeNet objects. An experiment is conducted to investigate the approximate inference of the training procedure by generating data from a Gaussian distribution with varying means across tasks. VERSATILE (VERSA) is a model that generates tasks with different train observations and test observations. It uses an inference network for amortization and is trained with Adam using mini-batches of tasks. The model infers posterior distributions accurately, even when minimizing predictive KL divergence. VERSA is evaluated on few-shot classification tasks like Omniglot and miniImageNet datasets. VERSATILE (VERSA) is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets. The model follows specific implementation and experimental protocols, training episodically with k c examples per class. Results show competitive performance compared to other approaches. VERSA achieves state-of-the-art results on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks. It is competitive on other benchmarks as well. VERS achieves state-of-the-art performance on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks. It outperforms amortized VI and non-amortized VI, showing significant improvements in accuracy and log-likelihood. The method adapts only the weights of the top-level classifier, leading to faster posterior formation compared to non-amortized VI. VERS achieves state-of-the-art performance on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks, outperforming amortized VI and non-amortized VI. VERSA allows for varying the number of classes and shots between training and testing, demonstrating flexibility and robustness. It is efficient, requiring only forward passes through the network, and significantly faster than MAML in evaluation time. The VERSA model outperforms MAML in speed and accuracy, with a 5\u00d7 speed advantage and 4.26% better accuracy. The ShapeNetCore v2 dataset consists of 37,108 objects from 12 categories, with 36 views generated for each object. VERSA is evaluated against a C-VAE using an episodic training approach. The VERSA model outperforms a C-VAE in generating images with more detail and sharper visuals. Quantitative comparison results show VERSA's superiority in view reconstruction tests, with improved metrics as the number of shots increase. ML-PIP is a probabilistic framework for meta-learning that unifies various methods and proposes alternative approaches. VERSA, a few-shot learning algorithm based on ML-PIP, achieves state-of-the-art performance without gradient-based optimization at test time. Prototypical Networks perform better when trained on a higher \"way\" than that of testing, achieving 68.20% accuracy when trained on 20-way classification and tested on 5-way. The new inference framework presented in Section 2 is based on Bayesian decision theory (BDT), which provides a recipe for making predictions for an unknown test variable by combining information from observed training data. BDT separates test and training data, making it a natural lens for recent episodic training approaches. A dense derivation leads to a stochastic variational objective for meta-learning probabilistic inference grounded in Bayesian inference and decision theory, generalizing to return a full predictive distribution over the unknown test variable. The new inference framework, based on Bayesian decision theory (BDT), generalizes to return a full predictive distribution over the unknown test variable. Amortized variational training involves amortizing q for quick predictions at test time and learning parameters by minimizing average expected loss over tasks. The optimal variational parameters are found by minimizing the expected distributional loss across tasks. The new inference framework, based on Bayesian decision theory, generalizes to return a full predictive distribution over the unknown test variable. Amortized variational training involves learning parameters by minimizing average expected loss over tasks, with the optimal variational parameters found by minimizing the expected distributional loss across tasks. Loss functions include log-loss, emphasizing the meta-learning aspect of the procedure. The log-loss is a proper scoring rule that approximates the true predictive distribution without requiring computation of the true predictive distribution. The section discusses the approximation of the true posterior distribution in the wake-sleep algorithm. It justifies the context-independent approximation using density ratio estimation and Bayes' theorem. The optimal softmax classifier constructs estimators for the conditional density for each class independently, similar to training a naive Bayes classifier. The text discusses the context-independent assumption in estimating weights for different classes in a task using free-form variational inference. An experiment is detailed to evaluate this assumption by randomly generating tasks from a dataset and examining the distribution of weights for a specific class across tasks. The experiment focuses on 5-way classification in the MNIST dataset. The experiment evaluates the assumption of context-independent class weights in 5-way classification on the MNIST dataset. The model achieves 99% accuracy on test examples, and t-SNE plots show class-weight clustering with some overlap. Tasks containing classes '1' and '2' exhibit class '2' weights located away from their cluster, suggesting class-weights are typically independent of the task. The experiment evaluates the assumption of context-independent class weights in 5-way classification on the MNIST dataset. The model achieves 99% accuracy on test examples, and t-SNE plots show class-weight clustering with some overlap. Tasks containing classes '1' and '2' exhibit class '2' weights located away from their cluster, suggesting independence of class weights. The VI-based objective for the probabilistic model involves amortized and non-amortized VI parameterization. An evidence lower bound (ELBO) is used for optimization. In this section, the few-shot classification experiments are detailed using the Omniglot dataset. The dataset consists of 1623 handwritten characters from 50 alphabets, with 20 instances each. The images are pre-processed by resizing to 28x28 pixels and augmenting character classes with 90-degree rotations. Training, validation, and test sets are split randomly into 1100, 100, and 423 characters, respectively. Training for C-way, k c -shot classification is done episodically. The study involves training, validation, and testing on a dataset with 4400 training, 400 validation, and 1292 test classes, each with 20 character instances. Training is done episodically for C-way, k c -shot classification, with tasks randomly selected from the training set. The Adam BID25 optimizer with a learning rate of 0.0001 is used, and models are trained for different iterations based on the number of classes and shots. The evaluation is performed on 600 randomly selected tasks from the test set. The 20-way -1-shot model is trained for 100,000 iterations using a Gaussian form for q with 10 \u03c8 samples. miniImageNet BID54 dataset consists of 60,000 color images divided into 100 classes with 600 instances each. Training is episodic like Omniglot, using Adam BID25 optimizer. The 5-way -5-shot model trains with 4 tasks per batch for 100,000 iterations, while the 5-way -1-shot model trains with 8 tasks per batch for 50,000 iterations. Neural network architectures for feature extractor \u03b8, amortization network \u03c6, and linear classifier \u03c8 are detailed in TAB0 to D.4. The output of the amortization network provides Gaussian parameters for the weight distributions of the linear classifier \u03c8. Sampling from the weight distributions involves the local-reparameterization trick. The feature extraction network \u03b8 is shared with the pre-processing phase of the amortization network to reduce the number of learned parameters. The network architecture includes convolutional layers with dropout and pooling for both Omniglot and miniImageNet datasets. The miniImageNet Shared Feature Extraction Network uses convolutional layers with dropout and pooling. The ShapeNetCore v2 BID5 database is used for experiments, with 12 object categories combined to create a dataset of 37,108 objects. The dataset used for training consists of 37,108 objects, with 70% for training, 10% for validation, and 20% for testing. Each object has 36 grayscale images at 32x32 pixels. Training is done episodically, with one object randomly selected for each task. The system is evaluated by generating views and computing metrics on the test set using an amortization network. The network architectures for the encoder, amortization, and generator networks are described. Training uses the Adam BID25 optimizer with a learning rate of 0.0001, 24 tasks per batch for 500,000 iterations. Parameters are set to d \u03c6 = 256, d \u03c8 = 256, and 1 sample for \u03c8. The ShapeNet Encoder Network (\u03c6) consists of convolutional and pooling layers with specific sizes and activations."
}