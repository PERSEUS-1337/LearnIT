{
    "title": "Byl5NREFDr",
    "content": "Model extraction in natural language processing involves an adversary reconstructing a victim model using only query access. The attacker can successfully extract the model without real training data by using random word sequences and task-specific heuristics. This exploit is possible due to the prevalence of transfer learning methods in NLP. Defense strategies like membership classification and API watermarking can be effective but may be circumvented by clever adversaries. Machine learning models are valuable intellectual property, often accessed through web APIs. Malicious users may attempt \"model stealing\" by training a local copy of the model using query access. Extracted models can leak sensitive information or be used to generate adversarial examples. NLP APIs based on ELMo and BERT are at risk. In this paper, the authors demonstrate that NLP models obtained by fine-tuning a pretrained BERT model can be extracted without access to the API provider's training data. Extraction attacks are possible with randomly sampled word sequences and simple heuristics, contrasting with prior work that required access to semantically-coherent data for large-scale attacks. The authors show that NLP models can be extracted without access to training data by fine-tuning a pretrained BERT model. Extraction attacks are possible with randomly sampled word sequences and simple heuristics, contrasting with prior work that required access to semantically-coherent data for large-scale attacks. The attacks are cost-effective, with the most expensive one estimated at $500. The study analyzes randomly-generated queries for model extraction, finding that despite being nonsensical, they are effective. Pretraining on the attacker's side facilitates extraction. Simple defenses like membership classification and API watermarking are ineffective against clever adversaries. The research aims to inspire stronger defenses against model extraction and a better understanding of vulnerabilities in models and datasets. Efforts on model extraction have mainly focused on computer vision applications, with studies on zero-shot distillation and rubbish inputs to NLP systems. Model extraction attacks have been explored empirically and theoretically, primarily against image classification APIs. Prior work on NLP systems attempted extraction using pool-based active learning, while this study focuses on nonsensical inputs for modern BERT-large models in tasks like question answering. This work is related to data-efficient distillation methods. The curr_chunk discusses the use of nonsensical inputs on modern BERT-large models for tasks like question answering. It mentions prior work on distillation methods and the impact of rubbish inputs on model predictions. Unnatural text inputs have been shown to produce overly confident predictions and trigger disturbing outputs from text generators. BERT, Bidirectional Encoder Representations from Transformers, is a 24-layer transformer model that converts word sequences into contextualized vector representations. Its parameters are learned through masked language modeling on unlabeled text, leading to state-of-the-art performance on various NLP tasks with minimal supervision. The model revolutionized NLP by achieving high accuracy without seeing real examples during training. The NLP system for task T utilizes fine-tuning methodology with a task-specific network and a composite function. Description of extraction attacks involves reconstructing a local copy of a victim model using a task-specific query generator. The attacker fine-tunes a public release of a model on nonsensical word sequences to obtain the extracted model. The extraction attacks involve obtaining models on diverse NLP tasks using different query generators, RANDOM and WIKI. Tasks include sentiment classification, natural language inference, extractive question answering, and boolean question answering. The input and output spaces vary for each task. The extraction attacks involve obtaining models on diverse NLP tasks using different query generators, RANDOM and WIKI. In the WIKI setting, input queries are formed from actual sentences or paragraphs from the WikiText-103 corpus. Additional task-specific heuristics are applied for tasks featuring complex interactions between different parts of the input space. Representative example queries and their outputs are provided for evaluation. In a controlled setting, extraction attacks on NLP tasks are evaluated using different query budgets. Commercial cost estimates for query budgets are provided using Google Cloud Platform's Natural Language API calculator. Two metrics are used for evaluation: accuracy of extracted models on the original development set and agreement with victim models. Extracted models show high accuracy even with nonsensical inputs, with diminishing gains at higher query budgets. The extracted models show high accuracy on original development sets, even with nonsensical inputs. However, their agreement with victim models is only slightly better than accuracy. On SQuAD, extracted models trained on WIKI and RANDOM data have low agreements despite being trained on the same data distribution. An ablation study with alternative query generation heuristics is conducted for SQuAD and MNLI. The API's argmax outputs were used in WIKI experiments for SST2, MNLI, and BoolQ, showing minimal accuracy drop. Hiding the full probability distribution is not an effective defense. Extraction algorithms are successful with varying query budgets, with accuracy gains diminishing quickly. The cost of attacks can be estimated from the results. Questions arise about the effectiveness of nonsensical input queries in model extraction and the performance without large pretrained language models. In this section, an analysis is conducted to understand the effectiveness of nonsensical input queries in model extraction without using large pretrained language models. Different victim models are tested to see if they produce the same answers when given nonsensical queries. The RANDOM and WIKI extraction configurations for SQuAD are specifically examined to determine the representativeness of these queries and the agreement between victim models. Five victim SQuAD models are trained on the original data, showing high agreement in their answers to different types of queries. The analysis examines the agreement between victim models when given nonsensical queries. Models show high agreement on SQuAD training and development sets but drop significantly on WIKI and RANDOM queries. High-agreement queries are found to be more useful for model extraction, with F1 improvements observed when using high-agreement subsets. This suggests that agreement between victim models is a good indicator of input-output pair quality for extraction. The analysis shows that high agreement between victim models on nonsensical queries is beneficial for model extraction. The study investigates if high-agreement nonsensical textual inputs have a human interpretation by comparing annotator responses on SQuAD questions. Annotators matched victim models' answers 23% of the time on WIKI and RANDOM subsets, while scoring significantly higher on original SQuAD questions. In contrast to high agreement on original SQuAD questions, annotators scored lower on WIKI and RANDOM subsets. Annotators used a word overlap heuristic to select answer spans, but many nonsensical question-answer pairs remained unclear. The study explores the impact of different pretraining setups on extraction accuracy, comparing BERT-large and BERT-base models. BERT comes in two sizes: BERT-large and BERT-base. Accuracy is higher when the attacker starts from BERT-large. Fine-tuning BERT gives attackers a headstart. QANet achieves high accuracy on SQuAD with no pretraining. BERT-large has 1.3 million randomly initialized parameters at the start of training. QANet achieves high accuracy with original SQuAD inputs using BERT-large labels. However, F1 drops significantly when training on nonsensical queries. Better pretraining allows models to start from a good language representation, simplifying extraction. BERT-based models are vulnerable to model extraction, leading to the investigation of defense strategies that preserve API utility and remain undetectable to attackers. Two defenses are explored, including membership inference, effective against weak adversaries. The defense strategy against weak adversaries involves using membership inference to detect nonsensical inputs or adversarial examples. By treating membership inference as a binary classification problem and using model confidence scores and rare word representations, classifiers are trained to identify fake examples. These classifiers transfer well to balanced development sets and remain robust to the query generation process. The defense strategy against weak adversaries involves using membership inference to detect nonsensical inputs or adversarial examples. Classifiers are trained to identify fake examples and remain robust to the query generation process. Watermarking is another defense strategy where a fraction of queries are modified to return wrong outputs, making extracted models vulnerable to detection. Watermarking is a defense strategy where a small percentage of queries are altered to return incorrect outputs, making extracted models easier to detect. Watermarked models show high accuracy in predicting the watermarked outputs but low accuracy in predicting the original labels. This technique is effective after an attack has occurred and assumes the attacker will make the extracted model publicly available with black-box query access. Model extraction attacks against NLP APIs serving BERT-based models are effective, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Further research is needed to develop robust defenses against adaptive adversaries in model extraction attacks on NLP APIs serving BERT-based models. Future directions include leveraging nonsensical inputs for model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity. Cost estimates from Google Cloud Platform's Calculator were used in the study. To estimate costs for different datasets, input instances with more than 1000 characters were counted multiple times. The costs of entity analysis and sentiment analysis APIs were extrapolated for tasks like natural language inference and reading comprehension. It is challenging to provide a universal estimate for the price of issuing queries, as some API providers offer a limited number of free queries. Attackers could potentially exploit multiple accounts to collect data in a distributed manner. Most APIs are freely available on webpages, making it easy for attackers to scrape information at scale. Additionally, API costs can vary based on the computing infrastructure used. In this section, details on the costs of extracting datasets using APIs are discussed. The costs can vary depending on factors like computing infrastructure and revenue models. For example, it costs -$430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translation queries. Input generation algorithms for datasets like SST2 involve building vocabularies from wikitext103 and randomly sampling tokens. The process involves building a vocabulary from wikitext103 by preserving the top 10000 tokens and discarding the rest. Tokens are randomly sampled from this vocabulary to create sentences. In another step, words not in the top-10000 vocabulary are replaced with random words from the same vocabulary. Additionally, words are randomly replaced in the premise and hypothesis using the same vocabulary. The final paragraph is constructed by sampling tokens from the unigram distribution of wikitext103. Questions are built by randomly sampling paragraph tokens and appending a question starter word before adding a question mark. This process is similar to how questions are generated in SQuAD and BoolQ datasets. In a study on query generation heuristics, it was found that starting questions with common question starter words like \"what\" improved performance. A comparison of extraction datasets for SQuAD 1.1 showed that RANDOM works better when paragraphs are sampled based on unigram frequency in wikitext103. An ablation study on MNLI revealed that low lexical overlap between premise and hypothesis leads to neutral or contradictory predictions, while high overlap results in entailment predictions, creating an unbalanced dataset. The study found that high overlap between premise and hypothesis leads to entailment predictions, creating an unbalanced dataset. Human annotators were asked to annotate different question sets, showing varying levels of agreement. In Table 10, inter-annotator agreement is shown with average pairwise F1 scores. The order of agreement is original SQuAD > WIKI, highest agreement > RANDOM, highest agreement \u223c WIKI, lowest agreement > RANDOM. An ablation study on input features for the membership classifier was conducted, comparing logits of the BERT classifier and last layer representations. Results in Table 9 indicate that last layer representations are more effective in distinguishing between real and fake inputs, but using both feature sets yields the best results. In an ablation study of membership classifiers, last layer representations are more effective in classifying points as real or fake compared to using both feature sets."
}