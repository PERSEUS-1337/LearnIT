{
    "title": "SJzqpj09YQ",
    "content": "Spectral Inference Networks is a framework for learning eigenfunctions of linear operators through stochastic optimization. It generalizes Slow Feature Analysis to various operators and is related to Variational Monte Carlo methods. This tool can be used for unsupervised representation learning from video or graph data. Training Spectral Inference Networks is formulated as a bilevel optimization problem, enabling online learning of multiple eigenfunctions. Results show accurate recovery of eigenfunctions and interpretable representations from video in an unsupervised manner. Spectral algorithms are essential in machine learning and scientific computing. Eigenfunctions are crucial tools in scientific applications, particularly in the study of PDEs and the behavior of classical and quantum systems. Full eigendecomposition can be achieved efficiently for reasonably-sized matrices, while iterative algorithms are used for larger matrices. In cases where the state space is too large to store in memory, eigenfunctions are approximated from a fixed number of points and Nystr\u00f6m method is used for other points. This is common in quantum physics and machine learning applications. In this paper, Spectral Inference Networks (SpIN) are proposed to approximate eigenfunctions of linear operators on high-dimensional function spaces using neural networks. The method is trained via bilevel stochastic optimization and has shown success in solving problems in quantum physics and generating interpretable representations from video data. This approach extends prior work on unsupervised learning without a generative model and has the potential to scale many applications of spectral methods. The paper outlines Spectral Inference Networks for approximating eigenfunctions using neural networks. It reviews related work on spectral learning and defines the objective function for the networks. The algorithm for training involves bilevel optimization to learn ordered eigenfunctions simultaneously. Experimental results are presented, and future directions are discussed. Supplementary materials include custom gradient updates, a TensorFlow implementation, and additional experimental results. Spectral methods have various applications in machine learning, such as spectral clustering and Laplacian eigenmaps. Spectral algorithms like Laplacian eigenmaps and manifold learning methods rely on eigendecomposition for various machine learning applications. In reinforcement learning, spectral decomposition is used for predictive state representations and planning. Proto-value functions have been proposed as a model for grid cells in the entorhinal cortex. PVFs are also utilized for discovering subgoals in reinforcement learning. Spectral learning methods like Laplacian eigenmaps and manifold learning rely on eigendecomposition for machine learning applications. Stochastic PCA, including Oja's rule, and online SVD algorithms have been used for learning fixed-size eigenvectors. Slow Feature Analysis (SFA) is related to optimizing eigenfunctions through parametric models. Spectral methods, including Laplacian eigenmaps and SFA, have been used in machine learning for feature optimization. SFA is typically applied to shallow or linear models, while SpIN allows for simultaneous learning of all eigenfunctions. Spectral networks have been developed for graph and manifold structured data, incorporating spectral decompositions in deep network architectures. Spectral decompositions have also been used with the kernelized Stein gradient estimator to improve learning in generative models like GANs. Our work focuses on using neural networks to solve large-scale spectral decompositions, specifically in the field of Variational Quantum Monte Carlo (VMC) for approximating eigenfunctions of a Hamiltonian operator. Neural networks have been utilized for calculating ground states and excited states in VMC methods, with stochastic optimization techniques dating back to at least 1997. Importance sampling is commonly used in these methods to reduce bias from finite batch sizes, but in machine learning, the data sampling distribution cannot be freely chosen, limiting the application of these techniques. In machine learning, bias from finite batch sizes is reduced using importance sampling techniques. Eigenvectors of a matrix A are defined as vectors u such that Au = \u03bbu for some scalar \u03bb. The Rayleigh quotient can be used to find the largest eigenvector of a symmetric matrix A. Algorithms like power iteration can converge to the global solution for finding eigenvectors. To compute the top N eigenvectors, a sequence of maximization problems can be solved. In machine learning, bias from finite batch sizes is reduced using importance sampling techniques. To find the top N eigenvectors of a matrix A, a sequence of maximization problems can be solved. This can be reframed as a single optimization problem, especially when dealing with large matrices and vectors that cannot be stored in memory. The problem can also be approached using a symmetric kernel instead of a matrix A. To compute the top N eigenfunctions of a linear operator, a constrained optimization problem needs to be solved over functions u : \u2126 \u2192 R N. The objective involves the covariance of features and a kernel-weighted covariance, leading to a simplified form of the optimization problem. If \u2126 is a graph, the kernel function k can further simplify the computation. The kernel k simplifies to Eq. 6, representing the graph Laplacian for neighboring points. Slow Feature Analysis (SFA) is a special case of SpIN, allowing for end-to-end online learning with arbitrary function approximators. The equivalent kernel for R^n converges to the differential Laplacian, with applications in physics. The Laplace-Beltrami operator generalizes to generic manifolds, enabling optimization solutions. In optimizing the optimization problems in Equations 6 and 7, various methods can be used such as the augmented Lagrangian method. However, directly optimizing the quotient in Eq. 6 is preferred as it is invariant to linear transformations and yields a function spanning the top N eigenfunctions of K. By masking the flow of information from the gradient of Eq. 6 correctly, all eigenfunctions can be learned simultaneously in order. The objective in Eq. 6 can be rewritten using the invariance of trace to cyclic permutation, allowing for independent optimization of each eigenfunction towards the objective \u039b ii. By masking the gradient, each eigenfunction can be optimized independently towards the objective \u039b ii. Orthogonalizing u(x) by multiplication by L \u22121 results in the true ordered eigenfunctions of K. The closed form expression for the masked gradient is provided, and it can be passed back as the error to parameters \u03b8 for optimization. A TensorFlow implementation of this gradient is available in the supplementary material. Learning Spectral Inference Networks involves dealing with biased gradient estimates due to the nonlinear nature of the problem. Reframing it as a bilevel optimization issue allows for the use of convergent algorithms. The process involves solving two coupled minimization problems simultaneously. Learning Spectral Inference Networks can be optimized as a bilevel stochastic problem, allowing for convergence to simultaneous local minima of f and g. By using a moving average in the optimization process, unbiased noisy estimates can be obtained for \u03a3 and J \u03a3. This approach involves alternately updating terms to improve the optimization process. Learning Spectral Inference Networks involves optimizing a bilevel stochastic problem to converge to simultaneous local minima of f and g. By using a moving average, unbiased noisy estimates for \u03a3 and J \u03a3 can be obtained. The approach includes alternately updating terms to enhance the optimization process. The Spectral Inference Network is defined as a machine learning algorithm that minimizes an objective by stochastic optimization, operates over deep neural networks, imposes an ordering on learned features using a modified gradient, and utilizes bilevel optimization to overcome bias from finite batch sizes. The full training algorithm for Spectral Inference Networks is provided in Alg. 1, with TensorFlow pseudocode in the supplementary material. An explicit estimate\u0134 \u03a3t of the Jacobian needs to be computed in this algorithm. The algorithm for Spectral Inference Networks involves computing an explicit estimate of the Jacobian of the covariance at each iteration. While proper learning rate schedules are important in theory, constant values of \u03b1 and \u03b2 are often used in practice for deep learning. Empirical results on quantum mechanics and unsupervised feature learning demonstrate the method's correctness. The SpIN method was tested on solving the Schr\u00f6dinger equation for a two-dimensional hydrogen atom, demonstrating the correctness of the approach. A neural network was trained to approximate the wavefunction \u03c8(x) with different energy levels E. The neural network was trained to approximate the wavefunction \u03c8(x) with different energy levels E for a two-dimensional hydrogen atom. The decay rate for RMSProp was set slower than the decay \u03b2 for the moving average of the covariance in SpIN. By using SpIN to correct biased gradients, accurate estimation of eigenfunctions and convergence to true eigenvalues were achieved. The effectiveness of SpIN was demonstrated on a problem with a known closed-form solution, leading to strong empirical support for the correctness of the method. A convolutional neural network was trained to extract features from videos using Slow Feature Analysis kernel. The model had 12 output eigenfunctions with similar decay rates as previous experiments. Training details are provided in Sec. C.2, including training curves in FIG4. The model trained with 12 output eigenfunctions, showing varying fitting times for different eigenfunctions. Analysis in FIG2 indicates positive or negative activation of features based on ball position. Most eigenfunctions encode ball positions independently, with higher eigenfunctions capturing more complex joint statistics. One eigenfunction, outlined in green in FIG2, does not directly relate to ball position but encodes other features. The eigenfunctions discovered in the model encode complex joint relationships between ball positions, with one feature capturing whether all balls are crowded in the lower right corner or one is there while the others are far away. This nonlinear feature could not be identified by a shallow model, and higher eigenfunctions may encode even more intricate relationships. The eigenfunctions do not seem to encode meaningful information about velocity due to rapid changes caused by collisions. The use of a different kernel may yield different results, and the framework allows for spectral decompositions through stochastic gradient descent without the need for the Nystr\u00f6m approximation. This approach extends unsupervised learning criteria by optimizing for slowness of features and addresses unresolved issues in physics and machine learning. The Spectral Inference Nets can be applied to more complex physical systems for which computational solutions are not yet available. The learned representations from video data show sensitivity to scene properties and can be used for tasks like object tracking and gesture recognition. The framework presented is general, with potential for exploring various kernels and architectures. The framework for Spectral Inference Nets allows for exploration of different kernels and architectures to break the symmetry between eigenfunctions and optimize functions to span top K eigenfunctions. The process involves using masked gradients and transforming functions into ordered eigenfunctions through orthogonalization and diagonalization of matrices. In practice, optimizing Eq. 6 by accumulating statistics on \u03a0 and \u03a3 to transform functions u * into w * at the end was found to be challenging due to numerical errors. The masked gradient approach was explored instead for improved numerical robustness. The Cholesky decomposition of \u03a3 and \u039b = L \u22121 \u03a0L \u2212T are discussed, with a claim that \u039b 1:n,1:n is independent of u n+1:n (x). This ensures correct optimization over a sufficiently expressive class of functions. The Cholesky decomposition of a positive-definite matrix yields a unique lower triangular matrix with a positive diagonal. The inverse of a lower triangular matrix is also lower triangular. The argument proceeds by induction, showing that the upper left block of \u039b can be written in terms of eigenvalues. Training with the masked gradient \u2207 u Tr(\u039b) ensures numerical robustness in optimization. The reverse-mode sensitivities for the matrix inverse and Cholesky decomposition are given by \u0100 = \u2212C TC C T where L is the Cholesky decomposition of \u03a3. Gradients can be computed in closed form by applying the chain rule, using matrices \u2206 k and \u03a6 k. The gradients for matrix inverse and Cholesky decomposition can be computed in closed form using matrices \u2206 k and \u03a6 k. The unmasked gradient with respect to u is expressed as a row vector, and masked gradients can be calculated using triu and diag functions. A TensorFlow implementation of the masked gradient is provided in pseudocode. The code provided includes custom operations for computing covariance and eigenvalues, with detailed gradient calculations. It is important to note that the code is not executable as is, lacking global variables and proper initialization. The moving average operation is also outlined in pseudocode for clarity. The provided code includes custom operations for computing covariance and eigenvalues, with detailed gradient calculations. It also outlines a moving average operation in pseudocode for clarity. The functions defined are for TensorFlow operations related to learning in SpIN, including creating neural network outputs and symmetric functions. To solve for the eigenfunctions with lowest eigenvalues, a neural network with 2 inputs, 4 hidden layers with 128 units, and 9 outputs was used. A softplus nonlinearity was chosen over ReLU to avoid Laplacian operator issues. RMSProp with a decay rate of 0.999 and learning rate of 1e-5 was used. Points were sampled uniformly at random during training to prevent degenerate solutions. During training, points were sampled uniformly at random from a box with a decay rate of 0.999 and learning rate of 1e-5. To prevent degenerate solutions, the network output was multiplied by a boundary condition factor. The Laplacian was approximated using a finite difference method with a small number. The neural network architecture was modified to enhance separation of eigenfunctions by introducing a block-sparse structure in each layer. The weight matrices were split into blocks for each eigenfunction, allowing shared features in lower layers and distinct features in higher layers. Training was done on 200,000 64x64 pixel frames using a network with 3 convolutional layers and 1 fully-connected layer before outputting 12 eigenfunctions. A constant first eigenfunction was added for zero-mean features. Block-sparse structure was used for weights, and training was done with RMSProp for 1,000,000 iterations. The network was trained with RMSProp using a learning rate of 1e-6 and decay of 0.999 for 1,000,000 iterations. The batch size was 24 clips of 10 frames each, and two consecutive frames were used as input for next-frame prediction. Another network was trained to compute successor features and \"eigenpurposes\" using PCA on 64k frames. The same convolutional network architecture was used with a batch size of 32 and RMSProp with a learning rate of 1e-4 for 300k iterations. Target network was updated every 10k iterations. The baseline presented here is stronger than in the original publication as the successor features were mean-centered when computing eigenpurposes. A spectral inference network was trained with the same architecture as the encoder of the successor feature network. SpIN was tested on 64k held-out frames and compared against PCA on pixels. Results are shown in FIG5, with more examples and comparisons at the end of this section. The comparison between SpIN and successor features shows that SpIN can encode features more effectively without the need for pixel reconstruction loss. SpIN's eigenfunctions clearly distinguish between top and bottom rows, indicating meaningful learning, while successor features struggle to do so for many games. The next step is to explore the usefulness of these features for exploration and learning options. The next step is to investigate the usefulness of computing eigenpurposes for exploration and learning options."
}