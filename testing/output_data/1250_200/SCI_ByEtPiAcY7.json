{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. A new method using \\textit{M-of-N} rules was proposed to map the complexity/accuracy landscape of rules describing hidden features in a Convolutional Neural Network (CNN). Results show an optimal trade-off between comprehensibility and accuracy, with rules in the first and final layers being highly explainable. The study explores the explainability of neural networks using the \\textit{M-of-N} rule. It highlights the challenges of extracting rules from deep networks and the importance of decompositional knowledge extraction for explainability in AI. The distributed representation in neural networks may not correlate with easily identifiable features of the data. Knowledge extraction aims to increase explainability by uncovering implicit knowledge learned by neural networks. Rule extraction techniques, either decompositional or pedagogical, generate symbolic rules from trained neural networks. The complexity of extracted rules is a major challenge in rule extraction. Rule extraction from neural networks faces challenges due to the complexity of the extracted rules. The distributed nature of neural networks, where important concepts are represented by patterns of activity over many neurons, makes it difficult to extract symbolic knowledge. This has led to the suggestion that methods like distillation should be used instead of symbolic extraction for improving explainability and robustness. In this paper, a method for empirically examining the explainability of latent variables in neural networks is developed. Rule extraction is used by searching through a space of rules describing a latent variable, measuring error and complexity. The relationship between rule complexity and accuracy in capturing network behavior is mapped out. Results on a 4-layer CNN trained on fashion MNIST show varying accuracy in extracted rules across layers. When applied to a 4-layer CNN trained on fashion MNIST, some layers have accurate rules while others do not, even with complex rules. A 'critical point' on the rule extraction landscape reveals an ideal M-of-N rule for each latent variable. The accuracy of rules varies depending on the variable, with explainability trends differing between layers. Convolutional layers have more complex rules compared to fully connected layers. Rules in the first and final layers have near 0% error, while those in the second and third layers have around 15% error. The paper also discusses previous algorithms for knowledge extraction and defines accuracy and complexity for M-of-N rules. One of the first attempts at knowledge extraction used a decompositional approach applied to feedforward networks, specifically the Knowledge-based Artificial Neural Networks (KBANN). This algorithm extracted symbolic rules known as M-of-N rules, where a given neuron is activated based on the activation of a set of neurons. More recent algorithms select M-of-N rules based on maximum information gain with respect to the output, using input units as concepts. These algorithms generate binary trees representing the rules, which can be reduced to IF-THEN propositional logic sentences. The extraction methods for rule extraction from neural networks can be pedagogical or eclectic, with some focusing on input/output relationships while others opt for visually oriented techniques. Most decompositional techniques have been applied to shallow networks, with a need for hierarchical rules in deep networks to explain latent features. In deep networks, explaining hidden features with decompositional techniques can lead to complex rules that are hard to understand. However, some layers may have explainable rules that can shed light on the network's behavior. This opens up the possibility of using rule extraction for modular explanation of network models and comparing extracted rules to understand latent features. In logic programming, a rule is defined as an implication A \u2190 B, where A is the head and B is the body. Rules explain neural network behavior by referring to neuron states, with literals representing binary or continuous values. Latent variables in neural networks are not easily described by single rules due to various input configurations activating neurons. M-of-N rules offer a compact representation for rule extraction in neural networks, allowing for a more general and structured explanation of neuron behavior compared to a simple lookup table. These rules soften the conjunctive constraint by requiring only a subset of variables to be true, reflecting the input/output dependencies of neurons and sharing structural similarities with neural networks. M-of-N rules provide a structured explanation of neuron behavior in neural networks, resembling weightless perceptrons. These rules have been used for knowledge extraction and are brought back into the spotlight for explainability. To extract rules from a network with continuous activation values, splitting values are chosen for each neuron based on information gain. When explaining a target neuron in a neural network, splitting values are chosen based on information gain to decrease entropy of network outputs. Input literals are then generated by maximizing information gain with respect to the target literal. Each target literal in a layer has its own set of input literals, corresponding to the same input neurons with different splits. In convolution layers, each feature map corresponds to neurons with different input patches, with only the one with maximum information gain tested for a single rule per feature map. In rule extraction from neural networks, the focus is on generating a single rule for each feature map based on maximum information gain. The metrics of concern are comprehensibility and accuracy, with accuracy defined in terms of the expected difference between rule predictions and network outputs. By using the network to compute the state of neurons and determine the truth of literals, discrepancies between rule outputs and network predictions can be measured. The discrepancy between rule predictions and network outputs is measured to assess accuracy in rule extraction from neural networks. Comprehensibility is evaluated based on the complexity of a rule, determined by the length of its body in disjunctive normal form. Complexity is normalized relative to a maximum complexity, with a logarithm taken to control for growth. The complexity of a rule is evaluated based on its length in disjunctive normal form, with a bias towards simplicity for higher accuracy. A loss function is defined with a parameter \u03b2 to balance soundness and complexity, allowing for the determination of the relationship between rule complexity and accuracy. A brute force search procedure with various values of \u03b2 reveals that for \u03b2 = 0, the rule with minimum error is chosen regardless of complexity, while for large \u03b2 values, a rule with minimum complexity is preferred. The complexity of a rule is evaluated based on its length in disjunctive normal form, with a bias towards simplicity for higher accuracy. A loss function is defined with a parameter \u03b2 to balance soundness and complexity, allowing for the determination of the relationship between rule complexity and accuracy. The search procedure involves generating splits for neurons based on weights, reordering variables, and maximizing information gain to minimize L(R) in generating rules. The search procedure involves generating splits for neurons based on weights to define M-of-N rules. The assumption is that accurate rules use literals corresponding to neurons with the strongest weights. Ordering literals by information gains reduces the search space from exponential to polynomial. The algorithm for rule extraction in neural networks involves defining an order on literals to reduce the search space. The process was implemented in Spark on IBM cloud services to handle large datasets efficiently. The accuracy of extracted rules was evaluated using examples from the training set, focusing on the network's learned behavior. Running the search in parallel allowed for mapping accuracy/complexity graphs for hidden neurons. The procedure is demonstrated on the first hidden feature of a CNN trained on the fashion MNIST dataset. The algorithm for rule extraction in neural networks involves defining an order on literals to reduce the search space. To demonstrate the procedure, we examine the extraction process for the first hidden feature in a CNN trained on the fashion MNIST dataset. 1000 random input examples are selected from the training set to compute neuron activations and predicted labels. Each neuron corresponds to a 5x5 patch of the input, with the optimal splitting value determined by information gain. Neuron 96, with an information gain of 0.015, is selected for testing. Input splits are defined based on the maximum information gain with respect to a defined variable H. The algorithm extracts rules from a CNN by defining input splits based on maximum information gain with respect to variable H. It searches for optimal M-of-N rules explaining neuron behavior, with increasing complexity penalties yielding different rules. The most complex rule is a 5-of-13 rule with 0.025 error, agreeing with the network 97.5% of the time. The algorithm extracts rules from a CNN by defining input splits based on maximum information gain with respect to variable H. It searches for optimal M-of-N rules explaining neuron behavior, with increasing complexity penalties yielding different rules. The most complex rule is a 5-of-13 rule with 0.025 error, agreeing with the network 97.5% of the time. Applying penalties to complexity results in simpler rules with higher errors. The technique is demonstrated on the DNA promoter dataset, showing an exponential relationship between complexity and error in the first layer. In the output layer, the rule 1 \u2212 of \u2212 {H 39 , H 80 } gives 100% fidelity to the network. The algorithm extracts rules from a CNN by defining input splits based on maximum information gain with respect to variable H. It searches for optimal M-of-N rules explaining neuron behavior, with increasing complexity penalties yielding different rules. The most complex rule is a 5-of-13 rule with 0.025 error, agreeing with the network 97.5% of the time. Applying penalties to complexity results in simpler rules with higher errors. The technique is demonstrated on the DNA promoter dataset, showing an exponential relationship between complexity and error in the first layer. In the output layer, the rule 1 \u2212 of \u2212 {H 39 , H 80 } gives 100% fidelity to the network. Each of the literals in our 1-of-2 rule with an M-of-N rule extracted from the input layer. The rules in question are of the form 64-of-119 for the variable H 3 9 producing a single incorrect classification, and 32-of-61 for the variable H 8 0 producing no incorrect classifications. The output of the network can be predicted by the rule DISPLAYFORM0. Errors propagate through layers when stacking rules that don't perfectly approximate each layer, exacerbated by choosing different splits for the same layer when treated as output. To replace a network with hierarchical rules, a single set of splits for each layer must be decided by selecting input splits based on information gain against all configurations of the new output layer, introducing more error. The experiments involved conducting layerwise rule extraction on a CNN trained on fashion MNIST in tensorflow. The CNN had a standard architecture with convolutional and max pooling layers followed by a fully connected layer. Rules were extracted using input splits based on information gain, aiming to provide a baseline for rule extraction algorithms. 1000 random inputs from the fashion MNIST training data were used to test the extracted rules. In the experiments, layerwise rule extraction was performed on a CNN trained on fashion MNIST in tensorflow. The final hidden fully connected layer had 1024 units with rectified linear activation functions. Testing involved selecting 1000 random inputs from the training data. Different values of \u03b2 were used to extract rules, resulting in 5 sets of rules with varying error/complexity trade-offs. A graph was created to illustrate the trade-off for rules extracted from each layer. The experiment involved layerwise rule extraction on a CNN trained on fashion MNIST in tensorflow. The Complexity/Error trade-off varied for rules extracted from each layer, with the first and final layers producing accurate rules with minimal error. The second and third layers showed a similar accuracy/complexity tradeoff, with the third layer unable to improve its minimum error with more complex rules. Despite differences in input nodes, the third layer performed as well as the second layer. The final layer provided more accurate rules with less complexity compared to the first layer. Overall, the results showed a consistent pattern in the rule extraction landscape for each layer. The study analyzed the rule extraction landscape in a CNN trained on fashion MNIST, revealing a critical point where error increases rapidly with complexity. Rule extraction algorithms typically do not consider complexity in optimization, but this study integrates rule complexity into the extraction process. Empirical evaluation of extraction algorithms is crucial for validation. The study highlights the importance of empirical evaluation of rule extraction algorithms in CNNs. It shows that while some layers may have complex rules with a 15% error rate, others have simple rules with near 0% error. This suggests that selective use of decompositional algorithms based on the layer being explained is necessary due to the black box problem in neural networks. The black box problem in neural networks presents an obstacle to their integration into society. Knowledge extraction has been challenging, with most large neural networks remaining difficult to interpret. A novel search method for M-of-N rules was applied to explain the latent features of a CNN, revealing that latent features can be described by an 'optimal' rule representing an ideal error/complexity trade-off. This trade-off varies between neurons in different layers, highlighting the need for selective use of decompositional algorithms based on the layer being explained. Rule complexity is used as a measure in the search for extracted rules in neural networks. The discrepancy in trade-off between neurons in different layers suggests that rule extraction may not adequately describe all latent variables. However, simplifying explanations without reducing accuracy indicates the usefulness of rule extraction for networks with easily understandable features. Decompositional rule extraction remains important for understanding network behavior, with potential for further research on different transfer functions, data sets, architectures, and regularization techniques."
}