{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is a key aspect of the natural world, where organisms collaborate despite individual incentives. This behavior is studied in the context of intertemporal social dilemmas (ISDs) using multi-agent reinforcement learning (MARL) and evolutionary theory. A modular architecture for deep reinforcement learning agents is introduced to support cooperation learning in a model-free way. Results in challenging environments are interpreted in the context of cultural and ecological evolution. Nature exhibits cooperation across various scales, from microscopic interactions to species-wide societies. In the context of intertemporal social dilemmas (ISDs) and multi-agent reinforcement learning (MARL), cooperation among self-interested agents is a significant topic. Evolutionary theory predicts a trade-off between collective welfare and individual utility in social dilemmas. Altruism can be favored by selection when individuals interact preferentially with other cooperators, avoiding exploitation by defectors. Various mechanisms such as kin selection, reciprocity, and group selection also play a role in promoting cooperation. Evolutionary theory predicts a trade-off between collective welfare and individual utility in social dilemmas. Self-interested reinforcement-learning agents tend to converge to defecting strategies instead of achieving the collectively optimal outcome. Different solutions have been proposed to promote cooperation among individuals in multi-agent training regimes, including opponent modeling, long-term planning, and intrinsic motivation functions. These hand-crafted approaches contrast with end-to-end model-free learning algorithms, which show better generalization abilities. The proposal suggests applying evolution to eliminate the hand-crafting of intrinsic motivation, similar to its use in optimizing hyperparameters and evolving neuroarchitectures in deep learning. The proposed system distinguishes between optimization processes unfolding over two time-scales: fast learning and slow evolution. Individual agents participate in a social dilemma with fixed intrinsic motivation, while that motivation is subject to natural selection in a population. Intrinsic motivation is modeled as an additional term in the reward of each agent. Evolutionary theory predicts that evolving individual intrinsic reward weights across a population does not lead to altruistic behavior. To address this, a \"Greenbeard\" strategy is implemented where agents choose interaction partners based on honest signals of cooperativeness, termed assortative matchmaking. This method is not a general solution for multi-agent reinforcement learning due to the lack of observable honest signals in typical models. The assortative matchmaking approach is not a general solution for multi-agent reinforcement learning due to the lack of observable honest signals. To address this limitation, a modular training scheme called shared reward network evolution is introduced, where agents have separate policy and reward network modules that evolve independently. The policy network is trained using modified rewards from the reward network on a fast timescale, while both networks evolve separately on a slow timescale. The fitness for the policy network is based on individual rewards, while the fitness for the reward network is based on the collective return of the group. This approach is inspired by ideas from multi-level selection theory. In the study of intertemporal social dilemmas within a MARL setting, the policy networks and reward networks are treated as separate modules to prevent overfitting. Different parameters were explored, including environments, reward network features, matchmaking, and reward network evolution. The conflict between individual and group-level actions characterizes these games, highlighting the intertemporal nature of the dilemmas. The intertemporal nature of social dilemmas is characterized by conflicts between individual and group-level rationality. Two dilemmas, the Cleanup game and the Harvest game, involve agents collecting apples with spawn rates affected by environmental factors. In the Cleanup game, agents must balance collecting apples with cleaning a polluted aquifer to maintain spawn rates, while in the Harvest game, apple spawn rates depend on the proximity of other apples. Agents collected rewarding apples with spawn rates affected by environmental factors. The dilemma arises between harvesting all apples quickly for short-term gain or preserving apples for a higher total yield in the long-term. The reward components in the model include total, extrinsic, and intrinsic rewards, each impacting agents' loss functions. Total reward is the sum of extrinsic and intrinsic rewards, with extrinsic reward obtained from environmental actions and intrinsic reward based on social preferences. The social preference u(f) is calculated using a neural network with evolved parameters based on fitness. The feature vector f_i is transformed into intrinsic reward by agents' reward network. Social preferences should not be influenced by the temporal alignment of rewards in Markov games. The model of social preferences in Markov games should not be influenced by the precise temporal alignment of rewards. Two ways of aggregating rewards are considered, using off-policy importance weighted actor-critic BID10. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The retrospective method derives intrinsic reward based on past rewards, while the prospective method looks at future rewards. The reward values for agents are updated at each timestep using value estimates and a stop-gradient mechanism to prevent gradient flow. Training involved a population of 50 agents with policies, sampled for 500 arenas running in parallel. Episode trajectories lasted 1000 steps and weights were updated using V-Trace. Learning parameters included learning rate, entropy cost weight, and reward network weights. Policy network parameters were inherited in a Lamarckian fashion. The objective function comprised three components: value function gradient, policy gradient, and entropy regularization, weighted by hyperparameters baseline cost and entropy cost. Evolution was based on a fitness measure calculated as a moving average of total episode return, with matches determined by random or assortative matchmaking methods. The study used two methods for matchmaking: random and assortative. Random matchmaking selected agents uniformly at random, while assortative matchmaking grouped agents based on recent cooperativeness metrics. This ensured that cooperative agents played with others like them, while defecting agents played with other defectors. Cooperative matchmaking was based on different metrics for Cleanup and Harvest games. It was not used in the multi-level selection model. The study evolved reward networks separately from policy networks, allowing for independent exploration of hyperparameters and generalization to a wide range of policies. Fitness was determined based on individual agent return for policy networks, while total episode return was used for evolving reward network parameters. This approach differed from previous work on intrinsic reward evolution. The study evolved reward networks separately from policy networks based on total episode return for fitness. This approach differs from previous work on intrinsic reward evolution, focusing on evolving social features for cooperation rather than remapping environmental events. Shared reward networks mix group fitness on a long timescale with individual reward on a short timescale, providing a biologically principled method for social settings. The study evolved reward networks separately from policy networks based on total episode return for fitness, focusing on evolving social features for cooperation. The network performs poorly on both games, asymptoting to 0 total episode reward in Cleanup and 400 for Harvest. Random matchmaking shows little benefit to adding reward networks over social features, while assortative matchmaking with individual reward networks leads to high performance. Shared reward network agents perform as well as assortative matchmaking, even with random matchmaking. Agents with the same intrinsic reward function, evolved based on collective episode return, can perform well without needing immediate access to honest signals of cooperativeness. The retrospective variant of reward network evolution outperforms the prospective variant, which relies on agents learning value estimates before reward networks become useful. Sustainability metrics show the average time step of positive rewards received by agents. The sustainability of agents is measured by the average time step of positive rewards received. Without a reward network, players collect apples quickly, but with reward networks, behavior is more sustainable. Equality is calculated using the Gini coefficient, showing that prospective reward networks lead to lower equality compared to retrospective ones. Tagging frequency is higher with prospective or individual reward networks compared to a retrospective shared reward network. The final weights of the best retrospective shared reward networks suggest different social preferences are needed to resolve each game. The final weights in the second layer of ISDs suggest different social preferences are needed to resolve each game. In Cleanup, a less complex reward network sufficed, while Harvest required a more complex one to prevent over-exploitation. The first layer weights tended to be arbitrary due to random matchmaking. Organisms develop internal drives based on primary or secondary goals, and intrinsic rewards based on features derived from other agents were examined. Intrinsic rewards based on features from other agents were examined to understand cooperation. Natural selection via genetic algorithms did not lead to cooperation, but assortative matchmaking did. A new evolutionary paradigm based on shared reward networks promotes cooperation by improving credit assignment and exposing social signals that correlate with selfishness. The shared reward network evolution model promotes cooperation by exposing social signals that correlate with selfishness. This model is inspired by multi-level selection and features lower level units constantly swapping with higher level units. Similar modularity can be seen in nature, such as microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids for cooperation. The curr_chunk discusses the spread of cultural norms and alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity. It also suggests exploring an assortative matchmaking model and combining an evolutionary approach with multi-agent communication for cooperative behaviors. The playable area sizes for Cleanup and Harvest are specified. The communication in cooperative behaviors during games like Cleanup and Harvest involves a 15x15 RGB window for observation. Actions include moving, rotating, tagging, and cleaning waste. Training involves joint optimization of network parameters and hyperparameters, with gradient updates every trajectory. Optimization is done via RMSProp with specific parameters, and the learning rates are adjusted throughout training using PBT. The baseline cost weight was fixed at 0.25, and the entropy cost evolved using PBT. Learning rates were initially set to 4 \u00d7 10 \u22124 and allowed to evolve. PBT uses genetic algorithms to search hyperparameters, resulting in joint optimization with network parameters learned through gradient descent. Evolution involved mutation rates and perturbations for hyperparameters. A burn-in period of 4 \u00d7 10 6 agent steps was implemented before evolution."
}