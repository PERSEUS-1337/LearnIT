{
    "title": "Byg-An4tPr",
    "content": "In this paper, a novel mechanism is developed to preserve differential privacy in adversarial learning for deep neural networks, ensuring provable robustness to adversarial examples. The approach leverages sequential composition theory in differential privacy to establish a connection between privacy preservation and robustness. An original differentially private adversarial objective function is designed to address the trade-off between model utility, privacy loss, and robustness, tightening the sensitivity of the model. The mechanism significantly enhances the robustness of differential privacy deep neural networks against attacks. The text discusses three solutions to prevent attacks: models preserving differential privacy, adversarial training algorithms, and provable robustness. Private models lack protection against adversarial examples, while robust models do not offer privacy protections. This one-sided approach poses risks to machine learning. The text highlights the need for machine learning models to be both private to protect training data and robust to adversarial attacks. Current approaches do not address both aspects simultaneously, leading to potential privacy risks and vulnerabilities. Developing a model that combines differential privacy preservation and provable robustness remains a significant challenge. The text introduces a novel differentially private adversarial learning mechanism to address the trade-off between model utility, privacy loss, and robustness. It aims to preserve the privacy of training data, be robust to adversarial examples, and maintain high model utility by injecting privacy-preserving noise and incorporating ensemble adversarial learning. The text introduces a new DP adversarial learning approach to tighten model sensitivity and reduce noise injection compared to previous works. It includes ensemble DP adversarial examples with dynamic perturbation sizes and a privacy analysis that shows privacy budget is not accumulated across training steps. The connection between privacy preservation, adversarial learning, and robustness is established by considering noise injection as randomizing mechanisms in different layers. Our mechanism establishes a connection between DP preservation and robustness against adversarial examples in adversarial learning. Experiments on MNIST and CIFAR-10 datasets show enhanced robustness of DP deep neural networks. The model outputs class scores that map inputs to categories. The model assigns a single true class label to input data, with class scores determining the predicted label. A loss function penalizes mismatches between predicted and original values. Differential Privacy (DP) techniques in deep learning aim to protect privacy by controlling data distribution differences. DP also applies to various metrics like l p -norms. Differential Privacy (DP) techniques in deep learning aim to protect privacy by controlling data distribution differences. DP-preserving algorithms involve introducing noise into gradients or objective functions. Adversarial Learning focuses on finding adversarial examples that are close to the original input but misclassified by the model. The goal is to achieve better sensitivity bounds compared to existing works. Adversarial learning aims to minimize risk over adversarial examples by training models with attacks to approximate solutions. Two basic adversarial example attacks include single-step and iterative algorithms. Prior work focuses on producing correct predictions on adversarial examples without compromising accuracy on legitimate inputs. Adversarial training is a key approach to improving model robustness by generating new adversarial examples during training. Some algorithms aim to achieve provable robustness by guaranteeing consistent predictions under perturbations. Several algorithms have been proposed to achieve provable robustness in machine learning models by ensuring consistent predictions under perturbations. Lecuyer et al. introduced PixelDP, an algorithm that randomizes the scoring function to enforce differential privacy on a small number of pixels in an image, guaranteeing robustness against adversarial examples. The PixelDP algorithm ensures robustness by randomizing the scoring function to enforce differential privacy on pixels. A certified robustness check is performed for each prediction, with a generalized condition proposed. The new DPAL mechanism aims to preserve differential privacy in learning private parameters, unlike PixelDP. The network architecture includes a feature representation learning model and class scores function. DPAL consists of three key components to maintain differential privacy. The DPAL model focuses on preserving differential privacy in learning private parameters. It consists of three key components: DP feature representation learning, DP adversarial learning, and provable robustness for inference. The network is trained using perturbed training examples and DP adversarial examples to ensure robustness. Verified inferring computes robustness bounds for input data. The DPAL model focuses on preserving differential privacy in learning private parameters through DP feature representation learning. An auto-encoder is used to learn DP parameters \u03b81 and ensure the output is DP. The model employs noise injection to maintain privacy while training. The DPAL model uses polynomial approximation and Functional Mechanism to inject noise into coefficients for privacy preservation during training. Laplace noise is injected into coefficients and input data to ensure privacy. The global sensitivity of the perturbed function is calculated to maintain differential privacy. By setting \u2206 R = d(\u03b2 + 2), the perturbed affine transformation h 1Bt tightens the privacy budget consumption in computing hidden layers. The computation of g(a(x, \u03b8 1 ), \u03b8 2 ) is (1/\u03b3)-DP without using additional information from the original data. The (1/\u03b3)-DP h 1Bt provides rigorous DP protection to the computation of g(\u00b7) and the output layer. The computation of the affine transformation h 1Bt is (1/\u03b3)-DP, while the batch B t as the input layer is (1/\u03b3 x)-DP. Optimizing R Bt (\u03b8 1 ) is (1/\u03b3 x + 1)-DP in learning \u03b8 1 given an (1/\u03b3 x)-DP B t batch. Adversarial learning is integrated for privacy preservation. The optimization of R Bt (\u03b8 1 ) tightens privacy budget consumption in computing hidden layers. Adversarial learning is integrated to enhance model robustness against unknown adversarial attacks. DP adversarial examples are crafted to preserve (1/\u03b3 x + 1)-DP in learning \u03b8 1. Training the auto-encoder with DP adversarial examples maintains privacy. A novel DP adversarial objective function L Bt (\u03b8 2 ) is proposed to optimize parameters \u03b8 2. The objective function L Bt (\u03b8 2 ) is defined to preserve DP in learning \u03b8 2 by protecting the true class labels y i and y j. The approach involves preserving DP in the objective function L for benign examples using the cross-entropy function. The optimization of the function L 1Bt \u03b8 2 ensures no information disclosure from the training data, requiring only 2-DP preservation in the function L 2Bt (\u03b8 2) accessing the ground-truth label y ik. The sensitivity of the objective function L 2Bt (\u03b8 2) is smaller than the state-of-the-art bound, crucial for model utility. The privacy budget for preserving DP in the adversarial objective function is (1/\u03b3 + 2). The mechanism achieves DP at the batch level B t \u222a B adv t, leveraging parallel composition and post-processing properties of DP. The mechanism achieves (1 + 1/\u03b3 x + 1/\u03b3 + 2)-DP parameters \u03b8 = {\u03b8 1, \u03b8 2} on private training data D across T training steps by leveraging parallel composition and post-processing properties of DP. The approach ensures DP across T training steps by using disjoint and fixed batches, preventing additional privacy leakage. The mechanism achieves (1 + 1/\u03b3 x + 1/\u03b3 + 2)-DP parameters \u03b8 = {\u03b8 1, \u03b8 2} on private training data D across T training steps. The correlation between the mechanism and provable robustness is established by injecting robustness noise \u03c3 r into the scoring function f (x) to derive a robustness condition against adversarial examples x+\u03b1. Privacy noise \u03c3 p is injected into the input x to avoid directly injecting noise into the coefficients h \u03c0i y ik, correlating privacy noise \u03c3 p and robustness noise \u03c3 r to derive a robustness bound. The mechanism establishes a correlation between privacy noise \u03c3 p and robustness noise \u03c3 r to derive a robustness bound against adversarial attacks. By applying group privacy size \u03ba, the scoring function satisfies r-PixelDP given \u03b1 \u2208 l p (\u03ba), achieving robustness against l p (\u03ba)-norm attacks. Additionally, perturbing the scoring function with \u03d5 results in robustness against l p (\u03d5)-norm attacks. These defensive mechanisms act independently against different types of attacks, providing provable protection during inference. The model establishes robustness against l p (\u03ba) and l p (\u03d5) attacks through independent defensive mechanisms. The general robustness bound is determined by \u03b1 \u2208 l p (\u03ba + \u03d5), leveraging sequential composition in DP theory. The composition scoring function f (M 1 , . . . , M S |x) is proven to be insensitive to small perturbations \u03b1 \u2208 l p (1), ensuring robustness. The composition of robustness is demonstrated through sequential mechanisms that are insensitive to small perturbations \u03b1 \u2208 l p (1). The expected output value of a sequential function f of independent mechanisms meets the robustness condition, ensuring robustness to adversarial examples x + \u03b1 with probability \u2265 \u03b7. Noise injections into the input x and its affine transformation h are considered as two mechanisms M x and M h for robustness. The noise injections into input x and its affine transformation h are treated as two mechanisms M x and M h for robustness. Group privacy is applied with sizes \u03ba and \u03d5, ensuring \u03ba r -DP and \u03d5 r -DP for scoring functions f x (x) and f h (x). The model is trained like typical deep neural networks, with parameters \u03b8 1 and \u03b8 2 updated independently using gradient descent. Inference time involves a verified procedure returning a robustness size guarantee for each example x. The goal is to maximize the robustness epsilon r by controlling the size of \u03ba + \u03d5. The text discusses maximizing the robustness epsilon r by controlling the size of \u03ba + \u03d5, with various hyper-parameters fixed in a well-trained model. The prediction on an example x is robust to attacks up to (\u03ba + \u03d5) max, and the failure probability can be minimized by increasing the number of invocations of the model. Hoeffding's inequality is used to bound the approximation error, and sensitivity bounds are defined for robustness. A new method for drawing independent noise is proposed, improving Monte Carlo Estimation without affecting the DP bounds and robustness. The new Monte Carlo Estimation of Ef (x) improves without affecting DP bounds and robustness. Extensive experiments were conducted on MNIST and CIFAR-10 datasets to evaluate the DPAL mechanism against state-of-the-art mechanisms in preserving differential privacy and providing robustness against adversarial examples. DP-SGD injects random noise into gradients, AdLM is a Functional Mechanism-based approach, and PixelDP offers provable robustness. SecureSGD combines PixelDP and DP-SGD for enhanced security. PixelDP and SecureSGD are advanced mechanisms that provide provable robustness using differential privacy (DP) bounds. SecureSGD combines PixelDP and DP-SGD with a heterogeneous noise distribution to improve robustness. Four white-box attacks were used in experiments, and model configurations are detailed in the appendices. Validation focuses on the interplay between accuracy metrics and robustness against attacks. The study compares the performance of different privacy-preserving mechanisms on the MNIST dataset under l \u221e (\u00b5 a )-norm attacks. The DPAL mechanism outperforms AdLM, DP-SGD, SecureSGD, and SecureSGD-AGM, showing significant improvements in accuracy. AdLM and DP-SGD algorithms exhibit the worst accuracies with no guarantee of robustness. The AdLM and DP-SGD algorithms show no guarantee of robustness and achieve the worst accuracies compared to other models. In contrast, the DPAL mechanism demonstrates better accuracy under adversarial attacks, with only a small degradation in conventional accuracy. This highlights the ability of the DPAL mechanism to offer tight differential privacy protections against adversarial examples, outperforming existing algorithms like SecureSGD and SecureSGD-AGM. Our DPAL mechanism outperforms baseline approaches in defending against adversarial example attacks, improving accuracy by 44.91% over SecureSGD, 61.13% over SecureSGD-AGM, 52.21% over AdLM, and 62.20% over DP-SGD. It remains resistant to different attack sizes, unlike other models that become defenseless. The certified accuracy is demonstrated as a function of attack size, with a privacy budget of 1.0 providing reasonable protection. Our DPAL model shows better certified accuracies under all attacks with small perturbations, outperforming other models. It maintains consistent accuracy against different attacks and sizes, with only a small drop in accuracy compared to significant drops seen in other models like PixelDP, SecureSGD, and SecureSGD-AGM. Incorporating ensemble adversarial learning into DP preservation enhances model consistency, robustness, and accuracy against different attack algorithms. DPAL outperforms baseline algorithms in terms of accuracy and certified accuracy. Results on the CIFAR-10 Dataset show DPAL outperforming baseline models in all cases, especially with small privacy budgets, providing strong privacy protections. On average, DPAL has a 10.42% improvement over SecureSGD in conventional accuracy and a 14.08% improvement in certified accuracy. The DPAL mechanism shows significant improvements over baseline algorithms in accuracy and certified accuracy, especially with small privacy budgets. Increasing the privacy budget from 2 to 10 results in a 4.74% average improvement in conventional accuracy. The model maintains consistency under different attacks with adversarial perturbations, outperforming baseline approaches. The DPAL model outperforms baseline approaches in accuracy and certified accuracy, with a smaller degradation in conventional accuracy compared to SecureSGD. It also shows better accuracies in all cases and achieves a 21.01% improvement in certified accuracy over SecureSGD. The model establishes a connection between DP preservation, adversarial learning, and provable robustness, introducing a sequential composition robustness theory and a DP-preserving mechanism to address the trade-off between model utility, privacy loss, and robustness. The original DP-preserving mechanism was designed to improve robustness under adversarial attacks, but limitations remain. These include low accuracy, scalability dependent on model structures, and the need to address threats from unseen attack algorithms. The study highlights challenges in providing DP protections in adversarial learning with complex networks like ResNet, VGG16, LSTM, and GAN. Efforts are required to explore alternative approaches for using DP adversarial examples. The study emphasizes challenges in providing differential privacy protections in adversarial learning with complex networks like ResNet, VGG16, LSTM, and GAN. Efforts are needed to explore alternative approaches for using differential privacy adversarial examples. The text discusses the sensitivity of inputs and reconstructed inputs in the context of adversarial training for differential privacy protection in complex networks like ResNet, VGG16, LSTM, and GAN. It also mentions the perturbation of inputs and the computation of global sensitivity for the function h1. The text discusses the sensitivity of inputs and reconstructed inputs in adversarial training for differential privacy protection in complex networks like ResNet, VGG16, LSTM, and GAN. It mentions perturbed inputs, sensitivity measurement, and parameter optimization for preserving differential privacy. The total privacy budget to learn perturbed optimal parameters is calculated as (1/\u03b3x + 1)-DP. The text discusses the computation of differential privacy in optimizing perturbed parameters for complex networks like ResNet, VGG16, LSTM, and GAN. It involves sensitivity measurement, parameter optimization, and the total privacy budget calculation as (1/\u03b3x + 1)-DP. The algorithm achieves differential privacy at the dataset level D by computing the first hidden layer with (1/\u03b3)-DP. This is consistent with the parallel composition property of DP, where batches are considered as disjoint datasets. The computation does not access the original data, only the perturbed batch inputs, making it (1/\u03b3x)-DP. The computation of h1Bt in each training step does not access the original data, only the perturbed batch inputs, ensuring (1/\u03b3x)-DP. The optimization of R Bt (\u03b81) is also shown to be (1/\u03b3x + 1)-DP across T training steps, maintaining privacy. The optimization of R B adv t (\u03b8 1) using DP adversarial examples is (1/\u03b3x + 1)-DP across T training steps, ensuring privacy. The computation of h1B adv t and R B adv t (\u03b8 1) are also (1/\u03b3)-DP and 1-DP, respectively. The left summation component in Eq. 26 has the same form as R Bt (\u03b8 1) in Eq. 7. By applying Proof 3 in Theorem 1 and Result 4, we can show that the optimization of R D adv (\u03b8 1) is (1/\u03b3x + 1)-DP for DP adversarial examples crafted from data D across T training steps. The Algorithm 1 preserves (1/\u03b3 + 2)-DP in optimizing the adversarial objective function L Bt\u222aB adv t (\u03b8 2) (Theorem 3). The optimization of the adversarial objective function in Algorithm 1 ensures (1 + 1/\u03b3 + 2)-DP across training steps using disjoint batches. The privacy guarantee is maintained by working on perturbed inputs and coefficients, preserving DP in learning private parameters. Algorithm 1 ensures (1 + 1/\u03b3 + 2)-DP in learning private parameters across training steps. Theorem 4, Lemma 5, and Theorem 5 hold, guaranteeing privacy with group privacy and Monte Carlo estimation. When the privacy protection is strong, a large distribution shift occurs between training and inference due to independent draws of Laplace noise. The model requires additional noise to make accurate predictions, leading to degraded inference accuracy with small privacy budgets. Increasing the number of noise invocations per prediction is impractical, so a novel method of drawing independent noise is proposed to address this issue. The proposed method involves drawing independent noise following a specific distribution for an affine transformation, ensuring both differential privacy and provable robustness in the training process. Experimental evaluations were conducted on the MNIST and CIFAR-10 datasets, showing improved performance without affecting privacy or robustness bounds. The dataset consists of 50,000 training samples and 10,000 test samples. Experiments were done on a single NVIDIA GTX TITAN X GPU with 3,072 CUDA cores. Models for MNIST and CIFAR-10 datasets have 2 and 3 convolutional layers. Each feature map in the convolution layer computes DP independently. Hidden neurons reconstruct a unit patch of input units, with d being the size of the unit patch and \u03b2 as the number of hidden neurons. MNIST model has two convolutional layers with 32 and 64 features, a fully-connected layer with 256 units, and a batch size of 2,499. The experimental setup involved using different adversarial attacks and data augmentation techniques on convolutional neural networks with varying architectures for the MNIST and CIFAR-10 datasets. The models had multiple convolutional layers with different numbers of features, fully-connected layers, and batch sizes. Privacy budgets were controlled using specific ratios, and learning rates were adjusted accordingly. The experiments focused on achieving computational efficiency and scalability in the context of privacy-preserving machine learning. Our mechanism ensures computational efficiency and scalability in privacy-preserving machine learning. It does not require extra computational resources compared to existing algorithms and can efficiently approximate robustness bounds. With tightened global sensitivities, it can handle deep neural networks of varying sizes without injecting noise or redrawing noise in each training step. The mechanism is versatile and has the potential to be applied in larger networks with larger datasets. Our mechanism has the potential to be applied in larger deep neural networks using larger datasets. Further study is needed to investigate this property. The error of our polynomial approximation approaches can be computed using known error bound results. The error of polynomial approximation approaches in deep neural networks can be computed using known error bound results. The error incurred by truncating the Taylor series approximate function depends on the maximum and minimum values of certain functions. This is consistent with previous studies. The error of polynomial approximation in deep neural networks can be computed using known error bound results. By using 2nd-order Taylor series with K categories, the functions F1j(zj) = xij log(1 + e\u2212zj) and F2j(zj) = (1 \u2212 xij) log(1 + ezj) can be proved to hold."
}