{
    "title": "SygMXE2vAE",
    "content": "BERT, a Transformer-based model, has achieved state-of-the-art results in various Natural Language Processing tasks. A layer-wise analysis of BERT's hidden states reveals valuable information beyond attention weights. By focusing on models fine-tuned for Question Answering, researchers examine how token vectors are transformed to find answers. Probing tasks show the information stored in each representation layer, with visualizations offering insights into BERT's reasoning process. The system can implicitly incorporate task-specific information into its token representations. Transformations within BERT go through phases related to traditional pipeline tasks, allowing implicit incorporation of task-specific information into token representations. Fine-tuning has minimal impact on semantic abilities, and prediction errors can be identified in vector representations of early layers. Transformer models have gained popularity in Natural Language Processing, especially with large pre-trained models. The main subject of this paper is BERT BID8, a popular Transformer model that has shown significant improvements in various benchmarks and tasks. The problem of black box models in deep learning is highlighted, with a focus on the lack of transparency and reliability. Transformers are considered moderately interpretable despite being black box models. This paper explores the lack of transparency and reliability in black box models like Transformers. It proposes a new approach to interpreting Transformer Networks by examining hidden states between encoder layers. The study aims to answer questions about how Transformers decompose questions, if different layers solve different tasks, the impact of fine-tuning on network states, and how evaluating network layers can explain prediction failures. The research focuses on fine-tuned models for Question Answering tasks, showcasing the complexity of solving various Natural Language Processing tasks. The paper explores the lack of transparency in black box models like Transformers and proposes a new approach to interpreting Transformer Networks by examining hidden states between encoder layers. It focuses on fine-tuned models for Question Answering tasks and showcases the complexity of solving various Natural Language Processing tasks. The analysis includes layer-wise visualization of token representations, general NLP Probing Tasks, Question Type Classification, Supporting Fact Extraction, and shows that BERT's transformations go through similar phases even when fine-tuned on different tasks. Transformations in BERT and other Transformer models encode general language properties in earlier layers and are used to solve downstream tasks in later layers. The paper also mentions other Transformer models like GPT-2, Universal Transformer, and TransformerXL, which aim to improve the flaws of the Transformer architecture. Interpretability and probing of neural models have become a significant area of research. Interpretability and probing of neural models, particularly focusing on probing tasks and methodologies applied to trained models like ELMo, BERT, and GPT-1. Recent advances include a novel \"edge-probing\" framework and analysis of BERT in a Ranking task. Attention values in different layers are probed to measure performance. The authors analyze BERT in a Ranking task, probing attention values in different layers and measuring performance for representations from different BERT layers. Previous work includes qualitative visual analysis of models, exploring phoneme recognition in DNNs, and studying word vectors' importance in sequence tagging and classification tasks. Liu et al. also performed a layer-wise analysis of BERT's token representations but focused solely on probing pre-trained models and disregarded fine-tuned models on downstream tasks. The authors focus on analyzing fine-tuned BERT models by examining token vectors qualitatively and evaluating their language abilities on QA-related tasks. They propose revisiting hidden states and token representations for explainability and interpretability, contrasting with previous work that only probed pre-trained models. The architecture of BERT and Transformer networks allows for analyzing token transformations in each layer. Hidden states are collected from each layer for both correctly and falsely predicted samples, removing padding to track token representations. Distances between token vectors indicate semantic relations, visualized through dimensionality reduction techniques like t-SNE and PCA. BERT's pre-trained models use 1024 (large) and 512 (base) vector dimensions. After analyzing token transformations in each layer of BERT and Transformer networks, dimensionality reduction techniques like t-SNE and PCA are applied to fit vectors into two-dimensional space. PCA reveals distinct clusters, which are verified using k-means clustering. Semantic probing tasks are then used to understand the information stored in transformed tokens at each layer. The goal is to determine if specific layers are reserved for certain tasks and how language information is maintained or forgotten by the model. In Beijing, China, from November 3rd-7th, 2019, Edge Probing is used to translate NLP tasks into classification tasks. Tasks like Named Entity Labeling, Coreference Resolution, and Relation Classification are adopted. Question Type Classification and Supporting Fact are also added. Named Entity Labeling involves predicting entity categories based on token spans. Coreference Resolution predicts if two mentions refer to the same entity. The Coreference task involves predicting if two mentions refer to the same entity. Relation Classification requires predicting the relation type connecting two entities. Question Type Classification identifies the question type, and Supporting Facts extraction is crucial for Question Answering tasks. BERT's token transformations are examined to understand how they distinguish important context parts from distracting ones in Question Answering tasks. A probing task is constructed to identify Supporting Facts, where the model predicts if a sentence contains relevant information for a specific question. Different probing tasks are created for datasets like HotpotQA and bAbI to test their ability to recognize relevant parts. SQuAD considers the sentence with the answer phrase as the Supporting Fact. All samples are labeled sentencewise as supporting facts or irrelevant. The text discusses a probing setup using BERT models to identify relevant parts in samples labeled as supporting facts. The Edge Probing concept focuses on tokens of \"labeled edges\" within a sample for classification. The process involves pooling tokens for representation and using a Multi-layer Perceptron classifier to predict label-wise probability scores. This analysis is done on pretrained BERT-base and BERT-large models to understand the model's abilities during pre-training or fine-tuning for complex downstream tasks like Question Answering. The text discusses using BERT models for Question Answering tasks, specifically analyzing three different datasets: SQUAD, bAbI, and HotpotQA. It mentions detention as a common punishment in various countries and describes the HotpotQA task involving 112,000 question-answer pairs designed to combine information from multiple sources. The HotpotQA task involves 112,000 question-answer pairs designed to combine information from multiple sources. The context includes supporting and distracting facts with an average size of 900 words, which are reduced by a factor of 2.7 for the pre-trained BERT model. The bAbI tasks are artificial toy tasks for understanding neural models, with 20 tasks requiring reasoning over multiple sentences. The tasks differ in simplicity and artificial nature compared to other QA tasks. The analysis is based on BERT BID8 and GPT-2 BID28 models. The analysis is based on BERT BID8 and GPT-2 BID28 models, which are Transformers that extend and improve on previous models. The models used for experiments are bert-base-uncased, bert-large, and GPT-2 small (117M Parameters). These models are fine-tuned on datasets with hyperparameters tuned through grid search. The models used for experiments are fine-tuned on datasets with hyperparameters tuned through grid search. The input length chosen is 384 tokens for bAbI and SQuAD tasks, and 512 tokens for HotpotQA tasks. Different tasks are evaluated, including span prediction and sequence classification for bAbI, and HotpotQA Support Only task. The HotpotQA tasks include Support Only (SP) and Distractor tasks, with the latter being more challenging. Evaluation results show that BERT performs well on SQuAD but struggles with HotpotQA tasks. GPT-2 excels in bAbI tasks but lags behind in SQuAD and HotpotQA. GPT-2 performs significantly better on bAbi tasks, reducing validation error to nearly 0. Most of BERT's errors in bAbI tasks come from tasks 17 and 19, which require positional or geometric reasoning. Qualitative analysis of vector transformations reveals recurring patterns in SQuAD and bAbI datasets. Results from probing tasks comparing BERT-base and BERT-large models are displayed, showing different phases in answering questions. In the middle layers of BERT-based models, entities are connected by their relation within a certain input context, forming task-specific clusters. These clusters are less connected by topical similarity and show a shift towards connecting entities with mentions and attributes. In BERT-based models, task-specific clusters of entities are formed in the middle layers, connected by their relation within a certain input context. These clusters include question-relevant entities and help in solving specific questions. The model's ability to recognize entities, identify mentions, and find relations improves in higher network layers. The ability of BERT models to identify named entities, coreferences, and relations improves in higher network layers. The models match question tokens with relevant context tokens to aid in question answering and information retrieval. This process is observed in both BERT-base and BERT-large models. The BERT models show improved ability to distinguish relevant information in higher layers, as seen in probing tasks. Performance increases over successive layers for tasks like SQuAD and bAbI, but the fine-tuned HotpotQA model does not reach high accuracy. The model struggles with identifying correct Supporting Facts, affecting its performance on certain datasets. In the last network layers, the model separates correct answer tokens from others, forming task-specific clusters for better transparency and decision retracing. The remaining tokens form homogeneous clusters, with vector representation being task-specific and learned during fine-tuning. Performance drop in NLP probing tasks is evident, especially in last-layer representations of large BERT models fine-tuned on HotpotQA. The phases of answering questions can be likened to human reasoning, involving semantic clustering, building relations between context parts, separating important from irrelevant information, and grouping answer candidates. BERT and GPT-2 models differ in their approach to processing input, with BERT able to run multiple processes concurrently while GPT-2 focuses on the first token of a sequence. This attention to the first token creates a separation of clusters in GPT-2's hidden states, affecting performance in NLP probing tasks. The issue is present in all layers except for the Embedding Layer, the first Transformer block, and the last one. As a result, the first token is masked during dimensionality reduction for further analysis. GPT-2, like BERT, separates Supporting Facts and questions in the vector space. It also extracts similar sentences, not necessarily Supporting Facts. Unlike BERT, the correct answer is not distinctly separated in GPT-2. This suggests that findings in GPT-2 extend beyond BERT and other Transformer networks. Future work will involve more probing tasks to confirm these observations. Visualizations can show failure states and the difficulty of tasks based on hidden state representations. The hidden state representations reveal insights into the network's predictions. For correct answers, transformations follow discussed phases, while for wrong answers, early layers can show reasons for the error. Low network confidence results in less meaningful transformations, often leading to incorrect predictions. The impact of fine-tuning on NLP abilities is minimal as the pretrained model already contains sufficient information. Fine-tuning only makes small weight changes and forces the model to forget some information to fit specific tasks. However, the model retains much of its previously learned encoding, indicating the success of Transfer Learning. Positional embedding is crucial for Transformer network performance, addressing the lack of sequential information compared to RNNs. The positional embedding in Transformers is crucial for maintaining effects into late layers, as shown in visualizations on the SQuAD dataset. Fine-tuning on SQuAD improves the model's ability to resolve question types, while the bAbI task fine-tuning diminishes this ability due to static structure. Surprisingly, fine-tuning on HotpotQA does not outperform the base model. The model fine-tuned on HotpotQA does not outperform the base model. Both models can solve the task in earlier layers, suggesting pre-trained recognition of question types in BERT-large. Token vectors reveal interpretable information in Transformer models, aiding in identifying misclassified examples and model weaknesses. Lower layers may be more suitable for certain problems, impacting Transfer Learning tasks. Future work can explore methods to process this information further. Transfer Learning suggests choosing layer depth based on the specific task. Further research on skip connections in Transformer layers could benefit information transfer between non-adjacent layers. The modularity of Transformer networks indicates that different layers solve different problems, allowing for task-specific training. Understanding state-of-the-art models and their problem-solving approaches can lead to improvements in downstream tasks."
}