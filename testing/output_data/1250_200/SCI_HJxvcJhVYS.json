{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A new method using Bayesian optimization and Stein variational gradient descent approximates these distributions, showing promise in likelihood-free inference for reinforcement learning environments. The approach aims to estimate parameters of a physical system from observed data by learning approximations to the posterior distribution. The forward model of the system is approximated by a computational model that generates data based on parameter settings. Likelihood-free methods like approximate Bayesian computation and conditional density estimation are used when the likelihood function is not available. Recent methods address efficiency challenges by constructing conditional density estimators or sequentially learning approximations to the likelihood function. Gutmann and Corander derive an active learning approach using Bayesian optimization to propose parameters for simulations, reducing the number of simulator runs. The paper proposes an approach that combines variational inference methods with Bayesian optimization to efficiently estimate a posterior distribution over simulator parameters. It utilizes a Thompson sampling strategy to refine variational approximations and proposes parameters for new simulations using Stein variational gradient descent over samples from a Gaussian process. The approach also includes a method to optimally subsample variational approximations for batch evaluations of simulator models. The paper introduces a Bayesian optimization approach to approximate a posterior distribution over simulator parameters without access to a likelihood function. It utilizes a black-box method with a Gaussian process model, Thompson sampling, and Stein variational gradient descent to optimize the distribution. The paper proposes using Stein variational gradient descent (SVGD) to learn the distribution directly, bypassing the need for the original connection with \u0398. A Gaussian process (GP) is used to model the synthetic likelihood function, allowing for the application of SVGD in the Bayesian optimization loop. Candidate distributions are selected using Thompson sampling, a GP posterior sampling approach. The paper suggests using Thompson sampling to select candidate distributions based on a Gaussian process (GP) posterior sampling approach. Thompson sampling accounts for uncertainty by sampling functions from the GP posterior, particularly for models like sparse spectrum Gaussian processes (SSGPs). The acquisition function is defined based on an approximation to the target posterior using SVGD, which represents the variational distribution as a set of particles. The particles are initialised as i.i.d. samples from the prior p(\u03b8) and optimised via smooth perturbations using the SSGP kernel. The gradients of logp n are available for SSGP models with differentiable mean functions. Selecting a distribution q n involves running evaluations of \u2206 \u03b8 from samples \u03b8 \u223c q n to update the GP model. Algorithm 1 outlines the DBO process, which involves maximising the acquisition function via SVGD and sampling simulator parameters. The algorithm involves selecting query parameters for the simulator using kernel herding to minimize error in empirical estimates under a given distribution. The procedure utilizes the GP posterior kernel to select informative samples for the model. The GP posterior kernel encodes information for selecting informative samples for the model. The DBO algorithm is summarized in Algorithm 1 and evaluated in synthetic data scenarios, comparing it against MDNs. Experimental results are presented for the cart-pole environment in OpenAI Gym, using a uniform prior with specific bounds for the environment. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations to the posterior compared to the MDN approach. Results indicated that distributional Bayesian optimization is more sample-efficient for inferring parameters in reinforcement learning environments. Future work includes scalability and theoretical analysis of the method. Open-source implementation and code are available online. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations to the posterior compared to the MDN approach. Results indicated that distributional Bayesian optimization is more sample-efficient for inferring parameters in reinforcement learning environments. Future work includes scalability and theoretical analysis of the method. The code is available online for further exploration. The method allows for fast incremental updates of the GP posterior with time complexity O(M^2)."
}