{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. A novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes, improving optimization robustness. Our approach offers an alternative surrogate to the original lattice measure, enhancing optimization robustness in the disjoint case while maintaining key properties. Demonstrated improvements in WordNet hypernymy prediction, Flickr caption entailment, and MovieLens market basket dataset, especially in sparse data scenarios. Embedding methods, crucial in machine learning, convert semantic problems into geometric ones, with a resurgence seen post-Word2Vec publication. Recent interest lies in structured or geometric representations over traditional point-based ones. In contrast to traditional point-based representations, recent interest lies in structured or geometric representations. These methods associate objects with complex geometric structures like density functions, convex cones, or axis-aligned hyperrectangles. The focus is on the probabilistic Box Lattice model for modeling transitive relations and complex joint probability distributions. Box embeddings generalize order embeddings by replacing vector lattice ordering with overlapping boxes. The probabilistic order embeddings (POE) BID9 replace vector lattice ordering with overlapping boxes. However, the \"hard edges\" of boxes can cause issues for gradient-based optimization, especially with sparse data. To address this, a new model is proposed that relaxes the hard edges of boxes into smoothed density functions using Gaussian convolution. The new model proposes smoothing the hard edges of boxes into density functions using Gaussian convolution, showing superior results in modeling transitive relations on various datasets. It extends existing order embeddings models by incorporating probabilistic and deterministic elements. The interpretation discusses probabilistic and deterministic models for learning hierarchical embeddings, including methods based on embedding points in hyperbolic space. The focus is on smoothing the energy landscape of the model using Gaussian convolution, which is common in mollified optimization and continuation methods. Our focus is on embedding orderings and transitive relations in knowledge graph embedding. We aim to learn a probabilistic model that maps concepts to subsets of event space, tailored for transitive relations and fuzzy concepts of inclusion and entailment. We introduce methods for representing ontologies as geometric objects using order theory and vector and box lattices. Posets are discussed as a formalism for acyclic directed graph data in knowledge bases. A lattice is a poset where subsets have unique upper and lower bounds. In a bounded lattice, there are additional elements denoting the least upper bound and greatest lower bound. Lattices have join and meet operations denoted by \u2228 and \u2227. Bounded lattices satisfy specific properties and can be dual lattices. Special cases include the extended real numbers and sets partially ordered by inclusion. In a lattice, subsets have unique upper and lower bounds with join and meet operations denoted by \u2228 and \u2227. The dual lattice can be obtained by swapping \u2227 and \u2228 operations and reversing the poset relation. A vector lattice, or Riesz space, is a vector space with a lattice structure. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order. The Order Embedding vector lattice representation embeds objects as vectors in a positive dual lattice. Vilnis et al. introduced a box lattice for knowledge graphs, where concepts are associated with minimum and maximum vectors forming hyperrectangles. The lattice structure includes least upper bounds and greatest lower bounds, with operations denoted by \u2228 and \u2227. The lattice meet is the largest box contained within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are given by the volume of boxes under a suitable probability measure. Using gradient-based optimization to learn box embeddings can lead to issues when two concepts are incorrectly seen as disjoint. The model faces issues when two concepts are mistakenly considered disjoint, hindering gradient flow. A surrogate function is proposed to address this problem, but a more principled framework is suggested to develop alternate measures. The approach involves optimizing a smoothing kernel to avoid the pathology of disjoint intervals, improving optimization and model quality. The approach involves optimizing a smoothing kernel to avoid gradient sparsity issues caused by hard edges in standard box embeddings. This relaxation maintains desirable properties while enabling better optimization and preserving geometric intuition. The joint probability between intervals is rewritten using indicator functions and replaced with functions of infinite support through kernel smoothing, specifically convolution with a normalized Gaussian kernel. Kernel smoothing, specifically convolution with a normalized Gaussian kernel, is used to replace indicator functions with functions of infinite support. This approach enables better optimization and preserves geometric intuition. The integral solution is given by a closed form solution involving the standard normal CDF and the softplus function. In the zero-temperature limit, the formula is recovered with equality in the last line due to intervals. However, multiplication of Gaussian-smoothed indicators does not give a valid meet operation on a function lattice for any \u03c1 > 0. A modification of equation 3 can retain smooth optimization properties while satisfying idempotency requirements. In the zero-temperature limit, a function p is obtained such that p(x \u2227 x) = p(x), maintaining smooth optimization properties of the Gaussian model. This leads to defining probabilities p(x) and p(x, y) using a normalized version of equation 7 instead of equation 3, satisfying idempotency requirements. In the interval case, a normalized version of equation 7 replaces equation 3 to satisfy idempotency requirements. Softplus upper-bounds the hinge function, allowing values greater than 1, requiring normalization. Two approaches are used: unconstrained learning for small entities and projection onto the unit hypercube for infeasible computations. The final probability p(x) is the product over dimensions. This function is not a valid probability measure on the entire joint space of events, similar to the Gaussian model. Our approach retains the inductive bias of the original box model, equivalent in the limit, and satisfies the necessary condition that p(x, x) = p(x). A comparison of different functions is shown in FIG2, with the softplus overlap performing better for highly disjoint boxes than the Gaussian model. Experiments on the WordNet hypernym prediction task were conducted to evaluate these improvements in practice. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, requiring less hyper-parameter tuning. Further experiments were conducted using different numbers of positive and negative examples from the WordNet mammal subset to confirm the performance in the sparse regime. The training data contains 1,176 positive examples, while the dev and test sets contain 209 positive examples. Negative examples are generated randomly. The experiments conducted on the WordNet mammal subset and Flickr entailment dataset show that the Smoothed Box model outperforms OE and Box models in all settings, especially on imbalanced data. The F1 scores of the models are compared for different levels of label imbalance, with the Smoothed Box model showing superior performance. The dataset used for the experiments is the same as in previous studies, with constraints on the boxes to the unit cube and the application of the softplus function before calculating the volume of the boxes. The experimental setup for the Smoothed Box model includes constraints on the boxes to the unit cube and the use of the softplus function before calculating box volume. The model is evaluated on various datasets, showing improved performance, particularly on unseen captions. In a market-basket task using the MovieLens dataset, the model predicts users' movie preferences based on their ratings. The dataset is pruned to include movies with significant user ratings, resulting in 8545 movies. The model's performance is compared with baselines like low-rank matrix factorization and complex bilinear factorization. The study evaluates the Smoothed Box model's performance on a dataset of 8545 movies. Various baselines are compared, and the results show that the smoothed box embedding method outperforms other methods, especially in Spearman correlation. Additional analysis on the model's robustness is presented, along with a theoretical justification for the smoothing approach. The study introduced a method to smooth the energy landscape of probabilistic box embeddings, showing improved performance on various datasets. The model is easier to train with fewer hyperparameters and is robust to sparse data and poor initialization. Further research is needed to address learning challenges posed by complex embedding structures. The Gaussian kernel is normalized to have total integral equal to 1, preserving the overall areas of the boxes. The antiderivative of \u03c6 is the normal CDF, allowing for evaluation of the difference \u03a6(x; a, \u03c3 2 ) \u2212 \u03a6(x; b, \u03c3 2 ). Fubini's theorem is applied to evaluate equation 8, leading to the rough distribution of probabilities in the MovieLens dataset. Additional experiments test the robustness of the smoothed box model to initialization. The models robustness to disjoint boxes is tested by adjusting the width parameter to control the percentage of disjoint boxes at initialization. Results show the smoothed model performs well in the disjoint regime, while the original box model degrades significantly. Methodology and hyperparameter selection methods for each experiment are briefly discussed. The experimental setup involves training models with hyperparameters determined on the development set. Negative examples are randomly generated based on a ratio for each batch of positive examples. A parameter sweep is done for all models to select the best result for each model. The architecture used is a single-layer LSTM that reads captions and produces a box embedding parameterized by min and delta, with embeddings produced by feedforward networks. The model is trained for a large number of epochs and tested on the development set. More details and code for reproducing experiments can be found at the provided GitHub link. The model is trained for a large fixed number of epochs and tested on the development data at each epoch. Hyperparameters are determined on the development set, and the best development model is used to report the test set score. Optimization is stopped if the best development set score fails to improve after 200 steps."
}