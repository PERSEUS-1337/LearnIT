{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive and outperforms previous baselines in molecular optimization tasks by improving biochemical properties of compounds through graph translation. The model is trained to translate an input molecular graph into a better form, facing challenges due to the vast space of candidates and complex molecular properties. Prior work utilized valid chemical substructures to generate graphs but had limitations in separate tree and graph encoding. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation involves a non-autoregressive attachment prediction process. The encoder represents molecules at different resolutions, while the decoder predicts substructure components and their attachments in a sequence of triplet predictions. This approach allows for modeling strong dependencies between successive attachments and substructure choices. The encoding of molecules involves three levels, with each layer capturing essential information for decoding. The decoding process is efficient, decomposing each generation step into smaller steps to avoid combinatorial explosion. Conditional translation is supported by feeding desired criteria as input. Interleaving tree and graph decoding steps solves the issue of generating invalid junction trees. An autoregressive decoder is proposed to address inconsistent local substructure attachments predicted during training. The autoregressive graph decoder proposed addresses inconsistent local substructure attachments during training. It outperforms previous methods in discovering molecules with desired properties, showing significant improvements on optimization tasks. The model runs faster during decoding and benefits from hierarchical decoding and multi-resolution encoding. Conditional translation can succeed even with limited training data on molecular pairs. Various approaches have been used for generating molecular graphs, including methods that generate molecules based on SMILES strings, output adjacency matrices and node labels at once, decode molecules sequentially node by node, and adopt node-by-node approaches in reinforcement learning. Our work is closely related to generating molecules based on substructures, using a two-stage procedure to realize graphs. Our method involves a two-stage procedure for realizing graphs. The first step generates a junction tree with substructures as nodes, capturing their coarse relative arrangements. The second step resolves the full graph by specifying how the substructures should be attached to each other. Our approach differs from existing methods by jointly predicting substructures and their attachments with an autoregressive decoder. Our method represents molecules as hierarchical graphs, utilizing graph coarsening algorithms to construct multiple layers of graph hierarchy. Unlike existing methods that seek to represent graphs as a single vector, our focus is on graph generation, encoding molecules into multiple sets of vectors for regression or classification tasks. Our method focuses on graph generation by encoding molecules into multiple sets of vectors at different resolutions. The graph translation task aims to map a molecule X to another molecule G with improved chemical properties using an encoder-decoder with neural attention. The decoder adds substructures in each generation step and predicts attachment points in the new substructure and the current graph. To support hierarchical generation, a hierarchical graph representation of a molecule X is proposed with three components. Our method focuses on graph generation by encoding molecules into multiple sets of vectors at different resolutions. The hierarchical graph representation of a molecule X consists of three components: substructure layer, attachment layer, and atom layer. Nodes in the graph are encoded into substructure vectors, attachment vectors, and atom vectors for decoding. The decoder adds substructures and predicts attachment points using attention mechanisms. Substructures are defined as subgraphs induced by atoms and bonds in the molecule, covering the entire molecular graph with rings and bonds considered. The paper discusses the generation of molecules using a graph decoder that incrementally expands a substructure tree in a depth-first order. The substructure tree is constructed by connecting substructures that share common atoms and ensuring it is tree-structured. The decoder predicts new substructures as rings and their attachment to the graph. The method focuses on encoding molecules into substructure, attachment, and atom layers for graph generation. The decoder predicts new substructures as rings and their attachment to the graph using a MLP with attention over substructure vectors. It first predicts whether a new substructure will be attached to the current node, then creates the substructure and predicts its type, and finally decides how it should be attached to the parent node. The decoder predicts new substructures as rings and their attachment to the graph using a MLP with attention over substructure vectors. It first predicts the atoms that will be attached to the substructure, then finds the corresponding atoms in the parent node based on the predicted attaching points. The predictions are autoregressive and dependent on each other, with teacher forcing used during training to determine the generation order. During training, teacher forcing is applied to the generation process of predicting substructures, with a depth-first traversal over the ground truth substructure tree. The attachment enumeration is manageable due to small substructure sizes, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The encoder represents a molecule as a hierarchical graph with atom and attachment layers, facilitating the decoding process. The attachment configuration of substructure S i in the vocabulary A(S i) provides necessary information for attachment prediction. The substructure layer in the hierarchical graph H X for molecule X offers essential information for substructure prediction in the decoding process. Edges connect atoms and substructures between layers to propagate information. The hierarchical graph H X for molecule X is encoded by a hierarchical message passing network (MPN) with three layers. The atom layer MPN encodes atom representations, while the attachment layer MPN encodes attachment information between nodes in the substructure layer. The hierarchical encoder uses message passing to compute substructure representations for molecules at multiple resolutions. During decoding, the same hierarchical MPN architecture is used to encode the hierarchical graph at each step, ensuring prediction depends on previously generated outputs. The training set contains molecular pairs where each compound X can have multiple outputs Y. A variational translation model is used to generate diverse outputs by sampling a latent vector z. The model is trained using variational inference, sampling z from a posterior distribution. The latent code z is passed to the decoder to reconstruct the output Y. The model extends to handle conditional translation by incorporating desired criteria as input during the translation process. This allows users to specify criteria to control the outcome, such as making the output drug-like and bioactive. The translation model is evaluated on single-property optimization tasks following a similar experimental design by Jin et al. (2019). The model is evaluated on single-property optimization tasks following Jin et al. (2019). A novel conditional optimization task is constructed where desired criteria are inputted. Molecular similarity between input X and output Y must be above a threshold at test time. Four different tasks are included, such as LogP Optimization, where the model translates input X into output Y with logP(Y) > logP(X). The model is evaluated on a conditional optimization task where desired criteria are inputted, and molecular similarity between input X and output Y must be above a threshold. Two similarity thresholds are experimented with, and the evaluation metrics include translation accuracy and diversity. The model aims to improve drug-likeness and activity properties in the translation process. The HierG2G method selects the best translation of compound X i as compound Y i based on property improvement and similarity constraints. Translation success rate and diversity are measured, comparing against baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. The study introduces a generative model for molecule decoding and optimization, along with an atom-based translation model for comparison. The model achieves state-of-the-art results in translation accuracy and output diversity on four tasks, outperforming existing methods like JTNN. The decoder is autoregressive, enhancing molecule generation capabilities. Our model outperforms AtomG2G on three datasets, showing the advantage of our hierarchical model in terms of translation accuracy and output diversity. Training our model on a specific criteria results in a low success rate, but our conditional translation setup can transfer knowledge from other pairs with different criteria. The study conducted ablation experiments to analyze the impact of different architecture choices on translation accuracy. Results showed that modifying the decoder's input to include both atom and substructure vectors led to a decrease in performance. Additionally, reducing the number of hierarchies in the encoder and decoder resulted in a slight drop in accuracy, with significant degradation when the attachment layer was removed. This highlights the importance of structure-based decoding, especially in tasks like DRD2 where specific functional groups play a crucial role in biological target binding. The study conducted ablation experiments to analyze the impact of different architecture choices on translation accuracy. Results showed that removing the attachment layer significantly degraded performance as substructure information was lost. The model needs to infer substructures and their layers for each molecule. The LSTM MPN architecture was used in the developed hierarchical graph-to-graph translation model, outperforming previous models. The attention layer in the LSTM MPN architecture for hierarchical graph-to-graph translation uses a bilinear attention function. AtomG2G is an atom-based translation method directly comparable to HierG2G, representing molecules as molecular graphs. Training set sizes and substructure vocabulary sizes for each dataset are provided. The training set and substructure vocabulary sizes for each dataset are listed in Table 3. The multi-property optimization is created by combining the training sets of QED and DRD2 tasks. The test set consists of 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters are set for HierG2G and AtomG2G models, with specific dimensions and regularization weights. CG-VAE is also used for molecule generation and property prediction. For CG-VAE experiments, three models were trained for logP, QED, and DRD2 tasks. Compound translation involved gradient ascent over latent representations to maximize property scores. A low KL regularization weight (\u03bb KL = 0.005) was crucial for meaningful results. Ablation studies included modifying the decoder to AtomG2G's atom-based decoder. In experiments with CG-VAE, models were trained for logP, QED, and DRD2 tasks using gradient ascent over latent representations. A low KL regularization weight was important. Modifications were made to the decoder to AtomG2G's atom-based decoder, including adjustments to the encoder and decoder MPN hierarchies. Topological and substructure predictions were made based on hidden vectors in different layer configurations."
}