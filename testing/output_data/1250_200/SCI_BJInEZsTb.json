{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for studying representation learning and generative modeling of geometric data represented as point clouds. The learned representations outperform state-of-the-art methods in 3D recognition tasks and enable shape editing applications. Different generative models, including GANs and Gaussian mixture models, are also studied, with GMMs trained in the latent space of the autoencoder producing samples of the best fidelity and diversity. The text discusses the challenges in designing generative models for 3D representations of objects. Various encoding methods like view-based projections and volumetric grids are effective but lack semantics. Editing and designing new objects with these representations require complex parametric models. Recent advances in deep learning have made it possible to eliminate the need for hand-crafting features and models in domains with abundant data. Deep learning architectures like autoencoders and Generative Adversarial Networks have been successful in learning complex data representations. Point clouds, a relatively unexplored 3D modality, offer a compact representation of surface geometry that is easily amenable to geometric operations, making them attractive for learning purposes. Point clouds are a compact representation of surface geometry, commonly used in range-scanning devices like the Kinect and iPhone. Deep architectures for 3D point clouds are limited, with PointNet BID17 being successful in classification and segmentation tasks. Generative models for point clouds have gained attention in the deep learning community, but training them is challenging and there is no standard evaluation method. Generative models for point clouds lack a standardized evaluation method. New AE architecture is designed for learning compact representations with good fidelity and coverage. The models can generate point clouds similar to training data and provide good dataset coverage without the need for joint learning and training. The proposed workflow involves training an AE with a compact bottleneck layer to learn a representation, followed by training a plain GAN in the fixed latent representation. This approach, supported by theory and empirical evidence, shows that latent GANs are easier to train and achieve superior reconstruction with better coverage. Multi-class GANs perform well when trained in the latent space, and various metrics are evaluated for learning good representations and evaluating generated samples. The curr_chunk discusses the evaluation of generated samples using fidelity and coverage metrics for generative models. It also introduces models for latent representations and point cloud generation. The paper outlines the necessary background on autoencoders and their low-dimensional representations. The curr_chunk discusses the use of bottleneck layers in autoencoders to create low-dimensional representations for datasets. It also introduces Generative Adversarial Networks (GANs) and their architecture, including the adversarial game between a generator and discriminator. The improved Wasserstein GAN is mentioned for improved stability during training, specifically for point cloud geometry challenges. The improved Wasserstein GAN is used for stability in training, especially for challenges specific to point cloud geometry. Point clouds present unique difficulties due to their lack of grid-like structure, making encoding more complex than images or voxel grids. PointNet bypasses this issue by avoiding 2D convolutions, and the unordered nature of point clouds requires permutation invariance for comparisons and defining loss functions. Two metrics for comparing unordered point sets have been proposed in the literature. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature: Earth Mover's distance (EMD) and Chamfer distance (CD). EMD transforms one set to the other through a transportation problem, while CD measures the squared distance between each point in one set to its nearest neighbor in the other set. These metrics are used to evaluate the quality of representation models in terms of matching training or test sets. To evaluate the quality of a representation model, metrics like Coverage, Jensen-Shannon Divergence, and Minimum Matching Distance are used to compare point-cloud distributions. Coverage measures how well a point-cloud distribution matches a ground truth distribution, while MMD captures the fidelity of the representation model. Jensen-Shannon Divergence is also used to assess the similarity between distributions. The Jensen-Shannon Divergence (JSD) measures the distances in pairwise matchings of point clouds, assessing the similarity between distributions. Architectures for representation and generative models for point clouds are described, including an autoencoder design and a GAN tailored to point-cloud data. The input to the AE network is a point cloud with 2048 points representing a 3D shape. The AE network for 3D shape representation uses Gaussian Mixtures with 2048 points input. The encoder employs 1-D convolutional layers to encode points independently, producing a k-dimensional vector for the latent space. The decoder transforms the latent vector to generate a 2048 \u00d7 3 output. Two AE models, AE-EMD and AE-CD, are explored using EMD-distance and Chamfer-Distance as structural losses. The architecture parameters for the latent-space were determined by constructing 8 AEs with bottleneck sizes ranging from 4 to 512. After training with point-clouds of a single object class, it was found that a bottleneck size of 128 had the best generalization error on test data. The raw point cloud GAN operates directly on the 2048 \u00d7 3 point set input, with a discriminator architecture identical to the AE. The latent-space GAN operates differently by not directly processing the raw point cloud input. The l-GAN uses a pre-trained autoencoder on the input data, operating on a 128-dimensional bottleneck variable. The architecture is simpler compared to the r-GAN, with shallow designs for the generator and discriminator. Gaussian Mixture Models are also trained on the latent spaces learned by the AEs, with varying numbers of components and covariance matrices. The GMMs can be turned into point-cloud generators by sampling the latent-space from the GMM distribution and using the AE's decoder. Shapes are reconstructed from the test split of input data using class-specific AEs trained on ShapeNet repository. Evaluation of unsupervised representation learning algorithms is done by applying them as feature extractors on supervised datasets. In this experiment, the Autoencoder (AE) was trained on 57,000 models from 55 categories of man-made objects. A larger bottleneck of 512 was used, along with batch-norm in the decoder. Features for a 3D shape were extracted using a 512-dimensional bottleneck layer vector. A linear classification SVM was trained on ModelNet BID32 for classification. The 512-dimensional feature was compared to a 7168-long feature from a previous state-of-the-art model. The decoupling of latent representation from generation allows flexibility in choosing the AE loss. On ModelNet10, EMD and CD losses performed equally, while CD performed better with increased variation within the collection. The EMD and CD losses perform equivalently on primarily larger objects with fewer categories than ModelNet40. However, CD produces better results when variation within the collection increases, possibly due to its more local and less smooth nature. The experiment also demonstrates the domain-robustness of learned features. Qualitative evaluation shows the ability of the learned representation to generalize to unseen shapes, enabling shape editing applications like interpolations and part editing. The study showcases the generalization ability of Autoencoders (AEs) in reconstructing unseen shapes, with results displayed in FIG0 and quantitative measurements in TAB5. Five generative models were trained on chair point-cloud data, including AE-CD and AE-EMD with a 128-dimensional bottleneck. Various models were trained in the latent spaces of AE-CD and AE-EMD, including an l-GAN and GMMs, as well as an r-GAN directly on point cloud data. The study trained five generative models on chair point-cloud data, including an l-GAN with Wasserstein objective and GMMs. Model selection was based on synthetic results matching the ground-truth distribution using JSD or MMD-CD metrics. GMMs performed better with full covariance matrices. The study trained five generative models on chair point-cloud data, including GMMs with full covariance matrices. Model selection was based on synthetic results matching the ground-truth distribution using JSD or MMD-CD metrics. The optimal number of Gaussian components for GMMs was 32 or 40, depending on the selection criterion used. Evaluation showed that the models were able to generate synthetic samples resembling the train and test splits of the ground truth distribution. The study evaluated five generative models on chair point-cloud data, with GMMs achieving the best results in fidelity and coverage. Training a simple Gaussian mixture model in the latent space of the AE yielded close-to-reconstruction results. The study compared generative models on chair point-cloud data, with GMMs showing the best fidelity and coverage. GMMs are easy to train and achieve results close to the reconstruction baseline. The generalization ability of the models was also demonstrated through training vs. testing splits. Synthetic datasets were generated for experiments, with the test datasets being three times bigger than the ground truth dataset to reduce sampling bias. The study compared generative models on chair point-cloud data, with GMMs showing the best fidelity and coverage. Synthetic datasets were generated for experiments, with the test datasets being larger than the ground truth dataset to reduce sampling bias. MMD-CD distance to the test set appears small for r-GANs, but qualitative inspection shows otherwise. Chamfer distance may not distinguish pathological cases well, as shown in image triplets comparing r-GAN and l-GAN results. The study compared generative models on chair point-cloud data, with GMMs showing the best fidelity and coverage. Synthetic datasets were generated for experiments, with the test datasets being larger than the ground truth dataset to reduce sampling bias. MMD-CD distance to the test set appears small for r-GANs, but qualitative inspection shows otherwise. Chamfer distance may not distinguish pathological cases well, as shown in image triplets comparing r-GAN and l-GAN results. The distances between nearest neighbors in r-GAN and l-GAN sets are reported using CD and EMD metrics, with EMD correlating more strongly to visual quality and heavily penalizing r-GAN results. Training trends were extensively measured to understand model performance. The study compared generative models on chair point-cloud data, with GMMs showing the best fidelity and coverage. Training trends were extensively measured to understand model performance. r-GAN struggles to provide good coverage of the test set, while l-GAN performs better in terms of fidelity with fewer epochs. The CD distance favors r-GAN results due to high-density areas in the synthesized point sets. Switching to an EMD-based AE for representation in GAN architecture improves coverage and fidelity. Mode collapse is a known issue in l-GANs, but switching to latent WGAN helps eliminate this problem. The study compares GANs on point-cloud data to voxel-based methods, showing promising results in terms of JSD on the training set of the chair category. The study compares GANs on point-cloud data to voxel-based methods, showing promising results in terms of JSD on the training set of the chair category. Categories can be found in the appendix (Table 10). The r-GAN outperforms BID31 in diversity and realistic results, while l-GANs perform even better in classification and diversity with less training epochs. Training time for l-GAN is significantly smaller than r-GAN due to its smaller architecture. Synthetic results produced by l-GANs and 32-component GMM are shown in Fig. 5. The study compares GANs on point-cloud data, showing promising results in terms of JSD on the training set of the chair category. Synthetic point clouds generated by l-GAN and 32-component GMM trained on AE-EMD latent space are of high quality. The l-GAN produces crisper results than the r-GAN, demonstrating an advantage of using a good structural loss on the decoupled, pre-trained AE. The study involved training and testing datasets for an autoencoder (AE) with 5 categories (chair, airplane, car, table, sofa). The multi-class AE was trained for 1000 epochs and compared against class-specific AEs. l-WGANs based on the multi-class AE performed similarly to dedicated class-specific ones. Visual quality was not sacrificed using the multi-class AE-EMD. MMD-CD measurements for l-WGANs trained on the latent spaces of dedicated and multi-class EMD-AEs were compared. The study compared l-WGANs trained on dedicated and multi-class EMD-AEs. Limitations included failure cases in decoding rare geometries and missing high-frequency details. The r-GAN struggled with creating realistic shapes for some classes, suggesting the need for more robust models. Training Gaussian mixture models in the latent space of an autoencoder is related to VAEs, with issues of over-regularization documented. In the study, it was found that fixing the autoencoder before training generative models yielded good results for 3D point-cloud representation learning and generation. The results showed good generalization to unseen data and meaningful semantics encoding. The best-performing generative model was a Gaussian Mixture Model trained in the fixed latent space of an autoencoder, suggesting that simple classic tools should not be overlooked. Further investigation is needed to understand the conditions under which simple latent GMMs are as powerful as adversarially trained models. The study found that fixing the autoencoder before training generative models led to good results for 3D point-cloud representation learning. The best-performing model was a Gaussian Mixture Model trained in the fixed latent space of an autoencoder. Different setups for autoencoders did not show significant advantages over the \"vanilla\" architecture. The study found that different setups for autoencoders did not show significant advantages over the \"vanilla\" architecture. The discriminator and generator architectures for the r-GAN model were detailed, including the number of layers, neurons, and training parameters used. The generator and discriminator architectures for the r-GAN model were described, with details on the number of layers, neurons, and training parameters used. The training parameters for the l-Wasserstein-GAN included a gradient penalty regularizer and specific iterations for the critic and generator. Classification experiments utilized a linear SVM classifier with specific optimization parameters listed in Table 5. The reconstruction quality of two AEs (CD and EMD-based) is compared in terms of JSD with ground truth datasets. AEs show generalization ability across training and test datasets. The AE-EMD embedding trained on 55 object classes can encode features for different shapes. Shape editing using vector arithmetic on AE latent space is demonstrated. Shape annotations are used to modify shapes, such as tuning car appearance, adding armrests to chairs, and removing handles from mugs. Using latent representations, shapes can be modified by changing properties such as adding armrests to chairs. Interpolating between different shapes is achieved by linearly interpolating between latent representations and decoding the result to obtain intermediate variants. This process creates a sequence with the two shapes at its endpoints. The latent representation allows for morphing between shapes, enabling variations between two shapes. It also supports shape analogies by finding similar shapes through linear manipulations and nearest-neighbor searching in the latent space. This process is demonstrated with images derived from meshes to aid visualization. In the geometry processing community, analogies have been a recent focus. Preliminary results show point-cloud generators working with voxel-based AEs. A full-GMM model with 32 centers was used for generation on ShapeNet's chair class. Results indicate that latent AE-based GMM models outperform voxel-based GANs significantly. For more details, refer to Table 7. The latent AE-based GMM models outperform raw GAN architecture significantly, showing an advantage of using latent representations for generation in the voxel modality. The performance of 64^3 voxel-based GMM is comparable to 32^3 resolution, indicating that high-frequency details in ground-truth data do not significantly affect fidelity. Point-cloud-based models outperform voxel-based models in fidelity, as measured by MMD. The coverage boost of voxel-based latent-space models compared to MMD is likely due to the coverage metric computation method. The voxel-based models produce shapes with missing components, leading to poor quality matchings with ground truth models. The histogram in FIG0 illustrates the distances between GMM-generated samples and their closest matches in the ground truth. The voxel-based method shows a heavier \"tail\" indicating poor quality matchings. The coverage from voxel-based output mostly comes from very poor quality partial shapes. The volumetric models use GMMs with full covariances and 32 centers, along with 64 or 256-dimensional latent codes. The mesh conversion is done using the marching cubes algorithm with an iso-surface value of 0.5. The voxel-based AEs are fully-convolutional with specific layer parameters listed. Each AE is trained for 100 epochs with Adam under binary cross-entropy loss. The learning rate, \u03b21, and batch size are specified. The voxel AE architectures are validated based on reconstruction quality. The voxel AE architectures were validated by comparing their reconstruction quality to the state-of-the-art method BID28 for the ShapeNetCars dataset. The comparison was based on the intersection-over-union measurement between input and synthesized voxel grids. Additionally, a GMM-generator was evaluated against a model that memorizes the training data of the chair class, showing slightly lower coverage/fidelity when memorizing the training set. The generative models show good fidelity in memorizing the training set, with a slight drop in coverage. Using a learned representation allows for compact data representation and generating novel shapes. Mode collapse is present, but the fidelity is excellent. Further comparisons with BID32 and ShapeNet classes are provided in Tables 10, 11, and 12. In Table 12, MMD/Coverage comparisons on the test split are provided for generative models, including GMM models trained on the latent space of an AE with EMD structural loss. Generalization error of various GAN models is shown in Figure 17, with measurements using JSD and MMD-CD metrics. GMM model selection is discussed in Figure 17, highlighting the impact of covariance type on JSD. The text discusses the evaluation of generative models, including GMM models trained on the latent space of an AE with EMD structural loss. The models are compared using metrics such as JSD and MMD-CD on synthetic datasets. The impact of covariance type on JSD is highlighted in the evaluation process."
}