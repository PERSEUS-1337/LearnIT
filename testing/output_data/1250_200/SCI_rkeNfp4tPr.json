{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is popular for training deep neural networks as it helps escape saddle points faster and find a second order stationary point more quickly. Theoretical justification for its use has been a significant open question. SGD with stochastic momentum improves deep network training by escaping saddle points faster and finding second order stationary points quickly. The ideal momentum parameter should be close to 1, as supported by theoretical and empirical findings. This algorithm is widely used in nonconvex optimization and deep learning, with applications in computer vision, speech recognition, natural language processing, and reinforcement learning. SGD with stochastic momentum is widely used in optimization and deep learning, with popular variants like Adam and AMSGrad incorporating momentum. Despite the lack of clear empirical justification, large momentum values (e.g. \u03b2 = 0.9) are observed to work well in practice. Theoretical analysis for setting momentum parameters is provided in this paper. SGD with stochastic momentum, including heavy ball momentum, accelerates escape from saddle points in optimization. The method involves updating iterates using stochastic gradients and a momentum term. The analysis focuses on finding second-order stationary points in non-convex optimization problems. In nonconvex optimization, the goal is to reach an approximate second-order stationary point with additional assumptions like Lipschitzness in gradients and Hessians. The use of momentum is shown to benefit in reaching these points, with a required condition ensuring updates correlate with negative curvature directions of the function. The dynamic procedure in Algorithm 2 ensures updates correlate with negative curvature directions of the function. Stochastic momentum under Correlated Negative Curvature (CNC) helps escape saddle points faster. With properties like Almost Positively Aligned with Gradient (APAG) and Gradient Alignment or Curvature Exploitation (GrACE), SGD with momentum reaches second-order stationary points in a certain number of iterations. Our theoretical result shows that a larger momentum parameter can help escape saddle points faster in optimization and deep learning. This sheds light on why SGD with momentum enables faster training. Stochastic momentum with properties like Almost Positively Aligned with Gradient and Gradient Alignment or Curvature Exploitation reaches second-order stationary points in a certain number of iterations. The challenge in optimization arises when the gradient updates drift slowly away from saddle points, hindering escape. While direct computation of negative eigenvectors is costly, relying on gradient methods is more practical. Studies on non-momentum SGD and stochastic momentum highlight the importance of update directions being strongly non-orthogonal to the direction of large negative curvature for efficient escape from saddle points. In this paper, the study focuses on stochastic momentum and the importance of the update direction being non-orthogonal to the direction of large negative curvature for efficient escape from saddle points. Momentum can accelerate the escape process by a factor of 1 - \u03b2, where \u03b2 is the momentum parameter. The study shows that momentum can speed up saddle-point escape by a factor of 1 - \u03b2, but \u03b2 has constraints. Empirical evidence demonstrates the benefits of stochastic momentum in escaping saddle points in two optimization tasks. In optimization tasks, all algorithms use the same initialization and step size. Convergence is measured in relative distance to the true model, showing acceleration with larger choices of \u03b2. Phase retrieval problem involves finding an unknown vector with limited samples. Empirical findings indicate faster convergence with stochastic momentum in escaping saddle points. The heavy ball method, proposed by Polyak in 1964, shows no speedup over standard gradient descent except in specific cases like convex quadratic objectives. Specialized algorithms aim at reaching a second-order stationary point, classified into two categories: specialized algorithms and simple GD/SGD variants. Specialized algorithms are designed to exploit negative curvature explicitly and escape saddle points faster. Our work falls into this category, with pioneer works by Ge et al. (2015) and Jin et al. (2017). Phase retrieval is nonconvex with the strict saddle property, where every local minimizer is global up to phase and each saddle exhibits negative curvature. Daneshmand et al. (2018) assume the Correlated Negative Curvature (CNC) for stochastic gradient to escape saddle points. Our work assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of gradient, comparing results with related works in Appendix A. We assume gradient \u2207f is L-Lipschitz and Hessian \u22072f is \u03c1-Lipschitz, ensuring convergence. Stochastic gradient has bounded noise and momentum norm is limited. Analysis relies on three key properties of stochastic momentum dynamics. Definition 2 states that SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) and Almost Positively Correlated with Gradient (APCG). Definition 3 introduces Gradient Alignment or Curvature Exploitation (GrACE) for SGD with momentum. APAG requires the momentum term to not be significantly misaligned with the gradient, ensuring progress in the algorithm. APCG is a related concept. In our analysis, a large gradient ensures algorithm progress; APCG requires momentum to be positively correlated with the gradient in the Mahalanobis norm induced by M t. The PSD matrix M t measures local curvature. Empirical evidence supports this property on natural problems. APCG is crucial in saddle regions with significant iterations. Gradient values are reported when \u2265 0.02, mostly nonnegative. Gs,t values are plotted in saddle points, with Mt mostly nonnegative. In experiments, SGD with momentum shows APAG and APCG properties. For phase retrieval, expected values may be nonnegative. GrACE measures alignment between momentum and gradient, and curvature exploitation. Small sum of terms allows bounding. The analysis follows a similar template to previous works and is structured into three cases based on the gradient and eigenvalues of the Hessian. The algorithm analyzed is similar to the previous one but with a larger step size. The algorithm analyzed in the current text chunk shows that with specific properties satisfied, it reaches a second order stationary point with high probability. The analysis is structured into three cases based on gradient and eigenvalues of the Hessian, with a focus on momentum analysis. The algorithm reaches a second order stationary point with high probability by satisfying specific properties. Higher momentum enables faster escape from saddle points, with constraints on the momentum parameter \u03b2 to avoid being too close to 1. The analysis focuses on reaching a second order stationary point efficiently. The text demonstrates escaping saddle points faster with momentum, showing that Algorithm 2 is better than CNC-SGD in the high momentum regime. It analyzes the process of escaping saddle points by SGD with momentum, showing that it takes at most T thred iterations to escape the region with a decrease in function value. The text discusses proving by contradiction that the function value must decrease by at least F thred in T thred iterations on expectation. It also highlights the impact of momentum on escaping saddle points faster and provides upper and lower bounds for the expected distance. The text discusses obtaining the lower bound of the expected distance in T thred iterations using the recursive dynamics of SGD with momentum. It introduces Lemmas 2 and 3 to show the critical component for ensuring the lower bound is larger than the upper bound. The proof involves analyzing the Hessian matrix and eigenvectors. The text discusses properties that guarantee SGD with momentum reaches a second-order stationary point faster by using a higher momentum parameter. It highlights the importance of a greater momentum in escaping strict saddle points faster. The conditions ensuring these properties are not clearly defined, making it an interesting area for further research. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. Recent research has focused on analyzing this method for non-quadratic functions, showing that the expected gradient norm converges at a rate of O(1/ \u221a t). This rate is not superior to that of the standard gradient descent. The heavy ball method, proposed by Polyak in 1964, does not offer a speedup in convergence over standard gradient descent. Recent research has analyzed its performance for non-quadratic functions, showing an expected gradient norm convergence rate of O(1/ \u221a t), which is not better than standard gradient descent. Additionally, there are works proposing stochastic accelerated algorithms with first-order stationary point guarantees, but they do not capture the stochastic heavy ball momentum used in practice. Kidambi et al. (2018) demonstrated that for specific problems, SGD with heavy ball momentum fails to achieve the best convergence rate compared to other algorithms. There are efforts to reach a second-order stationary point, categorized into specialized algorithms and simple GD/SGD variants. Specialized algorithms aim to exploit negative curvature explicitly to escape saddle points faster. The heavy ball method, proposed by Polyak in 1964, does not offer a speedup in convergence over standard gradient descent. Recent research has analyzed its performance for non-quadratic functions, showing an expected gradient norm convergence rate of O(1/ \u221a t). Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates, assuming an inherent property of stochastic gradients to escape saddle points. The algorithm can help escape saddle points faster compared to standard SGD under certain conditions. The analysis focuses on the effectiveness of stochastic heavy ball momentum and its advantage in practice. The heavy ball method, proposed by Polyak in 1964, does not offer a speedup in convergence over standard gradient descent. Recent research has analyzed its performance for non-quadratic functions, showing an expected gradient norm convergence rate of O(1/ \u221a t). Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates, assuming an inherent property of stochastic gradients to escape saddle points. The algorithm can help escape saddle points faster compared to standard SGD under certain conditions. In (Jin et al. (2019)), the advantage of stochastic heavy ball momentum is studied, building on the work of (Daneshmand et al. (2018)). Lemmas 6, 7, and 8 discuss the properties of SGD with momentum under certain conditions. Lemma 7 states that if SGD with momentum has the APAG property and the step size \u03b7 satisfies \u03b7 \u2264 2 8Lc 2 m, then the update step w t+1 = w t \u2212 \u03b7m t satisfies a certain inequality. Lemma 8 discusses the GrACE property of SGD with momentum, showing that the update rule w t+1 = w t \u2212 \u03b7m t satisfies a certain condition. The proof involves the \u03c1-Lipschitzness of the Hessian and an upper bound on the conditional expectation. To proceed, we need to upper bound E t0 [4\u03b7 where \u03be t 2 \u2264 \u03c3 2 and max t \u03b1 t \u2264 1 1\u2212\u03b2. By using \u03c1-Lipschitzness of Hessian, we have \u03b7 \u2207f (w t0+s\u22121 ), g t0+s\u22121 \u2264 f (w t0+s\u22121 ) \u2212 f (w t0+s ) + \u03b7 \u2207f (w t0+s\u22121 ), g t0+s\u22121. Lemma 2 defines a quadratic approximation at w t0, and the update step is given by w t0+t \u2212 w t0 = q v,t\u22121 + \u03b7q m,t\u22121 + \u03b7q q,t\u22121 + \u03b7q w,t\u22121 + \u03b7q \u03be,t\u22121. Lemma 5 states that if SGD with momentum has the APCG property, then certain inequalities hold. Constraints on parameter \u03b2 are also discussed, ensuring it is not too close to 1. The constraints on parameter \u03b2 ensure it is not too close to 1, upper-bounding its value. The dependence on L, \u03c3, and c is artificial and can be adjusted. Lemmas with parameter choices are used to prove Lemma 5, involving upper bounding E t0 [ q q,t\u22121 ]. The text discusses upper bounds derived from assumptions and inequalities related to Lipschitz gradients and Hessian functions. It also analyzes the bounds on certain terms using specific notations and parameter choices. The constraints on parameters are adjusted to ensure certain conditions are met. The text presents lower bounds derived from specific conditions and parameter choices related to Lipschitz gradients and Hessian functions. It discusses the bounds on various terms using specific notations and assumptions. The constraints on parameters are adjusted to meet certain conditions. The matrix B is symmetric positive semidefinite, and each G j can be written in the form of G j = U D j U. If SGD with momentum has the APCG property, then the function value must decrease at least F thred in T thred iterations on expectation. This leads to a contradiction, proving the function's behavior. The strategy involves showing that the lower bound is larger than the upper bound, leading to a contradiction and concluding that the function value must decrease by at least F thred in T thred iterations on expectation. This is achieved by proving certain inequalities and conditions, ultimately guaranteeing the desired result. The event Set T = 2T thred f (w 0 ) \u2212 min w f (w) /(\u03b4\u2206). We return w uniformly randomly from w 0 , w T thred , w 2T thred , . . . , w kT thred , . . . , w KT thred , where K := T /T thred. With probability at least 1 \u2212 \u03b4, we choose a w k where \u03a5 k did not occur. If SGD with momentum has certain properties, it reaches a second order stationary point in T iterations with high probability. The proof is available in Appendix G. With high probability, SGD with momentum reaches a second order stationary point in T iterations. The proof is based on Lemma 15, where conditions need to be satisfied for the theorem to hold. The proof is based on Lemma 15, where conditions need to be satisfied for the theorem to hold. By setting parameters according to Table 3, Algorithm 2 can find a second order stationary point faster than previous methods."
}