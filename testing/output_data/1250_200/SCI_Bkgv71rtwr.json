{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where the source and target domains may not share the same categories. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), a novel approach that incorporates category-agnostic clusters in the target domain to improve domain adaptation. By utilizing clustering information from unlabeled target samples, SE-CC enhances the generalization of Self-Ensembling for both closed-set and open-set scenarios. SE-CC is a novel approach for domain adaptation that utilizes category-agnostic clusters in the target domain to improve generalization. It enhances the learnt representation with mutual information maximization and achieves superior results in both open-set and closed-set scenarios compared to state-of-the-art approaches. Unsupervised domain adaptation aims to generalize a target model by leveraging labeled source samples and unlabeled target samples. Existing models struggle with open-set scenarios where unknown classes need to be distinguished from known ones. This limitation hinders the generalization of models in distinguishing target samples effectively. Unsupervised domain adaptation struggles with distinguishing unknown target samples from known ones. One approach is to use a binary classifier to assign known/unknown labels to target samples. However, this method may be suboptimal when target sample distribution is diverse or semantic labels are ambiguous. A novel approach involves clustering all unlabeled target samples to explicitly model the semantics of both known and unknown classes in the target domain. This clustering approach decomposes target samples into clusters that convey discriminative knowledge of unknown and known classes. The text discusses a new approach called Self-Ensembling with Category-agnostic Clusters (SE-CC) to address issues in domain adaptation. It involves decomposing target samples into category-agnostic clusters to refine representations for known and unknown classes in the target domain. The SE-CC framework integrates a clustering branch into the student model of Self-Ensembling to predict cluster assignments for target samples. KL-divergence is used to minimize the mismatch between estimated and inherent cluster distributions, preserving data structure. Mutual information is maximized to enhance feature representation. The framework is jointly optimized for unsupervised domain adaptation, addressing domain discrepancy through Maximum Mean Discrepancy. The curr_chunk discusses the use of Maximum Mean Discrepancy (MMD) in CNNs for learning domain invariant representation. Different approaches such as integrating MMD into CNNs and incorporating a residual transfer module have been explored. Unsupervised domain adaptation methods like domain confusion loss and domain discriminator are also mentioned. Open-set domain adaptation extends traditional domain adaptation to handle scenarios with new and unknown classes in the target domain. In the context of domain adaptation, various methods have been explored to tackle the realistic open-set scenario, where new and unknown classes are present in the target domain. Different approaches include exploiting known/unknown class assignments, adversarial training for feature separation, factorizing data into shared and private subspaces, and utilizing clustering to exploit the underlying data structure of the target domain. SE-CC utilizes self-ensembling loss to align classification predictions between teacher and student. Clustering is used to decompose unlabeled target samples into category-agnostic clusters, enhancing feature representation by aligning cluster assignment distribution with original clusters. Mutual information maximization among feature map, classification, and cluster assignment distributions further improves the student's feature representation. SE-CC leverages category-agnostic clusters for representation learning in open-set domain adaptation. It aligns sample distributions within known and unknown classes, discriminates between them, and enhances representation by maximizing mutual information among input features, clusters, and class probabilities. This approach integrates category-agnostic clusters into domain adaptation for both closed-set and open-set scenarios. In this paper, the SE-CC model integrates category-agnostic clusters into domain adaptation for closed-set and open-set scenarios. It aims to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown samples. The method of Self-Ensembling, based on Mean Teacher, is briefly explained as a foundation for this approach. The Self-Ensembling method, inspired by Mean Teacher, involves a student and teacher model with the same architecture. It aims to ensure consistent classification predictions between the two models under small perturbations of input images. The self-ensembling loss penalizes differences in classification predictions, with the teacher's weights updated as an exponential moving average of the student's weights. Additionally, an unsupervised conditional entropy loss is used to train the student's classification branch. The Self-Ensembling method, inspired by Mean Teacher, involves a student and teacher model with the same architecture. It aims to ensure consistent classification predictions between the two models under small perturbations of input images. The overall training loss includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data. The Open-set domain adaptation is more challenging as it requires classifying both inliers and outliers into known and unknown classes. To address this, clustering is used to model the diverse data distribution among unknown samples. To address the challenge of unknown samples spanning multiple classes, clustering is used to model diverse semantics in the target domain. This involves creating category-agnostic clusters integrated into Self-Ensembling for domain adaptation. The clustering branch aligns cluster assignment distribution with inherent cluster distribution, ensuring domain-invariant features for known classes and discriminative features for unknown and known classes. K-means clustering is utilized to group unlabeled target samples into clusters, revealing underlying structure tailored for the task. In the target domain, category-agnostic clusters are created to reveal underlying structure for target samples with similar semantics. Each target sample is represented by CNN output features and its inherent cluster distribution is measured through cosine similarities with cluster centroids. This helps in aligning cluster assignment distribution with domain-invariant features for known classes and discriminative features for unknown and known classes. The clustering branch in the student model predicts the distribution over category-agnostic clusters for cluster assignment of target samples. It infers cluster assignment distribution based on input features and is trained with supervision from inherent cluster distribution. The centroid of each cluster is the average of samples in that cluster, and cosine similarity is used to measure similarity with cluster centroids. The clustering branch in the student model predicts cluster assignments using KL-divergence loss to preserve data structure and inter-cluster relationships. The loss is relaxed to strengthen target features in a multi-task paradigm. The student model in SE-CC utilizes Mutual Information Maximization to enhance learned features in an unsupervised manner. A MIM module is designed to estimate and maximize mutual information among input features, output classification distribution, and cluster assignment distribution. Global Mutual Information is encoded into a global feature vector for downstream tasks. The global Mutual Information is estimated using a discriminator with fully-connected networks and nonlinear activation. The output score represents the probability of aligning the input feature with classification and cluster distributions. Jensen-Shannon MI estimator is used for estimation. Additionally, local Mutual Information is also utilized among local features. The local Mutual Information is estimated by replicating distribution maps and using a discriminator with convolutional layers. The output score map indicates the probability of matching input features with classification and cluster distributions. The SE-CC model integrates local and global Mutual Information estimations, along with other loss functions, for training on target data. Experimental validation is conducted on the Office, VisDA dataset, which includes synthetic-real image transfer tasks across different domains. The SE-CC model integrates local and global Mutual Information estimations, along with other loss functions, for training on target data. Experimental validation is conducted on the Office, VisDA dataset, which includes synthetic-real image transfer tasks across different domains. In the evaluation process, 3D CAD models are used for training, real images from COCO dataset for validation, and video frames from YTBB Real dataset for testing. Different classes are assigned for known and unknown categories in the source and target domains, with specific ratios of samples. Various metrics are used for evaluation, including accuracy for known classes, known & unknown classes, and overall target samples. ResNet152 is utilized as the backbone of CNNs for clustering. The SE-CC model uses ResNet152 for clustering and adaptation in closed-set and open-set scenarios. Results show better performance compared to other state-of-the-art models on Office dataset. The classifier in SE-CC \u2666 recognizes N-1 known classes and treats target samples as unknown if the predicted probability is below a threshold. The SE-CC model utilizes ResNet152 for clustering and adaptation in closed-set and open-set scenarios, outperforming other state-of-the-art models on the Office dataset. It improves classification accuracy on challenging transfers like D \u2192 A and W \u2192 A by exploiting category-agnostic clusters for domain adaptation. RTN and RevGrad align data distributions between source and target domains, while open-set adaptation methods (AODA, ATI-\u03bb, and FRODA) excel at rejecting unknown target samples during domain adaptation. The SE-CC model outperforms other state-of-the-art models on the Office dataset by utilizing ResNet152 for clustering and adaptation in closed-set and open-set scenarios. It improves classification accuracy on challenging transfers like D \u2192 A and W \u2192 A by injecting the distribution of category-agnostic clusters as a constraint for feature learning and alignment. The SE-CC achieves better performances than RTN, RevGrad, AODA, ATI-\u03bb, and FRODA in both open-set and closed-set domain adaptation tasks. The SE-CC model surpasses other state-of-the-art models on the Office dataset by using ResNet152 for clustering and adaptation in closed-set and open-set scenarios. It enhances classification accuracy on challenging transfers like D \u2192 A and W \u2192 A by incorporating category-agnostic clusters for feature learning and alignment. The SE-CC outperforms RTN, RevGrad, AODA, ATI-\u03bb, and FRODA in both open-set and closed-set domain adaptation tasks. Ablation Study reveals how different designs in SE-CC impact performance, with CE improving Mean accuracy from 65.2% to 66.3% on VisDA. KL and MIM further refine features for downstream tasks by aligning cluster assignment distribution and maximizing mutual information. The SE-CC model utilizes category-agnostic clusters for domain adaptation, improving Mean accuracy by 4.2% in total. It integrates clustering to separate unknown target samples and aligns cluster assignment distribution for enhanced feature learning. The results validate the effectiveness of exploiting target data structure and mutual information maximization in open-set adaptation. The SE-CC model integrates additional clustering to align cluster assignment distribution and enhance feature learning in domain adaptation. Mutual information among input features, classification, and clustering outputs is utilized to improve feature learning. Experiments on Office and VisDA show performance improvements compared to state-of-the-art techniques. The implementation is done with PyTorch and SGD optimization, with specific settings for learning rate, batch size, and training iterations. Global feature dimension and cluster number settings are detailed in Table 6. The dimension D 1 of global feature for Mutual Information estimation is set as 128/1,024 in AlexNet/ResNet backbone. Table 6 shows cluster number K, tradeoff parameters \u03bb 1 , \u03bb 2 , \u03bb 3 , \u03bb 4, and \u03b1 settings for open-set and closed-set adaptation tasks. Number of clusters (K) is determined using Gap statistics method. \u03bb 1 = 10 is fixed, other parameters are tuned. Evaluation of Clustering Branch compares KL-divergence in SE-CC with L 1 and L 2 distance, showing KL-divergence as a better measure. Evaluation of Mutual Information Maximization in SE-CC shows that CLS, CLU, and CLS+CLU improve performance by exploiting mutual information between input features and classification and clustering outputs. CLS+CLU provides the largest performance boost by combining outputs from both branches. This demonstrates the benefit of leveraging mutual information in downstream tasks. Compared to Source-only without domain adaptation, SE improves the alignment of classification and cluster assignment distributions. The SE-CC module improves domain-invariant representation by preserving the underlying target data structure for known and unknown classes. It separates unknown target samples from known ones while maintaining indistinguishability between known samples in different domains."
}