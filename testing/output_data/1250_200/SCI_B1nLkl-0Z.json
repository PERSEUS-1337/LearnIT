{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions (Q-values) are fundamental in reinforcement learning, with algorithms like SARSA and Q-learning. A new concept of action value is proposed, defined by a Gaussian smoothed version of expected Q-value from SARSA. Smoothed Q-values still satisfy a Bellman equation, making them learnable from experience. Gradients of expected reward with respect to a Gaussian policy's mean and covariance can be derived from the smoothed Q-value function. New algorithms are developed for training a Gaussian policy directly from a learned Q-value approximator, achieving strong results on continuous control benchmarks. In reinforcement learning, Q-values are crucial for algorithms like SARSA and Q-learning. Different Q-value functions lead to distinct families of RL methods. A new concept of action value, the smoothed action value function Q\u03c0, is introduced in this work, impacting algorithm outcomes by influencing policy expression and exploration methods. The smoothed action value function Q\u03c0 introduces a new notion of action value in reinforcement learning. It associates a value with a distribution over actions, allowing for Gaussian-smoothed or noisy versions of expected Q-values. Smoothed Q-values possess properties that make them useful for RL algorithms, such as single-step Bellman consistency and optimization objectives for Gaussian policies. The algorithm Smoothie utilizes derivatives of a trained smoothed Q-value function to update policy parameters, avoiding high variance in standard policy gradient algorithms. Unlike DDPG, Smoothie allows for exploratory behavior by default with a non-deterministic Gaussian policy, reducing the need for hyperparameter tuning. Additionally, Smoothie can easily incorporate proximal policy optimization techniques. Smoothie, a reinforcement learning algorithm, reduces the need for hyperparameter tuning and incorporates proximal policy optimization techniques. It improves stability and performance by including a KL-divergence penalty in the objective. Results on continuous control benchmarks are competitive, especially for challenging tasks with limited data. The algorithm involves an agent interacting with a stochastic environment to maximize cumulative rewards in a Markov decision process framework. The agent interacts with a state, emits an action, receives a reward, and transitions to a new state in a stochastic environment. A stochastic policy \u03c0 guides the agent's actions, and the optimization objective is the expected discounted return. The policy gradient theorem expresses the gradient of the objective function with respect to the policy parameters. Reinforcement learning algorithms balance variance and bias in estimating the expected action value function. In this paper, new RL training methods are developed for multivariate Gaussian policies over continuous action spaces. The policies are parametrized by mean and covariance functions, mapping the observed state to a Gaussian distribution. The focus is on balancing variance and bias in estimating the expected action value function. The paper introduces a new formulation of policy gradient for Gaussian policies, focusing on deterministic policies where the covariance approaches zero. This allows for estimating expected future returns and optimizing the policy and value function. Algorithms like DDPG alternate between improving the value function and policy. In practice, algorithms like DDPG improve the value function and policy by using off-policy distributions based on a replay buffer. Smoothed action value functions are introduced in this paper, providing an effective signal for optimizing Gaussian policies. Smoothed Q-values differ from ordinary Q-values by not assuming the first action is fully specified, but rather only the mean of the distribution is known. This allows for computing Q-values by performing an expectation of actions drawn in the vicinity of a. Smoothed action values are defined to perform an expectation of actions drawn in the vicinity of a for Gaussian policies. Instead of learning a function approximator for Q \u03c0 (s, a) and then drawing samples, a direct function approximator for Q \u03c0 (s, a) is learned. The Bellman equation enables direct optimization of Q \u03c0 by parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 in terms of mean and variance parameters. The Bellman equation allows for direct optimization of Q \u03c0 by parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 with mean and covariance parameters. The gradient of the objective with respect to mean parameters follows the policy gradient theorem. Estimating the derivative with respect to covariance parameters involves using the second derivative of Q \u03c0 with respect to actions. Two ways to optimize Q \u03c0 are discussed, one involving fixed target values and the other requiring a single function. The training procedure for optimizing Q \u03c0 involves using a single function approximator and drawing a phantom action from a replay buffer. The procedure reaches an optimum when Q \u03c0 satisfies the Bellman equation, without the need to track sampling probabilities. The training procedure for optimizing Q \u03c0 involves using a single function approximator and drawing a phantom action from a replay buffer. It is unnecessary to track probabilities q(\u00e3 | s) and recent work has benefited from ignoring importance weights. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods. The proposed formulation in this paper is easily amenable to trust region optimization. The paper proposes a method for optimizing Q \u03c0 using trust region optimization. It builds on previous work using Q-value functions to learn stable policies and is similar to methods that exploit gradient information from the Q-value function. The proposed method can be seen as a generalization of deterministic policy gradient, with updates for training the Q-value approximator and policy mean being identical. The Stochastic Value Gradient (SVG) method trains stochastic policies similar to DDPG but lacks covariance updates. Expected Policy Gradients (EPG) generalize DDPG by updating mean and covariance using Q-value gradients. Our approach estimates the smoothed Q-value function to avoid noisy Monte Carlo samples. Our approach estimates the smoothed Q-value function directly, avoiding approximate integrals and simplifying updates. The training scheme proposed for learning the covariance of a Gaussian policy relies on Gaussian integrals. This perspective on Q-values representing the averaged return of a distribution of actions is distinct from recent advances in distributional RL. The paper introduces a new RL algorithm called Smoothie, which utilizes insights from Gaussian policies. Smoothie maintains a parameterized Q function and trains a Gaussian policy using gradient and Hessian updates. Algorithm 1 provides a simplified pseudocode for the algorithm, which is evaluated against DDPG. Smoothie is evaluated against DDPG using a simple synthetic task with a reward function of two Gaussians. Smoothie learns both the mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima due to fixed exploratory noise. Smoothie successfully solves the task by adjusting the policy mean and covariance during training, while DDPG struggles to escape local optima due to fixed exploratory noise. Smoothie's smoothed reward function guides the policy mean towards the better Gaussian, allowing for effective learning. Smoothie successfully adjusts its policy mean and covariance during training to escape local optima, while DDPG struggles with fixed exploratory noise. Smoothie's ability to adapt to the smoothed reward function leads to effective learning and competitive performance with DDPG. Smoothie outperforms DDPG in tasks like Hopper, Walker2d, and Humanoid, showing advantages in final reward performance. TRPO is not sample-efficient. Smoothie learns optimal noise scale during training, while DDPG requires hyperparameter tuning. Despite DDPG's exploration advantage, Smoothie competes or performs better across tasks, with significant improvements in Hopper, Walker2d, and Ant. Smoothie shows competitive or superior performance in various tasks, with notable improvements in Hopper, Walker2d, and Humanoid. The algorithm's results for Humanoid are among the best published for methods training on millions of environment steps. The introduction of a KL-penalty enhances Smoothie's performance, particularly on challenging tasks, addressing the instability seen in DDPG training. Our algorithm, Smoothie, addresses the instability in DDPG training by introducing a new Q-value function, Q \u03c0, which allows for successful learning of mean and covariance during training. The use of Q \u03c0 results in smoother Q-values, making the reward surface easier to learn and improving performance in tasks like Hopper and Humanoid without sacrificing sample efficiency. Smoothie introduces Q \u03c0 to make the reward surface smoother and improve learning. Further investigation is encouraged to apply these techniques to other policies. The specific identity mentioned can be derived using standard matrix calculus."
}