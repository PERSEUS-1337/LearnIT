{
    "title": "rJfW5oA5KQ",
    "content": "Recent works have shown that Generative Adversarial Networks (GANs) suffer from lack of diversity or mode collapse, as powerful discriminators cause overfitting while weak discriminators cannot detect mode collapse (Arora et al., 2017a). Recent works have highlighted the issue of lack of diversity or mode collapse in Generative Adversarial Networks (GANs). However, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by designing discriminators with strong distinguishing power against specific generator classes. The Integral Probability Metric (IPM) induced by these discriminators can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution is close to the true distribution and cannot drop modes. Preliminary experiments suggest that the lack of diversity in GANs may be due to sub-optimality in optimization rather than statistical inefficiency. The lack of diversity in GANs may be due to optimization issues rather than statistical inefficiency. Various methods have been proposed to enhance the quality and stability of GAN training. Recent research has raised concerns about mode collapse in GANs, where the learned distribution misses significant modes of the target distribution. Properly designing discriminators with strong distinguishing power can potentially alleviate mode collapse. The paper discusses alleviating mode collapse in GANs by designing discriminators with strong distinguishing power. It focuses on the Wasserstein GAN formulation and introduces the F-Integral Probability Metric for comparing distributions. The goal is to learn the data distribution by optimizing the Wasserstein-1 distance using parametric families of functions like neural networks. The paper addresses mode collapse in GANs by enhancing discriminator strength to combat the issue. It highlights the weakness of IPM compared to W1, leading to the generation of high-quality but low-diversity examples. Increasing the discriminator's capacity to larger families like 1-Lipschitz functions is suggested as a solution. The paper proposes a solution to the mode collapse issue in GANs by designing a discriminator class F that addresses the dilemma of powerful discriminators causing overfitting and weak discriminators leading to diversity issues. This resolution aims to combat the lack of diversity observed in empirical studies. This paper addresses the mode collapse issue in GANs by proposing a discriminator class F that is strong against a specific generator class G. The discriminator class F has restricted approximability with respect to G and the data distribution p, aiming to approximate the Wasserstein distance W1 for the data distribution p and any q \u2208 G. The focus is on discriminators F that can approximate the Wasserstein distance W1, particularly in the realizable case where p \u2208 G. In this paper, the focus is on a discriminator class F with restricted approximability with respect to a specific generator class G. This approach aims to address mode collapse in GANs by ensuring that the Wasserstein distance is approximated accurately. The framework allows for non-realizable cases and provides solutions to prevent mode collapse and improve statistical properties of Wasserstein GANs. The paper introduces a theoretical framework for Wasserstein GANs, focusing on diversity and generalization properties. It is the first to address the statistical theory of GANs with polynomial samples. Techniques are developed for designing a discriminator class F with restricted approximability for various generator classes. The framework aims to prevent mode collapse and improve statistical properties of Wasserstein GANs. In Section 4, the paper explores distributions generated by invertible neural networks. A special type of neural network discriminator with one additional layer than the generator is shown to have restricted approximability. This discriminator class guarantees certain properties, such as producing an exponentially large number of modes. However, the invertibility assumption only produces distributions supported on the entire space, which may not align with the low-dimensional manifold often seen in natural image distributions. The distribution of natural images is believed to reside on a low-dimensional manifold. The KL-divergence is not suitable for cases where both distributions have low-dimensional supports. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning such distributions. Tools are developed to approximate the log-density of a neural network generator, demonstrating correlation between IPM and Wasserstein distance for low-dimensional distributions. The IPM correlates with the Wasserstein distance for low-dimensional distributions and with KL-divergence for invertible generator families. It can serve as an alternative measure of diversity and quality when KL-divergence or Wasserstein distance is not measurable in complex settings. The lack of diversity in real experiments may be due to sub-optimality of the IPM. The lack of diversity in real experiments may be caused by sub-optimality of the optimization in GANs. Various tests have been developed to measure diversity, memorization, and generalization. Mode collapse can occur due to a weak discriminator, and different architectures and algorithms have been proposed to address this issue. The curr_chunk discusses the challenges and advancements in training GANs, including the use of quadratic discriminators and statistical guarantees in Wasserstein distance for distributions. It also mentions the importance of diversity in GAN optimization and the impact of generator smoothness on sample complexity. The curr_chunk discusses the sample complexity for learning GANs and the use of invertible generator structures in Flow-GAN. It highlights the challenges in GAN training and the implications of successful GAN training in terms of the IPM. The theory suggests that real data cannot be generated by an invertible neural network and discusses the closeness between learned and true distributions in Wasserstein distance. The curr_chunk discusses the notion of IPM, including statistical distances like TV and Wasserstein-1 distance. It also mentions distances like KL divergence and Wasserstein-2 distance, focusing on distributions with finite second moments. The Rademacher complexity of a function class is defined, along with the training IPM loss for Wasserstein GAN. Generalization of the IPM is also touched upon. The curr_chunk discusses the training IPM loss for the Wasserstein GAN and the generalization of the IPM. It introduces discriminators with restricted approximability for simple parameterized distributions like Gaussian distributions. The set of Gaussian distributions with bounded mean and well-conditioned covariance is considered for the IPM induced by these discriminators. The curr_chunk discusses the restricted approximability of discriminators for Gaussian distributions in the context of training IPM loss for Wasserstein GAN. It shows that one-layer neural networks have limited approximability with respect to Gaussian distributions in G, with bounds differing by a factor of 1/ \u221a d. The proof is deferred to Section B.1, and an extension to mixtures of Gaussians is mentioned in Appendix C. Additionally, it explores the use of linear combinations of sufficient statistics as discriminators for exponential families. The curr_chunk discusses the restricted approximability of discriminators for exponential families. It introduces an exponential family G with linear combinations of sufficient statistics as discriminators. Theorem 3.2 states conditions for the log partition function log Z(\u03b8) and the diameter D of X. The Rademacher complexity bound for F and G is also discussed, with geometric assumptions on the sufficient statistics for the Wasserstein distance. The curr_chunk discusses designing discriminators with restricted approximability for neural net generators in GANs. It covers invertible and injective neural networks generators, with a focus on parameterized invertible neural networks. The curr_chunk discusses invertible neural networks generators parameterized by neural networks G = {G \u03b8 : \u03b8 \u2208 \u0398}, allowing variances to be non-spherical for different impacts on output distribution. It focuses on invertible neural networks G \u03b8 with standard -layer feedforward nets x = G \u03b8 (z) where W i \u2208 R d\u00d7d are invertible, b i \u2208 R d, and \u03c3 : R \u2192 R is the activation function, under Assumption 1 (Invertible generators) with specified parameters. The curr_chunk discusses the properties of neural networks G \u03b8 parameterized by \u03b8 = (W i , b i ) i\u2208[ ] with activation function \u03c3, satisfying certain conditions. It highlights the invertibility of such networks and the ability to compute log p \u03b8 with a neural network. Additionally, it mentions the family F of neural networks containing functions log p \u2212 log q. The curr_chunk discusses the parameterized family of neural networks G \u03b8 with activation functions, focusing on the ability to compute log p \u03b8 using a feedforward neural net. It mentions the family F containing functions log p \u2212 log q and the proof involving the change-of-variable formula. Theorem 4.2 states the properties of invertible-generator distributions in G. Theorem 4.2 discusses the properties of invertible-generator distributions in G, showing restricted approximability w.r.t. G. The proof involves relating the KL divergence to the IPM when log densities exist in the family of discriminators. Lemmas 4.1 and 4.3 are used to establish bounds on the Wasserstein distance for any p, q \u2208 G. Theorem 4.2 discusses properties of invertible-generator distributions in G, showing restricted approximability. The proof relates KL divergence to IPM when log densities exist in the family of discriminators. Lemmas 4.1 and 4.3 establish bounds on Wasserstein distance for any p, q \u2208 G. Theorem D.3 proves that for any p, q \u2208 G, transportation inequalities imply the inequality holds. In the invertible generator case, if G \u03b8 is suitably Lipschitz, f (G \u03b8 (Z)) is a sub-Gaussian random variable. The upper bound is obtained by either a truncation argument for a W 1 bound or a W 2 bound with Lipschitz constant growing linearly in x 2. This extends the result in (Polyanskiy & Wu, 2016). Combining restricted approximability and generalization bound, training success with small expected IPM leads to favorable outcomes. The training process aims to minimize the expected IPM between the estimated distribution q and the true distribution p in Wasserstein distance. The training error is measured by Eqm [W F (p n ,q m )], and efficient algorithms to achieve a small training error are a key open question for future work. Injective neural network generators are considered for generating distributions on a low-dimensional manifold, which is more realistic for modeling real images but technically challenging due to the KL divergence issue. The training process aims to minimize the expected IPM between the estimated distribution q and the true distribution p in Wasserstein distance. A novel divergence is designed to approximate the Wasserstein distance, utilizing a smoothed F-IPM between distributions p and q. The discriminator class F is constructed to ensure that the approximation holds, with a generalization bound when n poly(d). The discriminator class F ensures that the Wasserstein distance is minimized between distributions p and q during training. The generalization bound holds when n poly(d), preventing mode collapse. Theoretical results suggest that mode collapse is avoided as long as the discriminator family F has restricted approximability with respect to the generator family G. Certain specific discriminator classes are designed to guarantee this, with synthetic experiments confirming the theory's consistency in practice. The study confirms that the practice aligns with the theory by conducting experiments with synthetic datasets and GANs using different discriminator classes. Results show a correlation between IPM and Wasserstein / KL divergence, indicating that GAN training difficulty may stem from optimization challenges rather than statistical inefficiency. Experiments demonstrate good statistical behaviors with typical discriminator classes, supporting the theory of restricted approximability. In synthetic experiments with WGANs, the IPM is correlated with the KL divergence, showing that GANs can learn distributions like the unit circle and \"swiss roll\" curve well. The IPM W_F is strongly correlated with the Wasserstein distance W_1, supporting the theory of restricted approximability. Generators and discriminators use standard two-hidden-layer ReLU nets. The study utilized standard two-hidden-layer ReLU nets for both the generator and discriminator classes, with specific architectures. The RMSProp optimizer was used with defined learning rates for optimization. Two metrics, the neural net IPM W_F and the Wasserstein distance W_1, were compared between the ground truth distribution and the learned distribution. Results from experiments on a unit circle and Swiss roll curve showed the learned generator closely matched the ground truth. The study presented sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence. The analysis involved designing discriminators tailored to the generator class to avoid mode collapse and improve generalization. The hope is to extend these techniques to other distribution families with tighter sample complexity bounds in the future. The study focused on sample complexity bounds for learning distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence. The analysis included designing discriminators to prevent mode collapse and enhance generalization. The goal is to extend these techniques to other distribution families with tighter sample complexity bounds in the future. The text discusses the mean distance between two Gaussians using linear discriminators and ReLU functions. It presents calculations for covariance distance and neuron distance, showing how the distance can be bounded. The analysis includes perturbation bounds and comparisons with Wasserstein distance. The text discusses bounding the mean difference between two Gaussians using the W 2 distance. It involves upper bounding the growth of the gradient of the log likelihood function and applying the Rademacher contraction inequality. The analysis also includes perturbation bounds and comparisons with KL bounds. The text discusses bounding the mean difference between two Gaussians using the W 2 distance. It involves upper bounding the growth of the gradient of the log likelihood function and applying the Rademacher contraction inequality. The analysis also includes perturbation bounds and comparisons with KL bounds. The right-hand side can be directly bounded as part of the proof. The exponential family property and Wasserstein bounds are also considered. The Rademacher complexity is computed for a mixture of Gaussians using a neural network. The family F is suitable for learning mixtures of Gaussians based on restricted approximability and generalization criteria. The text discusses bounding the mean difference between two Gaussians using the W 2 distance. It involves upper bounding the growth of the gradient of the log likelihood function and applying the Rademacher contraction inequality. The analysis also includes perturbation bounds and comparisons with KL bounds. The right-hand side can be directly bounded as part of the proof. The exponential family property and Wasserstein bounds are also considered. The Rademacher complexity is computed for a mixture of Gaussians using a neural network. The family F is suitable for learning mixtures of Gaussians based on restricted approximability and generalization criteria. The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) is used in later proofs. The text discusses bounding the mean difference between two Gaussians using the W 2 distance. It involves upper bounding the growth of the gradient of the log likelihood function and applying the Rademacher contraction inequality. The analysis also includes perturbation bounds and comparisons with KL bounds. The right-hand side can be directly bounded as part of the proof. The exponential family property and Wasserstein bounds are also considered. The Rademacher complexity is computed for a mixture of Gaussians using a neural network. The family F is suitable for learning mixtures of Gaussians based on restricted approximability and generalization criteria. The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) is used in later proofs. (D 2 +1)-sub-Gaussian, X satisfies the Bobkov-Gozlan condition with \u03c3 2 = D 2 + 1. Applying Theorem D.1(a) we get Generalization Reparametrize the one-hidden-layer neural net eq. as It then suffices to bound the Rademacher complexity of f \u03b8 for \u03b8 and the Rademacher process we show that Y \u03b8 is suitably Lipschitz in \u03b8 (in the \u03c1 metric) and use a one-step discretization bound. Indeed, Therefore, for any \u03b5 > 0 we have for some constant C > 0. We now bound the expected supremum of the max over a covering set. Let N (\u0398, \u03c1, \u03b5) be a \u03b5-covering set of \u0398 under \u03c1, and N (\u0398, \u03c1, \u03b5) be the covering number. As \u03c1 looks at each \u00b5 i , c j separately, its covering number can be upper bounded by the product of each separate covering. Now, for each individual process Y \u03b8 is the i.i.d. average of random variables of the form \u03b5 i log k j=1 exp(\u00b5 j X+c j). The log-sum-exp part is D-Lipschitz in X, so we can reuse the analysis done precedingly to get that log. This shows that the term \u03b5 i log is 1)-subGaussian, and thus we have by sub-Gaussian maxima bounds that. The text discusses upper bounding the mean difference between two distributions using Wasserstein bounds. It involves truncation arguments, coupling, and Cauchy-Schwarz inequalities. The analysis considers the Lipschitz property of the gradient and provides bounds for the Wasserstein distance. The text discusses upper bounding the mean difference between two distributions using Wasserstein bounds, involving truncation arguments, coupling, and Cauchy-Schwarz inequalities. It considers the Lipschitz property of the gradient and provides bounds for the Wasserstein distance. The inverse network implementing G^-1\u03b8 has layers, parameters, and activation functions specified. The text discusses the implementation of the inverse network G^-1\u03b8 with specified layers, parameters, and activation functions. It shows how additional branches can compute the log determinant of the Jacobian, leading to a neural network that computes log p\u03b8(x) with a limited number of layers and parameters. The theorem is supported by three lemmas, including a restricted approximability bound in terms of the W2 distance. The text presents a restricted approximability bound in terms of the W2 distance, supported by three lemmas. It demonstrates the Gozlan condition satisfaction for p\u03b8 and the Lipschitzness of G\u03b8. Theorem D.1 is applied to show an upper bound on the Wasserstein distances. The text presents upper bounds on the Wasserstein distances using Theorem D.2, focusing on the Lipschitzness of log p\u03b8(x) for all \u03b8 \u2208 \u0398. It demonstrates the W2 bound and the W1 bound, showing tail bounds and substitution results under p\u03b81 or p\u03b82. The text discusses reparametrization of log-density neural networks to bound the Rademacher complexity of F. It introduces additional re-parametrization and defines a metric for the Rademacher process. The text discusses reparametrization of log-density neural networks to bound the Rademacher complexity of F. It introduces additional re-parametrization and defines a metric for the Rademacher process. The one-step discretization bound is dealt with in two separate lemmas, one for discretization error and the other for expected max over a finite set. Substituting these lemmas into the bound, a generalization error term is derived. The text also discusses the inverse network G^-1 and its hidden layers. The text discusses the Lipschitzness of hidden layers in the inverse network G^-1. It shows bounds for the layers and verifies results through induction. The text also addresses the sub-exponential nature of a random variable for a single \u03b8 in \u0398. The text discusses the sub-exponential nature of a random variable for a single \u03b8 in \u0398. It shows that the random variable is suitably sub-exponential and sub-Gaussian, with mean and sub-Gaussianity parameter O(Cd). The term h, A \u03b3 h, is a quadratic function of a sub-Gaussian random vector, hence is subexponential with a bounded mean. The parameter K is also upper bounded. The text discusses the sub-exponential nature of a random variable for a single \u03b8 in \u0398, showing it is sub-exponential and sub-Gaussian with mean-zero. It uses a covering argument to bound the expected maximum and states the theorem quantitatively by specifying relevant quantities of the generator class. The distribution obtained by adding Gaussian noise to a sample from G \u03b8 is truncated to a high-probability region. The text introduces regularity conditions for the family of generators G, including bounds on partial derivatives and the Lipschitz property of the inverse activation function. The main theorem states properties for certain F and d. The main theorem introduces regularity conditions for the generator class G and states properties for certain F and d, showing that F approximates the Wasserstein distance. The proof relies on a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. The main theorem introduces regularity conditions for the generator class G and states properties for certain F and d, showing that F approximates the Wasserstein distance. The proof relies on a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. N globally approximates the entropy by approximating p \u03b2 using Laplace's method of integration, with a greedy \"inversion\" procedure for typical x and a lower bound for atypical x. Theorem E.1 is proven by showing the approximation of log p \u03b2 and log q \u03b2 by neural networks N 1 and N 2, leading to the derivation of certain equations. The main theorem introduces regularity conditions for the generator class G and states properties for certain F and d, showing that F approximates the Wasserstein distance. The proof relies on a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. N globally approximates the entropy by approximating p \u03b2 using Laplace's method of integration, with a greedy \"inversion\" procedure for typical x and a lower bound for atypical x. Theorem E.1 is proven by showing the approximation of log p \u03b2 and log q \u03b2 by neural networks N 1 and N 2, leading to the derivation of certain equations. The proof of Theorem E.2 is discussed in the rest of the section, with helper lemmas and a reverse induction approach. The proof in Section E.3 involves proving helper lemmas using reverse induction. It shows the iterative production of estimates and the size/Lipschitz constant of the neural network. The proof relies on inequalities, the Lipschitzness of \u03c3 \u22121, and the properties of a Gaussian distribution. The algorithm presented approximates the integral and can be implemented by a small, Lipschitz network. It involves calculating gradients and nearest matrices, as well as approximate eigenvector/eigenvalue pairs. The algorithm approximates the integral using approximate eigenvector/eigenvalue pairs of matrices. It involves choosing matrices E i to ensure eigenvalues are separated, with a net S of bounded spectral norm matrices. The integral can be evaluated similarly to Theorem E.9. The integral can be evaluated using approximate eigenvector/eigenvalue pairs of matrices, ensuring eigenvalues are separated. Synthetic WGAN experiments are conducted with invertible neural net generators and discriminators designed with restricted approximability. The goal is to show the correlation between empirical IPM W F (p, q) and the KL-divergence on synthetic data. Data is generated from a ground-truth invertible neural net generator, using Leaky ReLU with negative slope 0.5 as the activation function. The study utilizes a layer-wise invertible feedforward net with Leaky ReLU activation function and Gaussian distribution Z. Weight matrices are well-conditioned, and the discriminator architecture is chosen based on restricted approximability guarantee. Training involves generating batches, solving min-max problem in Wasserstein GAN, and using RMSProp optimizer. Evaluation metrics are used to compare true and learned data. The study uses RMSProp optimizer for training the discriminator. Evaluation metrics include KL divergence, training loss, and neural net IPM for comparing true and learned generator distributions. The study uses RMSProp optimizer for training the discriminator to optimize the generator in norm balls without regularization. The theory shows that WGAN can learn the true generator in KL divergence, and F-IPM is indicative of the KL divergence. Experiments with a two-layer net in 10 dimensions show results for Vanilla WGAN and WGAN-GP, with findings plotted in FIG5. The study demonstrates that WGAN training with a restricted approximability discriminator design can effectively learn the true distribution in KL divergence. Results show a strong correlation between W F (eval) and KL divergence, indicating that GANs are finding the true distribution without mode collapse. Additionally, adding a gradient penalty improves optimization, as reflected in both the KL curve and the W F curve. The necessity of the specific discriminator design is tested, showing that IPM with vanilla discriminators also correlates well with the results. Results show that IPM with vanilla discriminators correlates well with KL-divergence, indicating that standard fully-connected discriminator nets can approximate the log density of generator distributions. The inferior performance of WGAN-Vanilla in KL divergence is attributed to inferior training performance in terms of IPM convergence, suggesting a similar phenomenon may occur in training GANs with real-life data. In this section, the correlation between perturbations and the KL divergence and neural net IPM is tested. By generating pairs of perturbed generators and denoising the training process, a positive correlation between the KL divergence and neural net IPM is observed. The majority of points align with the theory that the neural net distance scales linearly in the KL divergence, with a few outliers due to large perturbations causing poorly conditioned weight matrices. The experiments in Section G.2 tested the correlation between perturbations, KL divergence, and neural net IPM. Outliers with large KL were attributed to perturbations causing poorly conditioned weight matrices. Results showed that vanilla discriminator structures can still lead to good generator convergence, although slightly weaker than with restricted approximability. The left-most figure displays KL divergence, the middle shows estimated IPM, and the right illustrates training loss. The estimated IPM correlates well with the evaluation process. The experiments tested the correlation between perturbations, KL divergence, and neural net IPM. Vanilla discriminator structures can lead to good generator convergence, slightly weaker than with restricted approximability. The left figure displays KL divergence, the middle shows estimated IPM, and the right illustrates training loss. The estimated IPM in evaluation correlates well with the KL-divergence. Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators and plotted in FIG8. The correlation is roughly the same as for discriminators with restricted approximability."
}