{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. Extensive empirical investigations on the NSynth dataset show that GANs outperform WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis faces challenges in modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but have slow sampling speeds. Efforts to speed up generation introduce overhead. GANs have shown success in generating high-resolution audio efficiently. Local latent structure due to memory constraints BID9. Generative Adversarial Networks (GANs) have been successful in generating high-resolution images. Audio GANs have potential for domain transformations similar to images. However, adapting image GAN architectures for audio waveform generation has not achieved the same level of fidelity. Frame-based techniques for audio waveforms involve locally-coherent waves with periodicity. Transposed convolutional filters face the challenge of covering all frequencies and phase alignments to maintain phase coherence in audio. STFT allows unwrapping phase and deriving instantaneous radial frequency to show the relationship between audio and frame frequency. GAN progress in image modeling started with focused datasets like CelebA, gradually advancing to less constrained domains. The NSynth dataset was introduced with the motivation to focus on individual notes from musical instruments, aligning and cropping the data to reduce variance and highlight fine-scale details like timbre and fidelity. Various models have been explored for audio generation, including autoregressive WaveNet autoencoders, bottleneck spectrogram autoencoders, frame-based regression models, inverse scattering networks, VAEs with perceptual priors, and adversarial regularization for domain transfer. This work introduces adversarial training to further advance audio generation techniques. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms, focusing on maintaining the regularity of periodic signals over short to intermediate timescales. The synthesis network must learn appropriate frequency and phase combinations to produce a coherent waveform. The challenge for a synthesis network is to learn frequency and phase combinations to generate coherent waveforms. Phase precession, similar to STFT, occurs when filterbanks overlap. A new approach inspired by the phase vocoder involves unwrapping phase to observe constant instantaneous frequency. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Key findings include generating log-magnitude spectrograms and phases directly with GANs producing more coherent waveforms than generating waveforms with convolutions. Estimating IF spectra leads to even more coherent audio, and it is crucial to prevent harmonics from overlapping by adjusting STFT frame size and switching to mel frequency scale. GANs can outperform WaveNet on the NSynth dataset. The NSynth dataset contains 300,000 musical notes from 1,000 instruments, with samples aligned and recorded in isolation. It is structured with labels for pitch, velocity, instrument, and acoustic qualities. Training was focused on acoustic instruments and fundamental pitches ranging from MIDI 24-84 for natural sound. The dataset includes examples from mostly strings, brass, and woodwinds. The study focused on generating audio spectra from a dataset of 70,379 examples of strings, brass, woodwinds, and mallet instruments. A new test/train split was created, and progressive training methods were adapted from image generation techniques. The model used random vectors and transposed convolutions to generate output data, with a discriminator network estimating the difference between real and generated distributions. Gradient penalty and pixel normalization were employed for training. The study focused on generating audio spectra from various musical instruments using progressive training methods. The model incorporated a one-hot representation of musical pitch in the latent vector to achieve independent control of pitch and timbre. An auxiliary classification loss was added to encourage the generator to use the pitch information. Spectral representations were computed using STFT with specific parameters, resulting in an image size of (256, 512, 2). The study focused on generating audio spectra from musical instruments using progressive training methods. Spectral representations were computed using STFT with specific parameters, resulting in different variants of spectral images for analysis. These variants include \"phase\" models, \"instantaneous frequency\" models, high frequency resolution variants, and \"IF-Mel\" variants. The study compared different spectral representations of audio, including \"IF-Mel\" variants, and adapted WaveGAN and WaveNet models for waveform generation. WaveNet is currently the state of the art in generative modeling of audio. The study adapted WaveNet models for audio generation, using a one-hot pitch conditioning signal. They found that the 8-bit mu law model outperformed the 16-bit model. Evaluation of generative models is challenging due to the subjective nature of audio quality, so human evaluation was used as a gold standard. The study evaluated WaveNet models for audio generation using a one-hot pitch conditioning signal. They found that the 8-bit mu law model performed better than the 16-bit model. Evaluation was done through human perception tests on audio quality. Amazon Mechanical Turk was used for comparison tests on different models, collecting 3600 ratings. Metrics like Number of Statistically-Different Bins (NDB) and Inception Score (IS) were used to measure diversity and quality of generated examples. The Inception Score (IS) is a metric used to evaluate GANs by measuring the mean KL divergence between imageconditional output class probabilities and the marginal distribution. It penalizes models that don't easily classify examples into a single class or produce examples belonging to only a few classes. Additionally, Pitch Accuracy (PA) and Pitch Entropy (PE) measure the accuracy and distribution entropy of pitches generated by a pretrained pitch classifier. Fr\u00e9chet Inception Distance (FID) is another metric proposed for evaluating GANs based on the 2-Wasserstein distance. The Fr\u00e9chet Inception Distance (FID) is a metric for evaluating GANs based on the 2-Wasserstein distance between multivariate Gaussians fit to features extracted from a pretrained Inception classifier. Results show a correlation between FID and perceptual quality and diversity on synthetic distributions. Human evaluation indicates that audio quality decreases as output representations move from IF-Mel to Waveform, with IF-Mel being judged slightly inferior to real data. The WaveNet baseline produces high-fidelity sounds but occasionally breaks down, resulting in a score comparable to IF GANs. Sample diversity, measured by NDB, follows a similar trend as human evaluation. The NDB score correlates with audio quality, with high frequency resolution improving the score. The WaveNet baseline performs poorly due to lack of diversity in autoregressive sampling. FID scores are lower for models with high frequency resolution, while Mel scaling has less impact. Phase models have high FID, indicating poor sample quality. Classifier metrics like IS, Pitch Accuracy, and Pitch Entropy are good due to explicit conditioning. High-resolution models generate examples classified well. The high-resolution models generate examples classified well by metrics like IS, Pitch Accuracy, and Pitch Entropy. However, differences in these scores may not indicate sample quality due to mode collapse. Visualizing qualitative audio concepts is recommended, with accompanying audio examples provided for comparison. The real data waveform is extremely periodic, while models with low frequency resolution and baselines may have less reliable pitch generation. Listen to the audio examples for a better understanding. The WaveGAN and PhaseGAN models show phase irregularities and blurry waveforms, while the IFGAN model is more coherent with small variations. Rainbowgrams depict clear phase coherence in real data and IFGAN, while PhaseGAN has speckles and WaveGAN is irregular. Visualizations show consistent waveforms in real data and IF models, with PhaseGAN having phase discontinuities. The WaveNet autoencoder in BID9 learns local latent codes for generation on a millisecond scale but has limited scope. Comparing interpolations between raw waveforms, latent codes, and global codes shows WaveNet's improvement in mixing timbre but linear interpolation does not capture the complex prior on latents. WaveNet improves mixing timbre for two notes but linear interpolation fails to capture complex prior on latents, resulting in unrealistic sounds. IF-Mel GAN's global conditioning allows for high-fidelity audio examples during interpolation. The IF-Mel GAN's global conditioning enables high-fidelity audio examples during interpolation, ensuring smooth perceptual changes and consistent timbre morphing between instruments. The unique timbral identity of the GAN remains intact across different pitches, creating a distinct instrument identity in latent space. The IF-Mel GAN allows for high-fidelity audio interpolation with consistent timbre morphing between instruments. It offers faster synthesis times compared to WaveNet, enabling real-time neural network audio synthesis on devices for a broader range of expressive sounds. Previous applications of WaveNet autoencoders required prerendering all possible sounds due to long synthesis latency. Realtime neural network audio synthesis on device allows for a broader range of expressive sounds compared to speech synthesis models. Adapting GANs for variable-length conditioning or recurrent generators is a future research direction. Music audio generation is relatively under-explored compared to speech. Previous work on autoregressive models for music synthesis has shown slow generation times. Our work proposes a modification to GAN loss function for improved training stability and architectural advancements. Our work builds on recent advances in GAN literature by proposing a modification to the loss function, introducing progressive training for improved generation quality, and employing architectural tricks. The NSynth dataset was initially compared to \"CelebA of audio\" and used WaveNet autoencoders for timbre interpolation. BID23 expanded on this by incorporating an adversarial domain confusion loss for timbre transformations. BID5 achieved significant sampling speedups by training a frame-based regression model for raw waveform mapping. By controlling audio representation, high-quality audio generation with GANs on the NSynth dataset was achieved, surpassing WaveNet baseline fidelity. However, further validation and expansion to diverse signal types like speech are necessary. This study opens avenues for domain transfer and other applications of adversarial losses in audio generation, addressing issues like mode collapse and diversity. Further research can explore combining adversarial losses with encoders or regression losses for comprehensive data capture. Further research is needed to explore combining adversarial losses with encoders or regression losses for comprehensive data capture in audio generation. Different models were trained with various optimizer settings and learning rates, with the best performance achieved using a learning rate of 8e-4 and a classifier loss of 10. The models utilized box upscaling/downscaling and pixel normalization, with the discriminator appending the standard deviation of minibatch activations as a scalar channel. The use of a Tanh output nonlinearity for the generator was found to be beneficial. The models were trained with a Tanh output nonlinearity for the generator and normalized real data before passing to the discriminator. Each GAN variant was trained for 4.5 days on a single V100 GPU, with batch sizes of 8. Progressive models trained on 1.6M examples per stage, totaling to approximately 11M examples. The WaveNet baseline utilized a decoder with 30 layers of dilated convolution. The WaveNet model consists of 3 stacks of 10 layers each, with increasing dilation rates. The audio encoder stack is replaced with a conditioning stack operating on a pitch conditioning signal. The conditioning stack consists of 5 layers of dilated convolution and 3 layers of regular convolution. The model uses mulaw encoding for the 8-bit version and a quantized mixture for the 16-bit version. WaveNets converged in 150k iterations over 2 days using 32 V100 GPUs."
}