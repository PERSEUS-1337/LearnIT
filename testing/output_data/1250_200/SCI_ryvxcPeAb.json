{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, adversarial perturbations are decomposed into model-specific and data-dependent components, with the latter contributing significantly to transferability. A new approach using noise reduced gradient (NRG) to craft adversarial examples enhances transferability across various ImageNet models. Low-capacity models outperform high-capacity models in attack capability with comparable test performance. These findings offer insights for constructing successful adversarial examples and designing defense strategies against black-box attacks in the era of large neural network models applied in real-world applications. Recent works have shown that adversaries can manipulate inputs to fool computer vision models, creating adversarial examples. Understanding and defending against these attacks are open questions. Adversarial examples can transfer across different models, enabling attacks on black-box systems. The vulnerability is attributed to the nonlinearity of deep neural networks and high dimensionality. Recent works have shown that adversaries can manipulate inputs to fool computer vision models, creating adversarial examples. The primary cause of adversarial instability is the linear nature and high dimensionality of deep neural networks. Various methods like FGSM, DeepFool, and iterative gradient sign attacks have been proposed. Transferability of adversarial examples and defense mechanisms like defensive distillation, adversarial training, and image transformation have also been explored. In this work, the transferability of adversarial examples is explained, and insights are used to enhance black-box attacks. Adversarial perturbation is decomposed into model-specific and data-dependent components. The data-dependent part mainly contributes to the transferability of adversarial perturbations across different models. Adversarial examples are constructed by employing the data-dependent component of gradient instead of the gradient itself. The proposed noise-reduced gradient (NRG) method enhances black-box attacks by increasing success rates on the ImageNet validation set. Model-specific factors like capacity and accuracy influence the success rate, with higher accuracy and lower capacity models showing stronger attack capabilities. Transferability plays a key role in attacking unseen models, providing guidance for future attacks. In the context of attacking deep neural networks, adversarial perturbations can make models vulnerable to misclassification. Non-targeted attacks aim to misclassify inputs, while targeted attacks aim to produce a specific wrong label. In black-box attacks, the adversary lacks knowledge of the target model's architecture and parameters. In black-box attacks, the adversary lacks knowledge of the target model's architecture and parameters. They can construct adversarial examples on a local model and deploy them to fool the target model. Crafting adversarial perturbations involves optimizing a loss function to measure the discrepancy between prediction and ground truth. The commonly used loss function is cross entropy. In practice, distortion measurement is challenging to quantify. Ensemble-based approaches suggest using a large ensemble of source models to improve adversarial examples. Various optimizers are used to solve optimization problems, with a focus on normalized-gradient based optimizers. Fast Gradient Based Method attempts to solve optimization problems with a single step iteration. The Fast Gradient Based Method and Iterative Gradient Method are two approaches to solving optimization problems using normalized gradient vectors. The Fast Gradient Based Method performs a single step iteration, while the Iterative Gradient Method performs a projected normalized-gradient ascent for k steps. Both methods are empirically shown to be fast and have good transferability between models. The transferability of adversarial examples between models is crucial for black-box attacks and defense strategies. Previous works have suggested that similarity in decision boundaries enables transferability, especially in the direction of adversarial examples. Models with high performance on the same dataset learn similar functions on the data manifold, but their behavior off the manifold can differ due to architectural differences and random initializations. Perturbations can be decomposed into data-dependent and model-specific components to understand transferability better. The perturbation can be decomposed into data-dependent and model-specific components to understand transferability better. The data-dependent component mainly contributes to transferability between models, while the model-specific component contributes little due to different behaviors off the data manifold. This is illustrated in FIG0, where the data-dependent component can easily attack model B but the model-specific component contributes little to the transfer from A to B. The data-dependent component plays a significant role in transferability between models, while the model-specific component contributes little. To enhance success rates of black-box adversarial attacks, the NRG method reduces model-specific noise to focus on the data-dependent component. The NRG method reduces model-specific noise to focus on the data-dependent component, improving success rates of black-box adversarial attacks. The noise-reduced gradient captures more data-dependent information than the ordinary gradient, leading to smoother gradients and better generalization. The noise-reduced iterative sign gradient method (nr-IGSM) is proposed for attacks, with a special case called noise-reduced fast gradient sign method (nr-FGSM). To justify and analyze the effectiveness of NRG for enhancing transferability, start-of-the-art classification models trained on ImageNet dataset are used. The ImageNet ILSVRC2012 validation set with 50,000 samples is utilized, selecting 5,000 images for each attack experiment. Pre-trained models provided by PyTorch including resnet18, vgg16 bn, densenet121, alexnet, and others are employed to increase experiment reliability. The Top-1 and Top-5 accuracies of various models like densenet169, densenet201, alexnet, and squeezenet1.1 can be found on a website. Adversarial examples are evaluated using white-box attack performance and targeted attack success rates. The paper uses cross entropy as the loss function and measures distortion using \u221e norm and scaled 2 norm. FGSM and IGSM optimizers are considered, and the effectiveness of noise-reduced gradient technique is demonstrated. The noise-reduced gradient technique is shown to be effective when combined with fast gradient-based methods like FGSM and IGSM. Results show that nr-FGSM consistently outperforms FGSM in blackbox attacks, even improving white-box attacks. Additionally, nr-IGSM generates adversarial examples that transfer more easily than IGSM, indicating the effectiveness of noise-reduced gradients in guiding the optimization process. The noise-reduced gradient (NRG) from nr-IGSM guides the optimizer to explore data-dependent solutions, making adversarial examples transfer more easily. Large models like resnet152 are more robust to transfer attacks. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet. The higher confidence adversarial examples have in the source model, the more likely they will transfer to the target model. This may be due to inappropriate hyperparameter choices. The inappropriate choice of hyperparameters in BID6, such as \u03b1 = 1 and k = min(\u03b5 + 4, 1.24\u03b5), leads to underfitting of the source model. When attacking the alexnet as a target model, IGSM overfits more than FGSM, resulting in a lower fooling rate. Trusting the objective in Eq. (2) completely may cause overfitting to source model-specific information. The noise reduced gradient technique removes model-specific information from gradients, leading to better cross-model generalization. The NRG method is applied to ensemble-based approaches, with 1,000 images selected for evaluation due to computational costs. The NRG method is applied to ensemble-based approaches with 1,000 images selected for evaluation. Non-targeted attacks using FGSM, IGSM, and their noise reduced versions are tested. For targeted attacks, generating adversarial examples predicted by unseen target models is challenging. Different from non-targeted attacks, targeted adversarial examples are sensitive to the step size \u03b1 in optimization procedures. A large step size is necessary for generating strong targeted adversarial examples. The NRG method is applied to ensemble-based approaches with 1,000 images selected for evaluation. A large step size is necessary for generating strong targeted adversarial examples. Comparing success rates of normal methods and NRG methods shows a remarkable improvement in performance. Sensitivity of hyper parameters m, \u03c3 is explored for black-box attacks using the NRG method. The NRG method is used for black-box attacks, exploring the sensitivity of hyperparameters m, \u03c3. Larger m leads to higher fooling rates, while an optimal \u03c3 value improves performance. The optimal \u03c3 varies for different source models. The robustness of adversarial perturbations to image transformations is preliminarily explored, which is crucial for real-world applications. The study explores the influence of image transformations on adversarial examples' survival in the physical world. Densenet121 and resnet34 models are used, with rotations, Gaussian noise, Gaussian blur, and JPEG compression as transformations. Results show that NRG-generated adversarial examples are more robust. Decision boundaries of different models are analyzed to understand the superior performance of NRG-based methods. The study compares the performance of different target models using Resnet34 as the source model. The direction of sign \u2207f is found to be as sensitive as sign (\u2207f \u22a5 ) for Resnet34, but other target models are more sensitive along sign \u2207f. Removing \u2207f \u22a5 from gradients helps avoid overfitting to the source model and improves transferability to other target models. The study explores penalizing the optimizer along the model-specific direction to prevent overfitting to the source model and improve transferability to other target models. Different models exhibit varying distances for producing adversarial transfer, with larger distances for complex models compared to small models. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well for any target model. This observation highlights the varying performances of different models in attacking the same target model. The study investigates the varying performances of different models in attacking the same target model. Results show that models with smaller test error and lower capacity have stronger attack capabilities. This phenomenon is explained by the lower bias for approximating the ground truth and a smaller model-specific component. The study explores how different models perform in attacking a target model. Models with lower capacity and smaller test error exhibit stronger attack capabilities due to a lower bias for approximating the ground truth and a smaller model-specific component. Adversarial perturbations consist of model-specific and data-dependent components, with the latter contributing more to transferability. Noise-reduced gradient (NRG) based methods are proposed for crafting more effective adversarial examples. The study discusses the effectiveness of adversarial examples and the impact of model capacity on black-box attacks. It proposes using NRG-based methods for defense and explores the influence of hyperparameters on the quality of adversarial examples. The study evaluates the success rates of adversarial examples generated on ResNet152 and VGG16 BN models. The optimal step size for the attack is crucial, with both too large and too small step sizes affecting performance. More iterations with a small step size can lead to worse performance due to overfitting, while a large step size encourages exploration of model-independent areas. Additionally, an experiment on the MNIST dataset confirms the influence of model redundancy on attack capability. The experiment on attack capability involves using fully connected networks with different depths on the MNIST dataset. Results show that low-capacity models have stronger attack capability compared to large-capacity models. The success rates of cross-model attacks are reported, demonstrating the impact of model capacity on attack strength."
}