{
    "title": "H135uzZ0-",
    "content": "In this work, state-of-the-art visual understanding neural networks are trained on the ImageNet-1K dataset using Integer operations on General Purpose hardware. The focus is on Integer Fused-Multiply-and-Accumulate (FMA) operations with a shared exponent representation of tensors and a Dynamic Fixed Point (DFP) scheme. Efficient integer convolution kernel development is explored, including handling overflow of the INT32 accumulator. CNN training for various networks like ResNet-50, GoogLeNet-v1, VGG-16, and AlexNet achieves or exceeds SOTA accuracy. In this study, state-of-the-art neural networks are trained on ImageNet-1K using Integer operations on General Purpose hardware. Efficient integer convolution kernel development is explored, including handling overflow of the INT32 accumulator. CNN training for ResNet-50, GoogLeNet-v1, VGG-16, and AlexNet achieves or exceeds SOTA accuracy with a 1.8X improvement in training throughput using half-precision arithmetic. Half-precision training can use FP16 or INT16, offering different precision and range. INT16 has higher precision but lower dynamic range compared to FP16, leading to different residual errors. Algorithmic and semantic differences exist between the two data types, impacting tensor representation, multiply-and-accumulate operations, down-conversion schemes, scaling, normalization, and overflow management. Selecting the right combination of these factors is crucial for achieving state-of-the-art accuracy in half-precision training. In this work, a mixed-precision training setup is described, utilizing INT16 tensors with shared tensor-wide exponent and an instruction for multiplying two INT16 numbers. A down-convert scheme based on the maximum value of the output tensor is used, along with an overflow management scheme for accumulating partial results into FP32. Specialized low-precision instructions like AVX512_4VNNI can speed up compute for neural network training. The proposed mixed-precision training strategy utilizes INT16 tensors with specialized low-precision instructions like AVX512_4VNNI for speedup in dot-product operations. By keeping precision critical operations in single precision and compute intensive operations in half precision, Top-1 accuracies on the ImageNet-1K dataset matching or exceeding single precision results are achieved. The proposed mixed-precision training strategy utilizes INT16 tensors with specialized low-precision instructions like AVX512_4VNNI for speedup in dot-product operations, achieving Top-1 accuracies on the ImageNet-1K dataset that match or exceed single precision results. Results show significant improvement in accuracy for ResNet-50 and state-of-the-art accuracy with int16 training on GoogLeNet-v1, VGG-16, and AlexNet networks. The paper discusses literature on half-precision training, dynamic fixed point format, kernels, neural network training operations, and presents experimental results. Reduced precision data representations, including 16-bit floating point storage for activations, weights, and gradients, have been explored in deep learning research. The use of FP16/FP32 mixed precision training has shown minimal loss compared to baseline FP32 results, requiring loss scaling for near-state-of-the-art accuracy. This scaling ensures back-propagated gradient values are shifted into FP16 representable range, crucial for capturing small magnitude values essential for accuracy. Custom fixed point representations offer increased precision and dynamic range, making them more robust and accurate than floating-point schemes. Studies have shown that using dynamically scaled fixed point representation can significantly improve performance in convolutional neural networks. Research has also successfully trained smaller networks using 16-bit fixed point on specialized hardware, with potential for further reduced bit-widths for increased precision. Point data representations with increased precision and dynamic range have been explored in various publications, including custom fixed point schemes like DFXP and binary weight training. Different approaches have been proposed, such as using binary activations or quantizing activations and weights to 6-bits. While some models show promising results with smaller datasets, challenges arise with larger datasets like ImageNet. Flexpoint, a fixed point numerical format designed for deep neural networks, has shown superior performance compared to FP16 and achieves numerical parity with FP32 across various applications. Dynamic Fixed Point (DFP) tensors offer a more general approach compared to Flexpoint, allowing for utilization of general purpose hardware. DFP tensors combine an integer tensor I and an exponent E s, providing a trade-off between float and half-float data formats in terms of precision and dynamic range. The DFP-16 data type, for example, can achieve higher performance compared to full-precision floats. DFP-16 data type offers a trade-off between float and half-float with higher compute density and effective precision due to a larger 15-bit mantissa. Blocked-DFP representation extends dynamic range by using fine-grained quantization. Shared exponent and 2's complement representation are used for arithmetic operations on DFP tensors. Software manages exponent handling and precision. DFP-16 data type allows for mixed-precision training with arithmetic operations and data conversions between DFP and float tensors. Shared exponent is derived from the maximum value of the floating point tensor. Common DFP primitives for neural network training include multiplication and addition operations on DFP-16 tensors resulting in 32-bit tensors with new shared exponents. The DFP-16 tensor allows for mixed-precision training in neural networks. Fused Multiply and Add operations result in products with the same shared exponent. Down-Conversion scales DFP-32 output to DFP-16 for the next layer. Neural network training involves forward propagation, back-propagation, weight gradient computation, and solver operations. CNNs focus on compute-intensive steps like GEMM-like convolution operations. In this work, a method using INT16 operations for convolutions and GEMM is proposed. The core compute kernels in the mixed precision training scheme are FP, BP, and WU convolution functions. These functions take DFP-16 tensors as input and produce FP32 tensors as output. The FP and BP operations are followed by quantization steps to convert FP32 tensors to DFP-16 tensors for operations in the next layer. The text discusses the conversion of FP32 tensors to DFP-16 tensors for efficient implementations of core compute kernels using Integer FMA instruction sequences, particularly the AVX512_4VNNI instruction. This allows for 8 multiply-add operations per output (16 Integer-OPs) and prevents overflows during stats computation in batch-norm layers. The text discusses the implementation of FPROP convolution kernel using AVX512_4VNNI instruction, which performs 8 multiply-add operations per output. It highlights the data layout of weights capturing 2-way horizontal accumulation operation and the potential overflow issue in accumulate chains during neural network training. The implementation of FPROP convolution kernel using AVX512_4VNNI instruction performs 8 multiply-add operations per output. An accumulate chain of 3 products of INT16 multiplicative pairs can cause an overflow of the INT32 accumulator in neural network training. To prevent overflows, converting INT32 intermediate output into FP32 before accumulation is recommended. Performance impact is addressed by blocking input feature maps and using optimal register blocking. In optimizing the FPROP convolution kernel with AVX512_4VNNI instructions, blocking input feature maps and using optimal register blocking help manage overflow. The additional VCVTINTFP32 instruction in Algorithm 2 addresses overflow management, with an instruction overhead of <1% to at most 3%. The accumulate chain is optimized to more than 200 to balance instruction overheads and cache/instruction reuse, with inputs shifted by 1-bit for convolutions. This results in a DFP15 representation of all DFP tensors. In optimizing the FPROP convolution kernel with AVX512_4VNNI instructions, blocking input feature maps and using optimal register blocking help manage overflow. The additional VCVTINTFP32 instruction in Algorithm 2 addresses overflow management, with an instruction overhead of <1% to at most 3%. The accumulate chain is optimized to more than 200 to balance instruction overheads and cache/instruction reuse, with inputs shifted by 1-bit for convolutions. This results in a DFP15 representation of all DFP tensors. Trained several CNNs for ImageNet-1K classification task using mixed precision DFP16: AlexNet BID11, VGG-16 Simonyan & Zisserman (2014), GoogLeNet-v1 Szegedy et al. (2015), ResNet-50 He et al. (2016). Achieved SOTA accuracy for all models with DFP16 training, even surpassing full precision results in some cases. Highest achieved accuracy on ImageNet-1K classification task with mixed precision DFP16 is 75.77% top-1 accuracy and 92.84% top-5 accuracy for ResNet-50. DFP16 training shows improved generalization and better accuracies compared to full precision training. The convolution kernels in DFP16 are faster, resulting in a 1.5\u00d7 speedup. Batchnorm computation is also faster with DFP16 due to bandwidth saving. ReLU and EltWise layers are fused with batchnorm to reduce memory passes. The speedup in training throughput is primarily due to a 50% bandwidth saving from a smaller memory footprint. Fusion of ReLU and EltWise layers with batchnorm helps avoid additional memory passes. These optimizations result in a 1.8X speedup over FP32 for ResNet-50, with an overall training throughput of 276 images/sec. Additionally, improved SGD computation in Intel-Caffe pushes the training throughput to 317 images/sec, showing a 3X reduction in framework overhead. Mixed Precision DFP16 training can yield a high speedup of 1.6X with respect to time-to-train even in the case of the baseline FP32 version. Mixed Precision DFP16 training demonstrates high speedup in training throughput, showing on-par or better accuracies compared to FP32 baseline. It offers potential 2\u00d7 savings in computation, communication, and storage. A general dynamic fixed point representation scheme is proposed for large networks like ResNet-50, GoogLeNet-v1, VGG-16, and AlexNet, with plans to extend to other network types like RNNs, LSTMs, and GANs."
}