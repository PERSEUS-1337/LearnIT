{
    "title": "Byxpfh0cFm",
    "content": "Data augmentation is a common technique used in machine learning to enhance learning methods by expanding the training set with transformed examples. However, this process can be inefficient and costly due to the large dataset size created. In this study, a new approach using subsampling policies based on model influence and loss is proposed to reduce the augmentation set size by 90% while maintaining accuracy gains. This method aims to achieve the same benefits of data augmentation with fewer data points. Data augmentation is crucial for achieving high accuracy in machine learning pipelines, especially in image recognition tasks. While it simplifies the process by applying known invariances to the data, it can significantly increase dataset size and training costs. Applying multiple transformations can lead to a substantial growth in dataset size, impacting storage costs and training time. In this work, the aim is to make data augmentation more efficient and user-friendly by identifying subsamples of the full dataset that are good candidates for augmentation. The approach draws inspiration from the virtual support vector (VSV) method, which focuses on augmenting samples close to the margin to create a more robust decision surface. This method aims to reduce data storage costs and training time by augmenting only selected samples, improving the overall efficiency of the augmentation process. The method aims to create a more robust decision surface by augmenting samples close to the margin, specifically the support vectors. This approach reduces the number of samples augmented while maintaining transformation invariance. The goal is to develop policies that can effectively reduce the augmentation set size for a broader range of models. Two key metrics are used to rank the importance of data points for augmentation. The paper proposes policies for selecting important data points for augmentation based on metrics like training loss and model influence. It shows that high accuracy can be achieved by augmenting only a subset of the dataset, with policies based on training loss or model influence outperforming random sampling. Additionally, modifications like sample reweighting are suggested for further improvement. The paper suggests policies for selecting data points for augmentation based on training loss or model influence, outperforming random sampling. Modifications like sample reweighting and online learning can further enhance performance. Experiments are conducted on benchmark datasets like MNIST, CIFAR10, and NORB. Data augmentation is widely used in image classification pipelines, with various techniques applied to audio and text modalities as well. The selection of augmentation strategies can significantly impact performance and require careful tuning. The paper proposes data point selection policies for augmentation based on training loss or model influence, surpassing random sampling. These policies are complementary to existing transformation selection methods and can benefit reinforcement learning on large datasets. Recent works have also explored augmentation strategies using adversarial training approaches like GANs. The curr_chunk discusses training approaches like robust optimization frameworks and GANs for generating artificial points from target distributions. It mentions the Virtual Support Vector method for reducing points for augmentation and seeks ways to downsample candidate points using metrics beyond support vectors. The curr_chunk explores generalizing the notion of support vectors by measuring loss at each training point and studying model influence. It is related to subsampling for dataset reduction but focuses on metrics beyond support vectors. The curr_chunk discusses the motivation for subsampling the original training dataset before augmentation to make data augmentation more efficient. It highlights the difference between subsampling for dataset reduction and the goal of retaining accuracy in a fully augmented dataset. The curr_chunk discusses the effectiveness of subsampling the original training dataset before augmentation to improve efficiency. It shows the impact of translation augmentations on test accuracy for various datasets and highlights that augmenting only a portion of the dataset can still lead to significant accuracy gains. Subsampling a portion of the dataset before augmentation can lead to significant accuracy gains. Optimal policies for augmenting just 10% of the data can achieve similar results to full augmentation, with higher accuracy for some datasets. This serves as a starting point for the paper, detailing proposed policies in Section 4 and providing experimental details in Section 5. The goal of this work is to find a subset of the training set that, when augmented, achieves similar performance to augmenting the entire dataset. The proposed policies include an augmentation score for each training point and a policy for sampling points based on these scores. Two metrics, loss, and model influence, are used to generate augmentation scores. Different policies for subset selection based on these scores are explored, including deterministic and random policies. The text explores different policies for selecting a subset of training points based on augmentation scores. These policies include deterministic and random selection methods. The augmentation scores can be updated iteratively to adjust for model shifts. The goal is to achieve similar performance by augmenting a subset of data compared to augmenting the entire dataset. Two metrics are proposed to determine augmentation scores: training loss and model influence. Training loss is obtained by evaluating the loss at a point in the training set, allowing for extension beyond SVMs. Model influence is explored through Leave-One-Out (LOO) influence, measuring the impact of removing a data point on its own loss. The influence of upweighting a point on the loss at a test point is defined. The Hessian of the loss with respect to \u03b8 at the minimizer is denoted as H\u03b8. The influence of upweighting a point on the loss at a test point is defined as DISPLAYFORM0. LOO influence can be calculated as DISPLAYFORM1 when the test point is z. Augmentation scores focus on the magnitude of LOO influence. Model influence is measured across CIFAR10 and NORB datasets. Values are correlated before and after augmentation, indicating reliability in measuring future impact. Spearman's rank correlations range from 0.5 to 0.97 with p-values less than 0.001. Reweighting is proposed as a motivation. Reweighting individual samples in augmentation involves duplicating selected samples to reweight them with twice the original weight. Post-processing methods can correct class imbalanced sampling by adjusting the logistic regressor's bias. To normalize for reweighting effects, weights of original and augmented samples are divided by the size of the set. More advanced policies for reweighting samples based on their trustworthiness are still being explored. In augmentation, reweighting samples based on trustworthiness is a future research direction. It can impact performance, especially in cases of class imbalance. Updating influence information for augmented data points can help account for model behavior changes during training. This can reduce computation while enabling parallelism. In augmentation, modifying influence calculations can reduce computation and enable parallelism. The technique shows similar behavior to reweighting, with no significant effect on policy performance. This implies that selection metadata may only need to be computed once during augmentation. Detailed results on proposed policies for data subsampling using CNN bottleneck features are provided for MNIST, CIFAR10, and NORB datasets. In this study, the effects of augmentation policies on three datasets (MNIST, CIFAR10, and NORB) are explored using different architectures. Augmentations such as translation, rotation, and crop are applied exhaustively in a deterministic manner to selected samples. Regularization is controlled through cross-validation sweeps for the regularization parameter \u03bb. The study explores augmentation policies on three datasets using different architectures. Regularization is controlled through cross-validation sweeps for the regularization parameter \u03bb. Augmentations are applied using Imgaug and Keras CNN implementations. Different policies for augmentation are tested, including sampling points based on loss or influence value. The study explores augmentation policies on three datasets using different architectures. Regularization is controlled through cross-validation sweeps for the regularization parameter \u03bb. Augmentations are applied using Imgaug and Keras CNN implementations. Different policies for augmentation are tested, including sampling points based on loss or influence value. The policies based on loss and influence consistently outperform the random baseline, especially for rotation augmentation across all datasets. The study explores augmentation policies on three datasets using different architectures, with a focus on rotation augmentation. The policies based on loss and influence outperform the random baseline, achieving high accuracy with only a small percentage of the data. Additionally, using a reduced set of points for augmentation can lead to higher accuracy, possibly due to a bias towards harder examples in the dataset. The use of support vectors for augmentation is also explored. The study explores augmentation policies on three datasets using different architectures, with a focus on rotation augmentation. The approach of using support vectors for augmentation was proposed in the Virtual Support Vector literature. Tuning a linear SVM on bottleneck features of the original training set to find VSV points, which are then used as augmentation points for logistic regression. The transfer approach results in strong performance on some tests but is not as reliable as proposed policies in finding the optimal subset of points for transformation. The major limitation is that the augmentation set size is fixed to the number of support vectors. The study investigates the impact of two refinements on augmentation policies: reweighting samples and updating scores during augmentation. Results show that reweighting can have a positive effect on MNIST but may hurt performance on CIFAR10 and NORB. Score updating has a slight positive impact on NORB-rotate but overall matches the original policy's performance. The extra expense of continual model updating for online augmentation is noted. The study concludes that simpler policies are preferable despite the slight positive impact of updating scores. By examining the influence/loss of points in MNIST, it is evident that downsampling benefits diversity and removes redundancy, aiding in learning invariances efficiently. The proposed policies select the most viable subset of points based on training loss and model influence, applicable to various scenarios. The proposed policies for subset selection in machine learning models are based on training loss and model influence. These policies aim to efficiently select a subset of points for augmentation, which can lead to significant improvements in training performance. Future work could explore more advanced subset selection policies that consider second-order information and encourage subset diversity. The implementation details of the experiments include loading the dataset into a NumPy array, running it through a CNN model, training a logistic regression model on the featurized dataset, and measuring loss and influence for each training point. The experiments involve training a logistic regression model on a \"featurized\" dataset and testing it on a \"featurized\" test set. Augmentations are applied to the test set to create a \"poisoned\" test set, causing a performance gap. Augmentations are then applied to the training set to close this gap. A stochastic policy selects points to augment in rounds, with model scores recalculated and optionally retraining the CNN or logistic regression model. Each policy is tested 5 times with 95% confidence intervals shown in plots. The experiments involve testing the accuracy of stochastic policies 5 times with 95% confidence intervals shown in plots. Python, Keras, Tensorflow, Scikit-Learn, AutoGrad, and Imgaug are used for implementation. CNNs are wrapped in Scikit-Learn transformers, and new classes are created for classifiers. Augmentations are performed by Imgaug. Code is publicly available online. Bottleneck features from a CNN are used as inputs for a linear logistic regression model. The experiments involved testing the accuracy of stochastic policies using various models like LeNet, Keras, and ResNet. Different models were used for MNIST, CIFAR10, and NORB datasets, with varying levels of success. The ResNet model showed good performance on CIFAR10 without augmentations, while NORB required retraining with data augmentations for improved results. The ResNet model was retrained with random rotations, shifts, and flips applied to images after high prediction degradation from augmentations. Datasets were converted into binary classification tasks with specific class splits for MNIST, CIFAR10, and NORB. Augmentations included translate, rotate, and crop applied over a range of parameters to generate multiple augmented images. The augmentations applied to MNIST, CIFAR10, and NORB datasets include rotations, shifts, flips, translate, rotate, crop, and zoom. These augmentations aim to preserve labels, but it is possible to construct augmentations that utilize label information for the augmentation itself. Such augmentations are costly and require domain expertise. The AUC results from randomized and deterministic policies are compared for different augmentation methods and score functions. Using the VSV method, SVM margins are used to score points, considering the model mismatch between SVM and logistic regression. Different scoring methods result in uniformly worse performance. The model mismatch between SVM and logistic regression using different scoring methods results in uniformly worse performance. Augmentation with cluster-based stratified sampling is explored, where diversity is enforced to improve the process. Two policies are tested: Baseline Clustered and Random Proportional Influence Clustered. The performance of CIFAR10 k-DPP policies using bottleneck features or a combination of influence and bottleneck features is evaluated. Only 250 augmented points were used due to computational constraints. In the experiments, k-DPP policies using bottleneck features or a combination of influence and bottleneck features were evaluated. Only 250 augmented points were used due to computational constraints. The DPP results showed that \"Influence Bottleneck DPP\" performed better than \"Bottleneck DPP\". The experiments were run multiple times with up to 250 points augmentations. The influence weighted DPP performance is competitive with the influence driven approach. Using solely bottleneck features for L resulted in poor performance. Sampling a DPP takes O(N k^3), which can be limiting for large datasets. In the context of deep learning, performance is expected to scale linearly with an increased dataset. Training a ResNet50v2 network using Tensorflow BID0 version 1.10.1 with a variable number of training examples from CIFAR10 showed scaling performance. The test system had an Intel i7-6700k and Nvidia GTX 1080 with CUDA 9.2 and CuDNN 7.2.1. The test was run 5 times to ensure consistency, showing linear scaling performance. Subsampling led to a linear decrease in training time, observed in MNIST experiments. Using a pretrained model can amortize initial training costs. In CIFAR10 experiments, using SVM support vectors for augmentation achieved close to baseline accuracy with a fraction of the data. Augmenting a subset of the training set can decrease training time without significantly compromising model performance, as shown in experiments with a ResNetv2 model."
}