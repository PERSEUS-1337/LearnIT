{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features X^ci and \"orthogonal\" features X^orth. The former features have a consistent distribution across domains, while the latter features can vary significantly. To prevent adversarial domain shifts, it is ideal to use the \"conditionally invariant\" features for classification. The assumption is made that the domain is a latent variable and not directly observable, making it challenging to observe distributional changes across different domains. In data augmentation, images are generated from an original image, with an ID variable referring to the original image. Only a small fraction of images need to have an ID variable. In data augmentation, images are generated from an original image with an ID variable. The causal framework adds the ID variable to the model and treats the domain as a latent variable. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network with a graph Laplacian improves performance in settings with changing domains. Deep neural networks have excelled in prediction tasks, but issues can arise from dependencies that vanish in test distributions due to domain shifts. Domain shifts can impact predictive performance in machine learning systems when learned representations rely on dependencies that are not present in test distributions. An example is the \"Russian tank legend\" where a system trained to distinguish between Russian and American tanks based on image quality failed in practice due to sampling biases. Hidden confounding factors, like image quality, can lead to indirect associations in deep learning, highlighting the need for large sample sizes to mitigate these effects. Large sample sizes are crucial in deep learning to average out confounding factors and achieve invariance to known factors like translation and rotation through data augmentation. Adversarial examples, imperceptibly perturbed inputs misclassified by ML models, highlight the difference between human and artificial cognition. The goal is to mimic human ability to learn invariances from few instances and align DNN features with human cognition, considering fairness and discrimination in controlling input data characteristics. Counterfactual regularization (CORE) is proposed to control latent features extracted by an estimator from input data, aiming to address biases in ML algorithms caused by dataset replication. The approach categorizes latent factors into 'conditionally invariant' (core) and 'orthogonal' (style) features, emphasizing the use of core features for stable and coherent classification. CORE aims to create an estimator that is invariant to style features, ensuring robustness against domain shifts. It leverages knowledge of grouping within datasets to reduce the need for data augmentation and improve predictive performance in small sample size settings. The manuscript discusses motivating examples, related work, introduces counterfactual regularization, and evaluates CORE's performance in various experiments. In \u00a75, the performance of CORE is evaluated in various experiments using the CelebA dataset BID26, which contains face images of celebrities. The task involves classifying whether a person wears glasses, with grouping information used to ensure the same prediction for all images of the same person. The training set includes n = 10 identities, resulting in a total sample size of m = 321. Examples from the dataset are shown in FIG0, illustrating the grouping-by-ID approach. The CelebA dataset and augmented MNIST dataset are compared using counterfactual examples with the same ID realization. Exploiting grouping information reduces test error by 32% in CelebA and 50% in augmented MNIST. The average test error is reduced from 24.76% to 16.89% by utilizing the group structure. CORE can make data augmentation more efficient by generating additional samples through interventions on style features. Using CORE for data augmentation involves generating additional samples through interventions on style features, resulting in invariance of the estimator with respect to the transformations of interest. By enforcing invariance with grouping information, the average test error on rotated examples is reduced from 32.86% to 16.33%. This approach is similar to Domain-Adversarial Neural Networks (DANN) and other related works, but requires grouped observations for improved performance. The approach proposed in BID13, inspired by BID5, focuses on learning a representation without origin information through adversarial training. In contrast to BID14, which identifies independent features by minimizing MMD distance between domains, our approach assumes different realizations of the same object under interventions. The key difference lies in the data basis used, with BID14 having an observable domain identifier while ours is latent. Our approach differs from BID14 as we utilize a latent domain identifier instead of an observable one. We penalize the classifier using only conditionally independent features to guard against adversarial domain shifts in image classification. Causal modeling offers valid predictions even under large interventions on predictor variables, but challenges arise in anti-causal classification tasks and guarding against style shifts. The challenge in guarding against style shifts lies in leveraging causal inference for deep learning. Various methods have been proposed, such as the Neural Causation Coefficient (NCC) for estimating causal relations between image features. Different from other approaches, our focus is on anti-causal prediction and non-ancestral interventions on style variables. The use of generative neural networks for cause-effect inference and identifying v-structures in graphs. Different approaches like combining penalties with estimated probabilities for causal features. Connections between GANs and causal generative models, proposing causal implicit generative models for sampling from conditional and interventional distributions. Deep latent variable models and proxy variables are used for individual estimation. The use of deep latent variable models and proxy variables to estimate individual treatment effects and characterize fairness considerations in machine learning through causal reasoning. Algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, showing structural similarity to disentangling factors of variation in generative modeling. Matsuo et al. (2017) aim to learn a latent representation that excludes specific style features, such as location, image quality, posture, brightness, background, and contextual information. They address a confounding situation where style features vary based on class, utilizing grouped observations in a variational autoencoder framework to separate style and content. The text discusses a classification task without explicitly estimating latent factors, using a causal graph to compare adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. It describes the standard notation for classification and the prediction function for regression and classification tasks. The goal is to minimize the expected loss in training samples. The text discusses penalized empirical risk minimization for parameter estimation in a classification task. The model includes latent variables and an ID variable that can change based on the class label. The prediction is anti-causal, with predictors non-ancestral to the class label. The class label is causal for the image, with latent variables X ci and X \u22a5 mediating the effect. Interventions \u2206 can affect X \u22a5 but not X ci. The style features X \u22a5 and Y are confounded by the latent domain D, while X ci is conditionally independent of D given Y. The style variable includes various image attributes and can be context-dependent. The style intervention variable influences both the latent style and the image. Using potential outcome notation, the prediction under the style intervention is of interest in guarding against adversarial domain shifts. The intervention magnitude is typically assumed to be within a ball around the origin, with imperceptible changes in the input potentially causing significant output changes. The goal is to devise a classification that minimizes adversarial loss by considering interventions on style features that can cause misclassification. Adversarial domain shifts involve strong interventions on style features that can only change certain aspects of the image. This contrasts with domain adversarial neural networks where \"adversarial\" refers to the training procedure. The goal is to protect against shifts in test data distribution by distinguishing between core and style features. Causal inference involves observing the health outcome under different treatments, but we can never observe both simultaneously. Counterfactuals are situations where we keep class label and ID constant but allow style values to change. Counterfactuals involve keeping class label and ID constant while allowing style values to change, similar to treatments in causal inference. In image analysis, counterfactuals allow us to see the same object under different conditions. The style intervention \u2206 represents variables that determine different images of the same object. It is used to rule out parts of the feature space for classification rather than measuring treatment effects. The style intervention \u2206 is used to rule out parts of the feature space for classification, not to measure treatment effects. The pooled estimator treats all examples identically by summing over the loss, with a penalty parameter choice such as a ridge penalty. The adversarial loss of the pooled estimator may be infinite. The pooled estimator uses a ridge estimator with a cross-validated penalty parameter. The adversarial loss may be infinite. Conditions (i) and (ii) ensure the estimator works well in terms of adversarial loss. To minimize the loss, f \u03b8 (x(\u2206)) should be constant for all x \u2208 R p. The invariant parameter space I is based on core features x ci \u2208 R p. The invariant parameter space I is defined as a function of core features x ci \u2208 R p. The adversarial loss remains the same under interventions for all \u03b8 \u2208 I. To approximate the optimal invariant parameter vector, empirical risk minimization is used with an empirically invariant space I n. The regularization constant \u03c4 \u2265 0 defines variations in class label predictions across counterfactuals. The true invariant space I is a subset of the empirically invariant subspace I n. The graph Laplacian regularization penalizes variances \u03c3 2 i (\u03b8) induced by the identifier variable ID in the sample space. The regularization constant \u03c4 defines variations in class label predictions across counterfactuals. The graph Laplacian regularization penalizes variances induced by the identifier variable ID. The outcome is not strongly dependent on the penalty \u03bb value, and defining the graph in terms of the identifier variable ID is crucial for guarding against adversarial domain shifts. Experiments show that other regularizations do not perform as well in this aspect. The adversarial loss for the pooled and CORE estimator in logistic regression is analyzed, with the CORE estimator converging to the optimal adversarial loss as n approaches infinity. The CORE estimator converges to the optimal adversarial loss as n approaches infinity. Various experiments were conducted to assess CORE's performance with confounded training data and changing style features. Additional experiments included classifying elephants and horses based on color, gender, wearing glasses, and brightness. Experimental results for the settings introduced can be found in subsequent sections. The value of the tuning parameter \u03c4 or penalty \u03bb in Lagrangian form is discussed, with performance shown to be insensitive to the choice of \u03bb. An open question remains on setting the tuning parameter. In experiments, the performance of the CORE estimator is shown to be insensitive to the choice of tuning parameter \u03bb. Synthetic stickmen images are used to differentiate between adults and children based on height, with a hidden common cause affecting movement. The model's reliance on this dependence can lead to failures when presented with different scenarios. Test sets with intervened variables show the impact of removing the dependence between variables. In test sets 2 and 3, interventions are made to remove the dependence between variables X and Y. Movements are heavier in test set 3 compared to test set 2 for both children and adults. Results show that CORE outperforms the pooled estimator with as few as 50 counterfactual observations. Including more counterfactual examples would not improve the pooled estimator's performance due to bias. The CelebA dataset is used for classifying adults and children based on movement. The CelebA dataset is used to classify whether a person in an image is wearing eyeglasses. An intervention is made to change the image quality based on whether the person is wearing glasses. Counterfactual observations are created by sampling new image quality values. Different test sets show misclassification rates for CORE and the pooled estimator. In the CelebA dataset, interventions are made to change image quality based on whether a person is wearing glasses. Test sets with varying image quality distributions show that the pooled estimator outperforms CORE on test set 1 but struggles on test sets 2-4 due to using image quality as a predictor. CORE's performance is less affected by changing image quality distributions. In the \"Animals with attributes 2\" dataset, the study aims to assess if CORE can exclude color from its learned representation by including counterfactual examples of different colors for elephants. Counterfactuals are only available for one class, with a subtle shift in color, and the total sample size is 1850. Misclassification rates for CORE and the pooled estimator on different test sets are compared using examples from the training set. The study includes a total sample size of 1850 and compares misclassification rates for CORE and the pooled estimator on different test sets using examples from the training set. Test sets vary in color distribution, with CORE showing better performance than the pooled estimator on sets with color modifications. The CORE estimator is less affected by changing color distributions and can recognize colored elephants by including counterfactual examples of different colors. The CORE estimator aims to achieve color invariance by demanding prediction consistency for instances of the same object, such as elephants. It distinguishes core features from style features in images and uses counterfactual regularization to ensure robustness to interventions on style features. This approach satisfies fairness by not including color as a learned representation, unlike the pooled estimator which may use color for decisions. The CORE estimator aims to achieve invariance of classification performance with respect to style features like image quality or color. It works despite sampling biases and can achieve the same performance as data augmentation approaches with fewer instances. Regularization of CORE penalizes features that vary strongly between instances of the same object. Larger models like Inception or ResNet can be used for this purpose. The CORE estimator aims for classification invariance to style features like image quality or color. It can achieve similar performance as data augmentation with fewer instances. Using larger models like Inception or ResNet is beneficial for this purpose. Future directions could involve using video data for grouping and counterfactual regularization to potentially debias word embeddings. The core features Xci are conditionally invariant, with logistic regression used for predicting Y from image data X. Training involves estimating \u03b8 with logistic loss, with expected losses on test data including standard logistic loss and loss under adversarial style interventions. The second loss involves adversarial style interventions on X \u22a5 with large interventions allowed. Assumptions include sampling \u2206 from a distribution in R q, matrix W having full rank q, and c \u2265 q counterfactual examples in the samples. Sampling process involves collecting n independent samples and selecting c = m \u2212 n samples with redrawn values of \u2206. The pooled estimator has infinite adversarial loss for the CORE estimator, with probability 1 with respect to the training data. The oracle estimator constrained to be orthogonal to the column space of W implies W t\u03b8pool = 0 with probability 1. The pooled estimator has infinite adversarial loss for the CORE estimator, with probability 1 with respect to the training data. If W t\u03b8pool = 0, then \u03b8 pool = \u03b8 * and the directional derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8 * . The derivative g(\u03b4) of L n (\u03b8) in direction of \u03b4 is proportional to x i,j \u2208 R p. The oracle estimator \u03b8 * is identical under the true training data and the counterfactual training data x(0). The model assumptions state that the eigenvalues of WtW are positive. The estimator \u03b8* remains the same regardless of training on original or counterfactual data. Interventions \u2206i,j are drawn from a continuous distribution. The left hand side of the equation has a continuous distribution, and the probability of it not being 0 is 1. With probability 1, \u03b8core = \u03b8* where \u03b8* is defined as in (6). The invariant space for this model is the linear subspace I = {\u03b8 : Wt\u03b8 = 0}. The estimator \u03b8* remains unchanged whether trained on original or counterfactual data. The linear subspace I = {\u03b8 : Wt\u03b8 = 0} is the invariant space for the model.\u03b8* is in I, ensuring (y, x(\u2206)) = (y, x(0)). The proof involves uniform convergence of Ln to the population loss L(0) under assumed sampling. The CelebA dataset is used to classify gender based on images, creating confounding by including mostly men wearing glasses. Counterfactuals are used to assess results, with different test sets showing varying associations between gender and glasses. A comparison is made between training a four-layer CNN and using Inception V3 features with retraining the softmax layer. The study compares training a four-layer CNN with using Inception V3 features and retraining the softmax layer. Results show that as the number of counterfactual examples increases, the performance difference between CORE and the pooled estimator decreases. The analysis focuses on a confounded setting where the hidden common cause of variables indicates whether the image was taken outdoors or indoors, affecting the classification of whether the person in the image is wearing eyeglasses. The study compares training a four-layer CNN with using Inception V3 features and retraining the softmax layer. Results show that as the number of counterfactual examples increases, the performance difference between CORE and the pooled estimator decreases. The analysis focuses on a confounded setting where the hidden common cause of variables indicates whether the image was taken outdoors or indoors, affecting the classification of whether the person in the image is wearing eyeglasses. In test sets, the brightness intervention affects the image's appearance based on whether the person wears glasses or not. The pooled estimator performs better on test set 1 but struggles on test sets 2 and 4 due to differences in brightness distributions, while CORE's performance remains stable across all test sets. The study compares the performance of CORE and pooled estimator in different counterfactual settings. Counterfactual setting 1, where only brightness varies, yields the best results. Setting 2, using different images of the same person, poses challenges due to varying factors. Even grouping images of different persons can improve predictive performance. The study compares the performance of CORE and pooled estimator in different counterfactual settings, with brightness as the key factor. The tuning parameter \u03c4 or penalty \u03bb is discussed, showing that performance is not very sensitive to \u03bb. Results vary based on the number of identities included in the training dataset, with CORE improving predictive performance, especially with small sample sizes. The study compares the performance of CORE and pooled estimator in different counterfactual settings, with brightness as the key factor. Results show that as sample sizes increase, the performance of CORE and pooled estimator become comparable. Data augmentation is shown to be more efficient with CORE, leading to lower misclassification rates on rotated digits. The experiment in \u00a75.1 compares the performance of CORE and pooled estimator with different numbers of counterfactual examples. The CORE estimator shows consistent results for c \u2208 {50, 500, 2000}, indicating insensitivity to the number of examples. The pooled estimator struggles with predictive performance on test sets 2 and 3, using \"movement\" as a predictor for \"age\". Counterfactual setting 1 performs the best, with small differences between settings 2 and 3. There is a notable performance gap between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator, possibly due to insufficient image quality predictiveness. The study implemented models in TensorFlow and used the Adam optimizer for training. Experimental results were based on training each model five times to assess variance. Training data was shuffled in each epoch to ensure mini batches contained counterfactual observations. Mini batch size was set to 120, making optimization more challenging for small c values."
}