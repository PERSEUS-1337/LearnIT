{
    "title": "HJxhUpVKDr",
    "content": "In the context of multi-task learning, neural networks with branched architectures are used to jointly tackle tasks. These networks start with shared layers and then branch out into task-specific layers. Prior methods for determining layer sharing have been ad hoc or expensive. This paper proposes a principled approach to automatically construct branched multi-task networks based on task affinities. The approach generates architectures with task-agnostic shallow layers and task-specific deeper layers, optimizing for a given budget. Experimental results show that this method consistently produces high-performing networks. Deep learning researchers have developed multi-task networks with branched architectures to improve generalization and processing efficiency. These networks share early processing layers, inspired by biological data processing strategies. Experimental analysis shows that this method consistently yields high-performance networks with the least amount of learnable parameters for a given budget. Multi-task networks offer advantages over separate neural networks for individual tasks, such as lower memory footprint, faster inference speed, and potential performance improvements. Designing multi-task networks involves challenges in determining shared layers among tasks, leading to a complex process of defining optimal architecture. Resorting to neural architecture search techniques may not be a feasible solution due to the joint optimization required for layer sharing. Researchers have explored alternatives like routing, stochastic filter grouping, and feature partitioning for multi-task networks to avoid the complexity of joint optimization. Previous works on hard parameter sharing shared initial layers, leading to suboptimal task grouping and negative transfer. A novel approach is proposed to determine the degree of layer sharing between tasks automatically, eliminating the need for manual exploration. The novel approach proposed aims to automate the determination of layer sharing between tasks based on task affinity levels. This approach uses representation similarity analysis (RSA) to measure task relatedness by computing correlations between models pretrained on different tasks. Task clustering algorithm groups similar tasks together in common branches, creating a branched multitask network in a fully automated manner. The task clustering algorithm groups similar tasks in common branches, reducing negative transfer between tasks. It allows trading network complexity for task similarity and outperforms in multi-task performance vs computational resources. In the deep learning era, MTL models can be classified as utilizing soft or hard parameter sharing. Soft parameter sharing involves each task having its own parameters with a feature sharing mechanism, while hard parameter sharing divides parameters into shared and task-specific ones. Soft parameter sharing MTL networks are limited in scalability, growing linearly with the number of tasks, while hard parameter sharing MTL models are often based on selective sharing of parameters. In hard parameter sharing for MTL models, the parameter set is divided into shared and task-specific parameters. Various approaches have been proposed, such as multilinear relationship networks and hierarchical networks. Branched multi-task networks determine layer sharing based on task affinities, unlike ad hoc approaches. This method is similar to fully-adaptive feature sharing but dynamically grows the model based on task requirements. Neural architecture search (NAS) aims to automate network architecture construction. Different algorithms vary in search space, strategy, and performance estimation. Most NAS works focus on task-specific models, but a new method clusters tasks based on feature affinity scores, determines the tree structure offline, and achieves better results, especially on challenging datasets like Taskonomy. Neural architecture search (NAS) focuses on automating network architecture construction. Recent works explore evolutionary search and alternative methods like routing, filter grouping, and feature partitioning to reduce computation costs. Transfer learning is used to measure task affinity levels. Our work focuses on transfer learning to measure task affinity levels. We use RSA to compute correlations between models pretrained on different tasks and compare the usefulness of different feature sets for solving a particular task. Challenges in jointly learning multiple tasks include properly weighting loss functions, which can be addressed through methods like homoscedastic uncertainty, gradient normalization, and dynamic task prioritization. In this paper, the focus is on adapting gradient magnitudes in the network for multi-task learning. Different approaches such as dynamic task prioritization and modulation modules have been proposed to address challenges in jointly learning multiple tasks. The method aims to find an effective task grouping for shared layers in the network architecture. The proposed method in this paper focuses on adapting gradient magnitudes in the network for multi-task learning by finding an effective task grouping for shared layers in the network architecture. It derives task affinity scores at various locations in the encoder to construct a branched multi-task network within a computational budget. The proposed method focuses on adapting gradient magnitudes in the network for multi-task learning by finding an effective task grouping for shared layers. It calculates correlation between RDM matrices to create a branched multi-task network within a computational budget. Task affinity scores are derived at various encoder locations using RSA technique. The technique assesses task affinity in the sharable encoder to assign tasks in a branched multi-task network based on computational budget. Task affinity scores are calculated by training single-task models for each task using a shared encoder and task-specific decoder. Operations specific to each task are included in the decoder. The technique involves calculating task affinities in the sharable encoder by comparing dissimilarity matrices of single-task networks trained under the same conditions. This is done by selecting D locations in the encoder and creating a task affinity matrix. The dissimilarity scores between feature representations are calculated using a subset of images, resulting in a tensor of dissimilarity scores. The technique involves calculating task affinities in the sharable encoder by comparing dissimilarity matrices of single-task networks. RDMs are symmetrical with a diagonal of zeros, and Spearman's correlation coefficient is used to measure similarity. The method focuses on features used to solve tasks rather than task examples' difficulty. The task affinity tensor is derived based on computational budget and shared layers in the encoder. The text discusses how layers in a sharable encoder should be shared among tasks. Each layer is represented as a node in a tree structure, with task-specific decoders at the leaves. The goal is to separate dissimilar tasks by assigning them to separate branches, minimizing task dissimilarity scores. This approach differs from prior work by focusing on task affinity and dissimilarity. The network minimizes task dissimilarity scores by clustering tasks offline and selecting the tree that minimizes dissimilarity. The task dissimilarity score of a tree is defined as the average maximum distance between dissimilarity scores of elements in each cluster. This global approach contrasts with the local minimization of task dissimilarity in prior work. The network minimizes task dissimilarity scores by clustering tasks offline and selecting the tree that minimizes dissimilarity. A top-down approach is proposed for deriving the tree, starting at the outer layer and using spectral clustering for groupings. The method is evaluated on various multi-tasking datasets, including the Cityscapes dataset for urban scene understanding. The study focuses on urban scene understanding using a dataset of real images from Central European cities. Tasks include semantic segmentation, instance segmentation, and monocular depth estimation using a ResNet-50 encoder with dilated convolutions. Results are obtained after a grid search on hyperparameters for fair comparison. Task affinity decreases in deeper layers of the model, and performance of task groupings is compared with other approaches. The study focuses on urban scene understanding tasks using a ResNet-50 encoder with dilated convolutions. Task affinity decreases in deeper layers of the model, and performance of task groupings is compared with other approaches. The method generates specific task groupings based on computational budget, achieving higher performance within the budget. The proposed method selects the best performing task grouping in most cases. The proposed method selects the best performing task grouping within a fixed budget, comparing it with other approaches like cross-stitch networks and NDDR-CNNs. It strikes a balance between performance and number of parameters, offering a middle ground between baseline multi-task models and more complex architectures. The Taskonomy dataset is used, containing diverse tasks such as scene categorization, semantic segmentation, edge detection, monocular depth estimation, and keypoint detection. The study utilizes the Taskonomy dataset with diverse tasks like scene categorization, semantic segmentation, edge detection, monocular depth estimation, and keypoint detection. The architecture is based on ResNet-50 with a fully convolutional decoder for pixel-to-pixel prediction tasks. Task affinity is measured after each ResNet block, and the task groupings are compared against a method from Lu et al. (2017). Results are presented in Table 2, showing the impact of the task grouping technique on model performance. The study compares the performance of different task grouping techniques on multi-task models, showing that branched multi-task networks outperform other models in handling diverse tasks. The approach separates dissimilar tasks to limit negative transfer, resulting in consistent performance across various scenarios and datasets. This is a novel finding compared to existing approaches tailored for specific tasks. The study compares task grouping techniques on multi-task models, showing branched networks outperform others. The approach separates dissimilar tasks to limit negative transfer, resulting in consistent performance across scenarios and datasets. The Ours-Thin-32 and Ours-Thin-64 architectures optimize task clustering for different parameter budgets. Various models are compared based on accuracy and parameters. The CelebA dataset contains over 200k real images of celebrities labeled with 40 facial attribute categories. The training, validation, and test sets contain 160k, 20k, and 20k images respectively. Each facial attribute prediction is treated as a binary classification task. The study uses a thin-\u03c9 model from prior work and sets the parameter budget to ensure a fair comparison. Beam search adaptation is employed for optimization due to the large number of tasks, with n = 10 for grouping at each layer. Our branched multi-task networks outperform earlier works on the CelebA dataset, showing superior performance compared to models with similar parameters. The Thin-32 model achieves results comparable to the VGG-16 baseline while using significantly fewer parameters. Additionally, the Thin-64 model outperforms the ResNet-18 model with a uniform loss weighing scheme and matches the state-of-the-art ResNet-18 model with a specific loss weighing scheme. In this paper, a principled approach to automatically construct branched multi-task networks for a given computational budget is introduced. The method leverages tasks' affinities for layer sharing, optimizing only layer sharing without the need to jointly optimize layer types and connectivity. Experimental analysis shows that the proposed approach outperforms existing methods in multi-tasking performance vs number of parameters across various scenarios and datasets. Additionally, the MTAN model was re-implemented using a ResNet-50 backbone, based on the Wide-ResNet architecture, after extensive hyperparameter tuning. After re-implementing the MTAN model with a ResNet-50 backbone and extensive hyperparameter tuning, meaningful results were not achieved on the Cityscapes dataset for all three tasks jointly. Results were only shown for semantic segmentation and monocular depth estimation. The architecture used a ResNet-50 encoder with modifications in the last stride 2 convolution. A 15-layer fully-convolutional decoder with ReLU non-linearity and batch normalization was employed. Kaiming He's initialization was used for both encoder and decoder. L1 loss was used for depth, edge detection, and keypoint tasks, while KL-divergence loss was used for scene categorization. Performance on scene categorization was reported. The multi-task models were optimized with task weights for different tasks. Heatmaps were rescaled and depth maps were normalized during training. Single-task models used an Adam optimizer with specific parameters. The baseline multi-task model followed the same optimization procedure as single-task models. Branched multi-task models were generated using a specific method. The architectures generated by the method are shown in Fig. 4 and Fig. 5 displays architectures from another method. Predictions from a branched multi-task network are shown in Fig. 6 for qualitative evaluation. The CNN architecture is based on the VGG-16 model with specific hyperparameter settings. The model is trained using stochastic gradient descent with momentum and specific batch size and weight decay. The model is trained using stochastic gradient descent with momentum, a batch size of 32, weight decay of 0.0001, and a learning rate of 0.05. Training lasts for 120000 iterations with the learning rate decreasing by a factor of 10 every 40000 iterations, using a sigmoid cross-entropy loss function with uniform weighing."
}