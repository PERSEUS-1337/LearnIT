{
    "title": "SJfb5jCqKm",
    "content": "We develop an uncertainty estimation algorithm for deep neural classification that addresses biased estimates for highly confident predictions. The algorithm selectively estimates uncertainty using earlier model snapshots, outperforming known methods in extensive experiments. This is crucial for applications like autonomous driving and medical diagnosis. The Bayesian framework offers a principled approach to infer uncertainties from a model, but there are computational hurdles in implementing it for deep neural networks. Current uncertainty estimation methods for deep learning, such as raw softmax response, entropy, embedding layers, and MC-dropout, have shown effectiveness, but no conclusive evidence on their relative performance has been reported. An ensemble of softmax response values from multiple networks has been documented to perform better in uncertainty estimation. The paper presents a method of confidence estimation to improve deep learning models' performance. It addresses the issue of erroneous confidence estimates in deep classifiers, especially for highly confident instances. The approach is based on the observation that confidence estimates tend to be inaccurate, particularly for \"hard\" points during optimization. The method aims to provide uncertainty estimates in terms of probabilities, building on previous research on uncertainty estimation methods for deep learning. The paper proposes methods to improve confidence estimation in deep learning models by addressing inaccurate confidence estimates, particularly for difficult instances. It focuses on ranking uncertainties and probability calibration, aiming to provide more accurate confidence scores for classification tasks. The paper introduces methods to enhance confidence scoring functions for deep neural networks by improving uncertainty estimation through early stopped model selection and formulating a performance measure based on selective prediction concepts. Extensive experiments show consistent improvements over baseline methods, validated using calibrated uncertainty estimates. The paper introduces methods to improve uncertainty estimation in deep neural networks by utilizing calibrated uncertainty estimates. It focuses on enhancing confidence scoring functions through early stopped model selection and selective prediction concepts, showing consistent improvements over baseline methods in extensive experiments. The paper discusses improving uncertainty estimation in deep neural networks by enhancing confidence scoring functions through early stopped model selection and selective prediction concepts. It introduces a method to define a confidence score function for multi-class models based on signals extracted from the partially trained model. The function should induce a partial order over points in X and reflect true loss monotonicity. In the domain of uncertainty estimation, there is no consensus on measuring performance of ordinal estimators. Different approaches like Brier score, negative-log-likelihood, and area under the ROC curve have been used. A unitless performance measure for \u03ba functions is proposed in this section, borrowing elements from other approaches. Selective classification based on confidence score functions is discussed as a natural application for assessing performance. The section introduces the concept of selective classifiers and proposes a performance measure called \"excess AURC\" (E-ARUC) based on the area under the risk-coverage curve. It discusses the quantification of performance using coverage and risk measures, providing a framework for evaluating \u03ba functions in uncertainty estimation. The selective risk and empirical evaluation of classifiers can be measured using the risk-coverage curve (RC-curve). The performance measure is defined in terms of a selective classifier and selection functions, with the area under the (empirical) RC-curve (AURC) used as a performance metric. The performance of \u03ba is defined by the area under the RC-curve of the pair DISPLAYFORM6. A better \u03ba leads to a better selective classifier that rejects misclassified points faster. The RC-curve is calculated with an independent labeled set, showing monotonically increasing selective risk with coverage. The risk corresponding to coverage = 0.5 is approximately 0.06 for a selective classifier that rejects half of the points with least confidence. An optimal function \u03ba* yields the optimal risk coverage curve, with zero selective risk at rates below 1 - r(f|Vn). The RC-curve based on \u03ba* reaches zero at coverage of 0.71, with selective risk at full coverage being 0.29. The AURC of \u03ba* reaches zero at coverage of 0.71, with selective risk at full coverage being 0.29. A unitless performance measure is obtained by normalizing AURC values. The Excess-AURC (E-AURC) is defined as the difference between AURC values, with the optimal \u03ba having E-AURC = 0. The focus is on non-Bayesian methods in deep neural classification, with the Monte-Carlo dropout (MC-dropout) technique proposed for uncertainty estimation in DNNs. In deep neural classification, BID9 introduced the Monte-Carlo dropout (MC-dropout) technique for uncertainty estimation in DNNs. Confidence scores for DNNs are commonly obtained by measuring the classification margin, where large values indicate high confidence levels. BID4 and BID5 first proposed this approach for neural networks, which has shown better performance than MC-dropout on ImageNet. Recently, a K-nearest-neighbors (KNN) algorithm applied in the embedding space of a DNN was introduced by BID20, using KNN-distances as proxies for class-conditional probabilities. This is the first non-Bayesian method to estimate neural network uncertainties using activations from non-final layers. A new ensemble-based uncertainty score for DNNs was proposed by BID17, which requires large computing resources for training. BID14 suggested constructing an ensemble of snapshots during training to improve predictive performance with only one model. BID15 also proposed averaging weights across SGD. In contrast to previous ensemble methods proposed by BID15 and BID17, our technique utilizes \"premature\" ensemble members for improved classification performance. We illustrate this approach with an example involving a deep classification model trained over epochs on a set S m, monitored using an independent validation set V n. This allows us to assess the quality of softmax response values during the training process. The study focuses on the quality of softmax response values for unseen test points, comparing the confidence levels of the highest and lowest percentiles. It is noted that the model's predictions for confident points stabilize earlier in training compared to less confident points. The study shows that green points stabilize at their maximal values around Epoch 80, with early correct classification near Epoch 25. Red points continue to improve confidence scores throughout training. Intermediate model f 130 can estimate green points' confidence well, while the final model f [T] is one of the worst estimators for green points. In contrast, red points' confidence estimates improve monotonically, with the final model f [T] being the best estimator for red points. The study shows that green points stabilize at their maximal values around Epoch 80, with early correct classification near Epoch 25. Red points continue to improve confidence scores throughout training. The best estimator for red points is the final model f [T]. The learning of uncertainty estimators for easy instances resembles overfitting, where higher confidence points degrade as training continues. An algorithm is proposed for early stopping in uncertainty estimation. The Pointwise Early Stopping (PES) algorithm for confidence scores operates by extracting the most uncertain points from a training set at each iteration, finding the best model in a set of intermediate models, and using a confidence score function to determine the best model. The algorithm extracts the most uncertain points from a training set, finds the best model, and uses a confidence score function to determine the best model. It produces a partition of layers based on confidence levels, with each layer having the best performing model. The confidence rate for a point at test time is inferred by searching for the minimal i that satisfies a certain condition. The Averaged Early Stopping (AES) is a simplified version of the PES algorithm, aiming to address its computational complexity and the need for additional labeled examples. By averaging the confidence scores of intermediate models saved during training, AES leverages the observation that \"easy\" points are learned earlier. This approach has shown to work well, providing a more efficient alternative to the PES algorithm. The AES algorithm, a simplified version of the PES algorithm, is used in experiments due to its computational efficiency. Results of AES applied to various confidence scores are presented for different image datasets. Performance evaluation is done with k values of 10, 30, and 50, and t set to 0.4T. Results are reported in Table 4, showing E-AURC values for different baseline methods across four datasets. The E-AURC measure quantifies the difficulty level of learning problems, with CIFAR-10 and SVHN considered relatively easy, CIFAR-100 harder, and Imagenet the hardest. E-AURC values reflect this, supporting its usefulness as a measure. Our method consistently outperformed the baseline in 39 out of 42 experiments, with AES improving E-AURC values. The ensemble estimation approach by BID17 is currently the state-of-the-art for each dataset, and applying AES further enhances performance. In CIFAR-10, the softmax response method is the best single-classifier, with AES improving its E-AURC by 6%. Notably, AES also significantly improves the poor performance of NN-distance in this dataset. In the dataset, AES significantly improves the performance of NN-distance, bringing it on par with the best methods. For CIFAR-100, AES enhances the top method NN-distance by 22%. The AES method, when applied with Platt scaling, shows consistent results in terms of negative log-likelihood and Brier score across various datasets. Table 2 shows that probability scaling results align well with raw uncertainty estimates, indicating improved calibrated probabilities. The PES algorithm was implemented with the softmax response method on various datasets. Table 3 displays E-AURC values and % improvement for PES compared to SR on CIFAR-10, CIFAR-100, and SVHN. Time complexity for PES over NN-distance is nmT k, and for MC-dropout is O(dT C f (V n). The PES algorithm was applied to various datasets, reducing the E-AURC of softmax significantly, with the best improvement seen on CIFAR-100. Challenges were faced when applying PES to different confidence methods, motivating further research for algorithm enhancement. Novel uncertainty estimation algorithms were introduced based on observations during DNN training with SGD. The PES algorithm, along with its approximated version AES, improves estimation techniques by utilizing snapshot models generated during training to overcome confidence score deformations. Future research could focus on developing a loss function to prevent confidence deformations while maintaining high classification performance and incorporating distillation to reduce inference time. The text discusses various methods to reduce computational effort during inference time, such as distillation and using a single model per instance based on an early stopping criterion. Different techniques like Softmax Response, NN-distance method, MC-dropout, Ensemble method, and Platt scaling are implemented for uncertainty estimation and improving performance. The text also mentions the decision not to implement certain proposed extensions to avoid degrading performance. Platt scaling is applied to calibrate confidence measures using logistic regression on a validation set. The performance of scaled probabilities is evaluated using negative log likelihood and Brier score. Experiments on AES method show % improvement on CIFAR-10, CIFAR-100, SVHN, and ImageNET compared to baseline. E-AURC values are provided for clarity. In Section 5, the method divides the domain X into \"easy points\" and \"hard points\" based on overfitting observations during training. The strategy extracts information from early training stages to improve uncertainty estimates for easy points. Results show overfitting in all cases, with MC-dropout showing less overfitting compared to softmax \u03ba function. NN-distance also exhibits slight overfitting, affecting hard instances more than easy points. The proposed correction strategy shows potential usefulness in reducing overfitting for MC-dropout and NN-distance methods on CIFAR-100 dataset. The E-AURC values for both methods fluctuate during training, with NN-distance showing more severe effects on hard instances."
}