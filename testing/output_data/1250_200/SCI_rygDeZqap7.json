{
    "title": "rygDeZqap7",
    "content": "Natural language understanding research has shifted towards complex Machine Learning and Deep Learning algorithms, which often outperform simpler models. To address the issue of limited labeled data availability, a methodology for extending training datasets and training data-hungry models using weak supervision is proposed. This methodology is applied to biomedical relation extraction, a task crucial for drug discovery but challenging to create datasets for. Small-scale experiments show consistent performance enhancements of an LSTM network using this method, comparable to hand-labeled data. The increasing number of scientific papers in the biomedical field contains valuable but unstructured information, making it difficult for researchers to access. The increasing number of scientific papers in the biomedical field contains valuable but unstructured information, making it difficult for researchers to access. Efforts towards automating Information Extraction have been made to extract structured information from papers, which can have a significant impact on tasks like drug design and adverse drug effect detection. The focus is on automating semantic triple extraction from biomedical abstracts, specifically on Regulations (CPR) and Chemically Induced Diseases (CID) relations, crucial for drug design and safety. Researchers are working on automating the extraction of structured information from biomedical papers, focusing on Regulations (CPR) and Chemically Induced Diseases (CID) relations. They are using a methodology based on weak supervision to train multiple base learners on a small labeled dataset for relation extraction, aiming to increase the learning algorithm's capacity while minimizing the labor-intensive annotation process. The methodology involves training base learners on a small labeled dataset to predict labels for a larger unlabeled dataset, using a denoiser to derive weak labels, and training a meta-learner with weak supervision. Contributions include a detailed methodology for relation extraction, demonstrating effectiveness in experiments, and investigating denoising methods. The code is released for reproducibility. The related literature covers information extraction, relation extraction from biomedical text, and semi-supervised and ensemble learning methods. The curr_chunk discusses information extraction, relation extraction from biomedical text, and semi-supervised learning methods. It compares unsupervised, fully-supervised, and semi-supervised approaches, highlighting bootstrapping algorithms like DIRPE and other methods like Snowball and KnowItAll. These approaches focus on bootstrapping new data without excessive manual annotation, but do not use a combination of learning algorithms like the current work. The curr_chunk discusses distant supervision for generating weak labels in relation extraction, utilizing Knowledge Bases instead of pre-trained classifiers. It highlights the benefits of this approach on large-scale datasets without requiring human annotation. The work is complementary to distant supervision, as algorithms can serve as weak classifiers in the framework. It also mentions BioCreative competitions in biomedical relation extraction, with recent research showing the effectiveness of Support Vector Machines. The curr_chunk discusses the importance of ensemble methods in improving generalization for relation extraction tasks in the biomedical domain. It also aims to combine the advantages of ensemble techniques with semi-supervised learning, which has not been explored in this context before. Ensemble learning and semi-supervised techniques aim to enhance Machine Learning models by reducing variance and utilizing unlabeled data for better generalization. Combining ensembles with semi-supervised learning can improve performance by providing multiple views and increasing diversity among learners. Previous research has shown that this combination can be beneficial, with co-training being one proposed system for this purpose. Recent research has explored semi-supervised techniques like co-training, tri-training, and co-forest, where multiple learning algorithms leverage unlabeled data to improve performance. These methods have shown success in functional genomics without the need for manually labeled data, outperforming supervised methods trained on thousands of examples. The ensemble system decides whether to add an unlabeled example for re-training, using all learners. Base learners are used to generate weak labels, not for final prediction. Unlike previous methods, all unlabeled data is utilized. Weak supervision and data programming influence this methodology, focusing on training models with questionable quality labels. Weak supervision involves training models with labels of questionable quality using data programming, which creates training sets when no ground-truth labels are available. This process includes defining weak supervision sources, encoding them into Labeling Functions (LF), applying LFs to unlabeled data points, and denoising to derive weak labels close to the true labels using a probabilistic graphical Generative Model (GM). The methodology proposed involves a graphical Generative Model (GM) with trainable parameters for labeling data points, estimating correlations of Labeling Functions, and training a noise-aware discriminative model using weak labels generated through data programming. This approach aims to leverage weak supervision and data programming for semi-supervised learning when ground-truth labels are unavailable. The methodology involves using machine learning models of lower complexity as weak supervision sources to augment a gold-labeled training set. This allows for scaling the dataset size and adapting an already implemented pipeline with little effort. The approach requires a labeled training set, an unlabeled dataset drawn from the same distribution, a validation set for hyperparameter tuning, and a held-out test set for evaluation. Base learners are trained on solving a specific task to maximize individual performance in an ensemble learning scenario. In ensemble learning, 162 base learners are created by varying hyperparameters and design choices in the relation extraction pipeline. Important design choices include sentence pruning to remove irrelevant words and considering sequential features beyond bag-of-words approach. In ensemble learning, 162 base learners are created by varying hyperparameters and design choices in the relation extraction pipeline. Important design choices include sentence pruning to remove irrelevant words and considering sequential features beyond bag-of-words approach. Text representation involves converting the corpus to a numerical representation using token occurrences or TF-IDF weights. Machine learning algorithms such as Logistic Regression, Support Vector Machines, Random Forest Classifiers, Long-Short Term Memory Networks, and Convolutional Neural Networks are employed on the feature matrix. The base learners are selected based on individual performance and diversity to maximize performance. To maximize the performance of ensemble learning, base learners are selected based on individual performance and diversity. A simple method is used to discard classifiers below a performance threshold, while maximizing diversity. A similarity-based clustering method is employed to select the most diverse classifiers, using a KxK similarity matrix constructed from predictions on a validation set. K-means clustering is used to pick representative base learners closest to cluster centroids. The silhouette score is used to determine the appropriate number of clusters and base learners. To enhance ensemble learning performance, base learners are chosen based on individual performance and diversity. A similarity-based clustering method is used to select diverse classifiers, with K-means clustering identifying representative base learners closest to cluster centroids. The silhouette score helps determine the optimal number of clusters and base learners. The selected base learners predict labels for D U, generating a binary prediction matrix. A denoiser reduces the vote matrix to weak labels, with options including Majority Vote and Average Vote denoisers. A discriminative model serves as a meta-learner, trained with weak supervision to balance label quality and quantity for improved performance. In experiments using Snorkel, high-capacity models like Deep Neural Networks are used as meta-learners to learn features from a large, noisy training dataset. The BioCreative CHEMPROT and CDR datasets are utilized, with training, development, and test sets. The methodology requires three gold-labeled datasets and a held-out test set, with the original test sets used for evaluation. The training and development sets are merged and shuffled to create datasets for training base learners and hyperparameter selection. The methodology involves using three gold-labeled datasets and a held-out test set for training base learners and hyperparameter selection. The datasets are structured to ensure no bias in document selection and to control pre-processing steps. SpaCy is used for most text pre-processing tasks. The approach allows for comparing the performance of the meta-learner trained with weak supervision to optimal performance with ground-truth labels. The text pre-processing pipeline involves using SpaCy for tasks such as sentence splitting, tokenization, and dependency parsing. Named Entity Tags are manually annotated for candidate extraction, and Snorkel is used for mapping candidates to ground-truth labels. Entities of interest are replaced with tokens like 'ENTITY1' and 'ENTITY2' for prediction, along with 'CHEMICAL', 'GENE', or 'DISEASE' for additional entities of the same type within the same sentence. A relationship classifier is used to understand natural language interactions. In experiments, a bi-directional Long-Short Term Memory network is used for Natural Language tasks. Randomly initialized word embeddings and under-sampling for class balance are employed. Different hyperparameters are tested, including dropout values and training epochs selected based on validation data. Research questions focus on enhancing biomedical relation extraction with Machine Learning classifiers and optimal settings for weak supervision. Related literature suggests adding weakly labeled data can improve performance. The related literature suggests that adding weakly labeled data can improve the performance of the meta-learner in a setting where the weak supervision sources meet specific requirements. These requirements include having accuracy better than random guess, overlapping and disagreeing enough to estimate accuracy, and capturing diverse 'views' of the problem. However, it is unclear if Machine Learning classifiers can serve as weak supervision sources in this context, raising questions about the availability of a diverse and sufficiently large set of base learners trained on the same dataset. The study evaluates weak supervision's impact on meta-learner performance by conducting experiments with different training setups. It compares full-supervision on D B, weak-supervision on D U, and a combination of both. The optimal number of base learners is crucial, as adding more can sacrifice performance for diversity. The denoising component plays a fundamental role in dictating the method's quality. The study evaluates weak supervision's impact on meta-learner performance by experimenting with different training setups. The denoising component is crucial for dictating the quality of weak labels used for training. Different denoising methods are assessed, producing binary or marginal weak labels with varying distributions. An error analysis is conducted to understand their effect on training and final performance. The optimal number of base learners is selected using a specific strategy, and performance is benchmarked at intervals to maximize silhouette scores. The results are reported in detail in the Base Learners section. The study evaluates weak supervision's impact on meta-learner performance by experimenting with different training setups. Specifically, training the meta-learner with weak labels always performs better compared to training with fewer gold labels. Weak supervision can achieve comparable performance to full supervision, with some cases even showing slightly better results. Weak supervision can achieve comparable performance to full supervision, with some cases even showing slightly better results. Differences in performance are minor and not statistically significant due to high variance on the meta-learners' performance. Undersampling based on weak labels results in a larger training set size for the final learner in weak supervision. Majority Vote often outperforms the meta-learner, but this does not undermine the importance of the results. Learning curves of the meta-learner show an upward trend with statistically significant results. The F1 score on the training set is consistently higher than the test score, indicating high variance in the meta-learner's performance. The results show that the F1 score on the training set is consistently higher than the test score, indicating overfitting in the meta-learner. Additional training data is needed to improve performance. The number of base learners affects performance, with the weak Majority Vote labels showing the lowest F1 score. The performance of the meta-learner improves with more than 10 base learners when trained with Average Vote marginals. Using Generative model marginals also shows a slight improvement in performance. When more than 10 base learners are used, the metalearner performs better compared to when only 5 base learners are used. Generative model marginals slightly improve performance as the number of base learners increases, with exceptions. In most cases, the metalearner achieves the best performance when trained with Average Marginals. GM marginals also improve performance compared to Majority Vote weak labels, except for one case. GM marginals depend on hyperparameters chosen based on F1 score on a validation dataset. Marginal weak labels improve meta-learner performance compared to binary labels. Majority Vote weak labels always perform worse than Average marginals without hyperparameter tuning. Generative Model tends to create marginals following a U-shaped pattern. The Generative Model tends to create marginals following a U-shaped distribution, with Average Vote labels showing higher quality. The F1 score is deemed unsuitable for evaluating weak labels. Training with marginal labels results in higher training error, especially with Average weak marginals. LSTM quickly starts predicting binary training labels after a few epochs. Training a classifier using marginal labels can be seen as a regression problem, where the model predicts an exact number and is penalized for errors. The predicted logits become more spread as the training marginals distributions become more uniform. Applying this methodology on the CPR task with expanded datasets showed a decrease in meta-learner performance with added weakly labeled data, indicating issues with data quality or weak label generation. The meta-learner's performance decreases with additional weakly labeled data, suggesting issues with data quality or weak label generation. A class imbalance of 1:14 on the outgoing citations dataset compared to 1:4 on the original dataset indicates different distributions. Using t-SNE algorithm BID20 and features from the best base-learner, we visualize candidate samples from the original set versus both sets, revealing unsuitability of the new dataset. Weak supervision enhances complex model performance like deep neural networks by utilizing unlabeled data and multiple base learners. The methodology is feasible, defining a combination of base learners that sufficiently model the problem space. Further investigation into constructing appropriate unlabeled datasets is necessary. The proposed methodology is feasible for the task at hand, shifting human effort from hand-labeling to feature engineering and diverse learner construction. It allows for scaling training datasets while consistently improving performance. The method can be reused on similar tasks with appropriate datasets, eliminating the need for repeated hand-labeling. Further exploration is needed to construct large enough unlabeled datasets. Further exploration is crucial to construct a large enough unlabeled dataset for improving metalearner performance and drawing stronger conclusions on research questions. Collecting an appropriate unlabeled dataset is challenging, and semi-supervised algorithms should not assume its existence. Additionally, a more suitable metric than the F1 score is needed for evaluating weak labels. A more suitable metric than the F1 score is necessary for evaluating weak labels. This would eliminate the need for an additional step like training a meta-learner and could improve the Generative Model's performance by optimizing hyperparameters. Further investigation areas include experimenting with the meta-learner and defining a better selection method for Base Learners. It would also be interesting to explore how the system would perform if Base Learners abstained from voting on uncertain examples. This could provide a modeling advantage to the Generative Model compared to unweighted methods like Majority Voting. The Generative Model could gain a modeling advantage through weighted voting, as discussed in an analysis on weak supervision trade-offs."
}