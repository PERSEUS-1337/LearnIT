{
    "title": "rygFWAEFwS",
    "content": "We propose SWAP, an algorithm to speed up DNN training using large mini-batches and weight averaging. The resulting models generalize well on CIFAR10, CIFAR100, and ImageNet datasets. SGD and its variants are common methods for DNN training, with larger mini-batches allowing for faster training. Mini-batches with precise gradient estimates enable higher learning rates and larger reductions in training loss. In a distributed setting, nodes compute gradients on disjoint subsets and average estimates for consensus. Larger mini-batches require fewer updates and synchronization events, leading to good scaling behavior. However, there is a maximum batch size that can result in worse generalization performance. Stochastic Weight Averaging (SWA) produces models with good generalization by averaging weights from final training stages. Stochastic Weight Averaging in Parallel (SWAP) accelerates DNN training by averaging models from independent SGD sequences. It achieves comparable generalization performance to small-batch trained models in a shorter time. SWAP accelerates DNN training by averaging models from independent SGD sequences, achieving comparable generalization performance to small-batch trained models in a shorter time. The mechanism by which training batch size affects generalization performance is still unknown, with theories suggesting that larger mini-batches may lead to getting stuck in sharper global minima. In previous studies, it has been shown that the batch size in deep neural network training can impact generalization performance. Different research works have demonstrated various effects of batch size on model behavior, such as transforming flat minimizers into sharp ones, predicting optimal batch sizes for accuracy, and the relationship between batch size, model updates, and generalization performance. The batch size in deep neural network training impacts generalization performance. Training with large batches for longer times improves generalization performance but takes more time than small batches. Batch size also affects the optimization process, with a critical batch size for convergence. Adaptive batch size methods exist but may require extensive tuning and inefficiently use computational resources. Local SGD is a potential alternative. Post-local SGD is a distributed optimization algorithm that improves generalization by refining models with local-SGD. It achieves significant speedups compared to large-batch training. SWAP, on the other hand, averages models after tens of thousands of updates, different from Post-local SGD which does so after at most 32 iterations. This difference suggests that the mechanisms behind the success of SWAP and Post-local SGD are different in DNN optimization. Stochastic weight averaging (SWA) is a method where models are sampled from later stages of an SGD training run and their weights are averaged to improve generalization properties. SWAP is described as an algorithm in three phases for accelerating DNN training, involving large mini-batch updates, independent refinement of model copies, and different training parameters for workers. The SWAP algorithm involves three phases for accelerating DNN training: large mini-batch updates, independent refinement of model copies, and different training parameters for workers. The last phase includes averaging the weights of resulting models to produce the final output. Phase 1 stops before reaching 100% training accuracy to prevent optimization from getting stuck. Phase 2 involves reducing batch size and performing small-batch training independently and simultaneously. At the end of the training process, each worker will have produced a different model. Figure 1 shows the accuracies and learning-rate schedules for SWAP. In the large-batch phase, all workers share a common model with the same performance. In the small-batch phase, models diverge due to stochasticity. The averaged model performs better than individual models. The mechanism behind SWAP is visualized by plotting the error of the test network on a plane containing the outputs of three algorithm phases. Orthogonal vectors u, v are chosen to span the plane with \u03b8 1 , \u03b8 2 , \u03b8 3 . The loss value of model \u03b8 = \u03b8 1 +\u03b1u+\u03b2v is plotted at (\u03b1, \u03b2). Training and testing errors for the CIFAR10 dataset are shown in Figure 2, with 'LB' marking phase one output, 'SGD' marking a single worker output after phase two, and 'SWAP' marking the final model. The training error level-sets form a convex basin, with 'LB' and 'SGD' lying on the outer edges, while 'SWAP' is closer to the center. The final model 'SWAP' is closer to the center of the basin, less affected by topology changes, and has lower testing errors compared to 'LB' and 'SGD' points. The weight iterates in SGD behave like an Ornstein Uhlenbeck process, reaching a stationary distribution with a constant learning rate. In later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian centered at the local minimum. The covariance grows with the learning rate, inversely proportional to batch size, and depends on the Hessian of the mean loss and gradient covariance. Sampling weights from different SGD runs will choose weights spread out on the surface of an ellipsoid, with the average closer to the center. All runs starting in the same basin of attraction will converge to the same stationary distribution. In later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian centered at the local minimum. All runs starting in the same basin of attraction will converge to the same stationary distribution. SWAP and SWA have an advantage over SGD as they can generate independent samples from the stationary distribution. The cosine similarity between the gradient descent direction and the direction towards the output of SWAP decreases as training progresses, indicating faster progress towards the center of the basin. SWAP's performance was evaluated for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets using mini-batch SGD with Nesterov momentum. In the experiments, the best hyper-parameters were found using grid searches. Training was done using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation was done using cutout, and a custom ResNet 9 model was used. Experiments were run on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for computation distribution. Different settings were used for SWAP phases one and two, with varying batch sizes and epochs. The experiments compared models trained with small-batch only, large-batch only, and both. Training was conducted on 2 GPUs for 100 epochs. Results show that SWAP outperforms both small-batch and large-batch training in terms of test accuracies. SWAP achieves comparable training times to large-batch training while achieving accuracies on par with small-batch training. This approach shows promise in achieving state-of-the-art training speeds for CIFAR10. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch can train CIFAR10 to 94% test accuracy in 27 seconds. SWAP is also used to accelerate a fast-to-train ImageNet model with modified schedules for batch sizes and learning rates. Doubling the batch size reduces test accuracies compared to small-batch runs. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch can train CIFAR10 to 94% test accuracy in 27 seconds. Doubling the batch size reduces test accuracies compared to small-batch runs, but SWAP recovers generalization performance at reduced training times. The results are compiled in Table 3, showing that these accelerations were achieved with minimal tuning. SWAP is compared with SWA using the CIFAR100 dataset, maintaining the same number of models and epochs for both algorithms. In this section, experiments are conducted using the CIFAR100 dataset with SWA and SWAP algorithms. SWA involves sampling models with 10 epochs in-between and averaging them, while SWAP runs 8 independent workers for 10 epochs each and uses their average as the final model. The study aims to see if SWA can recover test accuracy from small-batch training on a large-batch training run. Results show that large-batch training achieves lower accuracy, and SWA does not improve it. Additionally, the effect of executing SWA using small batches after a large-batch training run is evaluated. In this case, small-batch SWA and SWAP are compared in terms of test accuracy and computation time. SWA achieves better accuracy than SWAP with a longer training time. The study aims to explore the speed-up that SWAP achieves over SWA if the precision of SWA is set as a target. SWAP is a variant of Stochastic Weight Averaging (SWA) that improves model generalization performance with large mini-batches. By relaxing constraints and increasing the training cycles, SWAP achieved a test accuracy of 79.11% in 241 seconds, 3.5x faster than SWA. This approach refines models trained with large mini-batches by averaging weights from models trained with small-batches, resulting in good generalization performance in less time. In the initial stages of training, using large mini-batches does not hinder models from achieving good generalization performance. Refining models through methods like SWA or SWAP allows them to perform as well as models trained with small-batches. Visualizations show that averaged weights are closer to the center of a training loss basin. The choice of the transition point between large and small batches is a key hyperparameter that requires further investigation. In future work, the transition point between large and small batches in SWAP will be further investigated. The design of SWAP allows for the substitution of optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad. Experiment parameters included momentum and weight decay constants, with remaining hyperparameters listed in Tables 5 and 6. Stopping accuracy of 100% indicates maximum epochs were used."
}