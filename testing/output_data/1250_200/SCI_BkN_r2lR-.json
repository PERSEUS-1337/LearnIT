{
    "title": "BkN_r2lR-",
    "content": "Identifying analogies across domains is a key task for artificial intelligence. Recent advances in cross-domain image mapping focus on translating images, but visual fidelity often falls short. This paper introduces AN-GAN, a matching-by-synthesis approach that outperforms current techniques in finding exact analogies between datasets. The cross-domain mapping task involves domain alignment and learning the mapping function, which can be iteratively solved to improve unsupervised translation quality. Humans excel at making analogies between domains without prior supervision, using previous knowledge to establish strong priors in new situations. Identifying analogies across domains is crucial for artificial intelligence. Recent success in AI has been in supervised problems, but analogy identification presents a different challenge as no explicit example analogies are given in advance. Unsupervised mapping between domains involves learning a mapping function that translates images from one domain to another, ensuring that the distributions of mapped images are indistinguishable from the target domain. The paper discusses the challenges of analogy identification in artificial intelligence, focusing on unsupervised mapping between domains using distributional and cycle constraints. The authors propose adding exemplar-based constraints to improve visual fidelity and performance in analogy identification tasks. The paper introduces a method for high-performance visual analogy identification, even when exact analogies are not present in sample sets. By aligning domains and using a two-step approach, the method outperforms previous unsupervised mapping techniques. The goal is to identify analogies between datasets without supervision, improving image matching methods through synthesis across domains. The paper discusses analogy identification in image matching without supervision, highlighting the use of unsupervised style-transfer and image-to-image mapping. Various methods for image matching, including deep neural networks and visual feature matching, are explored. The limitations of standard visual features in achieving analogies between different domains are also addressed. Generative Adversarial Networks (GAN) technology is mentioned as a breakthrough in image synthesis. Generative Adversarial Networks (GAN) technology has revolutionized image synthesis by training a generator network to produce realistic images from noise vectors. Image mapping techniques focus on creating images based on input images rather than random noise, with unsupervised mapping methods recently developed for image-to-image translation. These methods aim to generate mapped versions of samples in different domains without the need for supervision. Supervised mapping methods focus on generating a mapped version of samples in different domains, using matching pairs of input and output images. The discriminator in GANs receives pairs of images to strengthen the link between source and target images. While the algorithm in this work generates correspondences between domains, supervised mapping methods can be applied to the inferred matches. Our method for analogy identification involves finding matching indexes between two sets of images in different domains A and B. An iterative approach using a GAN-based distribution method is employed to map images from the source domain to the target domain and search for matches. A mapping function is trained to make images from domain A appear as if they came from domain B, enforcing distributional alignment through a discriminator. The method involves training a discriminator to distinguish between samples from different distributions, optimizing the mapping function to make discrimination difficult. Additional constraints like circularity and distance invariance have been added to improve performance. The cycle approach trains GANs in both directions and ensures that translated images recover the original. This two-sided approach provides matching between samples and synthetic images in the target domain. The method involves training a discriminator to distinguish between samples from different distributions, optimizing the mapping function to make discrimination difficult. Additional constraints like circularity and distance invariance have been added to improve performance. The cycle approach trains GANs in both directions and ensures that translated images recover the original. This two-sided approach provides matching between samples and synthetic images in the target domain. The current section introduces a method for exact matches between domains by finding a set of indices for matching A domain images to analogous B domain images. Once exact matching is achieved, a fully supervised mapping function can be trained to obtain high-quality results. The optimization process involves finding a binary matrix with specific weights for matching samples from different distributions. To enforce sparsity, an entropy constraint is added, and the final objective is optimized using a relaxed formulation. The constraints are enforced using an auxiliary variable and a Softmax function, and the solution can be optimized using SGD. By increasing the significance of the entropy term, exact correspondences can be recovered. The optimization process involves finding a binary matrix with specific weights for matching samples from different distributions. To enforce sparsity, an entropy constraint is added, and the final objective is optimized using a relaxed formulation. The constraints are enforced using an auxiliary variable and a Softmax function, and the solution can be optimized using SGD. By increasing the significance of the entropy term, exact correspondences can be recovered. AN-GAN is a cross domain matching method that uses exemplar and distribution based constraints, with a loss function consisting of distributional, cycle, and exemplar losses. The AN-GAN optimization problem involves minimizing the distributional, cycle, and exemplar losses. The optimization process includes adversarially training discriminators D A and D B, setting initial parameters, and using a burn-in period before optimizing the exemplar loss. The learning rate for the exemplar loss is decayed after 20 epochs, and shared \u03b2 parameters inform likelihood of matches in both mapping directions. In experiments with CycleGAN, shared \u03b2 parameters inform likelihood of matches in both mapping directions. Different loss functions were tested, with perceptual loss function yielding the best performance. VGG features are extracted for images I 1 and I 2, with L1 loss on pixels considered for color accuracy. Our method uses high-resolution images and L1 loss on pixels to consider colors. The perceptual loss function is defined using feature maps \u03c6m1 and \u03c6m2. The approach is unsupervised matching as features are off-the-shelf. Matching experiments were conducted on public datasets, comparing against other methods like U nmapped \u2212 Pixel and U nmapped \u2212 VGG. Our method utilizes high-resolution images and L1 loss on pixels for color consideration. The nearest neighbor in the target domain is found using VGG feature loss. Different training methods like CycleGAN-Pixel, CycleGAN-VGG, and AN-GAN are evaluated on public datasets like Facades and Maps. The Maps dataset was scraped from Google Maps and consists of aligned Maps and corresponding satellite images. The dataset includes images of shoes from the Zappos50K dataset and Amazon handbags. The edge images were automatically detected using HED. The datasets were down-sampled to 2k images each for memory complexity. The method was compared with five others on exact correspondence identification, with results presented in Table 1. The performance metric is the percentage of images with exact matches between domains A and B, calculated separately. Results show that matching using pixels or deep features alone is insufficient due to domain differences. CycleGAN with pixel-losses improves matching, but there is room for enhancement. Perceptual features, specifically VGG features, outperform pixel matching. Exhaustive search was too computationally expensive, requiring feature subsampling. Iterative steps on mapped images were also conducted. The method presented significant improvements by using perceptual features over pixel matching. By running iterative steps on mapped images, linear combinations were matched instead of single images. The method is less sensitive to outliers and uses the same parameters for both sides of the match. The optimization problem was aided by distributional auxiliary losses, allowing the exemplar loss to converge. The full-method AN-GAN optimizes the mapping function for better matches between source and target samples. AN-GAN significantly improves performance by optimizing the mapping function to match each source sample with the nearest target sample. In experiments with unavailable matches, our method successfully identifies correct matches for samples with matches in the other domain. The results show that our method can handle scenarios where not all examples have matches. The results in Table 2 show that AN-GAN can handle scenarios where not all examples have matches. Even with 25% of samples without matches, the method still performs well. AN-GAN achieved a 90% match rate with 75% of samples not having matches in some datasets. Additionally, the method was tested on scenarios with no exact analogies available, showing promising results. DiscoGAN architecture was used for mapping function in these experiments. The DiscoGAN architecture was used for mapping functions in the experiments. In the Shoes2Handbags dataset, examples showed varying quality of mapping with DiscoGAN. AN-GAN provided better analogies and alignment accuracy. A two-step approach was suggested for training mapping functions between unaligned datasets. Facades dataset achieved 97% alignment accuracy, used for training a self-supervised mapping function with Pix2Pix. The study achieved 97% alignment accuracy on the facades dataset and trained a self-supervised mapping function using Pix2Pix. Results showed that the method outperformed CycleGAN and performed similarly to fully supervised methods. The self-supervised approach also performed better than CycleGAN on the edges2shoes and edges2handbags datasets. The study utilized a Pix2Pix architecture with L1 loss for supervised learning, achieving improved performance over CycleGAN on point cloud matching tasks. The method was evaluated on the Bunny benchmark for 3D object alignment, showing success in achieving alignment for various rotation angles. Our method, using a fully connected network with hidden layers and BatchNorm, outperformed CycleGAN in achieving alignment accuracy for rotation angles. The success rate was measured with an RMSE of 0.05, showing superior performance especially for large transformations. The algorithm presented is effective for low dimensional transformations and settings without exact matches. Our method for cross domain matching in an unsupervised way outperformed baseline methods on public datasets for full and partial exact matching. The algorithm introduced the exemplar constraint to improve match performance, even in cases where exact matches are not available. Future work includes exploring matching between different modalities like images, speech, and text, requiring new distribution matching algorithms to be developed."
}