{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively in a bottom-up manner. This approach preserves semantic content better and follows a natural human language structure. The current research focuses on improving image captioning by addressing the limitations of sequential models. These models, while effective on benchmarks, struggle to capture hierarchical structures in natural language. This leads to drawbacks such as overreliance on n-gram statistics and a lack of hierarchical dependencies in captions. Sequential models in image captioning have drawbacks like relying on n-gram statistics, favoring frequent n-grams in training, and obscuring the dependency structure. To address these issues, a new paradigm proposes decomposing semantics extraction and caption construction into two stages. It first extracts explicit semantic representations like noun-phrases from the image, then recursively composes the caption using these phrases. The proposed paradigm decomposes semantics extraction and caption construction into two stages. It involves recursive composition to form higher-level phrases and uses modular nets for phrase composition and completeness evaluation. This approach offers advantages over conventional captioning models by preserving semantic content, making caption generation interpretable and controllable. The proposed paradigm for image captioning not only preserves semantic content but also allows for easy interpretation and control of caption generation. It effectively increases caption diversity while maintaining semantic correctness and generalizes well to new data, even with limited training data. The literature in image captioning has evolved from early bottom-up approaches to recent works using convolutional neural networks. Recent works on image captioning utilize convolutional neural networks for image representation and recurrent neural networks for caption generation. Various approaches have been proposed, including using a single feature vector for image representation, applying attention mechanisms to extract relevant image information, adjusting attention computation to consider generated text, and reformulating latent states as 2D maps to capture semantic information. Some approaches also directly extract phrases or semantic words from input images. Our proposed paradigm for image captioning proceeds in a bottom-up manner by representing the input image with noun-phrases and constructing captions through a recursive composition procedure. This approach aims to preserve semantics effectively, require less data to learn, and promote diversity in generated captions. The recursive composition procedure in image captioning preserves semantics effectively, requires less data to learn, and promotes diversity in generated captions by allowing the composition of any number of phrases using powerful neural networks. The proposed two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. Unlike traditional models, CompCap considers nonsequential dependencies among words and phrases in a sentence, providing a more interpretable representation of image semantics. In the proposed framework for image captioning, noun-phrases are used to represent image semantics explicitly. These noun-phrases capture object categories and attributes, extracted from input images. The task of noun-phrase extraction is formalized as a multi-label classification problem due to the smaller number of distinct noun-phrases compared to images in datasets like MS-COCO. The framework for image captioning uses noun-phrases to represent image semantics explicitly. Noun-phrases are extracted from training captions and treated as classes for binary classification. Visual features are extracted from images using a Convolutional Neural Network, and a recursive compositional procedure called CompCap is used to construct captions. Semantic Non-Maximum Suppression is applied to prune similar noun-phrases. The caption is constructed using a recursive compositional procedure called CompCap. It involves maintaining a phrase pool, scanning pairs of phrases, applying a Connecting Module to generate a sequence of words to connect phrases, and selecting the phrase with the maximum connecting score as the new caption. An Evaluation Module is used to determine if the new caption is complete, and the process continues until a complete caption is achieved. The Connecting Module (C-Module) selects connecting phrases and evaluates their scores based on left and right phrases. It treats phrase generation as a classification problem due to the larger space of incomplete captions. This approach differs from conventional methods and aims to address limitations in caption completion. The Connecting Module (C-Module) selects connecting phrases based on left and right phrases, treating phrase generation as a classification problem. It addresses limitations in caption completion by mining distinct connecting sequences from training captions and using a two-level LSTM model to encode phrases. The Connecting Module (C-Module) selects connecting phrases based on left and right phrases using a two-level LSTM model. The Evaluation Module (E-Module) determines the connection scores for phrases. The Evaluation Module (E-Module) encodes phrases into vectors and evaluates their probability of being complete captions. It can also check for other properties like caption quality using a caption evaluator. Extensions include generating diverse captions through beam search or probabilistic sampling to avoid local minima. The framework allows for generating diverse captions by sampling ordered pairs or connecting sequences based on normalized scores. User preferences can be incorporated by filtering noun phrases or modulating scores. Experiments were conducted on MS-COCO and Flickr30k datasets with a vocabulary size of 9,487. The vocabulary sizes for MS-COCO and Flickr30k datasets are 9,487 and 7,000 respectively. Training captions are truncated to 18 words, and ground-truth captions are parsed into trees using NLP toolkit BID31. The recursive compositional procedure is modularized for better generalization. Testing involves two forward passes for each module, with 2 or 3 steps typically needed to obtain a complete caption. CompCap is compared with other methods like NIC, AdapAtt, and TopDown, which utilize attention mechanisms for captioning. CompCap is compared with other state-of-the-art captioning models like AdapAtt and TopDown, all of which encode images as semantical feature vectors. CompCap also compares with LSTM-A5, which predicts semantical concepts as additional visual features. All methods are re-implemented and trained using the same hyperparameters, with ResNet-152 pretrained on ImageNet used to extract image features. Beam-search of size 3 is used for baselines, while CompCap empirically selects 7 noun-phrases for training. CompCap selects parameters for best performance on the validation set to generate captions. Beam-search of size 3 is used for baselines. Noun-phrases with top scores are empirically selected to represent the input image. The quality of generated captions is compared on MS-COCO and Flickr30k test sets using various metrics. CompCap performs well under the SPICE metric but lags behind in CIDEr, BLEU-4, ROUGE, and METEOR compared to baselines. These results highlight the differences in methods that generate captions sequentially and compositionally. The study compares the performance of CompCap in generating captions using different metrics like CIDEr, BLEU-4, ROUGE, and METEOR. The ablation study on the proposed compositional paradigm shows that using groundtruth noun-phrases from associated captions significantly improves all metrics. CompCap effectively preserves semantic content and generates better captions with a better semantic understanding of the input image. Additionally, integrating noun-phrases from a ground-truth caption further boosts metrics, except for SPICE. CompCap mainly focuses on composing semantics into a syntactically correct caption, resulting in improved metrics except for SPICE. It requires less data to learn and handles out-of-domain semantic content well. Two studies were conducted to verify this hypothesis, showing competitive results when trained on MS-COCO/Flickr30k and tested on Flickr30k/MS-COCO. CompCap shows competitive results when trained on in-domain and out-of-domain data, benefiting from disentangling semantics and syntax. It can generate diverse captions by varying noun-phrases and composing order, as shown by metrics evaluating diversity. This includes novel and unique captions, vocabulary usage, and pair-wise diversity analysis. The study evaluates the diversity of captions generated by CompCap using metrics like vocabulary usage and pair-wise editing distances. CompCap outperformed other methods in generating diverse and novel captions. The study evaluates the diversity of captions generated by CompCap, showing qualitative samples and error analysis. CompCap generates captions in a compositional manner, factorizing the procedure into two stages for better understanding of visual content. Our approach factorizes the captioning procedure into two stages: extracting an explicit representation of the input image in the first stage, and applying a recursive compositional procedure to assemble noun-phrases into a caption in the second stage. The proposed compositional procedure preserves semantics effectively, requires less data to train, generalizes better across datasets, and yields diverse captions. To suppress noun-phrases, we find semantically similar ones by comparing central nouns and using encoders in the C-Module. The C-Module uses encoders to compare semantically similar noun-phrases by computing normalized euclidean distances between their encodings. This approach is more robust than using a single encoding and helps determine similarity based on input images. The C-Module contains two independent encoders for ordered pairs to differentiate between phrases based on their position. The C-Module uses encoders with shared or independent parameters to compare noun-phrases. Tuning hyperparameters like beam search size can improve performance. Adjusting these hyperparameters has minor influence on CompCap's performance."
}