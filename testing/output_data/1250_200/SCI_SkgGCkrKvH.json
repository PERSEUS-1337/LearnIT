{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, such as Choco-SGD, enables data privacy and on-device learning over networks. This approach achieves linear speedup in the number of workers for high compression ratios on non-convex functions and non-IID training data. The practical performance of the algorithm is demonstrated in training scenarios over decentralized user devices and in datacenters. Distributed machine learning has enabled successful applications in research and industry by leveraging computational scalability and data-locality. Recent theoretical results show that decentralized training schemes can be as efficient as centralized approaches, especially in terms of convergence of training loss. Gradient compression techniques have been proposed to reduce data sent over communication links. CHOCO-SGD, introduced for convex problems, overcomes restrictions on compression operators for decentralized training of deep neural networks. CHOCO-SGD is a gradient compression technique introduced for convex problems, showing improved generalization performance on machine learning benchmarks. It is evaluated in two scenarios: training in a peer-to-peer setting with distributed data and in a datacenter setting with decentralized communication patterns for better scalability. The algorithm demonstrates speed-ups over decentralized baselines with less communication overhead and improves time-to-accuracy on large tasks like ImageNet training. CHOCO-SGD is a communication efficient algorithm that improves time-to-accuracy on large tasks like ImageNet training. Decentralized algorithms struggle to match the performance of centralized approaches when scaled to a larger number of nodes. The findings highlight deficiencies in current decentralized training schemes and call for further research in this area. The algorithm converges at a rate of O 1 / \u221a nT on non-convex smooth functions, showing a linear speedup in the number of workers. The text discusses the practical performance of CHOCO-SGD with momentum in on-device training and datacenter settings for deep learning models. It also explores decentralized learning schemes and challenges when scaling to a larger number of nodes. Various methods for training in communication restricted settings are mentioned, including decentralized schemes and gradient compression techniques. The text discusses decentralized SGD schemes with gradient compression, focusing on gossip averaging methods for convergence rate improvement. Lian et al. (2017) combine SGD with gossip averaging, showing convergence at a rate consistent with centralized mini-batch SGD. The spectral gap affects asymptotically smaller terms in the convergence rate. Decentralized optimization with quantization has gained popularity in the deep learning community, with successful implementations reported. Theoretical guarantees have been established for schemes with unbiased compression, and extended to biased compression as well. Error correction schemes show practical effectiveness and strong theoretical guarantees. Proximal updates and variance reduction have also been studied in combination with quantized updates. Gossip averaging methods may diverge in the presence of quantization noise. In decentralized optimization with quantization, various algorithms have been proposed to address convergence issues caused by quantization noise. Reisizadeh et al. (2018) introduced an algorithm that can still converge, albeit at a slower rate. Adaptive schemes have also been suggested to improve convergence at the expense of higher communication costs. For deep learning applications, Tang et al. (2018) proposed DCD and ECD algorithms that converge at a constant compression ratio. CHOCO-SGD algorithm, introduced by Koloskova et al. (2019), can handle arbitrary high compression and has shown promising results for non-convex functions. Tang et al. (2019a) also introduced DeepSqueeze as an alternative method for convergence with arbitrary compression ratio. In experiments, CHOCO-SGD demonstrated higher test accuracy under similar tuning conditions. In decentralized optimization, CHOCO-SGD algorithm achieves higher test accuracy with arbitrary compression ratio. The algorithm involves communication between nodes in a distributed setup using a mixing matrix and gossip-based stochastic optimization. The CHOCO-SGD algorithm utilizes a mixing matrix for communication between nodes in decentralized optimization. It involves assigning positive weights to edges based on local node degrees and transmitting compressed messages. The algorithm is summarized in Algorithm 1, where each worker updates its private variable using stochastic gradient descent. The CHOCO-SGD algorithm involves workers updating their private variables using stochastic gradient descent and a modified gossip averaging step to preserve averages of iterates despite compression errors. Nodes communicate with neighbors using compressed updates, sharing publicly available copies of their variables. Communication and gradient computation can be executed in parallel, with each node storing at most 3 vectors regardless of the number of neighbors. The CHOCO-SGD algorithm extends the analysis to non-convex problems by making technical assumptions on bounded variance of stochastic gradients. It converges asymptotically with a linear speed-up compared to SGD on a single node, affected by compression and graph topology. Further details and comparisons with baselines are provided in Section D. The newly developed momentum version of CHOCO-SGD is compared to baselines using commonly used compression operators. Experiments are conducted with momentum in all algorithms, implementing DCD, ECD, DeepSqueeze, and CHOCO-SGD with momentum on a ring topology with 8 nodes training ResNet20 on the Cifar10 dataset. Compression schemes are implemented on ResNet20 using various algorithms with momentum, including DCD, ECD, DeepSqueeze, and CHOCO-SGD. The momentum factor is set to 0.9 without dampening, and the initial learning rate is fine-tuned and warmed up gradually. The learning rate is decayed twice during training, and the training stops at 300 epochs. Two unbiased compression schemes are applied: quantization and sparsification. The top-1 test accuracy is evaluated on each node separately, and the average performance over all nodes is reported. The study implemented compression schemes on ResNet20 with momentum, including DCD, ECD, DeepSqueeze, and CHOCO-SGD. Different compression methods were used, such as quantization, sparsification, top-a, and sign compression. Results showed that ECD and DCD performed well with small compression ratios for unbiased schemes. Results showed that ECD and DCD performed well with small compression ratios for unbiased schemes, while DCD with biased top-a sparsification outperformed the unbiased random counterpart. CHOCO-SGD demonstrated good generalization in all scenarios with minimal accuracy drop. Sign compression achieved state-of-the-art accuracy with significantly fewer bits per weight compared to full precision. The focus now shifts to challenging decentralized real-world scenarios where centralized methods are inefficient, such as sensor networks, mobile devices, or hospitals collaborating on machine learning models. In decentralized scenarios, various applications like sensor networks, mobile devices, or hospitals train machine learning models together. Each device has limited access to local data, unknown global network topology, and a large number of connected devices. Privacy is a key motivation, keeping training data private on each device. Training data is permanently split between nodes, and no prior works have studied this decentralized setting for deep learning. The focus is on challenging real-world scenarios where centralized methods are inefficient. The study compares CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression on different network topologies. Results show performance differences in testing accuracy after 300 epochs, with centralized SGD performing well. The study compares CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression on different network topologies. Results show performance differences in testing accuracy after 300 epochs, with centralized SGD performing well. CHOCO-SGD slows down due to graph topology and communication compression, leading to slower convergence. Increasing epochs improves decentralized scheme performance, but the gap between centralized and decentralized algorithms remains. The focus is on reducing communication to minimize user data cost, with a fixed number of transmitted bits compared for testing accuracy. Experiments were conducted on a real social network graph using user devices connected by the Davis Southern women social network. Models were trained on mobile phones with a ResNet20 model for image classification and a three-layer LSTM architecture for language modeling on WikiText-2. The study fixed the number of transmitted bits to 1000 MB and compared testing accuracy, showing CHOCO-SGD performing the best with slight degradation as the number of nodes increased. Decentralized and Centralized SGD required significantly more bits to reach reasonable accuracy. The study compared different training algorithms on a real social network graph using user devices. For image classification, a ResNet20 model and for language modeling, a three-layer LSTM architecture were used. Results showed that the decentralized algorithm performed best in training accuracy, while the centralized scheme had the highest test accuracy. CHOCO-SGD outperformed other schemes in terms of test accuracy with less transmitted data. CHOCO-SGD outperforms centralized SGD in test perplexity for language modeling tasks. When considering perplexity for a fixed data volume, CHOCO-SGD performs best among decentralized and centralized algorithms. In large-scale training with Resnet-50 on ImageNet-1k, CHOCO-SGD benefits from scaling to more nodes and offers a way to address scaling issues in datacenters with fast connections. Decentralized schemes can outperform centralized ones in certain scenarios, as shown in previous studies. Assran et al. (2019) demonstrated impressive speedups for training on 256 GPUs with asynchronous gossip updates and exact communication. Their setup differs from CHOCO-SGD, which uses decentralized communication with compressed communication in a ring topology for ImageNet-1k training with Resnet-50 on 8 machines. CHOCO-SGD utilizes decentralized and parallel structure, achieving faster training times compared to all-reduce with a slight accuracy loss. The proposed approach shows a 20% gain in time efficiency over the common all-reduce baseline on commodity hardware clusters. The use of CHOCO-SGD is recommended for decentralized deep learning training in bandwidth-constrained environments, with theoretical convergence guarantees provided for the non-convex setting. The proposed CHOCO-SGD algorithm enables decentralized deep learning training in bandwidth-constrained environments, with theoretical convergence guarantees for the non-convex setting. It demonstrates a linear speedup in the number of nodes and performs well in various settings for image classification and language modeling tasks. This work expands the reach of decentralized deep learning by enabling training in strongly communication-restricted environments while respecting the constraint of data locality. The proof of Theorem 4.1 for fully decentralized deep learning involves analyzing CHOCO-SGD with arbitrary stepsizes. Algorithm 1 is a special case of a more general class of algorithms, involving stochastic gradient updates and averaging among nodes. Convergence is guaranteed as long as the averaging scheme exhibits linear convergence, which has been shown for CHOCO-SGD. Decentralized SGD with arbitrary averaging is presented in Algorithm 3. Decentralized SGD with arbitrary averaging is discussed in Algorithm 3. It involves an averaging scheme that preserves iterates' average and converges with a linear rate. Exact Averaging and CHOCO-SGD are examples of this, with CHOCO-GOSSIP as the consensus averaging scheme. The order of communication and gradient computation parts in Algorithm 1 is exchanged for better illustration. The order of communication and gradient computation parts in Algorithm 1 is exchanged to show their independence for parallel execution. The effect of this change does not impact convergence rate. The proof of Theorem 4.1 shows that Algorithm 3 with constant stepsize satisfies certain conditions. The averaged iterates of Algorithm 3 exhibit linear speedup compared to SGD on one node. The convergence rate affects the second-order term, with the rate recovering that of D-PSGD. The convergence rate of CHOCO-SGD with CHOCO-GOSSIP averaging scheme is affected by the second-order term, recovering the rate of D-PSGD. The theorem provides guarantees for the averaged parameter vector x, but in a decentralized setting, averaging all parameters across machines can be costly and sometimes infeasible. Similar guarantees can be obtained for individual iterates x_i. The convergence rate of CHOCO-SGD with CHOCO-GOSSIP averaging scheme is influenced by the second-order term, resembling the rate of D-PSGD. A decentralized setting may find it costly and impractical to average all parameters across machines. Similar guarantees can be achieved for individual iterates x_i as shown in (Assran et al., 2019). Theorems and corollaries provide convergence guarantees under specific assumptions and conditions, with Algorithm 3 converging at a certain speed determined by the convergence rate of the underlying averaging scheme. The convergence rate of CHOCO-SGD with CHOCO-GOSSIP averaging scheme is influenced by the second-order term, resembling the rate of D-PSGD. Algorithm 4 CHOCO-SGD demonstrates error feedback and mixing matrix initialization for decentralized settings. Nesterov momentum can be adapted for decentralized settings, with quantization errors saved in error feedback algorithms. In CHOCO-SGD, quantization errors are saved in internal memory and added to the compressed value at the next iteration. The model training procedure and hyper-parameter tuning are detailed. CHOCO-SGD is compared with decentralized SGD and centralized SGD without compression using ResNet20 and a three-layer LSTM architecture for image classification. The curr_chunk discusses the experimental setup for image classification on the Cifar10 dataset and a language modeling task on WikiText-2 using ResNet20 and a three-layer LSTM architecture. It includes details on model parameters, training epochs, mini-batch size, learning rate strategy, and gradient clipping. The momentum factor is applied only to ResNet20 training. The learning rate is gradually warmed up from a small value to a fine-tuned one. The experimental setup for image classification on the Cifar10 dataset and a language modeling task on WikiText-2 using ResNet20 and a three-layer LSTM architecture is discussed. The learning rate is gradually warmed up from a small value to a fine-tuned one, with details on hyperparameters tuning and optimization strategies provided in tables. The hyperparameters of CHOCO-SGD for training ResNet-20/LSTM on a social network topology are fine-tuned and presented in Table 5. The training data is split between 32 nodes with fixed partitions, no shuffling. The per node mini-batch size is 32, and the maximum node degree is 14. Learning curves and accuracy plots for the social network topology are provided in Figures 1, 6, 8, and 9, showing the training and test accuracy as well as model consensus towards the end of optimization. The local models reach consensus towards the end of optimization, with their test performances matching the averaged model. Initially, the local models diverge from the averaged model but start converging when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}