{
    "title": "rJxlc0EtDr",
    "content": "Recent research has focused on developing neural network architectures with external memory, often using the bAbI question and answering dataset. A classic associative inference task from human neuroscience literature was employed to assess the reasoning capacity of existing memory-augmented architectures. Current architectures struggle with reasoning over long distance associations, as shown in tasks involving finding the shortest path between nodes. A novel architecture called MEMO was developed to address this issue, featuring a separation between memories/facts and items in external memory, as well as an adaptive retrieval mechanism allowing for variable 'memory hops'. MEMO demonstrates improved reasoning over longer distances. MEMO is a novel architecture that utilizes an adaptive retrieval mechanism for reasoning tasks. It allows for flexible recombination of experiences to infer relationships, supported by the hippocampus. The hippocampus stores memories independently through pattern separation. The hippocampus stores memories independently through pattern separation, minimizing interference between experiences to recall specific events as 'episodic' memories. Recent research shows that the integration of separated experiences occurs at retrieval, allowing for inference through interaction of multiple pattern separated codes. This insight is used to investigate and enhance inferential reasoning in neural networks. Neural networks with external memory, such as DNC and EMN, have shown impressive abilities in computational tasks. Attention mechanisms and context utilization have also improved traditional neural networks. To address limitations in exploiting repetitions, a new task called Paired Associative Inference (PAI) was introduced, derived from neuroscientific literature to capture inferential reasoning. The PAI task involves finding distant relationships among elements distributed across multiple facts or memories. Unlike previous models, MEMO retains the full set of facts in memory and utilizes a linear projection with a recurrent attention mechanism for greater flexibility in memory usage. This approach allows for flexible weighting of individual memories, potentially enhancing memory-based reasoning capabilities. The new architectural components in MEMO allow for flexible weighting of individual elements in memory, supporting inferential reasoning. To address prohibitive computation time, the model adapts the amount of compute time to the complexity of the task, drawing inspiration from the REMERGE model of human associative memory. In the MEMO model, inspired by REMERGE, the network uses a halting policy to adjust computation time based on task complexity. The network outputs a probability of halting and is trained using REINFORCE to determine the optimal number of computation steps. Our approach adds an extra term to the REINFORCE loss to minimize the expected number of computation steps, encouraging the network to prefer minimal computation. Contributions include a task emphasizing reasoning, investigation of memory representation for inferential reasoning, a REINFORCE loss component for learning optimal iterations, and empirical results on reasoning tasks. The curr_chunk discusses the effectiveness of End-to-End Memory Networks in solving tasks like paired associative inference, shortest path finding, and bAbI. It explains the architecture and setup of the network for predicting answers based on knowledge inputs and queries. The prev_chunk introduces an approach that minimizes computation steps and emphasizes reasoning, memory representation, and learning optimal iterations. The curr_chunk discusses the implementation of End-to-End Memory Networks (EMN) and Memory Networks (MEMO) for various tasks. EMN uses one hot vector encoding for input words and calculates weights over memory elements to produce outputs. MEMO embeds input differently by deriving a common embedding for each input matrix. Unlike EMN, MEMO does not use hand-coded positional embeddings. MEMO uses multiple heads to attend to memory, with each head having a different view of the common inputs. Each item is kept separated in memory to learn how to weight them during a memory lookup, contrasting with the hand-coded positional embeddings used in EMN. MEMO uses multi-head attention to weight items during memory lookup, contrasting with hand-coded positional embeddings in EMN. The attention mechanism in MEMO is adapted for multi-head attention, DropOut, and LayerNorm to improve generalization and learning dynamics. MEMO's attention mechanism differs from Vaswani et al. (2017) by preserving query separation from keys and values, leading to linear computational complexity. It uses GRUs and MLP to learn the number of computational steps required for answering a query effectively. The input to the network is the Bhattacharyya distance between current and previous attention weights. The network in MEMO uses the Bhattacharyya distance between current and previous attention weights to determine when to stop querying memory. It is trained using REINFORCE and aims to minimize the expected number of hops in the policy. The new term introduced in the loss, L Hop, allows for minimizing the expected number of hops in the network. It encourages preferring representations that minimize computation. The variance in training discrete random variables is a concern, but for binary halting random variables, it is bounded by 1/4. The reward structure is defined by the target answer and prediction. The final layer of M LP R is initialized with bias init to increase the probability of doing one more hop. A maximum number of hops, N, is set for the network. The network is initialized with bias to increase the probability of doing one more hop. A maximum number of hops, N, is set for the network. Memory-augmented networks have gained interest for abstract reasoning tasks. The Differential Neural Computer (DNC) operates sequentially on inputs, learning to read and write to memory. The DNC struggled with scalability until sparsity was incorporated. Memory-augmented architectures like DNC, Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet have been developed for larger-scale tasks. These models enable relational reasoning over memory contents and have shown good performance on reference tasks like the bAbI task suite. Time to solve a problem is expected to increase with task complexity, but most machine learning algorithms do not adjust their computational budget accordingly. Adaptive Computation Time (ACT) and Adaptive Early Exit Networks are approaches to adjust computational steps based on task complexity in machine learning algorithms. REINFORCE is also used to dynamically adjust computation steps in recurrent neural networks. Graph Neural Networks (GNNs) involve iterative message passing to propagate embeddings in a graph, enabling various learning tasks. This approach differs from Adaptive Computation Time and Adaptive Early Exit Networks, which adjust computational steps based on task complexity. The message passing process in our work implements computation similar to attention mechanisms and self-attention can be seen as a fully-connected GNN. Our method differs from GNNs by performing adaptive computation to modulate the number of message passing steps and does not require message passing between memories. One contribution is introducing a task derived from neuroscience to probe reasoning. One contribution of this paper is introducing a task derived from neuroscience to probe the reasoning capacity of neural networks. The task involves paired associative inference (PAI), where two images are randomly associated together to test the appreciation of distant relationships among elements. The task includes direct and indirect queries to assess reasoning abilities. The study introduces a task called paired associative inference (PAI) to test neural networks' reasoning abilities. The task involves direct and indirect queries to assess the network's ability to appreciate distant relationships among elements. The network is presented with cues and images to make associations and answer questions based on memory retrieval or inference. The study compares MEMO with other memory-augmented architectures like End to End Memory Networks (EMN), DNC, and Universal Transformer (UT). The study compares MEMO with other memory-augmented architectures like End to End Memory Networks (EMN), DNC, and Universal Transformer (UT) on the paired associative inference (PAI) task. MEMO outperformed other models on harder inference queries, showing superior accuracy and efficiency in solving complex tasks. MEMO outperformed other memory-augmented architectures like End to End Memory Networks (EMN), DNC, and Universal Transformer (UT) on the paired associative inference (PAI) task by converging to 3 hops. The attention weights of an inference query were analyzed to associate a CUE with the MATCH and avoid interference of the LURE. The original sequence A \u2212 B \u2212 C, with class IDs 611 \u2212 191 \u2212 840, was not directly experienced together by the network. In the first hop, MEMO retrieved the memory in slot 10 containing the CUE and associated item, forming an A \u2212 B association. In the following hop, most mass was placed on slot 16, containing the memory association B \u2212 C, the MATCH. Slot 13 associated with the LURE also received some mass. MEMO assigned appropriate probability masses to all slots needed for a correct inference decision. In the second hop, MEMO assigned probability masses to support correct inference decisions, confirmed in the last hop. Different patterns of memory activation were observed with varying hops, indicating algorithm dependency. Ablation experiments on MEMO confirmed the importance of specific memory representations. The ablation experiments on MEMO confirmed the importance of specific memory representations and the recurrent attention mechanism for successful inference. Direct queries test episodic memory and can be solved with a single memory look-up. The adaptive computation mechanism in comparison with ACT was found to be more data efficient for the task. The ablation experiments on MEMO highlighted the significance of memory representations and recurrent attention for successful inference. In synthetic reasoning experiments, MEMO outperformed other models in predicting nodes in the shortest path on complex graphs, showcasing its scalability and superior performance. The study demonstrated MEMO's superior performance in predicting nodes in the shortest path on complex graphs, showcasing its scalability. Universal Transformer showed slightly lower performance compared to MEMO in certain cases. Results for the best hyper-parameters for MEMO, EMN, UT, and DNC are reported. Additionally, the model's accuracy on the bAbI question answering dataset was evaluated. In the study, MEMO demonstrated superior performance in solving tasks on the bAbI dataset by combining memory representations and recurrent attention. The use of layernorm in the recurrent attention mechanism contributed to stable training and improved performance. Ablation experiments were conducted to analyze the impact of different architectural components on the model's performance. In this study, MEMO showed state-of-the-art results in inferential reasoning tasks, including paired associative inference and graph traversal. It also matched the performance of the current state-of-the-art on the bAbI dataset. The results were achieved through flexible weighting of individual memory elements and a powerful recurrent attention mechanism. The study introduced MEMO, which demonstrated impressive results in inferential reasoning tasks by utilizing a recurrent attention mechanism. The dataset used for training included sequences of images with varying lengths, and batches were constructed by creating memory content based on pairwise associations within the sequences. The study introduced MEMO, a model that excelled in inferential reasoning tasks using a recurrent attention mechanism. The dataset consisted of image sequences with varying lengths, and memory content was created based on pairwise associations within the sequences. Queries were generated with 3 images: cue, match, and lure, testing episodic memory with 'direct' queries and requiring inference across episodes with 'indirect' queries. The network received queries as concatenated image embedding vectors. The inference trail in the study involved presenting queries to the network as concatenated image embedding vectors, with the cue always in the first position. The match and lure positions were randomized to avoid degenerate solutions. The task required appreciating the correct connection between images and avoiding interference from other items in memory. The batch was balanced with half direct queries and half indirect queries, with targets being the class of the matches. Longer sequences provided more direct queries and multiple indirect queries requiring different levels of inference. The study involved presenting queries as concatenated image embedding vectors, with randomized match and lure positions. Longer sequences generated direct and indirect queries requiring different levels of inference. Different models used memory and query inputs in various ways for prediction. Evaluation results were based on 600 items in the evaluation set at the end of training. The study involved generating graphs for shortest path experiments using uniformly sampled two-dimensional points. The task is represented by a graph description, query, and target. During training, a mini-batch of 64 graphs is sampled. During training, a mini-batch of 64 graphs is sampled, with queries represented as a matrix of size 64 \u00d7 2 and targets of size 64 \u00d7 (L \u2212 1). Graph descriptions are of size 64 \u00d7 M \u00d7 2, where L is the length of the shortest path and M is the maximum number of nodes allowed. Networks are trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is set as the contents of their memory, with the query as input. The model predicts answers for nodes sequentially, with MEMO and EMN differing in their approach. The model predicts answers for nodes sequentially, with MEMO using predicted answers as queries for the next node. For Universal Transformer and DNC, query and graph description are embedded, and the output is used as the answer for each round of hops. The weights for each answer are not shared. The DNC model embeds query and graph description for sequential prediction of nodes in a path. Pondering steps are used to output the sequence of nodes. Training is done using Adam with cross-entropy loss. Evaluation is based on accuracy over target paths. DNC and UT have a 'global view' to reason and work backwards for better performance. The DNC model uses a global view to reason and work backwards for better performance in sequential node prediction. In contrast, MEMO has a local view where the answer to the second node depends on the answer to the first node. Comparing MEMO and EMN performance, using ground truth for node 1 as query for node 2 improves MEMO's performance significantly. The study compares the performance of MEMO and EMN models in sequential node prediction tasks. Using ground truth for node 1 as query for node 2 significantly improves MEMO's performance. EMN performs poorly when trained with the same regime as MEMO. The experiment was conducted on a dataset with pre-processing steps like converting text to lowercase and ignoring punctuation. During training, a mini-batch of 128 queries and corresponding stories are sampled from the test dataset. Queries are 128 \u00d7 11 tokens, and stories are 128 \u00d7 320 \u00d7 11. Padding is applied to queries and stories that do not reach max sizes. Different models use stories and queries as inputs in their architectures. Optimization steps are performed using Adam after sampling the mini-batch. During training, a mini-batch of 128 queries and corresponding stories are sampled from the test dataset. Optimization steps are performed using Adam for all models with hyperparameters detailed in Appendix Section D.2. Tasks in bAbI require temporal context, so a time encoding column vector is added to the memory store in MEMO. Networks are trained for 2e4 epochs with 100 batch updates each. Evaluation involves sampling a batch of 10,000 elements and computing the mean accuracy over examples. MEMO is trained using cross entropy loss for tasks like paired associative inference and shortest path prediction. MEMO was trained for tasks in bAbI using cross entropy loss and had to predict class ID, node ID, and word ID in different tasks. The halting policy network parameters were updated using RMSProp. The temporal complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various parameters are fixed constants. MEMO is linear with respect to the number of sentences in the input, unlike the Universal Transformer which has quadratic complexity. The spatial complexity of MEMO is O(I \u00b7 S \u00b7 d), where the size of memory is fixed. The halting unit h in MEMO is defined differently from the original ACT, using a binary policy \u03c0 t. This change aims to enhance the model's representation power and align it more closely with our model. The halting mechanism in MEMO uses non-linearities for more powerful representations, similar to our model. The halting probability is defined, with specific architecture and hyperparameters used from previous works. The hyperparameters used in tensor2tensor/models/research/universal_transformer.py are described in Table 15. A search was conducted on hyperparameters for training tasks, with reported ranges."
}