{
    "title": "Bkl87h09FX",
    "content": "The paper discusses the progress in contextualized word representation through unsupervised pretraining tasks like language modeling. It compares different pretraining tasks and highlights the effectiveness of language modeling. However, strong baselines and varied results across tasks suggest limitations in the pretraining paradigm. During pretraining, a shared encoder and task-specific model are trained for each task, then the shared encoder is frozen and the task-specific model is retrained for each evaluation task. State-of-the-art NLP models include components for extracting sentence representations, typically trained directly for the task. Pretraining for sentence encoding is gaining interest due to the limited training data for NLP tasks. This approach aims to exploit external data and signals to effectively pretrain encoders for capturing sentence meaning. Pretraining sentence encoders is gaining interest in NLP due to limited training data. Recent papers show strong performance using pretrained sentence encoders on NLP tasks, exploring various pretraining tasks and combinations to determine effectiveness. The study evaluated 40 sentence encoders on 17 pretraining tasks and found that language modeling is the most effective task. Multitask learning during pretraining can offer further gains. However, using pretrained models without fine-tuning may be limiting as trivial baselines perform nearly as well as the best encoders. Representations from different pretraining tasks perform similarly, with small margins between them. Multitask pretraining does not offer general-purpose pretrained encoders. Various target tasks benefit differently from pretraining. Recent progress in reusable sentence encoders includes CoVe, ULMFit, ELMo, and Transformer LM. However, the question of which tasks to use for creating effective sentence encoders remains unanswered. Zhang & Bowman (2018) analyzed the strengths of translation and language modeling using a single architecture and training dataset. In unpublished work, Zhang & Bowman (2018) compared the strengths of translation and language modeling using a single architecture and training dataset. Encoders trained as language models were found to uncover syntactic structure effectively. Peters et al. delved into model design issues for ELMo, showing that standard architectures for sentence encoding can be pretrained with similar performance. Sentence-to-vector encoding has been explored for tasks requiring fast similarity-based matching, but replacing conventional sentence encoders with pretrained vectors does not consistently achieve top performance. Encoders trained conventionally on a target text classification task do not consistently achieve top performance. Multitask representation learning in NLP has been well studied, showing benefits in various tasks. The main experiment compares encoders pretrained on multiple tasks, with a random encoder serving as the primary baseline. Pretraining a sentence encoder with zero examples yields high scores, outperforming bag-of-words encoders. This approach is strengthened by a skip connection from input to output, allowing direct access to word representations. Nine tasks from GLUE are used for pretraining, including acceptability classification, sentiment classification, semantic similarity, and textual entailment. The curr_chunk discusses various datasets and models used for training language models, machine translation models, SkipThought model, and DisSent model. It includes information about tasks like MNLI, RTE, SQuAD, QNLI, WNLI, as well as datasets like WikiText-103 and 1 Billion Word Language Model Benchmark. The models are trained on datasets like WMT14 English-German and WMT17 English-Russian for machine translation, and on Reddit comment threads for reconstruction. The curr_chunk discusses models trained on Reddit comment threads using a dataset of 18M comment-response pairs collected from 2008-2011. The models include a two-layer bidirectional LSTM and use a pretrained character-level CNN word encoder from ELMo. The models aim to predict if a candidate response is the actual response or generate the true response to a comment. The curr_chunk discusses using a pretrained character-level CNN word encoder from ELMo in models trained on Reddit comment threads. The encoder acts as an input layer, allowing for multitask learning with pretrained ELMo models. This approach complements large-scale language model pretraining and involves training scalar weights for ELMo's three layers. The curr_chunk discusses training scalar weights for ELMo's three layers and using ELMo for evaluation in the GLUE benchmark, a set of classification tasks over sentences. The shared encoder is evaluated by training target-task models on the representations produced by the encoder for each of the nine tasks in the benchmark. The curr_chunk describes the procedure for training target-task models on the representations produced by a pretrained encoder for each task in the GLUE benchmark. Different approaches are used for single-sentence tasks and sentence-pair tasks, including linear projection, max-pooling, BiLSTM, attention mechanism, and heuristic matching. Additional details are provided in Appendices A and B. The curr_chunk discusses the pretraining process for GLUE tasks using BiLSTM with max-pooling over time, heuristic matching, and a final MLP. Different architectures are used for single-sentence and sentence-pair tasks, with details provided in Appendices A and B. Outside pretraining tasks involve sentence pair classification and LM tasks use separate forward and backward two-layer LSTM language models. Sequence-to-sequence pretraining tasks use an LSTM decoder with a single layer. Three sets of tasks are investigated for multitask pretraining. In pretraining tasks for GLUE tasks, LSTM decoder with a single layer is used. Three sets of tasks are explored for multitask pretraining, with sampling based on training data size. Early stopping is based on validation metrics, and models are trained with the AMSGrad optimizer. The text chunk discusses the optimization process and hyperparameter tuning for training models with the AMSGrad optimizer. The experiments took 1-5 days to complete on an NVIDIA P100 GPU, and hyperparameters were chosen without extensive tuning due to resource constraints. The results on the GLUE dev set for pretrained encoders are shown in TAB0. The results on the GLUE dev set for pretrained encoders, including variations with and without ELMo BiLSTM layers, are presented in TAB0. Only three pretrained encoders were evaluated on test data due to GLUE's test set access limits. Task-specific parameters were used for pretraining and target tasks, even when using identical data. Limited hyperparameter tuning was discussed, and prior work for comparable GLUE results can be found in BID41. The limited size of a US Letter page prevented the inclusion of baselines in the table. The best test result using a frozen pretrained encoder is 68.9, and the best overall result is 72.8. Variance in GLUE scores was estimated by re-running setups with different random seeds. Only one model reached the most frequent class performance on the WNLI dataset. In the WNLI dataset, only one model reached the most frequent class performance of 56.3. The CoLA task benefits significantly from ELMo pretraining, while the STS benchmark sees good results with various pretraining methods but not substantially with ELMo. Language modeling performs best among pretraining tasks, with MNLI following closely. Adding ELMo improves performance across all pretraining tasks, with MNLI and English-German translation performing the best. ELMo improves performance across pretraining tasks, with MNLI and English-German translation performing best. Multitask models with ELMo outperform random baseline, but single-task models generalize better on new data. Correlations between tasks and pretrained encoders vary, suggesting different tasks benefit from different forms. The study found that different tasks benefit from different forms of pretraining, with no single task yielding good performance on all target tasks. Models that performed best tended to overfit on certain training sets, leading to negative correlations. Some tasks, like CoLA, showed strong correlations with overall performance, while others, like STS, did not benefit from certain forms of pretraining. Learning curves were used to measure performance on overall metrics and target tasks with varying amounts of data. The study found that different tasks benefit from varying forms of pretraining, with no single task performing well on all target tasks. Models that performed best tended to overfit on specific training sets, leading to negative correlations. Some tasks, like CoLA, showed strong correlations with overall performance, while others, like STS, did not benefit from certain pretraining methods. Learning curves were used to measure performance on target tasks with different amounts of data. The second set of experiments focused on pretrained encoders and their performance on GLUE target tasks with varying amounts of data. Pretraining tasks showed improvement with increasing data, with LM and MT tasks showing the most promising results. Combining pretraining tasks with ELMo yielded less interpretable results, with some models achieving best performance by combining ELMo with restricted-data versions of other pretraining tasks like MNLI and QQP. Target tasks benefited from increasing data quantities, showing constant improvement in performance with pretraining using ELMo or multitask learning. This paper presents a systematic comparison of tasks and task-combinations for pretraining sentence-level BiLSTM encoders like ELMo and CoVe. Language modeling works well as a pretraining task, and multitask pretraining can outperform any single task, setting a new state-of-the-art among comparable models. Target task performance improves with more language model data, suggesting further scaling up of language model pretraining. The study compares pretraining tasks for sentence-level BiLSTM encoders like ELMo and CoVe. While language modeling works well for pretraining, the effectiveness of multitask pretraining is limited. The authors suggest that current methods may be reaching an upper performance bound, and future work will require a better understanding of pretraining strategies. The future of language modeling work will require a better understanding of how neural network models can benefit from outside knowledge and data, as well as new methods for pretraining and transfer learning. The study also involves extracting discourse model examples and performing a Reddit response prediction task. Additionally, the authors acknowledge the limitations of large-scale comparisons in NLP datasets. The paper discusses incomplete comparisons of NLP datasets and various experiments that did not surpass random encoder baseline performance. Evaluation is done every 1,000 steps during training tasks, with AMSGrad optimizer and specific learning rates for different tasks. During target-task training, a learning rate of 3e-4 is used for all tasks. The learning rate is decayed by 0.5 when validation performance does not improve for more than 4 checks. Training stops if the learning rate falls below 1e-6. Early stopping is implemented after 20 validation checks without improvement. Dropout of 0.2 is applied after specific layers, with an increase to 0.4 for small-data tasks. Moses tokenizer is used for preprocessing, with a maximum sequence length of 40 tokens. For English text generation, a word-level output vocabulary of 20,000 types is used. During target-task training, a learning rate of 3e-4 is used for all tasks, decayed by 0.5 when validation performance stagnates. Training stops if the rate falls below 1e-6, with early stopping after 20 checks without improvement. Dropout of 0.2 is applied, increasing to 0.4 for small-data tasks. Moses tokenizer preprocesses with a max sequence length of 40 tokens. For English text generation, a word-level output vocabulary of 20,000 types is used. Different models and training regimes are employed for larger and smaller target tasks to ensure competitive baseline performance. Attention is disabled during pretraining on GLUE tasks, with specific hyperparameter settings for target-task models and training. Attention is found helpful for SkipThought and Reddit pretraining tasks but not for machine translation. The text discusses the use of attention in various pretraining tasks and multitask learning experiments. The experiments involve mixing tasks with different amounts of training data and optimizing the quality of a shared encoder. The goal is to avoid overfitting or underfitting in the multitask mix, with a focus on randomly sampling tasks for training. In a small experiment, different hyperparameters were tested to control over-and underfitting in multitask learning. Task sampling probabilities were determined based on data availability, and loss scaling was applied to each task. Experiments were conducted on various subsets of tasks, showing results in TAB3. Different combinations of task sampling and loss scaling methods were also explored. In multitask learning experiments, various combinations of task sampling and loss scaling methods were tested on GLUE tasks. Results showed that applying only one method at a time was generally better than applying both simultaneously. Power 0.75 task sampling and uniform loss scaling were used in the experiments, with results shown in TAB0. Additionally, results on the GLUE diagnostic set categories indicated that ELMo and unsupervised pretraining were helpful for tasks involving world knowledge and lexical-semantic knowledge. The study found that language model pretraining is beneficial for tasks involving world knowledge and lexical-semantic knowledge but less effective for tasks requiring complex logical reasoning or sentence structure alternations. This contrasts with previous findings that pretraining is helpful for sentence structure tasks."
}