{
    "title": "S1e-0kBYPB",
    "content": "To ensure public acceptance of AI systems, methods must be developed to explain decisions made by black-box models like neural networks. Current explanatory methods face issues related to different explanation perspectives and lack of validation on complex models. A verification framework is introduced for explanatory methods under the feature-selection perspective, using a neural network architecture trained on real-world tasks. This framework provides guarantees on the inner workings of the model. The framework aims to provide a publicly available, off-the-shelf evaluation for feature-selection perspective explanations. Various post-hoc explanatory methods have been developed to shed light on black-box machine learning models, with two widely used perspectives: feature-additivity and feature-selection. The study shows that these perspectives illuminate the model's overall behavior but differ in explaining predictions on individual inputs. In comparing different explanatory methods for black-box machine learning models, it is important to consider the fundamental differences in perspectives. While feature-additivity and feature-selection perspectives shed light on overall model behavior, they diverge significantly when explaining predictions on individual inputs. Additionally, the reliability of explanatory methods in less biased models remains an open question due to the unknown decision-making process of neural networks. The text discusses the challenges of evaluating explainers for neural networks due to the unknown decision-making process. It highlights the issue of assuming target models behave reasonably and proposes a framework for generating evaluation tests from a feature-selection perspective. The framework proposed addresses the issue of evaluating explainers for neural networks by generating evaluation tests from a feature-selection perspective. It tests if explainers rank zero-contribution tokens higher than relevant tokens in the model's prediction. The tests were conducted on three pairs of (target model, dataset) for multi-aspect sentiment analysis. The framework provides guarantees on the behavior of the target model but does not introduce an explanation generation framework. The framework proposed evaluates explainers for neural networks by generating evaluation tests from a feature-selection perspective. It tests if explainers rank zero-contribution tokens higher than relevant tokens in the model's prediction. The framework provides guarantees on the behavior of the target model and introduces an automatic evaluation test that does not rely on speculations. LIME and SHAP perform better than L2X on the test. In Section 5, it is explained why LIME and SHAP outperform L2X in most cases. The error rates of these explanatory methods are provided to highlight potential failures in feature selection explanations. Some findings suggest that relevant tokens may be incorrectly predicted as having zero contribution. A generic evaluation methodology is introduced for testing explanatory methods, applicable across different tasks. Feature-based explainers typically explain predictions based on input unit features, such as tokens for text and super-pixels for images. In Section 5, it is explained why LIME and SHAP outperform L2X in most cases due to potential failures in feature selection explanations. Feature-based explainers focus on input unit features like tokens for text and super-pixels for images, with various evaluation methodologies to validate their faithfulness to the target model. While various explainers have been proposed, the challenge remains in validating their faithfulness to the target model. Evaluations commonly involve interpretable target models like linear regression and decision trees, synthetic setups with controlled important features, and assuming reasonable behavior. These evaluations aim to assess the explainer's faithfulness but may not fully represent the complexity of real-world neural networks. The evaluation of explainers involves identifying intuitive heuristics that high-performing models are assumed to follow, but neural networks may still produce surprising artifacts. Evaluating if explanations help humans predict a model's behavior involves presenting predictions and explanations to humans to infer the model's outputs. The evaluation of explainers involves presenting predictions and explanations to humans to infer the model's outputs. An explainer is considered better if humans can predict the model's output more accurately after seeing explanations from it. This evaluation method is costly and labor-intensive. In contrast, our automatic evaluation framework assesses a non-trivial neural network model with guaranteed inner-workings. Our test is more challenging than existing methods, requiring a strong fidelity of the explainer to the target model. The explanation of predictions from a model involves feature-additivity, where contributions from each feature approximate the model's output. Various methods follow this perspective, such as LIME and Shapley values from game theory. Lundberg & Lee (2017) showed that these contributions must satisfy local accuracy, missingness, and consistency constraints. The contribution of each feature in an instance is an average over a neighborhood of the instance, with the choice of neighborhood being critical. Different perspectives on explanation include feature-selection, where a subset of features leads to a similar prediction as the original model output. Various methods like L2X and mutual information maximization are used for feature selection. The L2X model learns by maximizing mutual information between S(x) and prediction, assuming known important features per instance. However, it may not always rely on a subset of features. Instance-wise explanations for sentiment analysis are provided to understand differences in perspectives. Real-world neural networks may heavily rely on specific tokens in input, as shown by Gururangan et al. (2018). The L2X model maximizes mutual information between S(x) and prediction, assuming known important features per instance. It may heavily rely on specific tokens in the input, which may not always be indicative of the correct target class. The feature-additive perspective provides an average explanation of the model on a neighborhood of the instance, while the feature-selective perspective focuses on pointwise features used by the model on the instance in isolation. The feature-selective perspective of instance-wise explanations reveals differences in ranking important features compared to the feature-additive perspective. A verification framework is proposed using the RCNN model architecture to identify relevant and irrelevant features for each datapoint. New metrics are introduced to evaluate explainers' ability to rank features accurately. The RCNN model consists of a generator and an encoder, both using recurrent convolutional neural networks. The generator selects a subset of tokens from the input text, which is then passed to the encoder for making predictions. The model is trained jointly with supervision only on the final prediction, using regularizers to encourage the selection of a short sub-phrase and fewer tokens. The generator in the RCNN model selects a subset of tokens to pass to the encoder for predictions. Training involves using regularizers to encourage selecting a short sub-phrase and fewer tokens. The model aims to eliminate \"handshakes\" where non-selected tokens contribute to predictions. The RCNN model aims to eliminate \"handshakes\" by selecting a subset of tokens for predictions. The proof of this concept is shown in Appendix B. In an example, selecting \"very\" yields a score of 1, while selecting only \"very\" results in a score of 0.5. Non-selected tokens are considered irrelevant or zero-contribution. Pruning the dataset ensures that non-selected tokens have no significant impact on predictions. After pruning the dataset to retain only relevant instances, further pruning is done to ensure that selected tokens are clearly relevant for prediction. This is determined by checking the absolute change in prediction when a token is removed, with a threshold \u03c4. If the change is greater than \u03c4, the token is considered clearly relevant. This step helps differentiate between noise and zero-contribution features. The dataset is pruned to retain only relevant instances, and further pruning is done to ensure selected tokens are clearly relevant for prediction. This is determined by checking the absolute change in prediction when a token is removed, with a threshold \u03c4. If the change is greater than \u03c4, the token is considered clearly relevant. This step helps differentiate between noise and zero-contribution features. The procedure does not provide an explainer itself but ensures that tokens changing the prediction by a high threshold are important and should be ranked higher. The dataset is further pruned to only include instances with at least one clearly relevant token. The procedure ensures that tokens with a high impact on prediction are considered important and should be ranked higher. Evaluation metrics are used to assess explainers that provide a ranking over features, with error metrics defined based on the importance of selected tokens. In this work, the framework is instantiated on the RCNN model trained on the BeerAdvocate corpus, consisting of human-generated beer reviews with three aspects: appearance, aroma, and palate. The RCNN is a regression model predicting ratings rescaled between 0 and 1. Three separate RCNNs are trained for each aspect independently. Three separate RCNNs are trained for each aspect independently, with ratings rescaled between 0 and 1. A threshold of \u03c4 = 0.1 is chosen to identify clearly relevant tokens. Statistics on dataset lengths and token relevance are provided in the appendix. The threshold of 0.1 is considered strict, with 1 or 2 relevant tokens per datapoint. Percentages of eliminated datapoints are also provided to ensure evaluation test integrity. The study evaluates three popular explainers: LIME, SHAP, and L2X. Default settings were used for LIME and SHAP, while adjustments were made for L2X. LIME and SHAP outperformed L2X in most metrics, possibly due to L2X's requirement to know the number of important features per instance. L2X learns a distribution over features by maximizing mutual information with the response variable. Testing with K as the average number of tokens highlighted by human annotators, L2X had an average K of 23, 18, and 13 for different aspects. In Table 1, explainers often rank zero-contribution tokens as most relevant, with LIME at 14.79% and L2X at 12.95% in the aroma aspect. Both explainers can also rank zero-contribution tokens higher than clearly relevant features. SHAP performs better than L2X in placing zero-contribution tokens ahead of relevant ones. In average, SHAP places one zero-contribution token ahead of a relevant one for the first two aspects and around 9 tokens for the third aspect, while L2X places 3-4 zero-contribution tokens ahead of a relevant one for all three aspects. Explainers' rankings on an instance from the palate aspect show both are prone to attributing importance to nonselected tokens. Both LIME and SHAP rank nonselected tokens as important, with LIME even ranking \"mouthfeel\" and \"lacing\" as top tokens. In contrast, L2X prioritizes \"taste\", \"great\", \"mouthfeel\", and \"lacing\". An evaluation test for post-hoc explanatory methods highlights failures in correctly predicting relevant tokens. This framework offers guarantees on neural network behavior, shedding light on the distinctions between explanation perspectives. The methodology discussed is generic and can be applied to various tasks. Evaluation of post-hoc explainers reveals limitations in predicting relevant tokens. Statistics of the dataset are provided in Table 2, showing the number of instances retained and average lengths of reviews. The core algorithms of current explainers are domain-agnostic, offering a representative view of their limitations. The methodology discussed is generic and can be applied to various tasks. Evaluation of post-hoc explainers reveals limitations in predicting relevant tokens. Statistics of the dataset are provided in Table 2, showing the number of instances retained and average lengths of reviews. The core algorithms of current explainers are domain-agnostic, offering a representative view of their limitations. The text chunk discusses the percentage of instances eliminated from the dataset due to potential handshakes and the impact of selected tokens on predictions. It explains how the model's prediction changes when certain tokens are eliminated, indicating the presence or absence of a handshake. The text chunk discusses the impact of selected tokens on predictions, showing how the model's prediction changes when certain tokens are eliminated, indicating the presence or absence of a handshake. The proof is concluded by showing that having S Sx = S x is sufficient to satisfy the condition. The LIME beer review describes a dark, fizzy beer with fruity notes and a smooth finish. The beer review describes a dark, fizzy beer with fruity notes and a smooth finish, tasting of fruit with minimal alcohol presence. The bottle is brown and reminiscent of \"grolsch\", pouring a fizzy yellow color with a fruity aroma. The beer has a small initial alcohol content, tastes of fruit with minimal alcohol presence, and provides a nice warming sensation. It is better than most American lagers and very smooth."
}