{
    "title": "rJgUfTEYvH",
    "content": "Generative models for video prediction face the challenge of uncertain futures. Recent works have explored probabilistic models, but they are computationally expensive or do not optimize data likelihood directly. This study introduces multi-frame video prediction using normalizing flows, enabling direct data likelihood optimization and high-quality stochastic predictions. The approach models latent space dynamics and demonstrates the effectiveness of flow-based generative models in video prediction. The advancement of machine learning, driven by improved computational hardware and innovative methods, has propelled the field into the mainstream. While progress has been made in various applications like image classification, machine translation, and game-playing agents, the reliance on large amounts of supervision or accurate simulations limits its scope. An alternative approach involves utilizing large unlabeled datasets with predictive generative models to build internal representations of the world for effective future event prediction. Predictive generative models can learn about real-world phenomena without labeled examples by training on large unlabeled datasets of video sequences. These models can be useful for downstream tasks or directly applied in decision-making scenarios like robotics. Video prediction faces the challenge of uncertain futures, with probabilistic models being computationally expensive or not directly addressing this issue. In this paper, the focus is on stochastic prediction, specifically conditional video prediction. The proposed video prediction models aim to provide exact likelihoods, generate diverse futures, and synthesize realistic video frames. The approach extends flow-based generative models into conditional video prediction, addressing the unique challenges of modeling high-dimensional video sequences. VideoFlow is a flow-based video prediction model that learns a latent dynamical system to predict future values, achieving competitive results in stochastic video prediction. It avoids common artifacts like blurry predictions and offers faster training compared to autoregressive models. VideoFlow is a flow-based video prediction model that achieves faster test-time image synthesis and directly optimizes the likelihood of training videos. It outperforms deterministic predictive models by focusing on architectural changes and different generation objectives. VideoFlow is a flow-based video prediction model that addresses stochastic environments by incorporating stochasticity through variational auto-encoders, generative adversarial networks, and autoregressive models. This allows for effective reasoning over uncertain futures in real-world videos. Variational autoencoders, generative adversarial networks, and autoregressive models are used in video prediction models. Variational autoencoders optimize an evidence lower bound on the log-likelihood, while autoregressive models generate videos pixel by pixel. Prior work has focused on speeding up training and synthesis with autoregressive models, but the proposed VAE model produces better predictions, especially for longer sequences. The proposed VAE model outperforms autoregressive models in video prediction, providing sharper and less noisy predictions for longer sequences. The VAE model exhibits faster sampling while optimizing log-likelihood and generating high-quality long-term predictions using a multi-scale architecture and flow-based generative models. The proposed VAE model outperforms autoregressive models in video prediction by using a tractable prior over latent variables and an invertible transformation approach. The model breaks up the latent space into separate variables per timestep and utilizes a multi-scale architecture for generating high-quality long-term predictions. The multi-scale architecture in the VAE model uses invertible transformations to encode information about frames at different scales. Techniques like Actnorm, Coupling, SoftPermute, and Squeeze are applied to process the input data efficiently. The multi-scale architecture in the VAE model utilizes deep networks to process input data efficiently. Techniques like SoftPermute and Squeeze reshape the input for better flow operation. The architecture infers latent variables at different levels and uses a multi-scale approach to obtain latent variables for each frame of the video. Autoregressive factorization is used for the latent prior. The latent prior in the VAE model utilizes autoregressive factorization, with a conditional prior specified for latent variables. A factorized Gaussian density is used, with a deep 3-D residual network predicting the mean and log-scale. The log-likelihood objective includes contributions from the invertible multi-scale architecture and the latent dynamics model. The architecture incorporates log Jacobian determinants of invertible transformations for mapping video frames to latent variables. The latent dynamics model is optimized by maximizing the objective function. Comparison of realism in generated trajectories is done using SAVP-VAE and SV2P models. VideoFlow model is conditioned on the first frame to generate trajectories for different shapes. Computational efficiency favors autoregressive priors over 3-D convolutional flows due to fewer operations and parameters. VideoFlow uses 2-D convolutions with autoregressive priors to generate long sequences without temporal artifacts. The generated videos show a blue border for conditioning frames and a red border for generated frames. The Stochastic Movement Dataset is modeled, where shapes move in eight directions with constant speed. A deterministic model averages out all directions in pixel space. The deterministic model in VideoFlow predicts the future trajectory of shapes in videos to be one of eight random directions with low bits-per-pixel. Comparison with SV2P and SAVP-VAE models shows high quality in generated videos, assessed through a real vs fake test on Amazon Mechanical Turk. VideoFlow outperforms baselines in generating realistic trajectories in a real vs fake test using the BAIR robot pushing dataset. The task is unsupervised due to partial observability and stochasticity of robot actions. VideoFlow is trained to maximize log-likelihood and achieves low bits-per-pixel compared to SAVP-VAE and SV2P models. Realism and diversity are measured using a 2AFC test and mean pairwise cosine distance. VideoFlow outperforms SAVP-VAE and SV2P models in realism and diversity on the BAIR action-free dataset. The models are compared based on bits-per-pixel and performance metrics like PSNR, SSIM, and VGG perceptual metrics. The high bits-per-pixel values of the baselines are attributed to their optimization objective, which does not directly optimize the variational bound on log-likelihood. The study evaluates the performance of VideoFlow compared to SAVP-VAE and SV2P models on the BAIR action-free dataset. VideoFlow is chosen based on PSNR, SSIM, and VGG perceptual metrics, generating 27 frames from ten target frames. The BAIR robot-pushing dataset's stochastic nature leads to diverse plausible futures, with generated videos aiming for realism but may differ from the ground truth perceptually. The evaluation uses metrics like PSNR, SSIM, and cosine similarity with VGG features to assess model accuracy. The study evaluates VideoFlow's performance on the BAIR action-free dataset using VGG similarity metrics to assess model accuracy. Pixel-level noise is removed to improve sample quality, and low-temperature sampling is applied to enhance video quality at the cost of diversity. The study evaluates VideoFlow's performance on the BAIR action-free dataset using VGG similarity metrics to assess model accuracy. Low-temperature sampling was applied to enhance video quality, but it was found to hurt performance. The model with optimal temperature performs well on VGG-based similarity metrics and competes with state-of-the-art video generation models. PSNR is a pixel-level metric, where VAE models excel, while VideoFlow underperforms on this metric. VideoFlow's performance on the BAIR action-free dataset is evaluated using VGG similarity metrics. The study shows that low-temperature sampling improves video quality but hinders performance. The model performs well on VGG-based similarity metrics and competes with state-of-the-art video generation models. PSNR is a pixel-level metric where VideoFlow underperforms compared to VAE models. The study evaluates VideoFlow's performance on the BAIR action-free dataset using VGG similarity metrics. Low-temperature sampling improves video quality but hinders performance. The model competes with state-of-the-art video generation models and performs well on VGG-based similarity metrics. PSNR shows VideoFlow underperforms compared to VAE models. Interpolations in the latent space show cohesive motion of the arm and background objects, with size smoothly interpolated for different shapes. During training, shapes are encoded into the latent space with different sizes and colors. Interpolations in the latent space show smooth size transitions for shapes. Colors are sampled from a uniform distribution, with all colors in the interpolated space matching those in the training set. VideoFlow generates frames into the future, maintaining temporal consistency even in the presence of occlusions. The model has a bijection between latent state and input, ensuring information consistency. The VideoFlow model has a bijection between the latent state and input, meaning that it can forget objects if they are occluded for a few frames. Future work aims to address this by incorporating longer memory in the model. The model is used to detect the plausibility of a temporally inconsistent frame occurring in the immediate future by conditioning it on the first three frames of a test-set video. The likelihood of a frame occurring as the 4th time-step is computed and averaged across the test set. VideoFlow is a flow-based video prediction model that assigns decreasing log-likelihood to frames further in the future. It introduces a latent dynamical system model for predicting future values and achieves competitive results with VAE models. The model optimizes log-likelihood directly for faster synthesis, making it practical for various applications. Future work includes incorporating memory for modeling long-range dependencies. In future work, VideoFlow plans to incorporate memory to model long-range dependencies and apply the model to challenging tasks using a dataset of i.i.d. observations of a random variable x. The data consists of 8-bit videos with added uniform noise to prevent infinite densities at datapoints, enabling optimization of log-likelihood. Evaluations show that decreasing temperature from 1.0 to 0.0 reduces the performance of VAE models in VideoFlow. The performance of VAE models in VideoFlow decreases as temperature decreases from 1.0 to 0.0. Lower temperature leads to a tradeoff between noise removal from the background and reduced stochasticity of the robot arm. The VideoFlow model shows high-quality video generation with lower bits-per-pixel values. The VideoFlow model is trained for 300K steps using Adam optimizer with specific hyperparameters. Different values of latent loss multiplier are used. For SAVP-VAE model, a linear decay on learning rate is applied. SAVP-GAN tunes gan loss multiplier and learning rate. A weak correlation of -0.51 is observed between VGG perceptual metrics and bits-per-pixel. In Figure 13, weak correlation between VGG perceptual metrics and bits-per-pixel with a factor of -0.51 is observed. Evaluations with a smaller version of VideoFlow model show competitiveness with SVG-LP on VGG perceptual metrics."
}