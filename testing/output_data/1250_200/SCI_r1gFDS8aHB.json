{
    "title": "r1gFDS8aHB",
    "content": "Recent advances in deep learning have highlighted the effectiveness of deep neural networks in extracting task-specific features. However, these features are limited to the initial task and do not capture general, task-agnostic features. Humans learn by disentangling task-agnostic features, which can be achieved by using Variational Auto-Encoders (VAEs) to capture latent variables. VAEs can represent latent features as continuous and/or discrete variables, making them suitable for learning disentangled features using a modified version of joint-vae in experiments. Feature learning is crucial in machine learning, with deep learning making significant advancements in this area. Deep neural networks excel at extracting features from raw data, but these features are often task-specific due to the use of specific loss functions tailored to the task at hand. To achieve true artificial intelligence, it is essential to learn task-agnostic representations that capture all necessary information. Recent efforts have focused on learning disentangled representations, where changes in the representation correspond to changes in specific factors. Experimentation with JointVAE aims to explore disentangled representations further. In this work, the focus is on experimenting with JointVAE BID2 to explore disentangled representations for a given dataset. Various state-of-the-art variants of VAEs, such as BID4, BID1, and BID2, have been reported to extract disentangled representations. The assumption of Gaussian distribution is common in these models, simplifying the sampling of latent variables. However, for discrete variables, a mixture of continuous and discrete latent variables is used in Joint-VAE to represent the data. In Joint-VAE Dupont (2018), latent features are represented using a mixture of continuous Gaussian and discrete multinomial variables. Continuous variables are sampled using the normal reparameterization trick, while discrete variables are sampled using the Gumbel Softmax trick. This approach allows for the representation of disentangled features using both continuous and discrete variables without making assumptions."
}