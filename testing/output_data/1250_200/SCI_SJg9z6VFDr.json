{
    "title": "SJg9z6VFDr",
    "content": "Recently, a new model called graph ordinary differential equation (GODE) has been proposed for graph data, inspired by neural ordinary differential equation (NODE) for Euclidean data. GODE uses continuous-depth models and two efficient training methods. It outperforms existing graph networks and can be easily adapted to different graph neural networks, improving accuracy in various tasks. The GODE model is designed for graph data, offering continuous model performance, memory efficiency, accurate gradient estimation, and generalizability across different graph networks. Unlike CNNs limited to grid-based data like images and text, graphs represent objects as nodes and relations as edges, making them suitable for irregularly structured datasets like social networks and protein interaction networks. Traditional methods like random walk and graph embedding have been used in early works. Graph neural networks (GNN) have been proposed as a new class of models to model graphs, inspired by the success of CNNs. There are two main types of methods for performing convolution on a graph: spectral methods and non-spectral methods. Spectral methods compute the graph Laplacian and filter in the spectral domain, while non-spectral methods perform convolution directly in the graph domain, aggregating information from node neighbors. GraphSAGE learns a convolution kernel in an inductive manner. The recently proposed GraphSAGE learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it difficult to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) views a neural network as an ODE, and we extend this concept to graphs with graph ordinary differential equations (GODE). NODEs have adaptive evaluation and accuracy-speed control but are inferior to discrete-layer models in image classification tasks. In benchmark image classification tasks, NODEs are significantly inferior to state-of-the-art discrete-layer models. This is due to error in gradient estimation during training of NODE. A memory-efficient framework for accurate gradient estimation is proposed, improving performance on benchmark classification tasks. The framework is also memory-efficient for free-form ODEs and generalizes to various model structures, achieving high accuracy for both NODE and GODE. Improved performance on various graph models and datasets has been demonstrated through the use of neural ordinary differential equations (NODE). Previous studies have viewed neural networks as differential equations and proposed new architectures based on numerical methods in ODE solvers. The adjoint method, widely used in optimal control and geophysical problems, has been applied to ODEs. Augmented neural ODEs have been proposed to enhance the expressive capacity of NODEs. However, gradient estimation issues have not been addressed, leading to inferior empirical performances of NODE compared to discrete-layer models in benchmark tasks. Spectral and non-spectral methods are two categories of Graph Neural Networks (GNNs). Spectral GNNs operate in the Fourier domain of a graph, requiring information of the entire graph for filtering. Non-spectral GNNs, on the other hand, focus on message aggregation around neighbor nodes, resulting in localized computations. Various spectral methods have been introduced to improve efficiency, such as using Chebyshev expansion to approximate filters without computing the graph Laplacian. Various methods have been proposed to accelerate the running speed of Graph Neural Networks (GNNs), including Laplacian and its eigenvectors, localized first-order approximation of graph convolution, fast localized spectral filtering, MoNet using a mixture of CNNs, GraphSAGE sampling fixed size neighbors, graph attention networks learning different weights for neighbors, and the graph isomorphism network (GIN) with expressive structure. Invertible blocks have also been utilized in normalizing flow. Invertible blocks are used in normalizing flow and bijective blocks for invertible networks. They enable backpropagation without storing activations, making the network memory-efficient. Discrete-layer models with residual connections can be represented as neural ordinary differential equations (NODE). The difference equation transforms into a neural ordinary differential equation (NODE) using hidden states represented by z(t) in the continuous case and x_k in the discrete case. The forward pass of a model with discrete layers involves applying an output layer on x_K, while the forward pass of a NODE applies an output layer on z(T). Integration in the forward pass can be done with various ODE solvers, and the adjoint method is commonly used in optimal process control and functional analysis. The adjoint method is widely used in optimal process control and functional analysis. Model parameters are denoted as \u03b8, which is independent of time. The adjoint is defined as a comparison of two methods for back-propagation on NODE. Direct back-propagation through the ODE solver saves evaluation time points during the forward pass and accurately reconstructs the hidden state for evaluating the gradient. The adjoint method is used for back-propagation on NODE. It involves solving equations forward and backward in time to accurately evaluate the gradient. The reverse-time ODE solver can cause inaccuracies in the gradient calculation. The reverse-time ODE solver can lead to inaccuracies in gradient calculation due to instability, as shown in Fig. 1. If the Jacobian of the ODE has eigenvalues with non-zero real parts, it indicates instability in either forward or reverse-time. This instability affects the accuracy of the solution and computed gradient, making the adjoint method sensitive to numerical errors. The reverse-time ODE solver can be inaccurate due to instability, affecting gradient calculation. To address this, direct back-propagation through the ODE solver is proposed for accurate hidden states. This can be achieved by saving activation z(t i ) in cache or reconstructing it at evaluated time points {t i }. The adjoint method with discrete time can be defined, ensuring accurate back-propagation regardless of Eq. 2 stability. Eq. 7 is a numerical discretization of Eq. 6, derived from an optimization perspective. The reverse-time ODE solver addresses instability by proposing direct back-propagation for accurate hidden states. Eq. 7 is a numerical discretization of Eq. 6, derived from an optimization perspective. Algorithm 1 outlines accurate gradient estimation in ODE solver for free-form functions, with adaptive step-size and error estimation. The algorithm described in the curr_chunk involves a solver that performs numerical integration with adaptive step sizes during the forward pass. It outputs integrated values and evaluation time points, rebuilding the computation graph during the backward pass without adaptive searching. The method supports free-form continuous dynamics and reduces memory consumption compared to a naive solver. The algorithm involves a solver for numerical integration with adaptive step sizes in the forward pass. By using a step-wise checkpoint method, memory consumption can be reduced. A more memory-efficient solver can be achieved by restricting the form of f to invertible blocks, leading to O(Nf) memory consumption. Graph neural networks (GNNs) use differentiable bijective functions for efficient memory usage. Theorem 1 states that the defined block is a bijective mapping. GNNs are represented with nodes and edges, and can be implemented in a message passing scheme. Graph neural networks (GNNs) utilize differentiable bijective functions for memory efficiency. GNNs involve nodes and edges in a message passing scheme. The uth node at the kth layer in the graph is represented by e u,v, denoting the edge between nodes u and v. N(u) signifies the set of neighbor nodes for node u. \u03b6 is a permutation invariant operation like mean, max, or sum. \u03b3(k) and \u03c6(k) are differentiable functions parameterized by neural networks. A GNN can be seen as a 3-stage model for a specific node u: (1) Message passing, where neighbor nodes v \u2208 N(u) send information to node u via message (v,u) using function \u03c6(\u00b7). (2) Message aggregation, where node u aggregates messages from its neighbors N(u) using permutation invariant operations like mean and sum. (3) Update, where the node's states are updated based on its original states x u k\u22121 and the aggregation of messages aggregation u. Discrete-time GNNs can be converted to continuous-time GNNs by replacing f with the message passing process, forming a graph ordinary differential equation (GODE). Graph neural networks (GNNs) utilize differentiable bijective functions for memory efficiency, involving nodes and edges in a message passing scheme. GNNs can be converted to continuous-time GNNs by replacing f with the message passing process, forming a graph ordinary differential equation (GODE). GODE, being an ODE, can capture highly non-linear functions and potentially outperform discrete-layer counterparts. The asymptotic stability of GODE is related to over-smoothing phenomena, with graph convolution being a special case of Laplacian smoothing. The continuous smoothing process involves eigenvalues of the symmetrically normalized Laplacian, leading to asymptotic stability if all eigenvalues are non-zero and negative. In experiments, the ODE is shown to be asymptotically stable with negative eigenvalues. Integration time T affects node features, impacting classification accuracy. The method is evaluated on image and graph classification tasks, including bioinformatic, social network, and citation datasets without pre-processing. Transductive inference is used for node classification tasks following a specific train-validation-test split. For node classification tasks, transductive inference is used with a train-validation-test split. Details of datasets are in appendix A. For image classification, a ResNet18 is modified into a NODE model with a specific function structure. GODE can be applied to various graph neural networks by replacing functions or structures. Different GNN architectures like GCN, GAT, ChebNet, and GIN are used for comparison. In a study comparing different graph neural network (GNN) models, including GCN, GAT, ChebNet, and GIN, various hyper-parameters were kept consistent across models for fair comparison. Different depths of layers were trained and the best results were reported for each model structure. The study also compared the adjoint method and direct back-propagation, showing that direct back-propagation yielded higher accuracy. Additionally, for node classification tasks, specific channel numbers and hops were set for each model. Direct back-propagation consistently outperformed the adjoint method for both image classification tasks and graph networks. The training method reduced error rates on CIFAR10 and CIFAR100 datasets, with NODE18 outperforming ResNet101. The method also showed robustness to ODE solvers of different orders. During inference, using different ODE solvers is equivalent to changing model depth without re-training the network. Our method supports NODE and GODE models with free-form functions, demonstrating robustness to different orders of ODE solvers. Bijective blocks defined as general neural networks can be easily adapted to different tasks. Most GODE models outperformed their discrete-layer counterparts. Results from experiments comparing GODE models with their discrete-layer counterparts showed that GODE models generally outperformed the latter significantly. Different \u03c8 functions had similar performance on node classification tasks, highlighting the importance of the continuous-time model over the coupling function \u03c8. Additionally, GODE models demonstrated lower memory cost. Various structures were tested, including GCN, ChebNet, and GIN, with corresponding GODE models showing improved performance. Integration time was found to be crucial for model performance, with shorter times leading to inadequate information gathering from neighbors. The influence of integration time on model performance is crucial, with short times leading to insufficient information gathering and long times causing over-smoothing issues. GODE is proposed to model continuous diffusion processes on graphs, with a memory-efficient back-propagation method for gradient determination. The paper addresses the gradient estimation problem for NODE and demonstrates superior performance on image classification and graph data tasks. Experiments are conducted on various datasets, including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with modifications to a family of bijective blocks, enabling accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch, emphasizing memory consumption reduction. The algorithm allows bijective blocks to be called multiple times while maintaining inversion accuracy. The bijective block in the backward function efficiently calculates x1, x2 from y1, y2, reducing memory consumption compared to conventional backpropagation. Memory usage with different depths shows our method increases minimally from 2.2G to 2.6G, while conventional methods increase significantly from 5.3G to 10.5G. The memory-efficient bijective block reduces memory consumption during backpropagation, with minimal increase from 2.2G to 2.6G compared to conventional methods. The algorithm for this block involves caching states of F and G, and ensuring stability in both forward and reverse-time ODEs. The bijective block ensures stability in both forward and reverse-time ODEs. The mapping is proven to be bijective by demonstrating injective and surjective properties. This memory-efficient block reduces memory consumption during backpropagation. The mapping is proven to be bijective by demonstrating injective and surjective properties. The gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. The continuous model follows an ODE with a differentiable function f represented by a neural network. The training process is formulated as an optimization problem. The training process for a neural-ODE model is formulated as an optimization problem. The Lagrangian Multiplier Method is used to solve the optimization problem, with Karush-Kuhn-Tucker (KKT) conditions being necessary for optimality. The derivative with respect to \u03bb is derived using calculus of variation, ensuring optimality at the optimal point. The Leibniz integral rule conditions are checked easily by switching integral and differentiation. At optimal \u03bb(t), dL/d| = 0 for all continuous differentiable \u03bb(t). In discrete cases, ODE conditions are replaced with finite sums, leading to the discrete version of the analysis."
}