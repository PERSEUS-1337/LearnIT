{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experiments show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves.GANs involve a discriminator distinguishing real data from generated data, while the generator aims to deceive the discriminator by recreating the real data's density distribution. The problem of matching target density can be minimized using the Wasserstein distance, known for its effectiveness in structured data applications. Optimal transport is used to define loss functions in generative models like Wasserstein GAN. The Wasserstein steepest descent flow is derived for deep generative models in GANs, utilizing the Wasserstein-2 metric function to introduce a Riemannian structure and natural gradient. The Fisher-Rao natural gradient induced by the KL divergence is problematic in GANs due to low dimensional support sets. To address this, the gradient operator induced by the Wasserstein-2 metric is proposed. The proximal operator for GAN generators is computed using the squared constrained Wasserstein-2 distance, which can be approximated by a neural network. The relaxed proximal operator simplifies computation by only involving the difference of outputs, leading to simple parameter updates. The paper introduces a Wasserstein proximal method for GAN generators, simplifying computation with simple parameter updates. It also discusses optimal transport and its proximal operator on a parameter space, focusing on the Wasserstein-2 distance. The paper extends the classic theory of Wasserstein-2 distance to cover parameterized density models, introducing a constrained Wasserstein-2 metric function on the parameter space. The paper introduces a constrained Wasserstein-2 metric on parameter space for parameterized density models, allowing for a steepest descent optimization scheme using the Wasserstein natural gradient operator. The paper introduces a constrained Wasserstein-2 metric on parameter space for parameterized density models, enabling a steepest descent optimization scheme using the Wasserstein natural gradient operator. The gradient descent iteration can be computed using the forward Euler method with a step size h. Another numerical scheme, the Jordan-Kinderlehrer-Otto (JKO) scheme, can be used with the proximal operator. The Semi-Backward Euler method provides an alternative first-order scheme for the gradient flow of loss. The Semi-Backward Euler method for the gradient flow of loss function F on parameter space is easier to approximate than the forward Euler method. It involves a generator function g and an update formula that can be implemented using a neural network to approximate variable \u03a6. See details in Appendix G. The update in Proposition 3 forms the constrained Wasserstein-2 metric in implicit generative models, allowing for a simpler formulation. This introduces a relaxed Wasserstein metric and a proximal operator algorithm on generators. The gradient constraint is satisfied when the sample space is 1 dimensional, but in general, finding \u03a6 involves computational difficulties. The update in Proposition 3 introduces a relaxed Wasserstein metric and a proximal operator algorithm on generators. When the sample space is high dimensional, computational difficulties arise in finding \u03a6. The algorithm aims to regularize the generator by minimizing the expectation of squared differences in sample space. The effectiveness of Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distributions. The proximal regularization for a loss function is defined, and various statistical distance functions between parameters are checked. The Wasserstein-2 and Euclidean distances are found to be effective, with the Euclidean distance not depending on the model structure. The Wasserstein-1 metric is considered as the loss function in the update steps. The Wasserstein-1 metric is used as the loss function in GAN training. The Relaxed Wasserstein Proximal algorithm improves speed and stability by applying regularization on the generator, offering better performance compared to the Euclidean proximal method. The Relaxed Wasserstein Proximal algorithm introduces novel regularization for the generator during GAN training. It modifies the update rule by introducing hyperparameters and updating the generator before the discriminator. The algorithm is tested on three GAN types using CIFAR-10 and CelebA datasets with the DCGAN architecture. The Fr\u00e9chet Inception Distance is used to measure the quality and convergence of generated samples. The Relaxed Wasserstein Proximal regularization improves the speed and stability of GAN convergence. It shows a 20% improvement in sample quality for DRAGAN and CelebA datasets, as seen in FIG1 and FIG2. The Fr\u00e9chet Inception Distance is used to measure performance and convergence, with lower FID values indicating better results. Multiple generator iterations are performed to compare results based on wallclock time. In DRAGAN, there is a 20% improvement in sample quality according to FID. The CelebA dataset also shows similar results, as depicted in FIG2. Multiple generator iterations can hinder Standard GANs on CelebA initially, but restarting the algorithm leads to successful runs. The defect may be resolved with a more stable loss function like WGAN-GP. Comparing multiple generator updates to discriminator updates, RWP shows better stability and lower FID. Samples from the models are available in Appendix E, and latent space walks were conducted to demonstrate RWP regularization. The use of RWP regularization in GAN training shows improved stability and lower FID compared to standard GANs. Through experiments, it is demonstrated that RWP leads to faster convergence and lower FID values. Additionally, RWP helps in preventing the GAN from memorizing data and shows consistent success in training runs. The use of RWP regularization in GAN training leads to improved stability and lower FID values. The training with RWP shows convergence and lower FID, while without it, training is highly variable. The Semi-Backward Euler method on the CIFAR-10 dataset is comparable to standard training with WGAN-GP loss. The experiment was averaged over 5 runs, and the results are presented in FIG3. Further investigation of the Semi-Backward Euler method is left for future work. In the literature, optimal transport aspects have been applied in machine learning and GANs, with the Wasserstein distance commonly used as the loss function due to its statistical properties and ability to compare probability distributions. In Wasserstein GAN, the Wasserstein-1 distance function is chosen as the loss function, requiring the discriminator to satisfy the 1-Lipschitz condition. Regularization techniques are employed to meet this condition. The regularization of the discriminator aims to meet the 1-Lipschitz condition in Wasserstein GANs. The Wasserstein-2 metric defines a metric tensor structure on the probability space, forming a density manifold. Gradient flows in this manifold are linked to transport-related partial differential equations, such as the Fokker-Planck equation. Different approaches in learning communities leverage the gradient flow structure in probability space and study stochastic gradient descent and nonparametric models like the Stein gradient descent method. Additionally, approximate inference methods are considered for computing Wasserstein gradient flow in the full probability set. The curr_chunk discusses the application of the constrained Wasserstein gradient and its relaxations on implicit generative models, focusing on regularizing the generator in Wasserstein GANs. The approach aims to compute the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space, leading to better minimization results. The proposed method computes the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space, leading to better minimization results in terms of FID with faster convergence speeds. It introduces a Riemannian structure in density space and considers a metric function in the full probability set. The inner product g W endows P + with a Riemannian metric tensor, defining the Wasserstein gradient operator in (P + , g W ). Analytical results on the Wasserstein-2 gradient flow are discussed. The Wasserstein-2 metric and gradient operator are then constrained on statistical models, defined by a triplet (\u0398, R n , \u03c1) with a locally injective parameterization function. A Riemannian metric g is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The Wasserstein statistical manifold is introduced, with associated metric tensor G(\u03b8) assumed to be smooth and positive definite. The metric tensor G(\u03b8) is defined in Theorem 2 for a smooth and positive definite Riemannian manifold. The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2. The proof involves the action function in the Wasserstein statistical manifold and the derivation of the semi-backward method. The geodesic path \u03b8 is denoted and equations 6 and 7 are proven. The proof of Proposition 4 involves the implicit model and the probability density transition equation of g(\u03b8(t), z) satisfying the constrained continuity equation. The Semi-backward method is derived, and the push-forward relation is utilized in the analysis. The proof of Proposition 5 involves computing the proximal operator explicitly for Wasserstein and Euclidean distances. Hyperparameter settings for the Relaxed Wasserstein Proximal experiments are provided, including batch size and optimizer details. The experiments involved different hyperparameter settings for various datasets and GAN models. The settings included batch sizes, optimizers, learning rates, and latent space dimensions. The Relaxed Wasserstein Proximal algorithm was showcased as an easy-to-implement regularization method. The experiments involved different hyperparameter settings for various datasets and GAN models, including batch sizes, optimizers, learning rates, and latent space dimensions. The Relaxed Wasserstein Proximal algorithm was presented as a simple regularization method. The algorithm involves updating the discriminator, performing Adam gradient descent, and repeating the process until a stopping condition is met. Notable differences from standard GAN training include specific terms and the number of generator iterations. Sample images generated using different GAN models and datasets were evaluated based on FID scores. Additionally, a latent space walk technique was used to detect memorization in the generator. The specific hyperparameter settings for training WGAN-GP on CIFAR-10 included a batch size of 64, DCGAN architecture, Adam optimizer with learning rate 0.0002, latent space dimension of 100, and updates to the discriminator, generator, and potential in each outer-iteration loop. Smooth transitions in the latent space walk technique indicated no memorization in the generator. The generator was updated multiple times during training, with iterations for sampling latent data."
}