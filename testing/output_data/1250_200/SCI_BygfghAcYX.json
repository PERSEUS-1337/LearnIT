{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is proposed for two layer ReLU networks, offering a tighter generalization bound. The capacity bound correlates with test error behavior with increasing network sizes and partly explains the improvement in generalization with over-parametrization. Additionally, a matching lower bound for Rademacher complexity is presented, surpassing previous capacity lower bounds for neural networks. Deep neural networks have been successful in various tasks, with over-parametrized networks being able to fit random labels to data. Increasing the capacity of neural networks, even without explicit regularization, has been found to improve generalization error. This goes against traditional wisdom in learning, where higher capacity models are expected to overfit. Empirical observations show that larger models lead to decreased test error in image classification tasks. Increasing the capacity of neural networks has been found to decrease test error in image classification tasks on MNIST and CIFAR-10. Various complexity measures have been proposed to explain this generalization behavior, as traditional measures like VC bounds do not capture it. Even when the network is large enough to fit the training data perfectly, test error continues to decrease for larger networks. The text discusses the observation that test error decreases for larger networks in image classification tasks. It mentions the importance of unit capacity and unit impact in network complexity, as well as existing complexity measures that fail to explain the benefits of over-parametrization. Dziugaite & Roy (2017) evaluated a generalization bound based on PAC-Bayes, showing that numerical generalization bounds increase with network size. The study focuses on simplifying the architecture while preserving key properties, choosing two layer ReLU networks to demonstrate similar behavior to more complex architectures. A tighter generalization bound is proven for two layer ReLU networks, with a capacity bound that correlates with test error and decreases with increasing hidden units. Complexity is characterized at a unit level, highlighting the impact of unit capacity on network complexity. The study focuses on characterizing complexity at a unit level in neural networks. It shows that as the number of hidden units increases, unit level measures decrease faster than 1/ \u221a h, impacting the overall measure. The generalization bound is influenced by the Frobenius norm of the top layer and the difference of hidden layer weights with initialization, which decreases with network size. In the over-parametrized setting, training only the top layer can minimize training error due to the large number of hidden units representing all possible features. Our study investigates the impact of over-parametrization in neural networks. The optimization problem involves selecting the right features to minimize training loss, with a large number of hidden units representing all possible features. Previous research has highlighted the importance of initialization in optimization algorithms, but generalization bounds for neural networks remain unproven. Our contributions include empirical analysis of over-parametrization's role. The study investigates over-parametrization in neural networks, showing that existing complexity measures increase with hidden units but do not explain generalization behavior. Tighter generalization bounds are proven for two-layer ReLU networks, with a proposed complexity measure that decreases with hidden units. A lower bound for Rademacher complexity is provided, surpassing previous bounds and being larger than the Lipschitz constant. The study explores over-parametrization in neural networks, revealing that current complexity measures increase with hidden units but do not clarify generalization behavior. Tighter generalization bounds are established for two-layer ReLU networks, with a complexity measure that decreases with hidden units. A lower bound for Rademacher complexity is introduced, exceeding previous bounds and surpassing the Lipschitz constant. The loss function L \u03b3 (.) is bounded between 0 and 1, with empirical estimates denoted as L \u03b3 (f). Rademacher complexity is a capacity measure capturing the ability of functions in a class to fit random labels. Generalization bounds for neural networks are derived based on the Rademacher complexity. The Rademacher complexity is crucial for understanding generalization error in neural networks. Choosing the right function class is essential to capture the behavior of network layers with increasing hidden units. Experimental observations on CIFAR-10 dataset reveal trends in spectral and Frobenius norms, highlighting the impact of network size on weight initialization. The increase in Frobenius norm of weights in larger networks is attributed to the increase in random initialization. The distance to initialization per unit decreases with network size, with a shift in angles between learned and initial weights. Unit capacity is defined as the per unit distance to initialization. In the second layer, Frobenius norm and distance to initialization decrease with increasing hidden units, suggesting a limited role of initialization for this layer. The Frobenius norm and distance to initialization decrease in the first layer, indicating a limited role of initialization. As network size grows, the norm of outgoing weights from hidden units decreases faster than 1/ \u221a h. This implies that the impact of each classifier on the final decision shrinks at a faster rate. Unit impact, defined as \u03b1 i = v i 2, plays a crucial role in two-layer neural networks. The hypothesis class of neural networks is represented using parameters in a restricted set W. The hypothesis class of neural networks is represented using parameters in the set W. Empirical observations show networks have bounded unit capacity and impact. Studying the generalization behavior of this function class can enhance understanding of these networks. A generalization bound for two-layer ReLU networks is proven by bounding the Rademacher complexity of the class F W. The complexity of the network is decomposed into complexity of hidden units in the proof. The proof introduces a new technique to decompose network complexity into hidden unit complexity, providing a tighter bound on Rademacher complexity for two-layer neural networks. The generalization bound holds for any function in a specific class, with potential for improvement over previous bounds. The generalization error is bounded for neural networks, with an improved bound over existing ones. The Rademacher complexity is explicitly lower bounded, showing tightness. The additive factor in the bound is small in practice, leading to a decrease in capacity with over-parametrization. The generalization bound is extended to p norms in Appendix Section B, with a finer tradeoff between terms. Comparison with Golowich et al. (2018) shows similarities in the first term of the bound. The key complexity term in the bound is U \u2212 U 0 1,2 V 2, which increases with h due to most hidden units having similar capacity. Experimental comparison on CIFAR-10 and SVHN datasets shows networks beyond size 128 can improve generalization even without regularization. Unit capacity and unit impact decrease with increasing h. The effective capacity of the function class decreases with increasing h, as shown in the right panel of FIG0 and second panel of FIG4. The number of epochs needed to achieve a 0.01 cross-entropy loss decreases for larger networks, as depicted in the third panel of FIG4. Generalization bounds scale as C/m, with our bound consistently lower than other norm-based data-independent bounds. Our bound even outperforms VC-dimension for networks larger than 1024. While numerical values are loose, they provide insight into generalization behavior relative to complexity measures. Our capacity bound decreases with network size, unlike other norm-based bounds. It may indicate properties allowing over-parametrized networks to generalize. Comparing networks trained on real vs. random labels shows correlation with generalization behavior. The lower bound for the Rademacher complexity of neural networks matches the dominant term in the upper bound. It extends the lower bound to a bigger function class and includes an additional constraint on the spectral norm of the hidden layer. The complexity lower bound aligns with the upper bound terms of Theorem 1, with adjustments for the Lipschitz constant and specific settings. The lower bound for neural network capacity tightens the upper bound by considering specific settings, such as c = 1 and \u03b2 = 0. This lower bound shows a gap between the Lipschitz constant of the network and the capacity of neural networks, excluding networks with rank-1 matrices as weights. The lower bound for neural network capacity excludes networks with rank-1 matrices as weights, showing a capacity gap between ReLU and linear networks. The construction can be extended to more layers by setting weight matrices in intermediate layers to the Identity matrix. This result improves on previous lower bounds and presents a new capacity bound for neural networks. In this paper, a new capacity bound for neural networks is presented, which decreases with the increasing number of hidden units. The focus is on understanding the role of width in generalization behavior of two layer networks. The interplay between depth and width in controlling network capacity is also explored. Matching lower bounds for capacity are provided, but the absolute values are still larger than the number of training samples. Future studies may investigate optimization algorithms' convergence to low complexity networks and the impact of hyperparameter choices on network complexity. In this experiment, a pre-activation ResNet18 architecture was trained on the CIFAR-10 dataset with specific settings for the network structure. The architecture includes a convolution layer, 8 residual blocks, and a linear layer. Different hyperparameter choices were explored, and the training was done using SGD with specific parameters. In experiments, various architectures were trained on CIFAR-10, SVHN, and MNIST datasets using SGD with specific parameters and data augmentation techniques. Different network sizes were tested, and training stopped when the cross-entropy loss reached a certain threshold or after a set number of epochs. Weight decay, dropout, and batch normalization were not used in the experiments. In experiments, architectures were trained on CIFAR-10, SVHN, and MNIST datasets using SGD. Training stopped when cross-entropy reached 0.01 or after 1000 epochs. Generalization bounds were calculated, and distributions were estimated using Gaussian kernel density estimation. Figures show network behavior on different datasets and comparison of generalization bounds. The left panel of FIG10 illustrates over-parametrization in the MNIST dataset, while the middle and right panels compare generalization bounds. Theorem 2 is generalized to p norm, with Lemma 11 constructing a cover for the p ball with entry-wise dominance. Theorem 5 provides a bound on generalization error for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d. Corollary 6 extends this bound for any function f(x) = V[Ux] +. The generalization error for any function f(x) = V[Ux] + is bounded by a vector-contraction inequality for Rademacher complexities. The Rademacher complexity of the class of networks can be decomposed to that of hidden units, with a bound provided in Lemma 9. The proof involves induction on t in an inequality statement. The proof involves induction on t in an inequality statement, where the Rademacher complexity of the class of networks can be decomposed to that of hidden units. Lemma 10 is used in the proof of Theorem 1, which bounds the generalization error for any function. Lemma 11 introduces a covering lemma to prove the generalization bound without assuming knowledge of network parameter norms. Lemma 13 provides a bound on generalization error based on certain parameters. Lemma 14 provides specific results for the case p = 2, bounding the generalization error for a given function f(x) = V[Ux] +. The proof involves upper bounding the generalization bound from Lemma 13 for p = 2 and a specific choice of parameters. The proof of Theorem 2 directly follows from Lemma 14, utilizing \u00d5 notation to conceal constants and logarithmic factors. Lemma 15 provides a generalization bound for any p \u2265 2, with additional constants and logarithmic factors compared to Lemma 14 for p = 2. The generalization error for a given function f(x) = V[Ux] + is bounded under certain conditions. The proof of Theorem 5 follows from Lemma 15 using \u00d5 notation. The proof of Theorem 3 starts with the case where h = d = 2k, m = n2k for some k, n \u2208 N. The dataset is divided into 2k groups, each with n copies of a different element in the standard orthonormal basis. The matrix U(\u03be) is defined as Diag(\u03b2) \u00d7 F(\u03be), where F(\u03be) is orthonormal. The matrix U(\u03be) is defined as Diag(\u03b2) \u00d7 F(\u03be), where F(\u03be) is orthonormal. U(\u03be) 2 \u2264 max i \u03b2 i."
}