{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a crucial aspect of intelligence, aiding in flexibility and creativity. Neuromorphic computing chips, which mimic the brain, have uncertainty but current neural networks do not account for it. A new training scheme, UATS, incorporates uncertainty into neural networks, resulting in improved performance on neuromorphic chips. Fuzziness and stochasticity are two types of uncertainties in intelligent systems, aiding in human thinking processes. The fuzziness and stochasticity in intelligent systems aid in human thinking processes by efficiently ignoring redundant information and enabling creativity. Existing artificial intelligence systems lack these characteristics, using 32-bit or 64-bit floating numbers to describe weights and activations. Some researchers suggest that 8-bit integers are sufficient for many applications. Methods like network quantization can address these issues. Neuromorphic computing chips, utilizing nanotechnology devices and crossbar structures, offer a hardware approach to address uncertainties in Deep Neural Networks (DNN). The crossbar structure efficiently performs vector-matrix multiplication (VMM) using Ohm's law and Kirchhoff's law, with nanoscale nonvolatile memory (NVM) providing additional storage capability. This computing in memory (CIM) architecture alleviates memory bottlenecks in traditional architectures, making neuromorphic computing chips more energy and area efficient for AI applications. Neuromorphic computing chips are promising for AI applications due to their energy and area efficiency. Uncertainty in these chips comes from fuzziness caused by analog to digital converters and stochasticity induced by NVM devices. The VMM result is analog output, requiring ADC for data transfer, similar to activation quantization in networks. Stochasticity in NVM devices is due to intrinsic physical mechanisms. The stochasticity of NVM devices is caused by random particle movement, leading to varied conductance and different output currents even with the same voltage. This randomness is often simulated as a non-ideal factor affecting network performance. A training scheme leveraging this stochasticity can enhance the performance of neuromorphic computing chips. Various types of NVM devices exist, each with different intrinsic physical mechanisms contributing to their unique stochastic behavior. The Gaussian distribution is commonly used to model device stochasticity, although it may not perfectly fit every device type. The Gaussian distribution is used to model device stochasticity in NVM devices, with the mean representing the conductance value of the stable state. The variance is typically correlated to the mean, and the standard deviation is assumed to be linearly related to the mean. Conductances below a minimum value are cut off, and the model for device stochasticity is defined accordingly. The model of device stochasticity in NVM devices involves sampling conductance values from a Gaussian distribution. Writing the conductance of each device in a neuromorphic computing chip is crucial for AI applications. Mapping processes determine target conductance based on neural network weights, with a mapping algorithm optimizing energy efficiency by using lower conductances to express weights. The mapping algorithm optimizes energy efficiency by using lower conductances to express weights in neuromorphic computing chips. The conductance values are affected by the stochasticity of the device and the precision of the ADC, leading to inaccuracies in writing conductance. A model using Gaussian distribution describes the fuzziness of the conductance values. The uncertainty in neuromorphic computing chips affects the performance of DNNs, leading to decreased classification accuracy. However, an uncertainty adaptation training scheme (UATS) can alleviate this issue by guiding neural networks to learn how to handle uncertainty. The stochasticity model introduces randomness in the feed forward process, using random variables to calculate weights. Conductances of stable states are determined based on the stochasticity model. The fuzziness model is introduced during training, where weights are replaced by random variables. Conductances are calculated based on the fuzziness model. The loss function is calculated by averaging the output of multiple feed forward processes. The uncertainty adaptation training scheme (UATS) was evaluated on various models and datasets, including the MNIST dataset. The uncertainty without UATS on MNIST dataset was investigated using different models. Two MLP models and a CNN model were used with 60,000 images in the training set. The models were tested with varying levels of uncertainty, showing higher test errors without UATS. Without using the UATS, uncertainty increases test errors for MLP and CNN models. CNN model (LeNet-5) performs best without uncertainty but is most affected by it. UATS significantly improves accuracies in fine-tuning and retraining experiments, with retraining showing better results. UATS achieves comparable results to the ideal case when uncertainty level is low. UATS is also validated on CIFAR-10 dataset with ResNet-44 DNN model. The UATS was validated on CIFAR-10 dataset with ResNet-44 DNN model, achieving lower error rates than ideal cases with proper hyper-parameters. It acts as a regularization method for easier DNN training, especially effective with more layers. UATS does not require additional circuits like Bayesian networks, making it more convenient for neuromorphic computing chips. The conductance distribution of the device is not as convenient as UATS, which requires no additional circuit. Various distributions have been tried to model device stochasticity, but the network performance remains similar. Methods to reduce the computation intensity of UATS include sampling weights and using uncertainty models. This accelerates simulation speed while achieving similar results. By sampling weights and using uncertainty models, simulation speed can be accelerated while achieving similar results compared to using the VMM for every input or batch."
}