{
    "title": "BkecJjCEuN",
    "content": "The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask and self-supervised learning. Training an end-to-end audio feature extractor based on WaveNet, the study demonstrates improved performance in supervised classification tasks by up to 6% through simultaneous training with self-supervised tasks and data augmentation. Deep neural networks are crucial for modeling and classifying auditory data. Deep neural networks are essential for modeling auditory data, but training on acoustic waveforms can be challenging due to limited labeled datasets. To address this, the study focuses on incorporating self-supervised auditory tasks during model training to improve generalization. The research identifies effective self-supervised audio tasks and demonstrates their joint training with supervised tasks significantly enhances performance. Additionally, WaveNet is utilized as a feature extractor for rich audio representations from raw waveform data. WaveNet is used as a feature extractor for rich audio representations from raw waveform data. The study explores self-supervised tasks to improve performance on supervised classification tasks like audio tagging, speaker identification, and speech command recognition. Leveraging unlabeled data and data augmentation techniques, the authors demonstrate the effectiveness of self-supervised tasks as a pre-training stage for performance improvements through transfer learning. The text discusses the use of self-supervised learning to improve performance on various tasks by leveraging unlabeled data. Different approaches, such as inpainting, image colorization, and motion segmentation, have shown promising results in the visual domain. The goal is to learn a single general-purpose representation through multitask learning, aiming to uncover underlying structures in sensory environments. The text discusses implementing an end-to-end audio processing network based on WaveNet architecture. The network includes a trunk and head networks trained jointly for various tasks, with a WaveNet trunk consisting of 3 blocks of 6 dilation stacks. The setup was tested on three distinct supervised tasks. The WaveNet trunk has an effective receptive field length of 190 samples. It was tested on audio tagging, speaker identification, and speech command recognition tasks using labeled and unlabeled datasets. The audio tagging task is trained on the FSDKaggle2018 dataset with audio segments cropped to 2 seconds. The output is averaged across time and fed into a fully-connected layer with 512 units and ReLU nonlinearity. Training minimizes a certain criterion. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The architecture includes a global average pooling layer, 2-layer perceptron, batch normalization, and a softmax output layer. The speech command recognition task is trained on the Speech Commands dataset. The speech command recognition task involves a stack of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The convolution layers have widths of 100, 50, and 25 with strides of 16, 8, and 4. Self-supervised tasks like next-step prediction, noise reduction, and upsampling were implemented alongside the main task using data from the Librispeech dataset. The auxiliary tasks in the multitask framework were trained on unlabeled data from the Librispeech dataset. They share a common head architecture with convolutional layers and a regression-type loss function. The focus was on using waveform inputs for audio tasks to allow for a broader range of processing capabilities compared to spectral/cepstral representations. This approach aims to overcome limitations in network architectures and maximize information gained from self-supervised tasks. The study focused on using waveform inputs for audio tasks to overcome limitations in network architectures and maximize information gained from self-supervised tasks. Multitask training with three self-supervised tasks improved performance on supervised tasks without increasing training data. Incorporating larger versions of Librispeech into training regimen showed further improvements. Incorporating larger versions of Librispeech into training regimen improved performance on supervised tasks. With additional unlabeled data, there was a significant increase in performance metrics, such as a MAP@3 increase of up to .056 with 500 hours of data. Multitask learning showed improvements in speech command classification and speaker identification tasks. Top-5 classification performance for speaker identification peaked at 75.22%. Comparing these results with data augmentation techniques highlighted the effectiveness of multitask learning in improving label efficiency. In comparison to data augmentation techniques, multitask learning with pitch-shift and noise augmentation showed significant improvements in label efficiency. The combination of both methods yielded the highest performance increase of .089 in MAP@3, suggesting their complementary nature. Transfer learning from self-supervised tasks on unlabeled data also played a role in enhancing performance metrics. Transfer learning from self-supervised tasks on unlabeled data can enhance performance metrics. By pre-training self-supervised tasks and then fine-tuning with labeled data, improved performance on supervised tasks can be achieved. This approach, using a WaveNet-based model on raw audio waveforms, shows scalability with the quantity of unlabeled data and can supplement existing data augmentation techniques. The results favor transfer learning over simultaneously training all tasks together, leading to performance gains on audio classification tasks. Our approach of using a WaveNet-based model on raw audio waveforms shows scalability with unlabeled data and can enhance performance metrics on supervised audio tasks. The multitasking model can learn to forecast audio frames, remove noise, and perform upsampling, forming a representation of the audio that can be further explored for broader auditory tasks. Our model, based on the WaveNet architecture, utilizes causal dilated convolutions to process high temporal resolution raw audio signals efficiently. The model consists of a task-agnostic trunk following the WaveNet structure, with stacked dilated causal convolutions and task-specific heads. The WaveNet trunk is composed of N blocks, each containing S dilated causal convolution layers with increasing dilation factors. The WaveNet trunk consists of N blocks, each with S layers. Each layer involves a \"residual atom\" computation producing hidden state vector h and layer output x. The first layer applies causal convolutions to raw audio waveforms to produce an output. Each block has an effective receptive field of 1 + b(2S-1), with a total effective receptive field of \u03c4 = 1 + N(2S-1). After hyperparameter search, N = 3 blocks with S = 6 layers each were chosen. The WaveNet trunk consists of N blocks with S layers, resulting in a total receptive field of \u03c4 = 190. Each task-specific head processes input data through the shared trunk independently. Tasks include supervised classification like \"audio tagging\" and self-supervised tasks like \"next-step prediction\" and \"noise reduction\". Task-specific optimizers and learning rates are customized. The next-step prediction task involves predicting the next value in a sequence of audio frames. This task allows for training on unlabeled data and uses a simple 2-layer convolutional architecture. The head takes in \u03c4 frames from the trunk and outputs a prediction for the next audio frame. The next-step prediction task involves predicting the next value in a sequence of audio frames using a regression approach. The model is trained to predict the clean sample given noisy input, treating noise as an additive random process on top of the true signal. The model is trained for denoising by predicting clean samples from noisy ones, using a structure similar to next-step prediction. A smoothed L1 loss is used for stability in convergence. Upsampling can be achieved by downsampling the audio source, with the original signal as the target, akin to super-resolution in computer vision. The original audio is downsampled to 4 kHz for the upsampling task, with the network inferring high frequency information lost during the transform. A smooth L1 loss function is used to compare the estimated upsampled audio with the original, trained on raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets. All code was written in the PyTorch framework, with audio samples cropped to two seconds and downsampled to 16 kHz for normalization. The experiments in the PyTorch framework involved cropping audio samples to two seconds, downsampling to 16 kHz, and normalizing inputs. Noise was added from ChiME3 datasets for a noise-reduction task. Hyperparameter search was conducted for the network architecture with varying block and layer numbers. Performance was not significantly affected by architecture specifications. The network's performance and training characteristics were not significantly affected by architecture specifications. Hyperparameter search was conducted for auxiliary task heads, with a focus on learning rate. The model was trained on all tasks simultaneously using a weighted sum of losses. The \"Adam\" optimizer was used with specific parameters, and the learning rate was decayed every 5 epochs. The optimizer BID6 was used with specific parameters, and the learning rate was decayed every 5 epochs. A batch size of 48 was utilized for all experiments. Noise reduction and upsampling tasks required separate forward propagation of noisy and downsampled audio. Important parameters can be found in TAB3."
}