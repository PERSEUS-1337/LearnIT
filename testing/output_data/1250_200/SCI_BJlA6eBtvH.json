{
    "title": "BJlA6eBtvH",
    "content": "Continual learning is a challenge for neural networks due to catastrophic forgetting. A Differentiable Hebbian Consolidation model is proposed to address this issue by adding a rapid learning plastic component to the fixed parameters of the softmax output layer. This allows learned representations to be retained for a longer timescale, even in scenarios with non-stationary data distribution or imbalanced datasets. The model is evaluated on various benchmarks including Permuted MNIST, Split MNIST, and an imbalanced variant of Permuted MNIST. Our proposed model addresses catastrophic forgetting in neural networks by introducing an imbalanced variant of Permuted MNIST. It outperforms comparable baselines by reducing forgetting, crucial for adapting in dynamic environments. The model is designed to handle non-stationarity in real-world deployment, where performance degrades when exposed to distributional changes over time. Catastrophic forgetting, also known as catastrophic interference, is a significant issue for deep neural networks in continual learning tasks. The goal of continual learning is to adapt to new tasks without forgetting previously learned ones, enabling scalable and efficient models over time. In real-world applications, the assumption of independent and identically distributed samples is often violated due to concept drift, imbalanced class distributions, and incomplete initial data. This leads to a \"stability-plasticity dilemma\" for learning systems, a well-known challenge for neural networks. In continual learning, the \"stability-plasticity dilemma\" poses a challenge for neural networks to balance plasticity for new knowledge integration and stability for preserving existing knowledge. Synaptic plasticity in biological neural networks is crucial for learning and memory, with theories like synaptic consolidation and the complementary learning system explaining continual learning in humans. Recent work on differentiable plasticity has shown that neural networks can be trained end-to-end through backpropagation and stochastic gradient descent to optimize both slow weights for long-term memory and fast weights for short-term memory. Fast weights change quickly based on input representations and enable reactivation of long-term memory traces in slow weights. Differentiable Hebbian Consolidation 1 model extends the work on differentiable plasticity to task-incremental continual learning. It adapts quickly to changing environments by adjusting the plasticity of synapses and consolidating previous knowledge. The model modifies the traditional softmax layer and augments the slow weights in the final fully-connected layer with plastic weights. The proposed model combines Hebbian plasticity with task-specific synaptic consolidation methods to adapt rapidly to new data while preserving previous knowledge. It is tested on various benchmark problems and outperforms traditional networks. Neural Networks with Non-Uniform Plasticity: Hebbian learning theory suggests that learning and memory are due to synaptic plasticity, where the strength of synapses is modified based on correlated activation of neurons. Recent meta-learning approaches incorporate fast weights for one-shot and few-shot learning. Recent advancements in meta-learning have introduced the incorporation of fast weights into neural networks for one-shot and few-shot learning. Different approaches have been proposed, such as augmenting FC layers with fast weights to bind labels to representations, utilizing Hebbian learning-based associative memory, and introducing a Hebbian Softmax layer for improved learning of rare classes. Additionally, differentiable plasticity has been suggested, optimizing the plasticity of synaptic connections alongside fixed weights. While these methods have shown promise in training neural networks, they have mainly been demonstrated on recurrent neural networks for tasks like pattern memorization and maze exploration with reinforcement learning. Our work focuses on overcoming catastrophic forgetting in neural networks by augmenting slow weights in the FC layer with fast weights using DHP. We update only the parameters of the softmax output layer to achieve fast learning and preserve knowledge over time. Two strategies are employed: Task-specific Synaptic Consolidation to protect previously learned knowledge and CLS Theory, which utilizes a dual memory system to extract structured representations and perform rapid learning. The text discusses regularization strategies in continual learning to prevent catastrophic forgetting. It involves estimating the importance of each parameter or synapse to retain memories for long timescales. A regularizer is added to the loss function when learning new tasks to adjust plasticity and prevent changes to important parameters of previously learned tasks. Regularization strategies in continual learning aim to prevent catastrophic forgetting by estimating the importance of network parameters or synapses. Different methods, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), compute the importance of parameters differently to control forgetting. EWC uses an offline approach with the Fisher information matrix, while SI and MAS use online methods based on cumulative changes or sensitivity to perturbations. These methods help adjust plasticity during new task learning to retain important memories. Our work is inspired by CLS theory and focuses on neuroplasticity techniques to alleviate catastrophic forgetting in continual learning. Previous approaches based on CLS principles include pseudo-rehearsal, exact replay, and generative replay methods. While iCaRL uses rehearsal and regularization with an external memory, our work explores techniques for preserving memories without forgetting. Neuroplasticity techniques inspired by CLS theory aim to address catastrophic forgetting in continual learning. Previous research has shown the use of slow and fast weights to store long-term knowledge and temporary associative memory. Recent studies have explored various methods such as replacing soft attention mechanisms with fast weights in RNNs, Hebbian Softmax layer, augmenting slow weights with fast weights matrix, differentiable plasticity, and neuromodulated differentiable plasticity. These methods focus on rapid learning on simple tasks or meta-learning. Our work focuses on evaluating methods for continual learning, specifically on metalearning a local learning rule for fast weights in the softmax layer. The model incorporates slow weights and an SGD optimizer to address the limitations of existing approaches designed for rapid learning on simple tasks or meta-learning over task distributions. Our model incorporates slow weights and a Hebbian plastic component in the softmax layer, with a scaling parameter \u03b1 and Hebbian traces accumulating hidden activations. The plastic connections adjust with a learning rate parameter \u03b7, dynamically acquiring new experiences. The model incorporates slow weights and a Hebbian plastic component in the softmax layer, with parameters optimized by gradient descent. The plastic connections adjust with a learning rate parameter \u03b7, preventing instability in Hebbian traces. During training, Hebbian traces are updated continually, while at test time, the most recent information is used. The model utilizes a Hebbian plastic component in the softmax layer, updating Hebbian traces during training and using them for predictions at test time. This approach improves initial representations and retains learned deep representations for longer timescales. The model's fast learning is enabled by a highly plastic weight component, which decays between tasks to prevent interference but consolidates into a stable component to protect old memories. This DHP Softmax method is simple to implement and scales easily without requiring additional space or computation. The DHP Softmax method utilizes a Hebbian plastic component in the softmax layer, allowing for rapid learning and sparse parameter updates to store memory traces efficiently. This approach improves learning of rare classes and speeds up binding by accumulating hidden activations into compressed episodic memory traces. The DHP Softmax method utilizes Hebbian synaptic consolidation to improve learning of rare classes and speed up binding of class labels without introducing additional hyperparameters. This approach updates synaptic importance parameters online and only regularizes the slow weights of the network, adapting existing task-specific consolidation approaches. The DHP Softmax method uses Hebbian synaptic consolidation to enhance learning of rare classes and accelerate binding of class labels. It updates synaptic importance parameters online and only regularizes the slow weights of the network. The approach includes a plastic component in the softmax layer to prevent catastrophic forgetting of consolidated classes. Experimental comparisons are made with vanilla neural networks using Online EWC, SI, and MAS. Additional slow weights are added to the softmax output layer to match the increased capacity of the DNN due to the plastic weights. The study tested a neural network on various benchmarks to evaluate memory retention and flexibility. They measured forgetting using the backward transfer metric and compared their model with other methods. The study evaluated memory retention and flexibility of a neural network on various benchmarks. They compared their model with other methods using the backward transfer metric. The neural networks were trained with Online EWC, SI, and MAS consolidation methods on all tasks sequentially. The input distribution changed between tasks, indicating concept drift. The benchmarks used permuted MNIST and imbalanced permuted MNIST with MLP networks. The plastic component's \u03b7 value was set to 0.001. The study compared their model with other methods using the backward transfer metric on various benchmarks, including permuted MNIST. The plastic component's \u03b7 value was set to 0.001, with little tuning effort. The network with DHP Softmax showed improvement in alleviating catastrophic forgetting compared to the baseline. Ablation study examined structural parameters and Hebb traces for interpretability. The behavior of the proposed model during training on the Permuted MNIST benchmark is analyzed. The synaptic connections become more plastic to acquire new information quickly, then decay to prevent interference between learned representations. The Hebb trace grows without runaway positive feedback, maintaining a memory of recent activity. Plasticity coefficients grow within each task, indicating the network leverages the structure in the plastic component. Gradient descent and backpropagation are used for meta-learning to tune structural parameters. The study introduces the Imbalanced Permuted MNIST problem. The Imbalanced Permuted MNIST problem introduces imbalanced class distributions, hindering predictive performance. DHP Softmax achieves 80.85% accuracy after learning 10 tasks sequentially, showing a 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer. DHP Softmax with MAS achieves a 0.04 decrease in BWT. DHP Softmax with MAS achieves a 0.04 decrease in BWT, resulting in an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The MNIST dataset was split into 5 binary classification tasks, with disjoint output spaces. DHP Softmax alone achieves 98.23% accuracy, providing a 7.80% improvement compared to a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreases BWT. Combining DHP Softmax with task-specific consolidation consistently decreases BWT, leading to a higher average test accuracy across all tasks, especially the most recent one, T5. Continual learning is performed on a sequence of 5 vision datasets using a CNN architecture with an initial \u03b7 parameter value of 0.0001. Training is done with mini-batches of size 32 and plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. DHP Softmax plus MAS decreases BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. Adding compressed episodic memory in the softmax layer through DHP and performing task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. This allows new information to be learned without interference, improving average test accuracy and reducing backward transfer. SI with DHP Softmax outperforms other methods, achieving an average test performance of 81.75% and BWT of -0.04 after learning all tasks. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically scales the magnitude of plastic connections, allowing for generalization across experiences. DHP Softmax showed improvement compared to traditional softmax, without introducing additional hyperparameters. The model's flexibility includes Hebbian Synaptic Consolidation with EWC, SI, or MAS to alleviate catastrophic forgetting. DHP Softmax, combined with consolidation methods like EWC, SI, or MAS, improves a model's ability to reduce catastrophic forgetting when learning multiple tasks sequentially. The combination of DHP Softmax and SI outperforms other methods on Split MNIST and 5-Vision Datasets. Additionally, combining DHP Softmax with MAS consistently yields superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The model's Hebbian plasticity enables continual learning and memory retention, leading to lower negative BWT and higher test accuracy compared to methods without DHP. Hebbian plasticity in neural networks enables continual learning and memory retention, reducing catastrophic forgetting in dynamic environments. Continual synaptic plasticity plays a key role in learning from limited labeled data and adapting at long timescales. This work aims to explore gradient descent optimized Hebbian consolidation for learning and memory in DNNs. In a continual learning setup, a neural network model is trained on a sequence of tasks, each with its own task-specific loss and associated training data. After training a neural network model on a sequence of tasks with task-specific losses and associated training data, a regularizer loss term is combined to prevent catastrophic forgetting. The model learns an approximated mapping to the true underlying function, mapping new inputs to target outputs for all tasks learned. Different classes are contained in each task, as seen in benchmarks like SplitMNIST and Vision Datasets Mixture. Experiments were conducted on Nvidia Titan V or RTX 2080 Ti, with training using mini-batches and plain SGD optimization. Early stopping is performed if validation error does not improve for 5 epochs, with training terminated and network weights reset if validation error increases for more than 5 epochs. For Permuted MNIST experiments, regularization hyperparameters were set for task-specific consolidation methods. Grid search was performed to find the best hyperparameter combination for each method. Training samples were artificially removed from each class in the Imbalanced Permuted MNIST problem. For the Imbalanced Permuted MNIST problem, training samples were removed from each class based on random probabilities. The distribution of classes for each task is shown in Table 2. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. A grid search was conducted to find the best hyperparameter combination for each method. In Online EWC, values of \u03bb were tested in the range of {50, 100,...,1\u00d710^3}, while for SI and MAS, \u03bb values were tested in different ranges. Random probabilities were used to remove training samples from each class. Regularization hyperparameters were determined for task-specific consolidation methods in the Split MNIST experiments. A grid search was performed to find the best hyperparameter combination for each method. The Vision Datasets Mixture benchmark consists of a sequence of 5 tasks with different image classification datasets. The notMNIST dataset includes font glyphs for letters 'A' to 'J'. The CNN architecture for image classification datasets MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 consists of 2 convolutional layers with 20 and 50 channels, LeakyReLU nonlinearities, max-pooling operations, and an FC layer before the final softmax output layer. The datasets vary in size and content, with different numbers of grayscale or color images for training and testing. The CNN architecture for image classification datasets includes 2 convolutional layers with 20 and 50 channels, LeakyReLU nonlinearities, max-pooling operations, and an FC layer before the final softmax output layer. A multi-headed approach was used due to different class definitions between datasets. The model has a trainable \u03b7 value for each connection in the final output layer, improving stability of optimization. Separate \u03b7 parameters for each connection allow for modulation of plasticity. Using a single \u03b7 value across all connections led to instability in optimization. Hyperparameters for the experiments include regularization values for task-specific consolidation methods. The regularization hyperparameters \u03bb for task-specific consolidation methods are \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. A random search was conducted to find the best hyperparameter combination for each method. Sensitivity analysis on the Hebb decay term \u03b7 showed that setting it to low values led to the best performance in continual learning setups. The sensitivity analysis on the Hebb decay term \u03b7 for continual learning setups showed that setting it to low values led to the best performance in alleviating catastrophic forgetting. The DHP Softmax model, implemented in PyTorch, adds compressed episodic memory to a neural network through plastic connections. It outperforms Finetune in class-incremental learning setups, showing significant improvements on tasks from CIFAR-10 and CIFAR-100 datasets. DHP Softmax (purple) outperforms Finetune (yellow) in class-incremental learning. It performs as well as training from scratch on some tasks. Test accuracies were compared with Finetune and SI (turquoise) from von Oswald et al. (2019)."
}