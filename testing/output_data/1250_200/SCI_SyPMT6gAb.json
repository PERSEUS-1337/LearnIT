{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. The challenge is to develop counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over conventional baseline algorithms. Off-policy learning is crucial for evaluating deterministic policies using historical data, avoiding costly and risky on-policy evaluations in real-world scenarios. Off-policy learning involves utilizing historic data to evaluate policies before deployment, minimizing risks and costs associated with trials. Various methods like Q learning and doubly robust estimator have been studied in reinforcement learning and contextual bandits. A new direction in off-policy learning involves using logged interaction data with limited feedback, such as scalar rewards. This approach aims to understand the relationship between policy changes and rewards, optimizing decision-making processes in systems like online recommendation systems. The text discusses off-policy learning in bandit feedback cases, focusing on the challenge of handling distribution mismatch between logging and new policies. A new counterfactual risk minimization framework is proposed, incorporating sample variance as a regularization term. However, limitations in policy parametrization and computational efficiency remain, making end-to-end training algorithms challenging to derive. The paper proposes a new learning principle for off-policy learning with bandit feedback, regularizing the generalization error of the new policy. It suggests parametrizing the policy as a neural network and solving the divergence minimization problem using recent techniques. Experimental results show significant performance improvement over conventional baselines. The curr_chunk discusses off-policy learning with logged bandit feedback, where a policy maps inputs to outputs using stochastic policies. Actions are taken by sampling from the distribution over the output space. Feedback is observed by comparing the selected action to an underlying 'best' action. In off-policy learning with logged bandit feedback, the goal is to find a policy with minimum expected risk on test data using data collected from a logging policy. Challenges include skewed distribution of the logging policy and the need to estimate expectations. In off-policy learning with logged bandit feedback, challenges arise due to skewed distribution of the logging policy and the need for expectation estimation. To address these issues, empirical estimation using finite samples is used, leading to generalization error and requiring additional regularization. The propensity scoring approach with importance sampling is employed to account for distribution mismatch. Counterfactual risk minimization is proposed to address flaws in the vanilla approach, such as variance regularization using empirical Bernstein bounds. The authors approximated the regularization term via first-order Taylor expansion to develop a stochastic optimization algorithm. Instead of estimating variance empirically, they derived a variance bound directly from the parametrized distribution, utilizing importance sampling weights. The authors developed a stochastic optimization algorithm by approximating the regularization term with a first-order Taylor expansion. They derived a variance bound from the parametrized distribution using importance sampling weights, leading to a generalization bound between expected risk and empirical risk. The authors derived a generalization bound between expected risk and empirical risk using distribution divergence. The proof involves Bernstein inequality and second moment bound, highlighting bias-variance trade-offs in empirical risk minimization problems. This motivates minimizing variance regularized objectives in bandit learning settings. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance. The authors explore an alternative formulation of variance regularized objectives in bandit learning settings. They introduce a constrained optimization formulation with a pre-determined constant \u03c1 as the regularization hyper-parameter. The new objective function provides a good surrogate for the true risk, with the difference bounded by \u03c1 and approaching 0 as N \u2192 \u221e. The new objective function in bandit learning settings removes the need to compute sample variance in existing bounds. Recent f-gan networks and Gumbel soft-max sampling can help minimize variational divergence. The stochasticity of the logging policy is crucial for effective learning, as deterministic policies with peaked masses make learning difficult. The derived variance regularized objective requires minimizing the square root of the conditional density. By connecting the divergence to f-divergence measures, a lower bound can be reached using the f-GAN for variational divergence minimization method. Applying Fenchel convex duality gives the dual formulation, where the bound is tight when T 0 (x) = f (h/h 0 ). The third inequality follows by restricting T to a family of functions. Neural networks can approximate continuous functions with any desired precision. The final objective is a saddle point of a function mapping input pairs to a scalar value, with the policy acting as a sampling distribution. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence, denoted by D f = sup T T dhdx \u2212 f * (T )dh 0 dx. The estimation error is decomposed into terms related to neural network approximation and empirical mean estimation. The strong law of large numbers applies to the difference between empirical and population distributions, with T 0 = h(y|x) h0(y|x) under bounded loss assumption. The law of large numbers applies to the terms involving probability density functions. By using a generative-adversarial approach, the T function is represented as a discriminator network and the policy distribution is parametrized as a generator neural network. Gumbel soft-max sampling methods are used for differential sampling in structured output problems. The complete training procedure is listed for optimization. The algorithm presented optimizes the generator distribution to minimize divergence from the initial distribution. It involves sampling real and fake data, updating parameters, and minimizing variance regularization. The training procedure for counterfactual risk minimization from logged data is detailed, along with the robust regularized formulation. The algorithm works in two separate training steps: 1) updating the policy parameters to minimize the reweighed loss and 2) updating the generator and discriminator parameters to regulate variance for improved generalization performance. Exploiting historical data is crucial in bandit problems, with approaches like doubly robust estimators being proposed. Bandit and its variants, such as contextual bandit, have wide applications. Approaches like doubly robust estimators have been proposed, and recent theoretical studies have explored the finite-time minimax risk lower bound of the problem. Bandit problems can be interpreted as single-state reinforcement learning problems, with techniques like Q function learning and temporal difference learning being alternatives for off-policy learning. Recent works in deep reinforcement learning have addressed off-policy updates using methods like multi-step bootstrapping and off-policy training of Q functions. Learning from logs traces back to where propensity scores are applied to evaluate candidate policies. In statistics, the problem is also described as treatment effect estimation, focusing on estimating the effect of an intervention. In statistics, treatment effect estimation focuses on estimating the effect of an intervention from observational studies. Techniques like unbiased counterfactual estimators and variance regularization are used in computational advertising and bandit learning. Variance regularization also applies to off-policy learning with bandit feedback and generalization bounds in importance sampling problems. Divergence minimization techniques can potentially be applied to supervised learning as well. Our divergence minimization technique can be applied to supervised learning and domain adaptation problems as an alternative to address distribution match issues. Regularization for our objective function is closely connected to distributionally robust optimization techniques, where we minimize the empirical risk over an ellipsoid uncertainty set. The Wasserstein distance between empirical and test distributions is a well-studied constraint that achieves robust generalization performance. To evaluate our algorithms, we convert supervised learning to bandit feedback method by constructing a logging policy and sampling predictions for each sample. Additionally, we use a conditional random field policy trained on a subset of the dataset as a benchmark. Our divergence minimization technique can be applied to supervised learning and domain adaptation problems as an alternative to address distribution match issues. For benchmarks, a conditional random field (CRF) policy trained on a subset of the dataset is used. Bandit feedback datasets are created by passing samples to the logging policy and recording actions, loss values, and propensity scores. Evaluation metrics for the probabilistic policy include expected loss and average hamming loss of maximum a posteriori probability prediction. The need for sampling in practice is highlighted, with a focus on the importance of considering diverse predictions for generalization performance. Various algorithms like IPS and POEM are compared using different optimization solvers. Neural network policies without divergence regularization are also examined. Four multi-label classification datasets are used for the supervised to bandit conversion, with details on hyperparameter selection and methods provided in the original paper. The study utilized four multi-label classification datasets for supervised to bandit conversion, implementing neural network policies with divergence regularization. Training was done using PyTorch on Nvidia K80 GPU cards, with results averaged over 10 experiments and reported in evaluation metrics. Gumbel-softmax sampling schemes were employed for regularized neural network policies. The study implemented neural network policies with divergence regularization using Gumbel-softmax sampling schemes. Results showed significant improvement in test performance compared to baseline CRF policies. Additional variance regularization further improved testing and MAP prediction loss. No significant difference was observed between the two Gumbel soft-max sampling schemes. The effectiveness of variance regularization was studied quantitatively by varying the maximum number of iterations in each divergence minimization sub loop. The study implemented neural network policies with divergence regularization using Gumbel-softmax sampling schemes, showing significant improvement in test performance. Results indicated that models with no regularization had higher loss and slower convergence rates. Increasing the number of maximum iterations for divergence minimization led to faster decrease in test loss and better test performance. The regularization term helped generalize better to test sets and improve convergence speed. Theoretical bounds suggested that generalization performance improves with more training samples. Varying the number of passes of training data in the bandit dataset also impacted the logging policy's action sampling. The study implemented neural network policies with divergence regularization using Gumbel-softmax sampling schemes, showing significant improvement in test performance. Increasing the number of training samples in the bandit dataset led to better test performance and stable results. Regularized policies demonstrated better generalization compared to models without regularization. However, as the number of training samples increased, MAP prediction performance started to decrease, indicating potential overfitting. Experimental results compared two training schemes and Gumbel-softmax sampling methods, highlighting differences in performance. In addition to comparing Gumbel-softmax sampling schemes, the study discusses the impact of logging policies on learning performance. The algorithm's ability to improve policy depends on the stochasticity of the logging policy, which is tested by modifying parameters. The study also explores the effect of introducing a temperature multiplier on CRF logging policies. The study explores the impact of a temperature multiplier \u03b1 on CRF logging policies. By varying \u03b1, it was found that NN policies outperform logging policies when the stochasticity of h0 is sufficient. However, as the temperature parameter increases beyond 2/3, it becomes harder to learn improved NN policies. Stronger regularization in NN policies shows slightly better performance, indicating the robustness of the learning principle. The decreasing stochasticity of h0 makes it challenging to obtain an improved NN policy. Our regularization helps the model be more robust and achieve better generalization performance. As the quality of h0 improves, the models constantly outperform the baselines, but the difficulty also increases. The impact of logging policies on our learned improved policies is discussed, showing a trade-off between policy accuracy and sampling biases. Varying the proportion of training data points used to train the logging policy affects the performance of our improved policies. In this paper, a new training principle was proposed to improve off-policy learning for logged bandit datasets by regularizing variance. The training objective combines importance reweighted loss with a regularization term measuring distribution divergence between logging and learned policies. Neural network policies were trained end-to-end using variational divergence minimization and Gumbel soft-max sampling techniques, proving effective on benchmark datasets. The study introduces a new training principle for off-policy learning in logged bandit datasets, utilizing importance reweighted loss and regularization to minimize distribution divergence. Neural network policies are trained end-to-end with soft-max sampling techniques, showing effectiveness on benchmark datasets. Limitations include the need for propensity scores, but learning to estimate them can enhance algorithm applicability. The work can be extended to general supervised and reinforcement learning. The study explores a new training principle for off-policy learning in logged bandit datasets, using importance reweighted loss and regularization to minimize distribution divergence. Neural network policies are trained end-to-end with soft-max sampling techniques, proving effective on benchmark datasets. The method requires propensity scores but learning to estimate them can broaden algorithm applicability. This approach can be extended to general supervised and reinforcement learning. The study introduces a new training principle for off-policy learning in logged bandit datasets, utilizing importance reweighted loss and regularization to minimize distribution divergence. Neural network policies are trained end-to-end with soft-max sampling techniques, showing effectiveness on benchmark datasets. The method requires propensity scores, and learning to estimate them can expand algorithm applicability. This approach can be extended to general supervised and reinforcement learning. The algorithm minimizes variance regularized risk through updating the generator and estimating the generator gradient. The statistics of the datasets are reported in a table, showing the effect of stochasticity of policies on test loss with MAP predictions. As the logging policy becomes more deterministic, neural network policies can still improve over the baseline policy in expected loss, but not necessarily in MAP predictions. Further investigation is needed to understand this phenomenon. Neural network policies can improve over the baseline policy in expected loss as logging policy quality increases, but struggle to beat MAP predictions if the logging policy was trained with full data in a supervised manner."
}