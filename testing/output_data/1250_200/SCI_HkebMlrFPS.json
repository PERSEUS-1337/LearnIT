{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning and outperform strong baselines on various NLP tasks. The model directly predicts the cluster centers from the input text sequence, providing a more interpretable semantic representation. The curr_chunk discusses the limitations of single embedding models in NLP and introduces multi-mode word embeddings to address the issue. These embeddings represent each target word as multiple points in a distributional semantic space by clustering words appearing beside the target word. This approach allows for a more nuanced analysis of words and phrases with multiple senses or topics. The curr_chunk discusses the challenges of extending multi-mode representations to arbitrary sequences like phrases or sentences due to efficiency issues in clustering-based approaches. The number of unique phrases and sentences in a corpus is much larger than the number of words, leading to difficulties in estimating and storing a large number of parameters. The curr_chunk discusses the challenges of estimating and storing a large number of parameters for unique sequences, especially for long sequences like sentences. It proposes a compositional model that learns to predict cluster centers' embeddings from the sequence of words in the target phrase to overcome sparseness in co-occurring statistics. The model aims to compress redundant parameters in local clustering problems to address efficiency issues in clustering-based approaches. The curr_chunk discusses using a neural encoder and decoder to compress redundant parameters in local clustering problems. Instead of clustering words at test time, a mapping is learned between target sequences and cluster centers during training. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. The model is trained jointly and end-to-end, showing promising results in experiments. The proposed model utilizes encoder and decoder to compress parameters in clustering problems, improving unsupervised phrase similarity tasks. It outperforms traditional word embedding methods and can measure asymmetric relations like hypernymy without supervision. The multimode representation excels in sentence representation, as seen in extractive summarization experiments. The model generates codebook embeddings to reconstruct co-occurring words, avoiding common topics. It uses neighboring words to train for sentence representation, distinguishing between phrases and sentences. Multiple embeddings are predicted by the model for clustering centers. The model generates codebook embeddings to reconstruct co-occurring words, focusing on semantics over syntax. It clusters words that could potentially occur together, learning from similar sequences. The model considers word order in the input sequence but not in the co-occurring words. It models the distribution of co-occurring words in a pre-trained word embedding space. The model uses codebook embeddings to cluster co-occurring words in a pre-trained word embedding space. It predicts cluster centers for input sequences and applies k-means clustering to generate centers in an arbitrary order. The reconstruction loss is calculated based on the clustering in the word embedding space. Non-negative sparse coding allows for positive coefficients in the clustering process. The neural network uses non-negative sparse coding to generate diverse cluster centers, as opposed to k-means clustering which collapses to fewer modes. The NNSC loss is smoother and easier to optimize, leading to better reconstruction of co-occurring words. The coefficient values are constrained to avoid instability in predicting centers. The proposed method uses non-negative sparse coding to generate diverse cluster centers, improving the reconstruction of co-occurring words. The loss function minimizes L2 distance in a pre-trained embedding space and efficiently estimates M Ot using convex optimization. The neural network architecture is similar to Word2Vec, encoding compositional meaning and decoding multiple embeddings. The neural network architecture proposed in the study is similar to Word2Vec but focuses on encoding compositional meaning and decoding multiple embeddings. Unlike typical seq2seq models, the decoder in this architecture does not make discrete decisions and outputs a sequence of embeddings instead of words. This allows for predicting all codebook embeddings in a single forward pass while capturing dependencies between outputs without auto-regressive decoding. Different linear layers are used to make codebook embeddings capture different aspects. The study proposes a neural network architecture similar to Word2Vec but focuses on encoding compositional meaning and decoding multiple embeddings. Different linear layers are used to make codebook embeddings capture different aspects, and attention on contextualized word embeddings significantly affects sentence representation. The framework is flexible, allowing for the incorporation of other input features and different architectures like (bi-)LSTMs. The attention connection between encoder and decoder is removed for phrase representation evaluation. The study introduces a neural network architecture that focuses on encoding compositional meaning and decoding multiple embeddings. Different linear layers are used to capture various semantic facets of a phrase or sentence. The model is trained on Wikipedia 2016 data without the need for additional linguistic resources. The codebook embeddings improve the performance of unsupervised semantic tasks, indirectly measuring the quality of generated topics. Our models do not require resources like PPDB or other multi-lingual resources, making them practical for domains with low resources such as scientific literature. The transformers have a dimension size of 300, trained on a single GPU within a week. However, due to their small size, the models tend to underfit the data. Comparing with BERT, our models have fewer parameters, output dimensions, and computational resources. BERT, trained on a masked language modeling loss, produces effective pretrained embeddings for downstream tasks. BERT uses a word piece model to address out-of-vocabulary issues and provides unsupervised performances based on cosine similarity. Standard benchmarks for evaluating phrase similarity include Semeval 2013 task 5(a) English and Turney 2012. BiRD and WikiSRS contain ground truth phrase similarities derived from human annotations. Different tasks involve distinguishing similar phrase pairs and identifying the most similar unigram to a query bigram. Scoring functions are used to measure phrase similarity in the model. Our model evaluates two scoring functions for measuring phrase similarity: Ours Emb averages contextualized word embeddings from a transformer encoder to compute cosine similarity between phrase embeddings. We calculate symmetric distance SC by comparing normalized codebook embeddings of two phrases. When ranking for similar phrases, negative distance represents similarity. Performance is compared with 5 baselines including GloVe Avg, Word2Vec Avg, BERT CLS, BERT Avg, and FCT LM Emb. Our models significantly outperform baselines in 4 datasets, especially in Turney. Non-linearly composing word embeddings improves performance compared to GloVe, Word2Vec, and FCT baselines. Ours (K=1) performs slightly better than Ours (K=10), supporting the idea that multi-mode embeddings may not always enhance performance. Despite this, Ours (K=10) remains strong compared to baselines, showing that similarity performance is not highly affected by the number of modes. The performances of Ours (K=10) remain strong compared to baselines, indicating that similarity performance is not sensitive to the number of clusters. STS benchmark is a widely used sentence similarity task where models predict semantic similarity scores. Additionally, comparisons are made with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). In STS benchmark, Arora et al. (2017) propose a method called GloVe SIF, which weights words in sentences based on their probability in the corpus. The method involves removing the first principal component from the weighted words. Another method, GloVe Prob_avg, does not involve this post-processing step. The performance of average embedding suggests considering word embeddings along with sentence embeddings for measuring sentence similarity. Multi-facet embeddings allow for estimating word importance in predicting possible outcomes. Multi-facet embeddings in the same space as word embeddings estimate word importance by computing cosine similarity with predicted codebook embeddings. Importance weighting is multiplied with original weighting vectors to generate results like Our Avg, Our Prob_avg, and Our SIF. Ours SC outperforms WMD and BERT Avg in STSB Low. Ours (K=10) scores significantly better than Ours (K=1), showing benefits of multi-mode representation. Proposed attention weighting boosts performance. The proposed attention weighting boosts performance in STSB Low, especially when not relying on the generalization assumption of the training distribution. A variant of the method using a bi-LSTM as the encoder and a LSTM as the decoder performs worse than the transformer alternative. The model is applied to HypeNet for unsupervised hypernymy detection, showing significant improvements over ST Cos. The approach ignores the order of co-occurring words in the NNSC loss. The text discusses how the predicted codebook embeddings of a hyponym often reconstruct the embeddings of its hypernym better than the other way around. The asymmetric scoring function is compared to symmetric similarity measurement in Table 4, showing that the proposed method outperforms baselines. The objective is to discover a summary that best reconstructs the distribution of word embeddings in the document. The extractive summarization method in the document by Kobayashi et al. (2015) optimizes sentence selection based on importance of words. Multiple codebook embeddings are generated to represent different aspects of each sentence. The approach is compared with alternative methods like average word embeddings and using all words in sentences as different aspects. The method W Emb optimizes sentence selection by normalizing the reconstruction loss based on sentence length. Results on the CNN/Daily Mail testing set show the effectiveness of different methods, including unsupervised approaches like Lead-3 and supervised methods like RL. The importance of sentence order information in English news corpora is highlighted, with unsupervised methods showing strong performance. The unsupervised method RL (Celikyilmaz et al., 2018) is compared with other unsupervised methods in evaluating sentence embeddings. Increasing cluster numbers (K) leads to better results, with K = 100 performing best after selecting 3 sentences. Larger K values are preferred for this application due to improved performance. Our method allows for setting a large K to address computational and sample efficiency challenges. Unlike global topic modeling, our approach efficiently discovers different topics/clusters on small word subsets that co-occur with target phrases or sentences. In topic modeling, sparse coding on word embedding space is used to discover different sets of topics efficiently. Parameterizing word embeddings with neural networks helps save storage space. Words are represented in Gaussian embeddings to capture asymmetric relations. Challenges include extending methods to longer sequences and designing a neural decoder for sets. Matching steps and computing distance loss are essential, with options like Chamfer distance widely adopted. The studies focus on measuring distances between ground truth and predicted sets, with various loss options proposed. Different methods for achieving permutation invariance in neural networks are discussed, including beam search and predicting permutations using CNNs, transformers, or reinforcement learning. The goal is to efficiently predict clustering centers for reconstructing observed instances, overcoming challenges in learning multi-mode representations for long sequences like phrases or sentences. In this work, the authors address the challenges of learning multi-mode representations for long sequences like phrases or sentences. They use a neural encoder and decoder to predict codebook embeddings as representations. The models outperform popular baselines in unsupervised benchmarks, showing that multi-facet embeddings excel with complex input sequences, while single-facet embeddings perform well with simpler inputs. The authors aim to train a single model to generate multi-facet embeddings for phrases and sentences, evaluating it as a pre-trained embedding approach for supervised or semi-supervised settings. They plan to apply this method to other unsupervised learning tasks relying on co-occurrence statistics. The model is kept simple to converge training loss quickly, without fine-tuning hyper-parameters. The transformer architecture is similar to BERT, with a sparsity penalty weight of 0.4 and specific settings for sentence and word limits. The maximal sentence size is set to be 50 and the maximal number of co-occurring words is 30. The number of dimensions in transformers is 300. For sentence representation, the number of transformer layers on the decoder side is 5 with dropout on attention at 0.1 for K = 10, and 1 for K = 1. For phrase representation, the number of transformer layers on the decoder side is 2 with dropout on attention at 0.5. The window size is set to be 5. Hyperparameters are determined by validation loss, except for the number of codebook embeddings. The performance of models with different embeddings K is not sensitive to the numbers as long as K is large enough. Larger K may require longer training time, and 1 week of training may not be enough for convergence. Skip-thoughts use a hidden embedding size of 600 and were retrained for 2 weeks in Wikipedia 2016 for fair comparison. Our model has fewer parameters and requires less computational resources than BERT base. We compare our method with BERT Large in various tasks, where BERT Large performs better in similarity tasks but worse in hypernym detection. Increasing the model size may be a promising future direction based on the performance gains in similarity tasks. The performance gains of BERT in similarity tasks suggest that training a larger model could be a promising future direction. Our method outperforms BERT in most cases, especially in phrase similarity tasks, possibly due to BERT's training on predicting masked words in longer sequences. Comparing our method with other baselines at the same summary length, we observe significant performance differences, indicating the effectiveness of our approach. In Figure 3, Ours (K=100) outperforms W Emb (GloVe) and Sent Emb (GloVe) for summaries of similar length. W Emb (*) generally outperforms Sent Emb (*) due to selecting more sentences. Ours (K=100) is best for summaries under 50 words, while W Emb (BERT) is better for longer summaries. Combining our method with BERT could lead to improved performance in extractive summarization tasks. The mixed results suggest that combining our method with BERT could be a promising direction for better performance in extractive summarization tasks. Visualizing predicted embeddings from randomly selected sentences in the validation set shows the format of the file similar to Table 1, with embeddings visualized by nearest neighbors in a GloVe embedding space."
}