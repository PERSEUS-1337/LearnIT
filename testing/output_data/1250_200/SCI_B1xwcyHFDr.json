{
    "title": "B1xwcyHFDr",
    "content": "The information bottleneck method is an information-theoretic approach to representation learning, aiming to retain relevant information for predicting labels while minimizing superfluous information. This work extends the method to the multi-view unsupervised setting, identifying unnecessary information by comparing two views of the same entity. The new multi-view model achieves state-of-the-art results on various datasets. Additionally, the theory is applied to the single-view setting using data augmentation techniques, showing improved generalization compared to traditional unsupervised approaches. The goal of deep representation learning is to transform raw input into a useful representation. Unsupervised representation learning aims to generate representations that allow for more efficient learning of supervised tasks with fewer labels. The information bottleneck principle states that discarding irrelevant information from the input leads to increased robustness for downstream tasks. This method can be applied by minimizing the mutual information between the representation and input while maximizing the mutual information between them. In the unsupervised setting, the InfoMax objective focuses on maximizing mutual information between x and z instead of minimizing it. This paper extends the information bottleneck method to the unsupervised multi-view setting by maximizing mutual information between representations of two views (Multi-View InfoMax) while reducing mutual information between each view and its corresponding representation. The paper extends the information bottleneck method to unsupervised multi-view settings by maximizing mutual information between representations of two views (Multi-View InfoMax) while reducing mutual information between each view and its corresponding representation. The resulting representation contains shared information between both views, leading to state-of-the-art results in low-data settings on standard multi-view datasets. Additionally, the model's representations are shown to be more robust in single-view settings compared to other unsupervised approaches, with a focus on data augmentation techniques. The paper introduces the Multi-View InfoMax method, which maximizes mutual information between representations of two views while reducing mutual information between each view and its representation. This results in shared information between views and state-of-the-art performance in low-data settings. The model's representations are robust in single-view scenarios and focus on data augmentation techniques. The sufficiency of a representation z for predicting label y is determined by the amount of target information retained after encoding the data. A representation z is considered sufficient for y if I(x; y|z) = 0, indicating that the encoding procedure does not alter the task information. The curr_chunk discusses the concept of minimizing superfluous information in a representation z for predicting label y. It explains how to subdivide the mutual information I(x; z) into three components and emphasizes the importance of reducing irrelevant information in supervised settings. The goal is to minimize I(x; z|y) to ensure a sufficient representation z for y. The curr_chunk discusses reducing information in a representation to predict a label y by utilizing redundant information from multiple views. It explains how a representation z can capture shared information from different viewpoints, reducing sensitivity to view changes. The theory supports the idea that z can be sufficient for predicting y without directly observing y. The curr_chunk discusses redundancy in multi-view settings, where z can predict y by capturing shared information from v1 and v2. Redundancy is defined as I(y; v1 | v2) = 0, indicating when v1 is irrelevant for predicting y given v2. When v1 and v2 are mutually redundant, z1 can predict y as effectively as observing both views together. The curr_chunk discusses reducing mutual redundancy between views v1 and v2 to improve the predictive task. By minimizing the unique information in z1 that is not shared by v2, the resulting representation becomes more robust. This concept is supported by formal proofs in the appendix. The curr_chunk discusses minimizing information redundancy between views v1 and v2 to enhance the robustness of the representation. By ensuring that I(v1; v2 | z1) = 0 and reducing I(z1; v1 | v2), the objective function aims to discard irrelevant information while maintaining label sufficiency. The Lagrangian objective function is defined to achieve this goal. The encoder parameters are optimized using a Lagrangian multiplier in a Multi-View Information Bottleneck model. The model aims to minimize information redundancy between views v1 and v2 while maintaining label sufficiency. The loss functions L1 and L2 are used to optimize the parameters for robust representation. The trade-off between sufficiency and robustness is controlled by the coefficient \u03b2. The resulting model is visualized in Figure 1. The MultiView Information Bottleneck (MIB) model optimizes encoder parameters to minimize information redundancy between views v1 and v2 while maintaining label sufficiency. The symmetrized KL divergence and mutual information are key components in maximizing the representations. Jensen-Shannon and InfoNCE estimators are used with an auxiliary parameteric model C \u03be (z1, z2) for optimization. The methodology involves building mutually redundant views from single observations x by exploiting task symmetries. The MultiView Information Bottleneck (MIB) model aims to minimize information redundancy between views v1 and v2 while maintaining label sufficiency. By selecting functions that do not affect label information, views can be artificially created to satisfy mutual redundancy for y. The representations z1 and z2 must contain the same amount of predictive information as x. Independent transformations introduce uncorrelated variations in the views, such as small shifts for translations in T. For single-view datasets, two views are generated by sampling functions from the same class T with uniform probability. The MultiView Information Bottleneck (MIB) model minimizes information redundancy between views v1 and v2 while maintaining label sufficiency. Views are generated by sampling functions from the same class T with uniform probability, allowing for parameter sharing in conditional distributions. Representations z1 and z2 in the Information Plane are characterized by their information content regarding the raw observation and accessible predictive information for a given task y. Good representations are maximally informative about the label while retaining minimal information from the observations. The InfoMax principle focuses on unsupervised representation learning by maximizing information preservation from raw observations. However, Tschannen et al. (2019) argue that the effectiveness of InfoMax models is influenced more by architectural biases than the training objective itself. In contrast, Variational Autoencoders (VAEs) balance compression and reconstruction error through a hyper-parameter \u03b2, aiming for lossless representation when \u03b2 is close to 0. The InfoMax principle focuses on unsupervised representation learning by maximizing information preservation from raw observations. In contrast, Variational Autoencoders (VAEs) balance compression and reconstruction error through a hyper-parameter \u03b2, aiming for lossless representation when \u03b2 is close to 0. As \u03b2 increases, the representation becomes more compressed, showing increased generalization and disentanglement. The transition between low and high \u03b2 regimes depends on how well label information aligns with the inductive bias introduced by encoder, prior, and decoder architectures. Concurrent work applies the InfoMax principle in Multi-View settings to maximize mutual information between different data views. The MultiView InfoMax (MV-InfoMax) models aim to maximize mutual information between two data views, ensuring the representation contains predictive information. Results from the Sketchy dataset show comparisons with other models in sketch-based image retrieval tasks. The model with \u03b2 = 0 belongs to the family of objectives optimizing for InfoMax principle. Our work is the first to explicitly identify and discard superfluous information from the representation in the unsupervised multi-view setting. This is in contrast to \u03b2-VAE models which remove information indiscriminately without identifying which part is superfluous. The MIB objective results in the representation with the least superfluous information, making it the most robust. The MIB objective aims to create robust representations by removing superfluous information. It outperforms state-of-the-art baselines in both multi-view and single-view settings. Results are compared using the Jensen-Shannon estimator, showing better performance for MIB and other InfoMax-based models. The effectiveness of MIB is demonstrated on tasks like sketch-based image retrieval and multiclass image classification. The Sketchy dataset consists of images and handdrawn sketches of objects. A total of 73,002 natural object images are included for the sketch-based image retrieval task. Representations are generated for query sketches and natural images using neural networks. The MIB objective aims to create robust representations and outperforms state-of-the-art baselines. The MIB model, with hidden layers of 2048 and 1024 units, achieves strong performance on the retrieval task for images and sketches. The regularization introduced with symmetrized KL divergence aligns the representations, making it suitable for retrieval tasks. The MIR-Flickr dataset consists of 1M images with hand-crafted features and 800K user tags. The dataset used in the study consists of 1M images with hand-crafted features and 800K user tags. Each image is represented by a vector of 3,857 image features and a 2000-dimensional multihot encoding based on the most frequent tags. The dataset is divided into labeled and unlabeled sets, with the labeled set containing 975K images with 38 topic classes. Training images with less than two tags are removed, resulting in 749,647 training samples. The model is trained on unlabeled image-tag pairs and a multi-label logistic classifier is trained on 10K labeled images for macro-categories. The Multi-View InfoMax objective does not consistently produce representations for two views, making it challenging to use for ranking. The model consists of a multi-layer perceptron with ReLU activations learning 1024-dimensional representations for images and tags. The MIB model outperforms Multi-View InfoMax with fewer labels available, especially when a larger beta value is chosen. This leads to increased accuracy in scarce label scenarios but slightly reduces accuracy when all labels are observed. The MIB model outperforms Multi-View InfoMax in scarce label scenarios by using mutual information estimators that do not require reconstruction, leading to increased accuracy with fewer labels available. This effect may be due to a violation of the mutual redundancy constraint, which can be compensated with smaller beta values for less aggressive compression. The performance of different unsupervised learning models is compared by measuring data efficiency and estimating their representation coordinates on the Information Plane. The dataset is generated from MNIST by creating two views, v1 and v2, through data augmentation. Encoders are trained using the unlabeled multi-view dataset, and a logistic regression model is trained using resulting representations and a subset of labels. Mutual information estimation networks are used to estimate I(x; z) and I(y; z) on the final representations. The Multi-View Information Bottleneck method utilizes joint samples to train models with 64-dimensional representations. The models aim to retain predictive information while discarding irrelevant data, resulting in better classification performance at low-label regimes. The MIB model with \u03b2 = 1 focuses mainly on label information, maintaining classification performance even with one label per data point. This approach produces robust representations by leveraging multiple data views. The Multi-View Information Bottleneck method introduces a novel approach using multiple data views to create robust representations for various tasks. It does not rely on the assumption that each view alone can determine the label, making it applicable to traditional multi-view tasks. Empirical comparisons show strong performance in sketch-based image retrieval, multi-view, and unsupervised representation learning tasks. The model can work well even when mutual redundancy is only approximate, with potential extensions to explore in future research. The Multi-View Information Bottleneck method introduces a novel approach using multiple data views to create robust representations for various tasks, showing strong performance in various tasks. Future research could explore extensions such as considering more than two views and the role of data augmentation in bridging the gap between the Information Bottleneck principle and invariant neural networks. The properties of mutual information are used to prove the theorems in this work, with potential for further exploration. Theorem B.1 states that if a representation z of x satisfies I(x; z) > I(x; z), then there exists a label y for which z is not predictive while y is. This is proven by factorizing x as a function of two independent random variables and considering the mutual information between y and z. Corollary B.1.1 extends this to show that if z discards observational information, there will always be a label y for which z is not predictive. Theorem B.1 states that if a representation z of x satisfies I(x; z) > I(x; z), then there exists a label y for which z is not predictive while y is. Corollary B.1.1 extends this to show that if z discards observational information, there will always be a label y for which z is not predictive. By construction using Theorem B.1, it is shown that a representation z = x exists where I(y; x) > I(y; z) = 0, with various hypotheses and proofs provided. Theorems and corollaries in this section consider the independence assumption derived from a graphical model G. Theorems and corollaries in this section discuss the independence assumption derived from a graphical model G, focusing on the relationship between observations x, label y, functions t1, t2, and representation z1. Proposition B.4 states that when I(t1(x); y) = I(t2(x); y) = I(x; y), views t1(x) and t2(x) are mutually redundant for y. Thesis T1 shows that if I(t1(x); y) = I(t2(x); y) = I(x; y), then I(t1(x); y|t2(x)) + I(t2(x); y|t1(x)) = 0. The proof involves various conditional independence relations determined by the graphical model G. The text discusses the relationship between observations x, label y, functions t1, t2, and representation z1 in a graphical model. It explores the redundancy of views t1(x) and t2(x) for y, and presents the theorem that if I(t1(x); y) = I(t2(x); y) = I(x; y), then I(t1(x); y|t2(x)) + I(t2(x); y|t1(x)) = 0. The proof involves conditional independence relations determined by the graphical model G. The text discusses the constraints for representations z of observations x in relation to label y. It states that the representation must contain more information about the observations than the label. The mutual redundancy condition between views for a label y cannot be extended to an arbitrary number of views due to higher order interactions. The text discusses the limitations of extending the mutual redundancy condition between views for a label y to more than two views due to higher order interactions. This non-trivial theory requires an extension to ensure sufficiency for the label. The theory discussed in the text is challenging to generalize to more than two views, requiring an extension to ensure label sufficiency. It shows equivalence between supervised Information Bottleneck and Multi-View Information Bottleneck when views share only label information. The theory discussed in the text shows equivalence between supervised Information Bottleneck and Multi-View Information Bottleneck when views share only label information. It demonstrates that a minimal representation z1 of v1 for v2 that is sufficient for v2 is also minimal for the label y. The proof involves equations and the concept of InfoMax for creating minimal sufficient representations z1 and z2. The text discusses the equivalence between supervised Information Bottleneck and Multi-View Information Bottleneck when views share only label information. It explains how a minimal representation z1 of v1 for v2 that is sufficient for v2 is also minimal for the label y. The proof involves equations and the concept of InfoMax for creating minimal sufficient representations z1 and z2. Stochastic encoders p\u03b8(z1|v1) and p\u03c8(z2|v2) are modeled by Normal distributions parametrized with neural networks. The symmetrized KL-divergence can be computed directly as the encoders' densities are evaluable. Mutual information estimator I\u03b8\u03c8(z1; z2) is required and the hyper-parameter \u03b2 is slowly increased during training. The experiments used Adam optimizer with a learning rate of 10 and input features from two VGG-16 network models for sketch-based classification. The sketch-based classification task utilizes 4096 dimensional sketch and image features from pre-trained VGG-16 network models. The feature extractors are frozen during training, with batch size B = 128. The encoder and critic architectures consist of multi-layer perceptrons, with \u03b2 exponentially increasing from 10^-4 to 1.0 over 250,000 iterations. All natural images are used for training and retrieval galleries. The training procedure involves keeping \u03b2 fixed at one for 500,000 iterations. Evaluation includes comparing 64-dimensional outputs of sketch and image representation using Euclidean distance. Hamming distance is used for comparison with other methods relying on binary hashing. Mean average precision (mAP@all) and precision at top rank 200 (Prec@200) are reported for both real and binary representations. Whitening is applied to handcrafted image features, with batches of size B = 128 for each update step. The encoders consist of a multi-layer perceptron with 4 hidden ReLU units of size 1,024. The MIR-Flickr dataset includes examples of pictures, tags, and category labels. The tags may not always predict the label accurately. The representations in the study have a size of 1,024, resulting in a total of 2x1,024 parameters defining mean and variance. The critic consists of a multi-layer perceptron with hidden ReLU units. The beta update policy involves exponential increase from an initial value of 10^-8. Evaluation involves computing representations of labeled images for training a logistic regression classifier. The study involves computing representations of labeled images for training a logistic regression classifier. The encoders and decoders consist of neural networks with two hidden layers, while the critic architecture includes two hidden layers with ReLU activations. The beta update policy starts at 10^-3 and increases exponentially until the 50,000th iteration, then remains constant until the 1,000,000th iteration. The study involves training \u03b2-VAEs with a specific annealing policy and evaluating the trained representations using Jensen-Shannon mutual information. The final values for I(x; z) and I(y; z) are computed by averaging mutual information estimations on the dataset, with the lowest and highest 5% removed for consistency. Additional quantitative results and visualizations are included for singleview MNIST experiments. Table 2 compares input information I(x; z), label information I(z; y), and classifier accuracy for models in Figure 4 using different labeled examples. Results from Jensen-Shannon and InfoNCE estimators are reported. The MIB model shows a latent space with ten clusters corresponding to digits. With distinct clusters, 10 labeled examples align centroid coordinates with digit labels. Ablation studies on data augmentation show the effect on the MV-InfoMax Model. The MV-InfoMax Model does not benefit from increased corruption levels, while models trained with the MIB objective can utilize augmentation to improve representation. As corruption levels approach 100%, MIB performance deteriorates. MIB models lose label information at low corruption probabilities but regain it as corruption increases. This behavior may be due to optimization issues with biased Monte-Carlo estimation. The Monte-Carlo estimation for the symmetrized Kullback-Leibler divergence is biased. Using more examples of views from the same data-point within the same batch could mitigate this issue. The hyper-parameter \u03b2 determines the trade-off between sufficiency and minimality of the representation for the second data view. Different values of \u03b2 result in varying trade-offs between information preservation and removal. The Multi-View Information Bottleneck model shows a better trade-off between information preservation and removal compared to \u03b2-VAE, as demonstrated by predictive accuracy values. The Multi-View Information Bottleneck model demonstrates a superior trade-off between information preservation and removal compared to \u03b2-VAE, as evidenced by predictive accuracy values. Published at ICLR 2020."
}