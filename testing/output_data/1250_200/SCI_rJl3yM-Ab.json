{
    "title": "rJl3yM-Ab",
    "content": "In this paper, the authors propose two re-ranking methods, strength-based and coverage-based, to aggregate evidence from multiple passages for open-domain question answering. Their model outperformed existing QA models on three public datasets, showing an 8% improvement on two of them. The authors propose re-ranking methods to aggregate evidence from multiple passages for open-domain question answering, showing an 8% improvement on two public datasets. The method aims to improve QA by combining evidence from various sources. The method proposed in this paper aims to enhance open-domain question answering by aggregating evidence from multiple passages. The correct answer is often suggested by multiple passages, which collaboratively enhance the evidence. Additionally, when a question covers multiple answer aspects spread across passages, aggregating the evidence effectively can help infer the correct answer. The method proposed in this paper aims to enhance open-domain question answering by aggregating evidence from multiple passages. By aggregating both strengthened and complementary evidence, the correct answer \"Galileo Galilei\" can be inferred. This evidence aggregation is formulated as an answer re-ranking problem, efficiently incorporating global information from multiple pieces of textual evidence without significantly increasing complexity. The proposed method enhances open-domain question answering by aggregating evidence from multiple passages. Two re-rankers are used to score answer candidates based on the strength and coverage of their evidence. The strength-based re-ranker considers how often evidence occurs in different passages, while the coverage-based re-ranker ranks candidates higher if their contexts cover more aspects of the question. The re-ranker aims to rank answer candidates higher by aggregating evidence from multiple passages. Two re-rankers, strength-based and coverage-based, are used to aggregate evidence in open-domain QA datasets. The proposed approach achieves state-of-the-art results on various datasets and outperforms previous methods. The proposed approach achieves state-of-the-art results on three different datasets (Quasar-T, SearchQA, and TriviaQA) by using an IR model to find relevant web passages and a reading comprehension model to extract answers. This method outperforms previous best results by up to 8% on F1 score for Quasar-T and SearchQA. Unlike standard reading comprehension tasks, this approach operates in an open-domain setting without labeled answer positions in the passages during training. The models are trained under distant supervision without labeled answer positions in passages. The RC model matches passages containing the answer with the question. The R3 model is used to extract candidate answers, which are then reranked based on evidence strength and coverage. This re-ranking step aims to prioritize correct answers not easily found by the base system alone. The method achieves state-of-the-art results on various datasets by combining an IR model to retrieve passages and a reading comprehension model to extract answers in an open-domain setting. The method described in the curr_chunk focuses on re-ranking top-K answers generated by the RC model based on evidence strength and coverage. It utilizes features like counting answer occurrences and summing up probabilities to prioritize answers with stronger evidence. This approach aims to improve answer selection in open-domain QA by considering multiple passages and answer spans. The method described focuses on re-ranking top-K answers based on evidence strength and coverage. It aims to improve answer selection in open-domain QA by considering multiple passages and answer spans. The re-ranking scenario does not require exhaustive consideration of all probabilities, as many answer spans are irrelevant. Two methods, strength-based and coverage-based re-rankers, are proposed to prioritize answers with stronger evidence and better coverage of the question. The method focuses on re-ranking top-K answers by considering evidence strength and coverage. It involves concatenating passages containing the answer into a \"pseudo passage\" to measure how well it entails the answer. The match-LSTM model is used for this purpose. The method involves concatenating passages with the answer to form a union passage and using a model to identify which union passage better entails the answer for the question. Word-by-word attention and comparison modules are used to measure how each aspect of the question is matched by the union of multiple passages. The bidirectional LSTM is utilized to enhance the question representation for better matching with the answer candidate. The method involves concatenating passages with the answer to form a union passage. Word-by-word attention is used to measure how well each aspect of the question can be matched by the union passage. The attention weight matrix is computed to achieve this matching. The method involves concatenating passages with the answer to form a union passage. Word-by-word attention is used to measure how well each aspect of the question can be matched by the union passage, integrating lexical matching representations into the final aspect-level matching representations. Additionally, a bi-directional LSTM is added to aggregate aspect-level matching information and measure how well the entire question is matched by the union passage. The method involves concatenating passages with the answer to form a union passage. Word-by-word attention is used to measure how well each aspect of the question can be matched by the union passage. Additionally, a bi-directional LSTM is added to aggregate aspect-level matching information and measure how well the entire question is matched by the union passage. The matching process can be made aware of an answer's positions in the passage, but the study of better usage of answer position information is left for future work. LSTM is used to capture conjunction information among aspects, while simple pooling methods treat aspects independently. Re-ranking is based on the entire matching representation, transforming all representations into scalar values for each candidate answer. The method involves matching answers to questions and passages through a series of transformations and normalization processes. The objective function uses KL distance to determine the ground-truth answer. The strength-based re-ranker focuses on common cases in open-domain QA datasets. The full re-ranker combines strength-based and coverage-based approaches without additional training. Experiments were conducted on three open-domain QA datasets: Quasar-T, SearchQA, and TriviaQA. These datasets contain passages retrieved from search engines like Google or Bing. The Quasar-T dataset is based on trivia questions with 100 unique sentence-level passages collected for each question. The data set uses the \"Lucene index\" on the ClueWeb09 corpus for question sets. Human performance is evaluated in an open-book setting where subjects have access to passages retrieved by the IR model. Baseline models include GA, BiDAF, AQA, and R3. TriviaQA does not provide a leaderboard for the open-domain setting, making it challenging to compare with existing baselines. The TriviaQA dataset does not have a public leaderboard for open-domain settings, making it difficult to compare with existing baselines. The R3 model is used as a baseline, and a pre-trained R3 model is utilized to generate candidate spans for training. The coverage-based re-ranker is optimized using Adam BID19, with word embeddings initialized by GloVe BID23. Various parameters such as batch size, learning rate, and dropout probability are tuned for re-ranking candidate answers. Results and analysis of different re-ranking methods on public datasets are presented. The R3 model serves as a strong baseline for TriviaQA data, achieving competitive results in open-domain settings. Performance metrics include F1 score and Exact Match (EM), with results displayed in a table. The code for the model will be released on GitHub. The performance of our models, including the full re-ranker and coverage-based re-ranker, significantly outperforms previous best results, especially on Quasar-T and SearchQA datasets. Our model even surpasses human performance on SearchQA. Additionally, the BM25-based re-ranker improves F1 scores but is still lower than our coverage-based re-ranker with neural network models. The BM25-based re-ranker improves F1 scores compared to the R3 model but is still lower than the coverage-based re-ranker with neural network models. The BM25 re-ranker sometimes gives lower EM scores due to its reliance on bag-of-words representation and preference for shorter answers. The F1 score can be improved by considering context information and phrase similarities. Performance is analyzed based on answer lengths and question types on TriviaQA and Quasar-T datasets. The coverage-based re-ranker outperforms the baseline in answer lengths and question types. The strength-based re-ranker shows improvement but is less consistent across datasets. The coverage-based re-ranker and strength-based re-ranker have similar trends, except for \"why\" questions where the strength-based re-ranker performs worse. The strength-based re-ranker performs worse on \"why\" questions due to non-factoid answers. The top-K predictions of the baseline R 3 method show potential improvement for re-rankers. The re-ranking approach achieves a large improvement by covering the correct answer in top-K predictions. There is still a clear gap in performance between top-5 and re-ranking, indicating room for improvement in future work. The re-ranking model shows potential improvement with a 10% gap in performance on both datasets. The selection of K for coverage-based re-ranker significantly impacts recall, with a trade-off between list coverage and re-ranking difficulty. Results vary based on K values, with smaller K leading to lower performance due to limited coverage. The selection of K for the strength-based re-ranker impacts performance, with K=50 yielding the best results. Higher K values lead to a drop in performance. The re-ranker is fast, and different K values do not affect computation speed significantly. The strength-based re-ranker's performance is influenced by the choice of K, with K=50 producing the best results. Increasing K to 200 leads to a significant decrease in performance due to a higher ratio of incorrect answers. The re-ranker successfully corrected a wrong answer in an example from the Quasar-T dataset, showcasing its effectiveness in improving answer accuracy. Open Domain Question Answering involves producing answers to questions by utilizing resources such as documents, webpages, or knowledge bases. Recent advancements in machine reading comprehension have led to deep learning methods that use a document retrieval module to retrieve passages for extracting answers. Various techniques have been proposed to improve the retrieval and ranking steps, including using bi-gram passage indexing and reducing the length of retrieved passages. Our work focuses on noise reduction in passage ranking through joint training of a ranker module with the reading comprehension model using reinforcement learning. This is the first work to enhance neural open-domain question answering. Our work is the first to improve neural open-domain QA systems by using multiple passages for evidence aggregation. We focus on the novel problem of \"text evidence aggregation\", modeling the relationship between the question and multiple passages. Previous research did not address this issue, as traditional QA systems and KB-QA systems only utilized single passages for answer scoring features. Our approach introduces re-ranking methods to neural open-domain QA and multi-passage RC. Our work introduces re-ranking methods to neural open-domain QA and multi-passage RC, focusing on aggregating evidence from multiple passages. Compared to previous models like Epireader, our two-step approach differs in how it handles noisy sentences and utilizes sentence embedding vectors for matching evidence. Our work introduces re-ranking methods to neural open-domain QA and multi-passage RC, focusing on aggregating evidence from multiple passages. Unlike previous models like Epireader, our approach utilizes sentence embedding vectors for matching evidence and addresses noisy sentences more effectively. Our approach focuses on answer generation in open-domain QA by combining evidence from multiple passages using two types of re-rankers. These re-rankers significantly improved results individually and even more so when used together, advancing the state-of-the-art on three open-domain QA datasets. While our methods show success in modeling the union of multiple passages, challenges in open-domain QA requiring reasoning and commonsense inference abilities remain. Future work will explore these directions, with potential generalization to more difficult multipassage reasoning scenarios."
}