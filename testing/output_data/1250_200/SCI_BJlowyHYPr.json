{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. The D-Conv operator can be integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM is applied to mobile service traffic and air quality indicator forecasting, showing accurate long-term predictions with real-world datasets. Point-cloud stream forecasting involves predicting future values from geospatial point-cloud data, such as mobile network antennas and air quality sensors. Unlike traditional spatiotemporal forecasting, point-cloud stream forecasting operates on irregular and unordered sets of points with complex spatial correlations. Point-cloud stream forecasting requires models that can handle irregular and unordered point sets with complex spatial correlations. While vanilla LSTMs struggle with spatial features, ConvLSTM and PredRNN++ are limited to grid-structured data. PointCNN is proposed to leverage spatial-local correlations of point clouds, regardless of point order. The proposed PointCNN leverages spatial-local correlations of point clouds, regardless of input order. CloudLSTM architecture is introduced for forecasting over point cloud-streams, utilizing the DConv operator. CloudLSTM is combined with Seq2seq learning and attention mechanisms for precise forecasting. A point-cloud is defined as containing N points with value features and coordinates at each time step. An ideal point-cloud stream forecasting model should have five key properties: order invariance, information intactness, interaction among points, and robustness to transformations. These properties ensure that the model can handle point clouds without specific order, maintain the same number of points in the output as in the input, capture local dependencies among points, and be robust to transformation operations. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM, satisfying key properties such as order invariance, information intactness, interaction among points, and robustness to transformations. DConv generalizes convolution on point-clouds, ensuring the same number of elements in the output as in the input. The Dynamic Point Cloud Convolution (DConv) operator takes U channels of a point-cloud S and outputs U channels of a point-cloud with the same number of elements as the input to preserve information intactness. It involves subsets of points in S, element-wise products, and aggregation of features to obtain values and coordinates of points in the output. The operator ensures dynamic spatial correlations by considering value features related to their positions at the previous layer/state. The Dynamic Point Cloud Convolution (DConv) operator aggregates coordinate features to exploit spatial correlations. Learnable weights are defined as 5D tensors shared across anchor points. Bias terms are used for output maps. Sigmoid function limits predicted coordinates to (0, 1) for robustness. Raw point-cloud coordinates are normalized before feeding into the model. The K nearest points can vary for each channel at each location. The DConv operator improves transformation robustness by allowing each channel to find the best neighbor set for spatial correlations, which vary between different measurements due to human mobility. This approach enhances forecasting performance and adaptability across different types of datasets. The DConv operator in CloudLSTM improves forecasting performance by weighting its K nearest neighbors across all features to produce values and coordinates in the next layer. It is a symmetric function that captures local dependencies and improves robustness to global transformations. Additionally, DConv learns the layout and topology of the cloud-point for the next layer. The DConv operator in CloudLSTM improves forecasting by learning the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step. It can be efficiently implemented using 2D convolution and builds upon PointCNN and Deformable Convolution, introducing variations for pointcloud structural data. The DConv operator in CloudLSTM improves forecasting by learning the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step. It introduces variations for pointcloud structural data, maintaining order invariance without information loss. The DConv operator can be plugged into LSTMs for transformation modeling flexibility. The DConv operator in CloudLSTM enhances forecasting by learning the layout and topology of point clouds for dynamic positioning in each channel and time step. It maintains order invariance without information loss and can be integrated into LSTMs for flexible transformation modeling. The CloudLSTM combines Seq2seq learning and soft attention mechanism for forecasting tasks. The Seq2seq CloudLSTM model combines CloudLSTM with soft attention mechanism for forecasting tasks. The architecture includes an encoder and decoder, with Point Cloud Convolutional layers processing the data before forecasting. In this study, a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is employed. The study also explores new models like CloudRNN and CloudGRU by plugging DConv into vanilla RNN and Convolutional GRU. These models are compared with 12 baseline deep learning models using measurement datasets of traffic and air quality indicators. The proposed CloudLSTM is used to forecast future demands and indicators in target regions. In this study, 12 baseline deep learning models are compared using TensorFlow and TensorLayer libraries. Models are trained with two NVIDIA Tesla K40M GPUs, optimized with Adam optimizer, and evaluated on spatiotemporal point-cloud stream forecasting tasks. Coordinate features are omitted in the final output for fixed location data sources. Mobile Traffic Forecasting is conducted on large-scale multi-service datasets. In mobile traffic forecasting, large-scale datasets from European metropolitan areas are analyzed. The data includes traffic volume from devices connected to antennas, aggregated over 5-minute intervals for 38 different mobile services. Air quality forecasting is also explored using a public dataset from China with six air quality indicators collected from monitoring stations. In China, a public dataset with six air quality indicators collected from 437 monitoring stations is used for air quality forecasting. The dataset includes data on PM2.5, PM10, NO2, CO, O3, and SO2 over a one-year period. The monitoring stations are divided into two city clusters based on geography, with data collected hourly. Missing data is filled using linear interpolation, and measurements are transformed into input channels for the models. The dataset is split into training, validation, and test sets in an 8:2 ratio. The performance of the proposed CloudLSTM model is compared with baseline models like PointCNN. The performance of the proposed CloudLSTM model is compared with baseline models like PointCNN, CloudCNN, PointLSTM, CloudRNN, and CloudGRU. Other baseline models such as MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are also discussed. The accuracy of CloudLSTM is evaluated in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) on mobile traffic snapshots. The CloudLSTM model is evaluated for accuracy using MAE and RMSE on mobile traffic snapshots. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are also used to measure forecast fidelity. Various neural networks are employed for city-scale mobile traffic consumption prediction over different time horizons. In the air quality forecasting task, models receive input measurements and forecast indicators for specific time periods. The CloudLSTM model is evaluated for accuracy in predicting mobile traffic consumption over different time horizons. It outperforms other architectures, achieving lower MAE/RMSE and higher PSNR/SSIM on urban scenarios. CloudLSTM performs better than CloudGRU and CloudRNN, showing insensitivity to the number of neighboring points (K). The CloudLSTM model shows good forecasting performance, with insensitivity to the number of neighbors (K) and improved results with the attention mechanism. Long-term forecasting up to 36 time steps shows reliable performance in city 1, while low K may affect CloudLSTM in city 2. This provides guidance on choosing K for different forecast lengths. The CloudLSTM model demonstrates strong forecasting performance, especially for 12-step air quality forecasting across six indicators. Results show that CloudLSTMs outperform ConvLSTM by up to 12.2% and 8.8% in terms of MAE and RMSE, respectively. Lower K values lead to better prediction performance, with CloudCNN consistently outperforming PointCNN. Overall, CloudLSTM models are effective for modeling spatiotemporal point-cloud stream data. Performance evaluations for long-term forecasting up to 72 future time steps are also presented. The CloudLSTM model, utilizing D-Conv operator, shows superior forecasting performance compared to other models like ConvLSTM. The attention mechanism does not significantly impact performance. CloudLSTM is tailored for spatiotemporal forecasting on pointcloud data streams, emphasizing the importance of core operator > RNN structure > attention in terms of contribution. The DConv operator performs convolution over point-clouds to learn spatial features while maintaining permutation invariance. It predicts values and coordinates of each point, adapting to changing spatial correlations. It can be combined with RNN models, Seq2seq learning, and attention mechanisms efficiently using a standard 2D convolution operator. The input and output tensors of DConv are 3D tensors. Top K nearest neighbors are found for each point in the input, transforming it into a 4D tensor for DConv operation. The DConv operator performs convolution over point-clouds to learn spatial features while maintaining permutation invariance. It predicts values and coordinates of each point, adapting to changing spatial correlations. The complexity of DConv is analyzed by separating the operation into two steps: finding neighboring sets for each point and performing weighting computations. The complexity of finding K nearest neighbors for one point is O(K * L log N), and the complexity of computing one feature of the output point is O((H + L) * K). This complexity is equivalent to that of a vanilla convolution operator. The DConv operator introduces extra complexity by searching for K nearest neighbors for each point, with a complexity of O(K * L log N). Normalizing coordinates enables transformation invariance to shifting and scaling. CloudLSTM is combined with an attention mechanism for encoding and decoding states. CloudLSTM with attention mechanism is compared against baseline models like MLP, CNN, and 3D-CNN for mobile traffic forecasting. DefCNN and ConvLSTM are also discussed, while PredRNN++ is highlighted as the state-of-the-art for spatiotemporal forecasting. CloudRNN and CloudGRU formulations are presented. The CloudRNN and CloudGRU models share a Seq2seq architecture with CloudLSTM but do not use the attention mechanism. Different configurations and parameters for various models are detailed in Table 3 for comparison. The use of 3x3 filters in image applications is highlighted for its effectiveness, providing a receptive field equivalent to K=9 in CloudLSTMs. PredRNN++ has a unique structure compared to other Seq2seq models. CloudLSTM models with different configurations and parameters are optimized using the MSE loss function. Evaluation metrics such as MAE, RMSE, PSNR, and SSIM are employed. PSNR is calculated based on traffic volume and air quality indicators, with coefficients used for stabilization. Anonymized locations of antenna sets in cities are shown in Figure 5. The data collection process involved anonymized locations of antenna sets in two cities, with measurements conducted under privacy regulations and without personal subscriber information. The dataset used for the study is fully anonymized, ensuring no privacy concerns. The dataset used for analysis includes 38 different services, with streaming being the dominant type of traffic. Web, cloud, social media, and chat services also consume significant fractions of mobile traffic, while gaming accounts for a small percentage. The air quality dataset consists of information from 43 cities in China, with over 2.8 million air quality records collected. The Urban Computing Team at Microsoft Research collected air quality information from 43 cities in China, totaling 2,891,393 records from 437 monitoring stations over a year. The dataset is divided into two clusters based on geographic locations. Missing data was filled through linear interpolation. The forecasting accuracy of Attention CloudLSTMs was evaluated for each mobile service, showing similar performance across cities. Services with higher traffic volume had higher prediction errors due to more frequent fluctuations in traffic evolution. The higher traffic volume, such as streaming and cloud services, leads to higher prediction errors due to more frequent fluctuations in traffic evolution. The MAE for long-term air quality forecasting increases with time for all models. Larger K values in CloudLSTM improve robustness, with slower MAE growth over time. Hidden features visualization provides insights into the knowledge learned by the model. In Fig. 10, scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM are shown for encoders and decoders in City 2. Fig. 11 and 12 display NO2 forecasting examples in City Cluster A and B generated by RNN-based models, highlighting the superior performance of (Attention) CloudLSTMs in capturing spatial correlations. The proposed architectures, particularly Attention CloudLSTMs, show better prediction capabilities in capturing trends in point-cloud streams and maintaining high visual fidelity over time compared to other models. By using Sigmoid functions and stacking multiple DConv via LSTM, the CloudLSTM model enhances representability and refines the positions of input points, allowing for accurate forecasting even with outlier points. The model's effectiveness is demonstrated using density-based spatial clustering to identify outlier points. Our model, CloudLSTM, maintains good performance in forecasting with outlier points. Using DBSCAN, we identify outliers in the air quality dataset and observe that CloudLSTM has the lowest prediction error. CloudCNN, with the DConv operator, shows the best forecasting performance among CNN-based models. Further experiments show CloudLSTM's robustness to outliers in controlled scenarios. In controlled scenarios, outliers in weather stations are identified and moved away from the center by different distances. The CloudLSTM model performs well in forecasting with outliers, outperforming the PointLSTM model. Additionally, comparisons with simple baselines show the robustness of CloudLSTM to outliers. Our CloudLSTM model outperforms MLPs and LSTMs that rely on k-nearest neighbors for forecasting. The number of neighbors K affects the model's receptive field, with a small K focusing on local spatial dependencies and a large K considering global spatial correlations. However, the results show that K does not significantly impact the baseline models' performance. In contrast, our CloudLSTM, utilizing DConv kernels for local spatial dependencies and stacks of time steps and layers for global spatial dependency, outperforms these simple baselines. Seasonal information in mobile traffic series can improve forecasting performance, but feeding models with data spanning multiple days is impractical due to the large input size. To efficiently capture seasonal information, 30-minute sequences are concatenated with a sub-sampled 7-day window to create an input with a length of 90. Experiments conducted on a subset of the mobile traffic dataset show promising forecasting performance. By incorporating seasonal information in mobile traffic forecasting, models show improved performance. However, this also increases input length and model complexity. Future work aims to find a more efficient way to integrate seasonal data with minimal complexity increase."
}