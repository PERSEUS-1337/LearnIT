{
    "title": "BJJLHbb0-",
    "content": "Unsupervised anomaly detection on multi- or high-dimensional data is crucial for machine learning research and industrial applications. The Deep Autoencoding Gaussian Mixture Model (DAGMM) presented in this paper combines a deep autoencoder with a Gaussian Mixture Model for anomaly detection. DAGMM optimizes the parameters of both models simultaneously, improving the balance between autoencoding reconstruction and density estimation of latent data points. The DAGMM model combines a deep autoencoder with a Gaussian Mixture Model for unsupervised anomaly detection on high-dimensional data. It optimizes parameters simultaneously, balancing reconstruction and density estimation. Experimental results show significant performance improvement over state-of-the-art techniques, with up to 14% better F1 score. Detection on high-dimensional data without human supervision is challenging. Dimensionality reduction is often used before density estimation, but this can lead to suboptimal performance. Recent works have explored combining dimensionality reduction and density estimation using deep networks, but face limitations in performance due to reduced low-dimensional space. Deep Autoencoding Gaussian Mixture Model (DAGMM) is a deep learning framework proposed to address challenges in unsupervised anomaly detection. It preserves key information of input samples in a low-dimensional space, combining features from dimensionality reduction and reconstruction error. DAGMM utilizes a compression network for dimensionality reduction and a Gaussian Mixture Model for density estimation in anomaly detection tasks. DAGMM addresses the training challenge of learning Gaussian Mixture Models (GMM) by utilizing an estimation network to facilitate joint optimization of dimensionality reduction and density estimation. This approach allows for the direct estimation of GMM parameters and enables end-to-end training, overcoming the limitations of traditional two-step approaches. DAGMM is friendly to end-to-end training, allowing for direct estimation of GMM parameters and superior performance in anomaly detection with up to 14% improvement in F1 score. The regularization introduced by the estimation network helps the autoencoder escape less attractive local optima, resulting in low reconstruction error compared to pre-trained counterparts. The end-to-end trained DAGMM outperforms baseline methods using pre-trained autoencoders in anomaly detection. Various methods for unsupervised anomaly detection include reconstruction-based methods like PCA, kernel PCA, and Robust PCA. However, these methods are limited by analyzing anomalies solely based on reconstruction error. DAGMM offers a comprehensive approach to anomaly detection by considering both density estimation in a reduced low-dimensional space and reconstruction error. Unlike traditional reconstruction-based methods, DAGMM can detect anomalies that may have normal reconstruction errors but reside in low-density areas. Other popular methods for anomaly detection include clustering analysis with models like multivariate Gaussian Models, Gaussian Mixture Models, and k-means. Recent works propose deep autoencoder based methods to jointly learn dimensionality reduction and clustering components, addressing the issue of key information loss during dimensionality reduction in traditional two-step approaches. However, current state-of-the-art methods are limited by oversimplified clustering models and pre-trained dimensionality reduction components. DAGMM addresses issues with dimensionality reduction and anomaly detection by using an estimation network to evaluate sample density in a low-dimensional space. It allows for end-to-end training and improves clustering analysis and density estimation quality. Unlike traditional methods, DAGMM estimates data density in a jointly learned low-dimensional space for more robust performance. DAGMM improves anomaly detection by estimating data density in a jointly learned low-dimensional space, using deep autoencoders for non-linear dimensionality reduction. It also learns density under the GMM framework through mixture membership estimation. Deep Autoencoding Gaussian Mixture Model (DAGMM) combines a deep autoencoder for dimensionality reduction with a Gaussian Mixture Model (GMM) for anomaly detection. The compression network reduces input samples and feeds their low-dimensional representations to the estimation network, which predicts their likelihood in the GMM framework. The features used include reduced representations from the autoencoder and reconstruction error features. The compression network in DAGMM utilizes low-dimensional representations from a deep autoencoder and features derived from reconstruction error. These features are then used in the estimation network for density estimation within a Gaussian Mixture Model framework. The estimation network in DAGMM utilizes a multi-layer neural network to predict mixture membership for samples in Gaussian Mixture Model density estimation. It estimates parameters and evaluates likelihood/energy without alternating procedures like EM, achieving this by predicting membership for each sample and inferring sample energy with estimated parameters. The objective function in DAGMM training includes components for reconstruction error and sample energy estimation using GMM parameters. The compression network aims to minimize reconstruction error, while the estimation network models the probabilities of observing input samples. By optimizing these networks, DAGMM maximizes the likelihood of observing the input data. DAGMM addresses the singularity problem in GMM by penalizing small values on diagonal entries. Meta parameters \u03bb1 and \u03bb2 are typically set to 0.1 and 0.005 for desirable results. The estimation network in DAGMM predicts sample membership, similar to latent variable inference in probabilistic graphical models. Neural variational inference uses deep neural networks for difficult latent variable inference problems. In DAGMM, the estimation network predicts sample membership using an energy function that can be upper-bounded. By minimizing the negative evidence lower bound, the estimation network approximates the true posterior and tightens the bound of the energy function. Unlike neural variational inference, DAGMM explicitly uses a tractable and efficient energy function. DAGMM employs a deep estimation network to parametrize a sample-dependent prior distribution, unlike neural variational inference. It is a powerful deep unsupervised version of adaptive mixture of experts combined with a deep autoencoder, utilizing end-to-end training. Pre-trained compression networks show limited anomaly detection performance, and the compression network and estimation network can boost each other's performance. The compression network and estimation network in DAGMM can enhance each other's performance. The regularization from the estimation network helps the deep autoencoder in the compression network reduce reconstruction error. Meanwhile, the well-learned low-dimensional representations from the compression network enable the estimation network to make meaningful density estimations. In Section 4.5, the choice between pre-training and end-to-end training in DAGMM is discussed using public benchmark datasets to demonstrate the effectiveness of DAGMM in unsupervised anomaly detection. The dataset used for anomaly detection tasks includes samples with 41 dimensions, where 34 are continuous and 7 are categorical. Categorical features are encoded using one-hot representation, resulting in a dataset of 120 dimensions. Different datasets like Thyroid, Arrhythmia, and KDDCUP-Rev are used with specific anomaly and normal class definitions for each. In anomaly detection tasks, datasets with 41 dimensions are used, including 34 continuous and 7 categorical features. Categorical features are encoded using one-hot representation, resulting in a dataset of 120 dimensions. Various datasets like Thyroid, Arrhythmia, and KDDCUP-Rev are employed with specific anomaly and normal class definitions. Different anomaly detection methods such as OC-SVM, DSEBM-e, DSEBM-r, and DCN are utilized as baselines for comparison. The Deep Clustering Network (DCN) is a state-of-the-art clustering algorithm used for anomaly detection. It regulates autoencoder performance by k-means and identifies anomalies based on the distance between a sample and its cluster center. Variants of the DAGMM model, such as GMM-EN and PAE, demonstrate the importance of different components in anomaly detection. The DAGMM variant discussed is equivalent to a deep autoencoder. Different approaches like E2E-AE, PAE-GMM-EM, PAE-GMM, and DAGMM-p are used for anomaly detection, with sample reconstruction error or sample energy as the criteria. Training procedures involve pre-training the compression network and using the output for further training. The DAGMM variants, including DAGMM-p and DAGMM-NVI, use sample energy for anomaly detection. DAGMM-NVI incorporates neural variational inference and different objective functions. Reconstruction features like relative Euclidean distance and cosine similarity are important for DAGMM, with details on their selection provided in Appendix D. The DAGMM variants, DAGMM-p and DAGMM-NVI, use sample energy for anomaly detection. DAGMM-NVI incorporates neural variational inference and different objective functions. Reconstruction features like relative Euclidean distance and cosine similarity are important for DAGMM. The KDDCUP dataset uses a compression network with 3-dimensional input and a GMM with 4 mixture components. The Thyroid dataset also uses a compression network with 3-dimensional input and 2 mixture components. The models are implemented using TensorFlow and trained with the Adam algorithm. Training epochs and batch sizes vary for each dataset. In DAGMM instances, training epochs range from 200 to 400, with mini-batch sizes varying from 128 to 1024. The parameters \u03bb 1 and \u03bb 2 are set as 0.1 and 0.005, respectively. Baseline methods undergo exhaustive search for optimal meta parameters. Anomaly detection performance is evaluated using average precision, recall, and F 1 score, with anomalies identified based on energy levels. Experiment settings involve clean training data with random sampling for training and testing. In DAGMM instances, training epochs range from 200 to 400 with mini-batch sizes varying from 128 to 1024. Parameters \u03bb 1 and \u03bb 2 are set as 0.1 and 0.005. Anomaly detection performance is evaluated using average precision, recall, and F 1 score. DAGMM demonstrates superior performance over baseline methods in terms of F 1 score on all datasets, with significant improvements on KDDCUP and KDDCUP-Rev datasets. Other methods like OC-SVM, DSEBM, and GMM-EN have limitations in performance due to various factors. In DAGMM instances, training epochs range from 200 to 400 with mini-batch sizes varying from 128 to 1024. Parameters \u03bb 1 and \u03bb 2 are set as 0.1 and 0.005. Anomaly detection performance is evaluated using average precision, recall, and F 1 score. DAGMM demonstrates superior performance over baseline methods in terms of F 1 score on all datasets, with significant improvements on KDDCUP and KDDCUP-Rev datasets. Other methods like OC-SVM, DSEBM, and GMM-EN have limitations in performance due to various factors. GMM-EN struggles with density estimation without reconstruction constraints, while E2E-AE shows poor performance on KDDCUP and Thyroid due to loss of key information during dimensionality reduction. DAGMM and DAGMM-NVI exhibit similar performance, with no significant improvement from neural variational inference. In the second set of experiments, DAGMM's response to contaminated training data was investigated. Results showed that as the contamination ratio increased from 1% to 5%, detection accuracy decreased for all methods except DAGMM, which maintained good accuracy with 5% contaminated data. It was noted that OC-SVM was more sensitive to contamination ratio. To achieve better accuracy, training with high-quality data is crucial. Overall, DAGMM achieved state-of-the-art performance through end-to-end training. In summary, training a model with high-quality data is crucial for achieving state-of-the-art accuracy in unsupervised anomaly detection. The DAGMM model, learned through end-to-end training, outperforms baselines like pre-trained deep autoencoders in separating anomalous samples from normal ones in low-dimensional space. In our study, we found that pre-trained deep autoencoders like DAGMM-p and DCN struggle to effectively separate anomalous samples from normal ones in low-dimensional space. Despite efforts to fine-tune the autoencoder, significant changes were not observed. The reconstruction error in a trained DAGMM was comparable to that of a pre-trained deep autoencoder, indicating the challenge of reducing error through end-to-end training. The compression and estimation networks in the model mutually boost performance during training, helping the autoencoder escape less attractive local optima. The Deep Autoencoding Gaussian Mixture Model (DAGMM) combines compression and estimation networks to improve anomaly detection through end-to-end training. The regularization from the estimation network helps the autoencoder escape local optima, while the compression network provides meaningful low-dimensional representations. DAGMM shows promise for density estimation and anomaly detection by combining dimensionality reduction and density estimation. The DAGMM model utilizes compression and estimation networks for anomaly detection, with end-to-end training benefiting density estimation tasks. It outperforms existing techniques on benchmark datasets, showing up to a 14% improvement in F1 score and offering a promising approach for unsupervised anomaly detection in high-dimensional data. The study discusses the use of OC-SVM and DSEBM for unsupervised anomaly detection on high-dimensional data. OC-SVM requires setting parameter \u03bd during training, while DSEBM is configured based on the network structure of DAGMM. The optimal \u03bd values for different datasets are determined through exhaustive search, with a focus on reconstruction features for detecting anomalies in network flows. Our investigation focuses on using deep autoencoders to reduce the dimensions of a private network security dataset from 20 to 1. This reduction helps in separating anomalies, specifically spoofing attacks, from normal network flows. By analyzing the L2 reconstruction error, anomalies with similar representations to normal samples in the reduced space can be easily identified. This observation leads to the inclusion of reconstruction features into DAGMM for improved anomaly detection. In our study, we include reconstruction features into DAGMM to separate anomalies from normal samples. Guidelines for reconstruction feature selection include using error metrics that are continuous, differentiable, and output small values. Cosine similarity and relative Euclidean distance are chosen based on these rules. As long as an error metric meets these criteria, it can be used as a candidate metric for deriving a reconstruction feature in DAGMM. In a case study on the KDDCUP dataset, anomalies with low cosine similarity and high relative Euclidean distance are easily captured by both joint training in DAGMM and decoupled training in PAE-GMM. However, anomalies with medium relative Euclidean distance and high cosine similarity are more challenging for PAE-GMM to separate from normal samples. DAGMM assigns lower cosine similarity to these anomalies, making it easier to differentiate them from normal samples. The DAGMM model assigns lower cosine similarity to anomalies compared to PAE-GMM, making it easier to differentiate anomalies from normal samples. The objective function of DAGMM includes components from deep autoencoder, estimation network, and penalty function for covariance matrices. The ratio among these components (1:\u03bb1:\u03bb2) is crucial, with a ratio of 1:0.1:0.005 consistently delivering expected results across datasets. Varying the ratio can impact the effectiveness of the model. In our investigation, we found that varying the base of the ratio (1:0.1:0.005) in DAGMM does not significantly affect anomaly detection accuracy. The experiment involved adjusting \u03bb 1 and \u03bb 2 to 0.2 and 0.01 respectively when the base is set to 2. Results from 20 runs on the KDDCUP dataset show consistent performance of DAGMM as the base varies from 1 to 9 with a step of 2."
}