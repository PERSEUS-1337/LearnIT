{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for transformations remains challenging without supervision. A non-statistical framework based on modular organization of the network allows for targeted interventions on image datasets, enabling applications like style transfer and robustness assessment in pattern recognition systems. Deep generative models like Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) have been successful in creating realistic images in various domains. Efforts have been made to produce disentangled latent representations for interpretable properties of images, but models lack a mechanistic or causal understanding. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds. In this paper, a causal framework is proposed to explore modularity in deep generative architectures, aiming to understand how to perform extrapolations using trained models. The framework is based on the principle of Independent Mechanisms, allowing for direct interventions in the network without affecting other causal mechanisms. The study focuses on the effect of direct interventions in the network to modify individual mechanisms in generative models, assessing causal mechanisms and counterfactuals. It analyzes disentanglement in deep generative models through unsupervised counterfactual manipulations, showing modularity in VAEs and GANs. The work is related to interpretability in convolutional neural networks. Generative models like InfoGANs and \u03b2-VAEs address disentanglement of latent variables in deep networks. The concept of intrinsic disentanglement uncovers internal network organization, highlighting statistically dependent transformations. This contrasts with supervised approaches and group representation theory for disentanglement. Our approach to disentanglement, introduced independently, is more flexible than previous proposals. We introduce a general framework to precisely define disentanglement and connect it to causal concepts, focusing on intrinsic disentanglement in generative models. For mathematical details, readers can refer to the provided appendix. The text discusses the generation of samples from a model using a latent representation in a Euclidean space. It introduces a Causal Generative Model (CGM) implemented by a non-recurrent neural network. The model involves a mapping from endogenous variables to compute the latent representation. The text introduces a Causal Generative Model (CGM) implemented by a non-recurrent neural network, involving a mapping from endogenous variables to compute the latent representation. The framework allows defining counterfactuals in the network following Pearl (2014). The CGM framework allows defining counterfactuals in the network following Pearl (2014). Counterfactuals induce a transformation of the output of the generative model, introducing faithfulness of a counterfactual mapping to account for interventions on internal variables. Non-faithful counterfactuals may result in artifactual outputs or allow extrapolation to unseen data. The classical notion of disentangled representation involves sparsely encoding real-world transformations in individual latent variables. Supervised approaches manipulate relevant transformations explicitly, while unsupervised learning seeks to learn these transformations from unlabeled data. State-of-the-art approaches enforce conditional independence between latent factors, leading to issues with statistical independence constraints on the prior distribution of latent variables. The statistical approach to disentangled representation faces issues with independence constraints on latent variables, leading to an ill-posed problem. Current unsupervised methods struggle with real-world datasets, showing lower visual quality compared to non-disentangled models. A new non-statistical definition of disentanglement is proposed, focusing on transformation-based insights. The non-statistical definition of disentanglement focuses on transformation-based insights, where transformations on the latent space act on individual variables while leaving others available for encoding different properties. This notion, called extrinsic disentanglement, emphasizes independent mechanisms in transformations, regardless of the property being disentangled or statistical independence. The functional definition of disentanglement, known as extrinsic disentanglement, emphasizes independent mechanisms in transformations on the latent space. This definition allows for the exploration of statistically related properties that are disentangled according to the defined criteria. The text discusses the concept of modularity in internal representations of a network, where a subset of endogenous variables is considered modular if any transformation applied to it stays within its input domain and is disentangled. This concept allows for the implementation of arbitrary disentangled transformations. The framework discussed in the current text introduces the idea of a disentangled representation achieved through partitioning the intermediate representation into modules. This partitioning allows for valid transformations in the data space, highlighting the need for a partition of latent variables into modules for a truly disentangled representation. This insight challenges traditional approaches to disentanglement by emphasizing the interconnected nature of neurons in artificial and biological systems. The concept of representation in artificial and biological systems involves grouping neurons into modules at a mesoscopic level for independent intervention. Proposals suggest that finding a modular structure allows for disentangled transformations, with counterfactual interventions defining transformations. Assigning a constant value to endogenous variables aims for faithful counterfactuals. Sampling from the marginal distribution of variables helps avoid characterizing the variable space. The hybridization procedure involves generating original examples of output by taking two independent latent variables, memorizing the values of variables indexed by a subset E, and creating a hybrid example by mixing features from the two original examples. This framework allows for assessing the impact of interventions on modular structures in neural networks. The counterfactual hybridization framework assesses how module E affects the generator output by generating pairs of latent vectors and estimating causal effects through influence mapping. The approach considers unit-level causal effects and averages over multiple interventions, providing a unique perspective on assessing modular structures in neural networks. The counterfactual hybridization framework evaluates the impact of module E on the generator output by creating latent vector pairs and estimating causal effects through influence mapping. It considers unit-level effects and averages over various interventions, offering a novel way to analyze modular structures in neural networks. The challenge lies in selecting the subsets E to intervene on, especially in networks with numerous units or channels per layer. A fine to coarse approach is used to extract groups, focusing on convolutional layers to estimate elementary influence maps (EIM) for each output channel c. These EIMs are then grouped by similarity to define modules at a larger scale. The EIMs for channels of convolutional layers in a VAE trained on the CelebA face dataset show functional segregation, with some influencing finer face features while others affect the background or hair. To group channels into modules, clustering is performed using EIMs as feature vectors. Pre-processing involves smoothing and thresholding the maps before applying Non-negative Matrix Factorization to obtain cluster template patterns. Non-negative Matrix Factorization (NMF) is used to factorize the matrix S into WH, resulting in K cluster template patterns. Each pattern contributes to individual maps with weights encoded in W. NMF is chosen for isolating meaningful image parts. A comparison with k-means clustering is planned. Additionally, a toy generative model is introduced involving neural networks and random model parameters. The model parameters for the neural network involve random choices for coefficients and indices, enforcing the assumption of modular influence in different areas of the image. The identifiability result shows that the hidden layer partition corresponds to a disentangled representation. The partition of the hidden layer in the neural network corresponds to a disentangled representation, justifying the use of NMF for generating a binary matrix summarizing significant influences. Modularity of generative models trained on the CelebA dataset was investigated using a basic \u03b2-VAE architecture. The full procedure included EIM calculations, clustering of channels into modules, and hybridization of generator samples using these modules. The study involved EIM calculations, clustering channels into modules, and hybridizing generator samples. Setting the number of clusters to 3 resulted in interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed the robustness of the clustering with consistency over 90%. The study confirmed the robustness of clustering with 3 clusters, showing high consistency (>90%) and outperformance of NMF-based clustering over k-means. Cosine similarity between templates also supported the choice of 3 clusters, with an average similarity of .9. Hybridization of the 3 modules resulted in feature replacement, as observed in the influence maps. The study demonstrated the effectiveness of clustering with 3 modules, leading to feature replacement in the image while maintaining its overall structure. Further research is needed to explore better disentanglement methods, especially in models where disentanglement is not explicitly enforced, such as GAN-like architectures. These findings were also replicated in the tensorlayer DCGAN implementation. The study showed successful feature replacement in images using clustering with 3 modules. Results were replicated in the tensorlayer DCGAN implementation and further tested on a pretrained BEGAN model for higher resolution face images. Notable effects required interventions on channels from the same cluster in two successive layers. Selective feature transfer was observed between Original 2 and Original 1 by intervening on layers 5 and 6. The study demonstrated successful feature replacement in images using clustering with 3 modules. Selective transfer of features from Original 2 to Original 1 was observed by intervening on layers 5 and 6. The hybridization procedure had a mild effect on image quality, as shown by the Frechet Inception Distance evaluation. The approach was also tested on high-resolution generative models like BigGAN-deep, showing potential for scaling to complex image datasets. The study demonstrated successful feature replacement in images using clustering with 3 modules. Selective transfer of features from Original 2 to Original 1 was observed by intervening on layers 5 and 6. The hybridization procedure had a mild effect on image quality, as shown by the Frechet Inception Distance evaluation. In a more challenging situation, meaningful combinations of objects of different nature were generated, such as a teddy bear in a tree or a \"teddy-koala\" merging textures on a uniform indoor background. The study investigated the use of counterfactual images to improve classifier robustness. Different SOTA classifiers showed varying recognition rates for hybrid images of teddy bears and koalas. The Inception resnet performed better at intermediate blocks 5-6. Non-consensual classification results suggested that classifiers rely on different aspects of image content for decision-making. The study introduced a mathematical definition of disentanglement and applied it to characterize the representation encoded by different groups of channels in deep generative architectures. Evidence for interpretable modules of internal variables in generative models was found, leading to better understanding and interpretability of complex generative architectures. This research direction aims to enhance the interpretability of deep neural networks and enable applications such as style transfer and automated assessment of object recognition systems' robustness to contextual changes. The curr_chunk discusses the potential of utilizing trained generator architectures as mechanistic models to enhance interpretability and expand their use to tasks they were not originally trained for. It proposes using structural causal models to represent these architectures and highlights the importance of understanding structural equations for interventions. The curr_chunk introduces a Causal Generative Model (CGM) to capture computational relations in neural network implementations. It focuses on the interdependent modules represented by a directed acyclic graph and the decomposition of the generator's output into successive steps. The model comprises a graph G and a set S of deterministic continuous structural variables. The CGM M = G(Z, S, G) consists of a directed acyclic graph and a set of deterministic structural equations. It aligns with Pearl's definition of a deterministic structural causal model and allows for modeling feed-forward networks with latent inputs. The model guarantees unambiguous assignments of endogenous variables and outputs based on chosen inputs or subsets of variables. The CGM M = G(Z, S, G) is a deterministic structural causal model that ensures unambiguous assignments of endogenous variables and outputs based on chosen inputs or subsets of variables. The latent and endogenous mappings define mappings from latent variables and endogenous variables to outputs, respectively, with constraints on their values in subsets of their euclidean ambient space. This framework allows for the verification of specific examples within the model. An embedded CGM is a CGM that is invertible and has well-defined image sets constrained by its parameters. The image set Y M of a trained model should approximate the data distribution. Transformations on the output respect the topology of Y M, allowing for inversion of the generator. An embedded CGM is a generative model that is invertible and has well-defined image sets. The model should approximate the data distribution, with transformations respecting the topology of the output space Y M. Injectivity of the model is a key requirement for embedded CGMs, and if the latent space is compact, the model is embedded if and only if it is injective. This framework allows for defining counterfactuals in the network. The CGM framework enables defining counterfactuals in the network by replacing structural assignments for endogenous variables. This concept aligns with potential outcome theory and induces transformations in the generative model. Additionally, counterfactuals are related to disentanglement, allowing for transformations of internal variables within the network. In a CGM, endomorphism T: YM \u2192 YM is intrinsically disentangled with respect to a subset E of endogenous variables. This implies a transformation T of endogenous variables affects only variables indexed by E. Intrinsic disentanglement relates to a causal interpretation of the generative model's structure, showing robustness to perturbations in subsystems. Counterfactuals exemplify such perturbations and can be disentangled based on faithfulness. Armstrong (2013) states that a continuous and injective gM is an embedding due to Z being compact and the codomain of gM being Hausdorff. In a CGM, endomorphism T: YM \u2192 YM is intrinsically disentangled with respect to a subset E of endogenous variables. This implies that T affects only variables indexed by E. The proof of the equivalence between faithful and disentangled transformations is discussed, showing that T is well defined and an endomorphism. The subsets of endogenous variables associated with each V k are modular, leading to a disentangled representation in the hidden layer. The choice of increasing dimensions and i.i.d. sampling ensure an injective mapping, satisfying the embedded CGM assumptions. Counterfactual hybridization of V k components results in an influence map covering I k. The conditions on I k and thresholding guarantee a rank K binary factorization of matrix B, with uniqueness ensured by classical NMF identifiability results. The \u03b2-VAE architecture, similar to DCGAN, consists of three blocks of convolutional layers with skip connections for image sharpness. The pretrained model used was from Tensorflow-hub and BigGan-deep architecture by Brock et al. on 256x256 ImageNet. No retraining was done on the model, which includes ResBlocks in the generator. The architecture includes ResBlocks in the generator, with BatchNorm-ReLU-Conv Layers and skip connections. Influence maps are generated by a VAE on the CelebA dataset, showing variance and perturbation influence. FID analysis of BEGAN hybrids indicates closeness to generated data, producing visually plausible images. The study explores the closeness of Hybridization to generated data, suggesting visually plausible images. Entropy values vary based on Gblock interventions, with Gblock 4 hybrids showing well-rendered object textures. Larger collections of hybrids for BIGAN between classes \"cock\" and \"ostrich\" are also examined. The study investigates the use of intervention procedures to assess classifier robustness by creating koala+teddy hybrids. The resulting hybrids combine teddy bear features in a koala context, highlighting the importance of object recognition over contextual information. The study explores classifier robustness using koala+teddy hybrids, emphasizing object recognition over contextual information. Nasnet large is shown to be more resilient to context changes compared to other classifiers."
}