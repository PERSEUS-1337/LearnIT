{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks that can reprogram the target model to perform a task chosen by the attacker without specifying the desired output for each input. This attack introduces a single perturbation that can cause the model to perform a task chosen by the adversary, even if it was not trained for that task. Adversarial reprogramming was demonstrated on six ImageNet classification models, repurposing them for tasks like counting and classification of MNIST and CIFAR-10 examples. The study focuses on adversarial attacks that aim to cause model prediction errors with small changes to inputs, such as tricking a self-driving car with a sticker or manipulating photos to inflate insurance claims. Various methods have been proposed to construct and defend against these attacks, which can be untargeted or targeted to produce specific outputs. In this work, a novel adversarial goal is considered: reprogramming a model to perform a task chosen by the attacker without needing to compute the specific desired output. Adversaries can achieve this by learning reprogramming functions that map between tasks, converting inputs from one domain to another. In this work, the concept of adversarial reprogramming is introduced, where a model is repurposed to perform a new task by learning reprogramming functions that map between tasks. The parameters of the adversarial program are adjusted to achieve the desired task without the need for specific output computation. The attack does not have to be imperceptible or subtle to be successful. Adversarial reprogramming involves repurposing a model for a new task by adjusting parameters. The attack does not require imperceptibility to humans to succeed. Risks include theft of resources, misuse of AI-driven assistants, and ethical violations. Flexibility in neural networks allows for repurposing through changes in inputs. The paper discusses how the number of unique output patterns increases exponentially with network depth. It introduces adversarial reprogramming, where neural networks are trained to perform new tasks by adjusting parameters. Experimental results show successful reprogramming of convolutional neural networks for tasks like counting squares and classifying different datasets. The paper explores adversarial reprogramming of neural networks, demonstrating the ability to alter network functions for tasks like counting squares and classifying various datasets. Adversarial examples are intentionally designed inputs to cause model mistakes, either untargeted or targeted. The study also shows the concealment of adversarial programs and data, with results discussed in Sections 5 and 6. Adversarial attacks can be untargeted or targeted, affecting model predictions. Reprogramming methods aim to produce specific functionality rather than a hardcoded output. Adversarial examples can be applied to different inputs to form specific outputs. For example, an \"adversarial patch\" can switch model predictions to a specific class when placed in their field of view. Adversarial reprogramming involves using a single adversarial program to manipulate a model to process images in a specific way. It is a form of parasitic computing that repurposes neural networks to perform new tasks, similar to transfer learning. This method does not involve leveraging communication protocols or gaining access to the host computer. Transfer learning and adversarial reprogramming aim to repurpose neural networks for new tasks. Transfer learning uses knowledge from one task as a base to learn another, while adversarial reprogramming manipulates the model through input. Transfer learning allows changing model parameters for the new task, unlike adversarial reprogramming where the model remains unchanged. In adversarial reprogramming, an attacker manipulates the input to achieve new tasks without altering the model. The adversary has access to the neural network parameters and aims to reprogram the model for a different task by crafting an adversarial program to be included in the input. This approach is more challenging than transfer learning as it involves adding an adversarial program to the network input, which is not specific to a single image but applied to all images. The adversarial program parameters are defined as W \u2208 R n\u00d7n\u00d73, with n as the ImageNet image width, and a masking matrix M. The mask is used to improve visualization, and the perturbation is bounded by tanh to (-1, 1). A sample x from the dataset is transformed into an ImageNet size image X. The probability P(y|X) is defined for ImageNet labels, and a mapping function h g(y adv) maps adversarial task labels to ImageNet labels. The adversarial program aims to maximize the probability of mapping adversarial task labels to ImageNet labels. Optimization includes a weight norm penalty and Adam optimization with decaying learning rate. Adversarial reprogramming exploits target model nonlinear behavior and requires minimal computation cost for the adversary during inference. Adversarial reprogramming exploits the nonlinear behavior of the target model to perform different tasks, such as counting squares and image classification. Experiments were conducted on six architectures trained on ImageNet, showing resistance to reprogramming and susceptibility of trained networks compared to random networks. The possibility of reprogramming networks with adversarial data dissimilar to the original data was also explored, along with concealing the adversarial program. The study demonstrated the feasibility of adversarial reprogramming by embedding a counting task in ImageNet models using adversarial images. The adversarial program consisted of a frame around the counting task images, with ImageNet labels unrelated to the new task. The study showed how adversarial reprogramming can be achieved by embedding a counting task in ImageNet models using adversarial images. Despite the dissimilarity of ImageNet labels and the new task, the adversarial program mastered the counting task for all networks. Additionally, the study also demonstrated adversarial reprogramming on the more complex task of classifying MNIST digits. The study demonstrated successful adversarial reprogramming of ImageNet models to function as MNIST classifiers using an additive adversarial program. The reprogramming generalized well from training to test sets and was not brittle to input changes. Additionally, adversarial programs were crafted to repurpose ImageNet models for classifying CIFAR-10 images, achieving moderate accuracy with minimal computation cost. The study showed successful adversarial reprogramming of ImageNet models to classify MNIST digits, even with adversarial training. Standard defense methods were found to have little effectiveness against this type of reprogramming. The study demonstrated successful adversarial reprogramming of ImageNet models for MNIST classification, despite standard defense methods being ineffective. Adversarial reprogramming differs from traditional attacks in its goal to repurpose the network rather than induce specific errors, with larger adversarial programs compared to small perturbations. Reprogramming attacks on models with random weights showed significant challenges in training accuracy. Training random networks for adversarial reprogramming proved to be challenging, with lower accuracy compared to ImageNet pretrained models. The appearance of adversarial programs was different, highlighting the importance of the original task performed by neural networks. Randomly initialized networks may perform poorly due to factors like weight scaling, while trained weights are better conditioned. Adversarial reprogramming may rely on similarities between original and adversarial data, suggesting a potential link to transfer learning. The study explored reprogramming pretrained ImageNet networks to classify shuffled MNIST and CIFAR-10 images. Despite lacking spatial structure, the networks achieved comparable accuracy to standard datasets. The results suggest that transferring knowledge between original and adversarial data is feasible. The study demonstrated the successful reprogramming of a pretrained Inception V3 model to classify MNIST digits with limited visibility of adversarial perturbations. The results suggest the potential for reprogramming across tasks and domains, even with small adversarial programs. In the next experiment, the study successfully reprogrammed a pretrained Inception V3 model to classify MNIST digits with nearly imperceptible adversarial programs. The adversarial task was concealed by shuffling pixels and limiting the scale of both the adversarial data and program, then adding it to a random ImageNet image. The study successfully reprogrammed a pretrained Inception V3 model to classify MNIST digits with nearly imperceptible adversarial programs. The adversarial task was hidden by shuffling pixels and limiting the scale of the data and program, then adding it to a random ImageNet image. The resulting adversarial images closely resemble normal ImageNet images, demonstrating the possibility of concealing the adversarial task. Trained neural networks were found to be more susceptible to reprogramming than random networks, even when the data structure is different. This highlights the flexibility of repurposing trained weights for new tasks. Our results show the flexibility of repurposing trained weights for new tasks in artificial neural networks. Adversarial reprogramming was demonstrated on image classification tasks, raising questions about its applicability to other domains like audio, video, and text. The reduced performance on random networks and CIFAR-10 classification tasks prompts further exploration into limitations in expressivity and trainability. Our findings suggest that reprogramming trained networks for different domains is possible. Adversarial reprogramming of RNNs could lead to various malicious activities, including theft of computational resources. This highlights the potential risks associated with machine learning systems being reprogrammed by specially crafted inputs. Adversarial attacks can lead to theft of computational resources by reprogramming neural networks for novel tasks, violating ethical guidelines. This poses a significant risk to machine learning systems and highlights the need for further research on mitigating such attacks. The investigation focuses on adversarial reprogramming, its properties, limitations, and ways to defend against it. Neural networks can be reprogrammed using adversarial data, even when unrelated to the original task data. This poses a risk to machine learning systems and highlights the need for research on mitigating such attacks."
}