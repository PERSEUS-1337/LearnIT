{
    "title": "SkzK4iC5Ym",
    "content": "In this paper, a generalization of the BN algorithm called diminishing batch normalization (DBN) is proposed, updating BN parameters in a diminishing moving average way. The DBN algorithm maintains the structure of BN while introducing weighted averaging updates to some parameters. Convergence analysis shows that DBN converges to a stationary point with respect to trainable parameters, applicable to any activation function meeting common assumptions. This analysis is the first of its kind for convergence with Batch Normalization introduced, focusing on a two-layer model with arbitrary activation function. The convergence analysis applies to any activation function meeting common assumptions, showing necessary conditions for stepsizes and diminishing weights to ensure convergence. In numerical experiments, more complex models with ReLU activation are used, showing that DBN outperforms BN on various datasets. Deep neural networks have been successful in object detection, but training them until convergence takes a long time. Batch normalization addresses the problem of internal covariate shift by normalizing the input of hidden layers using mini-batch statistics. This approach has led to a significant performance improvement. The paper introduces a new algorithm called diminishing batch normalization (DBN) as a generalization of batch normalization (BN). DBN updates BN parameters in a diminishing moving average way, adjusting its output based on all past mini-batches. This helps to address the issue of BN depending on the current mini-batch for output. The algorithm is analyzed for convergence in a two-layer batch-normalized neural network with diminishing stepsizes. The paper introduces a new algorithm called diminishing batch normalization (DBN) as a generalization of batch normalization (BN). It provides a convergence analysis for DBN in a two-layer batch-normalized neural network with diminishing stepsizes, showing that DBN converges to a stationary point under certain conditions. The main contribution is a general convergence guarantee for DBN, with necessary conditions for stepsizes and weights to ensure convergence. The paper discusses the new algorithm called diminishing batch normalization (DBN) as a generalization of batch normalization (BN). It provides a convergence analysis for DBN in a two-layer batch-normalized neural network with diminishing stepsizes, showing that DBN converges to a stationary point under certain conditions. The algorithm outperforms the original BN algorithm numerically, with proofs for main steps collected in the Appendix. Various preprocessing techniques like input whitening and decorrelation have been known to speed up the training process in deep learning. The text discusses different variations of normalization techniques in neural networks, such as layer normalization, weight normalization, and normalization propagation. These methods aim to improve training efficiency and performance by reducing internal covariate shift and dependencies between examples in a minibatch. Additionally, new approaches like applying batch normalization to RNN and LSTM models are being explored for further enhancements in training algorithms. The text discusses the application of batch normalization to RNN and LSTM models, highlighting its popularity and the lack of prior analysis. The analysis considers the workings of batch normalization and its optimization problem in neural networks. The text discusses the application of batch normalization to deep networks, focusing on the optimization problem and the structure of the network. It includes details on the layers, activation functions, and parameters involved in the analysis. The text discusses the application of batch normalization to deep networks, focusing on the optimization problem and the structure of the network. It includes details on the layers, activation functions, and parameters involved in the analysis. The set of trainable parameters updated by gradients includes matrices W1, W2, \u03b2, and \u03b3 introduced by BN. The objective function for each sample is nonconvex with respect to \u03b8 and \u03bb. The algorithm studied deviates from the standard BN algorithm by using full gradient instead of stochastic gradient, with potential future research on the latter. The text discusses the application of batch normalization to deep networks, focusing on optimization and network structure. It introduces trainable parameters like W1, W2, \u03b2, and \u03b3 updated by gradients. The algorithm uses full gradient instead of stochastic gradient, with potential future research on the latter. The method updates BN parameters using moving averages with diminishing \u03b1(m), making the output more reflective of the entire dataset. The text discusses the application of batch normalization to deep networks, focusing on optimization and network structure. It introduces trainable parameters like W1, W2, \u03b2, and \u03b3 updated by gradients. The algorithm uses full gradient instead of stochastic gradient, with potential future research on the latter. The output of the BN layer becomes more general, reflecting the dataset distribution. Algorithm 1 outperforms the original BN algorithm in numerical experiments, showing convergence in nonconvex objective functions with Lipschitz continuity and bounded parameters. The text discusses assumptions related to optimization problems in deep networks, including bounded weights and parameters, diminishing stepsizes, Lipschitz continuity of loss functions, existence of stationary points, and Lipschitz activation functions. These assumptions are standard in convergence proofs and hold for popular loss functions and activation functions like softmax, MSE, ReLU, and LeakyReLU. The text discusses conditions for convergence in optimization problems for deep networks, including assumptions on activation functions like ReLU and LeakyReLU. Theorems and lemmas are presented to show convergence results with diminishing stepsizes, with references to classical convergence rate analysis. The main result is listed as Theorem 11. The text presents Theorem 11, which characterizes the convergence property of Algorithm 1 under specific assumptions. It discusses the conditions required for convergence and conducts computational experiments using various datasets. The text presents Theorem 11, characterizing the convergence of Algorithm 1 under specific assumptions. It compares DBN and BN algorithms using MNIST, CIFAR-10, and Network Intrusion datasets with different neural network architectures and optimization techniques. Hazan, Elad and Singer (2011) updated learning rates for trainable parameters in DBN, testing different choices of \u03b1 (m) which showed proper convergence. The algorithms performed similarly even with very small \u03b1 (m) values, but became erratic when \u03b1 (m) was set to 0. The study found that non-zero choices of \u03b1 (m) converged at a similar rate, with \u03b1 (m) = 1/m and 1/m^2 outperforming the original BN algorithm. The importance of considering mini-batch history for updating BN parameters was highlighted, with all constant choices of \u03b1 (m) less than one performing better than the original BN. The DBN algorithm outperforms the original BN algorithm on various datasets with deep FNN and CNN models. The importance of mini-batch history for updating BN parameters is emphasized, with \u03b1 (m) = 0.25 being the most robust choice. Analytically, extending to more than 2 layers is feasible with additional notation. Proof of Proposition 12 shows the existence of a constant M for any \u03b8 and fixed \u03bb. The Lipschitz-continuous condition from Bottou et al. (2016) is used to prove the convergence of series {\u03bc(m)}. By Algorithm 1, \u03b1 is defined, leading to the convergence of {\u03bc(m)}. The sequence {\u03bc} is shown to be a Cauchy series, and the existence of constants is established for its convergence. Equations 35 to 39 establish the convergence conditions for the series {\u03c3(m)}. The proof of Theorem 7 is established through lemmas, and Proposition 17 provides bounds for |\u03bb(m) - \u03bb|\u221e. The convergence conditions for {\u03c3(m)j} are the same as for {\u03bc}. Equations 35 to 39 establish convergence conditions for series {\u03c3(m)}. The proof of Theorem 7 is through lemmas, and Proposition 17 provides bounds for |\u03bb(m) - \u03bb|\u221e. The convergence conditions for {\u03c3(m)j} are the same as for {\u03bc}. In equation 38, \u03c3j := \u03c3. The inequalities in equations 41, 30, and 44 lead to further bounds on the variables. The upper bound of \u00b5 is defined by Am := \u03bc(m) - \u03bc(\u221e) and Bm := \u03bcj. The series {\u03bc(m)} is a Cauchy series, with constants M1 and M2 defined. Equations 47 to 52 provide bounds and inequalities for the variables in the proof. Proposition 18 and Proposition 19 establish conditions under the assumptions of Theorem 7. The update of \u03b8 in Algorithm 1 is defined using a full gradient approach. The proof involves substituting values into equations to derive further inequalities. Equations 47 to 52 provide bounds and inequalities for variables in the proof. Proposition 18 and Proposition 19 establish conditions under the assumptions of Theorem 7. The update of \u03b8 in Algorithm 1 is defined using a full gradient approach. The proof involves substituting values into equations to derive further inequalities. By equation 56, we have \u03b8. Substituting values into equation 54 yields additional inequalities, leading to the proof of Theorem 11 based on Theorem 7 and Lemmas 8, 9, and 10. Lemma 8 is shown as a consequence of Lemmas 20, 21, and 22, with conditions ensuring the finiteness of certain equations. Under Assumption 4, a set of conditions is provided to ensure the finiteness of certain expressions. Equations 47 to 52 provide bounds and inequalities for variables in the proof. Proposition 18 and Proposition 19 establish conditions under the assumptions of Theorem 7. The update of \u03b8 in Algorithm 1 is defined using a full gradient approach. The proof involves substituting values into equations to derive further inequalities. By equation 56, we have \u03b8. Substituting values into equation 54 yields additional inequalities, leading to the proof of Theorem 11 based on Theorem 7 and Lemmas 8, 9, and 10. Lemma 8 is shown as a consequence of Lemmas 20, 21, and 22, with conditions ensuring the finiteness of certain equations. Under Assumption 4, a set of conditions is provided to ensure the finiteness of certain expressions. To show the finiteness of equation 64, we only need to show the following two statements: DISPLAYFORM8 and DISPLAYFORM9. Proof of equation 65: For all j we have DISPLAYFORM10. The inequality comes from |W. Finally, we invoke Lemma 14 to assert that DISPLAYFORM11. Proof of equation 66: For all j we have DISPLAYFORM12. The first term in equation 68 is finite since {\u00b5. Noted that function f (\u03c3) = 1 \u03c3 + B is Lipschitz continuous since its gradient is bounded by 1. Next we show that each of the four terms in the right-hand side of equation 75 is finite, respectively. For the first term, DISPLAYFORM13 is by the fact that the parameters {\u03b8, \u03bb} are in compact sets, which implies that the image of fi(\u00b7) is in a bounded set. For the second term, we showed its finiteness in Lemma 21. The right-hand side of equation 77 is finite because DISPLAYFORM14 and DISPLAYFORM15. The second inequalities in equation 78 and equation 79 come from the stated assumptions of this lemma. For the fourth term, DISPLAYFORM16 holds, because we have In Lemmas 20, 21 and 22, we show that {\u03c3 (m) } and {\u00b5 (m) } are Cauchy series, hence Lemma 8 holds. This proof is similar to the proof by Bertsekas & Tsitsiklis (2000). In Lemmas 20, 21, and 22, it is shown that {\u03c3 (m)} and {\u00b5 (m)} are Cauchy series, thus Lemma 8 holds. The proof is similar to Bertsekas & Tsitsiklis (2000). The conditions for \u03b7 (m) and \u03b1 (m) to satisfy the assumptions of Lemma 8 are h > 2 and k \u2265 1."
}