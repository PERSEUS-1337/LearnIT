{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters and improved performance. This arrangement also introduces an additional form of regularization. The branches are tightly coupled by averaging their log-probabilities, promoting better representations. This branched architecture, known as \"coupled ensembles\", can be applied to various neural network architectures. Results show that with coupled ensembles of DenseNet-BC and a parameter budget of 25M, error rates on CIFAR-10, CIFAR-100, and SVHN tasks are lower compared to DenseNet-BC alone. The study explores deep convolutional network architectures, introducing \"coupled ensembling\" as a regularization technique. This approach reduces parameters and improves performance by averaging log-probabilities of parallel branches. Results show lower error rates on CIFAR-10, CIFAR-100, and SVHN tasks compared to DenseNet-BC alone. The design philosophy follows a template with fixed filter size and features maps, down-sampling, and skip-connections in architectures like ResNet and DenseNet. The study introduces \"coupled ensembling\" as a regularization technique in deep convolutional network architectures. This approach splits the network into branches, achieving comparable performance to state-of-the-art models with fewer parameters. By combining activations through an arithmetic mean, the proposed method improves performance on CIFAR and SVHN datasets. Further ensembling of coupled ensembles leads to additional improvements. The paper discusses related work, introduces the concept of coupled ensembles, and evaluates the proposed approach. In section 2, related work is discussed. Section 3 introduces coupled ensembles and their motivation. Section 4 evaluates the proposed approach and compares it with existing models. The network architecture proposed is similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, but differs in training a single model with branches, having a fixed parameter budget, combining branch activations through log-probabilities, and using the same input for all branches. Multi-branch architectures have been successful in vision applications, with recent modifications using grouped convolutions for spatial and depth-wise feature extraction. A generic modification at the global model level involves replicating an \"element block\" as parallel branches to form the final model. Shake-Shake regularization is proposed to further enhance performance. Our method involves a generic rearrangement of a given architecture's parameters, leading to efficient parameter usage. In contrast to other techniques, it does not require modification at a local level of each residual block. Additionally, we confirm empirically that our configuration is effective. Ensembling is a reliable technique to improve model performance by combining outputs from multiple trainings of the same neural network architecture. Our proposed model architecture involves parallel branches trained jointly, similar to ResNet and Inception modules. Arranging parameters into parallel branches can increase performance. Classical ensembling can still be applied for fusion of independently trained ensemble models, leading to significant performance improvement. Snapshot ensembles have been used in previous studies. Our approach aims to improve performance while keeping model size constant or reducing it. The model comprises multiple branches, each using DenseNet-BC or ResNet with pre-activation as element blocks. A fuse layer combines the parallel branches. In our model, we use DenseNet-BC and ResNet with pre-activation as element blocks. The fuse layer combines parallel branches by averaging their log probabilities. The neural network outputs a score vector for the target classes, followed by a fully connected layer and SoftMax layer for probability distribution. This setup is common in recent network architectures for image classification tasks. The differences among recent network architectures for image classification lie in the setup before the last FC layer. Ensemble models are fused by averaging individual predictions from separate instances. A \"super-network\" with parallel branches and an AVG layer is functionally equivalent but not commonly implemented due to GPU memory constraints. Alternatively, the AVG layer can be placed after the last FC layer and before the SoftMax layer in element block instances. In our model setup, parallel branches produce score vectors for target categories, fused through a \"fuse layer\" during training to generate a single prediction. Three options for combining score vectors are explored: FC average, LSM average, and LL average. This transformation leads to improved performance with fewer parameters in all experiments. The proposed architecture involves parallel branches generating score vectors for target categories, which are fused through a \"fuse layer\" during training to produce a single prediction. By averaging the log probabilities of the target categories, a performance improvement is achieved with a lower parameter count in all experiments. The parameter vector W of this composite branched model is the concatenation of parameter vectors from element blocks. The architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets, each with specific training and testing image sets categorized into different classes. All hyperparameters are set according to the original descriptions of the \"element block\" used. During training, input images for CIFAR-10, CIFAR-100, and SVHN are normalized by subtracting the mean image and dividing by the standard deviation. Data augmentation includes random flips and crops for CIFAR datasets, while SVHN does not use data augmentation but applies a dropout ratio of 0.2 for DenseNet. Testing involves normalizing input similarly to training. Error rates are given as percentages averaged over the last 10 epochs. DenseNet-BC is implemented using PyTorch with a single NVIDIA 1080Ti GPU. Experiments are conducted on CIFAR-100 with DenseNet-BC, depth L = 100, growth rate k = 12. The baseline reference point for experiments in Section 4.5 is the same configuration with a single branch. The proposed branched architecture is compared with an ensemble of independent models in experiments. Results show that the jointly trained branched configuration outperforms averaging predictions from separate models. Additionally, the error rate from the multi-branch model is significantly lower compared to a single branch model with a similar number of parameters. This demonstrates the effectiveness of the branched configuration in reducing test error. The error rate of the multi-branch model is lower than that of a single branch model, indicating the efficiency of arranging parameters into parallel branches. The study analyzes the relationship between the number of branches and model performance, comparing different \"fuse layer\" combinations in a branched model with e = 4. Performance is evaluated on the CIFAR-100 test set, showing the superiority of the branched model over a single branch model. The study compares the performance of multi-branch models with different \"fuse layer\" combinations to a single branch model on the CIFAR-100 test set. Results show that a branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model. The study compares multi-branch models with different \"fuse layer\" combinations to a single branch model on the CIFAR-100 test set. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with significantly fewer parameters. Coupled ensembles with LSM fusion show lower error rates for \"element blocks\" trained jointly, indicating better representations and complementary feature learning. Ensemble combinations outperform single branch networks, with all except Avg. FC training showing significant improvements. The study compares multi-branch models with different \"fuse layer\" combinations to a single branch model on the CIFAR-100 test set. For a parameter budget of 3.2M, using 4 branches reduces the error rate to 17.61. Avg. FC prediction works better than Avg. SM prediction due to the spread of information at the FC layer. All experiments have Avg. LSM for the training \"fuse layer\" in branched models. The optimal number of branches e for a given model parameter budget is investigated. In this section, the optimal number of branches e for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100. The results are shown in table 3, with different configurations of branches e, depth L, and growth rate k. The parameter counts are quantified according to L and k values, making it critical in moderate size models like the 800K one targeted here. Model configurations with parameters just below the target were selected for a fair comparison. The study investigated the optimal number of branches for DenseNet-BC on CIFAR-100 with different configurations of branches, depth, and growth rate. The results showed that using 2 to 4 branches significantly improved performance compared to a single branch, while 6 or 8 branches performed worse. The model's performance was robust to slight variations in parameters, but increased training and prediction times were observed with smaller values of k. The study evaluated coupled ensembles with different architectures like DenseNet-BC and ResNet BID8. Results showed that coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperformed single branch models. Performance improvements were observed even with comparable or higher parameter counts. The study compared ensembles with ResNet pre-act as element block and e = 2, 4 to single branch models, showing better performance despite similar or higher parameter counts. Different network sizes were considered for DenseNet-BC architecture, with experiments showing that the trade-off between depth L and growth rate k is not critical for a given parameter budget. Additionally, experiments were conducted with both single-branch and multi-branch versions of the model, with varying numbers of branches. The study compared ensembles with ResNet pre-act as element block and e = 2, 4 to single branch models, showing better performance despite similar or higher parameter counts. Different network sizes were considered for DenseNet-BC architecture, with experiments showing that the trade-off between depth L and growth rate k is not critical for a given parameter budget. Coupled ensemble approach outperforms DenseNet-BC's reported performance, with larger models performing better than current state-of-the-art implementations. The coupled ensemble approach is limited by the size of the network that can fit into GPU memory and the training time. The coupled ensemble approach is limited by GPU memory and training time. The classical ensembling approach was used for larger models, showing significant performance gains. Ensembles of coupled ensemble networks outperform state-of-the-art implementations. The proposed approach of using ensembles of coupled ensemble networks outperforms all state-of-the-art implementations, including other ensemble-based models. By replacing a single deep convolutional network with multiple \"element blocks\" connected via a \"fuse layer,\" significant performance improvements are achieved at the cost of slightly increased training and prediction times. The proposed approach of using ensembles of coupled ensemble networks outperforms all state-of-the-art implementations by achieving significant performance improvement over a single branch configuration. This comes with a small increase in training and prediction times. The individual \"element block\" performance is better when trained together compared to independently. The increase in processing times is due to sequential processing of branches, making data parallelism less efficient on GPUs. Solutions include extending data parallelism to branches and spreading branches over multiple GPUs for better performance. The proposed approach of using ensembles of coupled ensemble networks outperforms state-of-the-art implementations by achieving significant performance improvement over single branch configurations. Preliminary experiments on ImageNet show that coupled ensembles have lower error for the same parameter budget compared to single branch models. The structure of the test and train versions of networks used as element blocks is illustrated in Figures 3 and 4, showing how the averaging layer can be placed in different locations. The reuse of \"element blocks\" from other groups is done for efficiency and meaningful comparisons. PyTorch implementations are used when available. Each branch is defined separately. The global network architecture is determined by hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. Coupled networks use a single global parameter vector split into branch parameter vectors for training and prediction. This allows for combining different training and prediction conditions, although not all combinations are equally efficient. The approach of using ensembles of coupled networks outperforms single branch models, with lower error rates on ImageNet. The global network architecture is determined by hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. For larger models, data batches are split into micro-batches to accommodate batch size limitations. Gradient accumulation and averaging over micro-batches are used to approximate processing data as a single batch. BatchNorm layer uses micro-batch statistics, which may not be exact but does not significantly impact results. To ensure fair comparison among different models, parameter updates are done using gradient for a batch while forward passes are done with micro-batches for optimal throughput. Memory requirements in single branch cases depend on network depth and mini-batch size. The use of micro-batches helps adjust memory needs while maintaining a default mini-batch size. In multi-branch versions, memory remains constant if branch width is unchanged. Hyper-parameter search experiments showed that reducing both width and depth was the best option, with the exact trade-off not critical. In hyper-parameter search experiments, it was found that reducing both width and depth was the best option, with the exact trade-off not critical. For \"full-size\" experiments, training was done within 11GB memory using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch ones. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but does not significantly increase speed or improve performance. Using only two branches provides a significant gain over a single branch architecture of comparable size. TAB8 presents an extended version of a comparison between different depth and growth rate combinations for a fixed parameter count. The performance remains stable across variations. A validation set experiment confirms that the (L = 82, k = 8, e = 3) combination is predicted to be the best, with slight variations observed. TAB9 compares parameter usage and performance of branched coupled ensembles with models recovered using meta learning techniques, highlighting issues with reproducibility and statistical significance in performance measures. The performance measures in experiments can vary due to different factors such as the framework used, random seed for network initialization, CuDNN non-determinism during training, fluctuations in batch normalization, and the choice of model instance. These variations can significantly impact the results obtained. The best performing model is chosen based on test data, taking into account factors like numerical determinism, Batch Norm moving average, and epoch sampling. Different random seeds can lead to different local minima, causing a dispersion in evaluation measures. Despite small dispersion, comparisons between methods are complicated as differences below the dispersion may not be significant. Experiments show that properly designed and trained neural networks should have similar performance despite variations in random initialization. Experiments on a moderate scale model show dispersion due to different random seeds. The study focuses on DenseNet-BC with L = 100, k = 12 on CIFAR 100, comparing Torch7 and PyTorch with same or different seeds. Results show error rates for various configurations and performance measures. The study compares Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100 using same or different seeds. Results show no significant difference between implementations or seed usage, indicating inability to reproduce exact results. Standard deviation is slightly smaller when computed on last 10 epochs compared to single last epoch. The study compares Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100 using same or different seeds. Results show no significant difference between implementations or seed usage, indicating inability to reproduce exact results. Standard deviation is slightly smaller when computed on last 10 epochs compared to single last epoch. Averaging measures on the last 10 epochs reduces fluctuations due to moving average and standard deviation in batch normalization. The mean of measures from 10 runs is lower when taken at the best epoch compared to single last epoch or last 10 epochs. Choosing the measure as the minimum error rate for all models during training is not realistic or good practice. In experiments with CIFAR and SVHN datasets, it is recommended to use the average error rate from the last 10 epochs for more robust and conservative performance estimation. For SVHN experiments, using the last 4 iterations is suggested due to smaller epochs. Tuning on the test set or selecting the best model based on error rate is not a good practice as it introduces bias in performance estimation. In this study, comparisons between single-branch and multi-branch architectures were made at a constant parameter budget, showing an advantage for multi-branch networks. However, multi-branch networks currently have longer training times. The study investigates if multi-branch architectures can still outperform single-branch ones with the same training time budget. Ways to reduce training time include reducing iterations, parameter count, or increasing width while decreasing depth. Reducing training time can be achieved by reducing iterations, parameter count, or increasing width while decreasing depth. Results from different options are compared using DenseNet-BC models with varying parameters and training times. Multi-branch networks outperform single-branch ones with the same parameter budget, despite longer training times. In a comparison between single branch models and coupled ensembles in a low training data scenario, results show that coupled ensembles significantly outperform single branch models for a fixed parameter budget. The study involved training on two datasets, STL-10 and a 10K balanced random subset of CIFAR-100, with DenseNet-BC models of varying parameters. Results from experiments comparing single-branch models and multi-branch coupled ensembles on ILSVRC2012 dataset show that coupled ensembles outperform single-branch models significantly. The experiments were conducted on images of size 256\u00d7256 due to constraints. The baseline single-branch model used was DenseNet-169-k32-e1, compared to the coupled ensemble DenseNet-121-k30-e2. The results in table 11 demonstrate that the coupled ensemble approach with two branches shows a significant improvement over the baseline model, even with a constant training time budget."
}