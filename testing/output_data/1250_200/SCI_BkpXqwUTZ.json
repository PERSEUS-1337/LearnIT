{
    "title": "BkpXqwUTZ",
    "content": "In vanilla backpropagation, the choice of activation function is crucial for non-linearity and differentiability. This study introduces a new approach using iterative temporal differencing with fixed random feedback weight alignment to replace the derivative of the activation function. This method transforms backpropagation into a more biologically plausible way for learning deep neural networks. The curr_chunk discusses the integration of spike-time dependent plasticity (STDP) in deep learning, starting with the proposal of VBP in 1987. It mentions the introduction of biologically-inspired convolutional networks and the development of deep learning approaches. The text also touches on the implementation of deep reinforcement learning mimicking dopamine effects and the use of hierarchical convolutional neural networks inspired by the visual cortex. Additionally, it highlights the discovery of fixed random synaptic feedback weight alignments in error backpropagation for deep learning. The curr_chunk discusses the use of segregated dendrites in deep learning to address the symmetrical synaptic weights problem in backpropagation. It visually demonstrates the integration of spike-time dependent plasticity (STDP) using Feedback Alignment (FBA) in VBP, comparing it with traditional VBP and FBA. The experiments were conducted on the MNIST dataset using Tanh activation function and compared using maximum cross entropy (MCE) as the loss function. The hyperparameters for both experiments included 5000 iterations, 0.01 learning rate, 100 minibatch size, and 32 hidden layers in a 2-layer deep network. In this paper, a feed-forward neural network with 32 hidden layers and 2-layer deep networks is used. The focus is on a more biologically plausible backpropagation for deep learning, integrating STDP learning processes in the brain. Future steps involve investigating STDP processes, dopamine-based unsupervised learning, and generating Poisson-based spikes."
}