{
    "title": "HyerxgHYvH",
    "content": "We propose a Lego bricks style architecture for evaluating mathematical expressions, using small neural networks for fundamental operations. These networks can be combined to solve more complex tasks like n-digit multiplication and division. Our approach allows for reusability and generalization up to 7 digit numbers. Our solution for evaluating mathematical expressions using small neural networks allows for generalization up to 7 digit numbers, including both positive and negative numbers. However, existing methods lack generalization and rely on memorization rather than understanding inherent rules, leading to performance degradation on unseen data. This highlights the lack of quantitative reasoning and systematic abstraction in Artificial Neural Networks (ANNs). The decision-making process in Artificial Neural Networks (ANNs) lacks quantitative reasoning and systematic abstraction, unlike other intelligent beings such as children who can extrapolate arithmetic operations from single digits to higher digits by reusing memorized examples. To improve ANNs' ability for complex numerical extrapolation and quantitative reasoning, fundamental operations need to be identified and learned for developing complex functions. In this work, fundamental arithmetic operations are identified and learned using neural networks to solve various problems like n-digit multiplication and division. This is the first proposed generalized solution that works for both positive and negative numbers, unlike existing methods. Neural networks are known for approximating mathematical functions, and exploratory work has been done on simple arithmetic operations like multiplication and division. Recent works in the field of neural networks have focused on training models to generalize over minimal training data for arithmetic operations. Various approaches such as Resnet, highway networks, and dense networks have been explored. However, existing models like EqnMaster and NALU still struggle with generalization, particularly with extrapolation issues. Recent works in neural networks have focused on generalizing over minimal training data for arithmetic operations. NALU has highlighted extrapolation issues in end-to-end learning tasks. Optimal Depth Networks using binary logic gates can solve simple arithmetic functions efficiently. Inspired by digital circuits, neural networks can be designed to tackle arithmetic problems, building on the premise of Binary Multiplier Neural Networks. Our work builds on the premise of Binary Multiplier Neural Networks to predict arithmetic functions for both positive and negative decimal integers. Instead of using a single neural network for different tasks, we train smaller networks for specific sub tasks like signed multiplication and division. By combining these smaller networks, we can perform complex arithmetic operations. Additionally, we propose a loop unrolling strategy similar to LSTMs to generalize solutions from 1-digit to n-digit arithmetic. Neural networks are used to simulate digital circuits for accurate arithmetic operations. Multiplication and division involve 1-digit operations, place value shifting, and sign calculation. Six neural networks are designed for fundamental operations, with additional networks for complex functions. Neural networks are utilized for accurate arithmetic operations, including multiplication and division involving 1-digit operations, place value shifting, and sign calculation. Complex functions are designed using several neural networks for fundamental operations. The addition and subtraction modules consist of single neurons with specific weights, facilitating shift-and-add multiplication. The neural network for single digit multiplication involves combining outputs of multiplications from right to left, using a place value shifter and addition. It has a single neuron with linear activation, can handle n inputs, and uses fixed weights for each digit. The model has 2 input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons to predict the result of single digit multiplication. The neural network for single digit multiplication involves computing absolute values and extracting signs using specific activation functions and hidden layers. The process includes operations like x + x, x \u2212 x, maxpooling, and modulus calculations to determine the final output. The neural network for single digit multiplication involves using specific activation functions and hidden layers to compute absolute values and extract signs. The process includes operations like x + x, x \u2212 x, maxpooling, and modulus calculations to determine the final output. The network takes sign and magnitude as input, passes it to a hidden layer of 10 neurons, and uses a soft-sign activation in the output layer to predict the sign multiplication result. The module assigns a sign to the output of complex operations like multiplication and division by performing multiplication between 2 n digit signed integers using fundamental operations. The multiplication model involves single digit operations to multiply tokens of multiplicand with tokens of multiplier, adding results with carry forward. The final output is assigned a sign using 1-digit sign multiply. The division model separates sign and magnitude, with n-digit divisor controlling output computation. Smallest outputs are selected from results of multiplying divisor with single digit multipliers and subtracting from dividend chunks. The division model involves selecting the smallest non-negative integer by introducing additional layers. The selected node represents the remainder and quotient result of division for the n-digit dividend and divisor. The quotient is combined over all iterations and the remainder is knitted to the next digit in the divisor. The architecture of the multiplication network is shown in Figure 2(b,d). A division model based on digital circuitry for decimal digits is generated. Comparisons are made with signed arithmetic operations and with the Neural Arithmetic and Logic Unit (NALU) implementation. Our model, based on Neural Arithmetic and Logic Unit (NALU) by Trask et al. (2018), outperforms recurrent and discriminative networks in arithmetic operations. It achieves 100% accuracy within the testing range and excels in signed multiplication. Comparison with NALU shows our model's superiority, especially in division architecture. In this paper, the NALU network is trained within a specific range and compared with the division architecture proposed. Results show 100% accuracy for various digit operations. The study demonstrates that complex tasks can be divided into smaller sub-tasks, each accomplished by independent neural networks. Fundamental arithmetic operations are learned using feed forward neural networks, which can be combined to solve more complex tasks. The study demonstrates that complex tasks can be divided into smaller sub-tasks, each accomplished by independent neural networks. Fundamental arithmetic operations are learned using feed forward neural networks, which can be combined to solve more complex tasks. The proposed work involves developing larger and more complex networks by reusing smaller networks for tasks like n-digit multiplication and division. A limitation is the use of float operation in the tokenizer, but it does not hinder the current work as only pre-trained smaller networks representing fundamental operations are used. Future work aims to resolve this issue and develop a point cloud segmentation algorithm using a larger number of identical smaller networks."
}