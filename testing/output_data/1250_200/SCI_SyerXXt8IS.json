{
    "title": "SyerXXt8IS",
    "content": "Auto-generate enhanced input features for ML methods with limited training data. Biological neural nets (BNNs) like the insect olfactory network excel at fast learning by utilizing competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming traditional methods like PCA and NNs. These \"insect cyborgs\" show significantly better performance on data sets like MNIST and Omniglot, reducing test set error averages by 20% to 55%. This highlights the potential value of BNN-inspired feature generators in the ML context. In the ML context, limited data poses challenges for neural nets with backprop. To address this, an architecture is proposed to automatically generate new class-separating features from existing ones. Inspired by the rapid learning ability of biological neural nets like the insect olfactory network, the architecture incorporates competitive inhibition, sparse layers, and feedforward network elements. The MothNet model incorporates competitive inhibition, sparse layers, and a Hebbian update mechanism in a feedforward network. It demonstrates rapid learning of vectorized MNIST digits with superior performance compared to standard ML methods, even with limited training samples per class. The model includes competitive inhibition in the AL, sparsity in the MB, and weight updates affecting only MB\u2192Readout connections. The MothNet model incorporates competitive inhibition, sparse layers, and Hebbian updates in a feedforward network. Weight updates only affect MB\u2192Readout connections. The model was tested as a feature generator for an ML classifier, improving accuracy on a non-spatial dataset derived from vMNIST. The MothNet model, incorporating competitive inhibition, sparse layers, and Hebbian updates, significantly improved ML methods' accuracies on a non-spatial dataset. MothNet-generated features outperformed PCA, PLS, NNs, and transfer learning in enhancing ML accuracy. vMNIST, downscaled and vectorized, provided samples with 85 pixels-as-features, showing advantages in training accuracy control. The full network architecture details of the AL-MB model (MothNet) are provided. The MothNet model, detailed in [11], improved ML accuracies on vMNIST dataset. Experiments involved training ML methods and MothNet on N samples per class, then retraining ML methods with MothNet outputs as features. Trained accuracies were compared to assess gains. Full Matlab code for experiments can be found at [12]. The MothNet model improved ML accuracies on the vMNIST dataset by using a feature generator joined to a ML classifier. Trained accuracies were compared to assess gains, with experiments involving different feature generators such as PCA, PLS, and NN pre-trained on vMNIST data. The baseline NN method was also used for comparison. The MothNet model significantly improved ML accuracies on the Omniglot dataset by generating new class-relevant features. Adding extra hidden layers to the baseline NN method did not enhance performance, indicating that MothNet features were unique. MothNet features outperformed other feature generators like PCA, PLS, and NN, increasing accuracy by 10% to 88% on the vMNIST dataset. The MothNet model improved ML accuracies on the Omniglot dataset by introducing new class-relevant features. It outperformed other feature generators like PCA, PLS, and NN, increasing accuracy by 10% to 88% on the vMNIST dataset. MothNet features led to a 20% to 55% relative reduction in test error across all ML models, with NN models benefiting the most. The gains were significant in almost all cases with N > 3, and even when ML baseline accuracy exceeded MothNet's ceiling. The MothNet model introduced new class-relevant features that improved ML accuracies on the Omniglot dataset. It outperformed other feature generators like PCA, PLS, and NN, leading to significant gains in accuracy. The MothNet architecture consists of a competitive inhibition layer (AL) and a high-dimensional, sparse layer (MB). Results showed that the high-dimensional layer (MB) was crucial, but the competitive inhibition of the AL layer also added value in generating strong features. NNs benefitted the most from the AL layer, with automated feature generation based on a simple BNN. The MothNet model introduced new class-relevant features that significantly improved ML accuracies on vMNIST and vOmniglot datasets. The automated feature generator based on a simple BNN with competitive inhibition, sparse projection, and Hebbian weight updates enhanced learning abilities. MothNet features outperformed standard methods like PCA, PLS, NNs, and pre-training, making class-relevant information more accessible. The competitive inhibition layer created attractor basins for inputs, increasing the effective distance between samples of different classes. The insect MB, similar to sparse autoencoders, pushes samples towards class attractors, increasing distance between classes. It differs from SAs by having more active neurons and no pre-training step. MBs also differ from Reservoir Networks as MB neurons lack recurrent connections. The Hebbian update mechanism in MB is distinct from backprop, with weight updates occurring on a \"use it or lose it\" basis. The dissimilarity of optimizers (MothNet vs ML) may increase total encoded information."
}