{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are neural generative models successful in modeling high-dimensional continuous measures. A scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework is presented in this paper. The formulation involves learning a transport map and a scaling factor to push a source measure to a target measure in a cost-optimal manner. The algorithm for solving this problem is based on stochastic alternating gradient updates, similar to GANs, and is demonstrated through numerical experiments in population modeling. The goal is to find a cost-optimal way to transform one measure to another using mass variation and transport, particularly useful in modeling population transformations. In population modeling, optimal transport methods aim to transform a source population into a target population efficiently by modeling mass transport and local mass variations. The Kantorovich formulation seeks the optimal probabilistic coupling between measures, with recent advancements using the Sinkhorn algorithm for more efficient solutions. These methods have applications in various fields like computer graphics and domain adaptation. Transport maps can also be learned in cases where a transport cost is not available. In various applications like computer graphics and domain adaptation, transport maps can be learned using generative models such as GANs to push a source distribution to a target distribution. Problems in image translation, natural language translation, domain adaptation, and biological data integration have been addressed using GAN variants with strategies like conditioning and cycle-consistency. In applications like computer graphics and domain adaptation, GANs have been used to learn transport maps for pushing a source distribution to a target distribution. Various methods have been proposed for extending optimal transport theory to handle unbalanced masses, with algorithms developed to approximate solutions for optimal entropy-transport problems. These algorithms have been applied in fields such as computer graphics, tumor growth modeling, and computational biology. However, while optimal entropy-transport allows for mass variation, it cannot explicitly model it, and there are currently no methods for performing unbalanced optimal transport between measures. In response to the limitations of optimal entropy-transport in modeling mass variation, a novel framework for unbalanced optimal transport is proposed. The framework includes a Monge-like formulation for learning a stochastic transport map and scaling factor, scalable methodology for solving the problem, and practical applications in population modeling using handwritten datasets. The methodology proposed involves an alternating gradient descent method for population modeling using various datasets, including handwritten digits and single-cell RNA-seq data from zebrafish. Additionally, a new scalable method for solving the optimal-entropy transport problem is introduced in the continuous setting. The optimal transport problem addresses transporting measures in a cost-optimal manner, with Monge formulating it as a search over deterministic transport maps. The Kantorovich OT problem is a convex relaxation of the Monge problem, formulating OT as a search over probabilistic transport plans. Introducing entropic regularization simplifies the dual optimization problem, solvable efficiently with the Sinkhorn algorithm. The introduction of entropic regularization simplifies the dual optimization problem, leading to efficient solutions using the Sinkhorn algorithm. Various formulations have been proposed to handle mass variation in optimal transport, with numerical methods based on optimal-entropy transport. State-of-the-art methods in the discrete setting include iterative scaling algorithms that extend the Sinkhorn algorithm. In the context of optimal transport, a new algorithm is proposed for unbalanced optimal transport that directly models mass variation. This algorithm aims to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. The algorithm addresses the challenge of unbalanced optimal transport between continuous measures in high-dimensional spaces. The algorithm proposed for unbalanced optimal transport aims to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. It models mass variation and considers stochastic maps for practical problems like cell biology. The unbalanced Monge OT problem can be used to model the transformation between source and target measures in various applications. The algorithm for unbalanced optimal transport learns a transport map and scaling factor to move points from a source to a target measure efficiently. It addresses class imbalances by adjusting the scaling factor to balance classes between distributions. Relaxation techniques are used to handle optimization challenges in satisfying constraints. The relaxation of the optimal-entropy transport problem involves using a divergence penalty instead of an equality constraint. This Monge-like version specifies a joint measure \u03b3 \u2208 M + (X \u00d7 Y) and reformulates the objective function in terms of \u03b3. The search space differs from the original formulation, as not all joint measures \u03b3 can be specified by a choice of (T, \u03be). The relaxation of the optimal-entropy transport problem involves using a divergence penalty instead of an equality constraint. Equivalence can be established by restricting the support of the joint measure \u03b3 to supp(\u00b5) \u00d7 Y. Theorem 3.4 shows that solutions of the relaxed problem converge to solutions of the original problem under certain conditions. The relaxation of unbalanced Monge OT involves learning the transport map and scaling factor using stochastic gradient methods. The optimization procedure resembles GAN training and involves minimizing the divergence between transported samples and real samples from \u03bd. This is achieved by parameterizing T, \u03be, and an adversary function f with neural networks and using alternating stochastic gradient updates. The objective is to minimize the divergence between transported samples and real samples from \u03bd using neural networks to parameterize the transport map T, scaling factor \u03be, and adversary function f. The optimization procedure involves stochastic gradient updates and aims for a cost-efficient strategy. Practical considerations for implementation and training are discussed in the appendix. The transport map T and scaling factor \u03be are parameterized by neural networks for scalable optimization using stochastic gradient descent. The neural architectures imbue their function classes with a specific structure, enabling effective learning in high-dimensional settings. A new stochastic method based on the same dual objective as BID8 is proposed in the Appendix for handling transport between continuous measures, overcoming scalability limitations. The method proposed in the Appendix generalizes the approach for handling transport between continuous measures and overcomes scalability limitations. However, the output in the form of the dual solution is less interpretable for practical applications compared to Algorithm 1. Learning a scaling factor to \"balance\" measures arises in causal inference, where the goal is to scale the importance of different members from a control population based on their likelihood to be present in a treated population. Algorithm 1 performs unbalanced optimal transport on MNIST data to eliminate selection biases in treatment effects. It applies to population modeling with class distribution adjustments between source and target datasets. Algorithm 1 is used to model the evolution of class distribution between source and target datasets through unbalanced optimal transport. The scaling factor learned reflects class imbalances and can be used to track growth or decline of different classes in a population. Experiments validate the effectiveness of Algorithm 1 in reweighting distributions, as shown in FIG4 and FIG1. The unbalanced optimal transport process involves determining the transport cost between images from the MNIST and USPS datasets. The scaling factor indicates the prominence of MNIST digits in the USPS dataset. Brighter MNIST digits with higher scaling factors cover more pixels, reflecting similarities with USPS digits. This analysis was also applied to the CelebA dataset. The study applied Algorithm 1 on the CelebA dataset to perform unbalanced optimal transport from young faces to aged faces. A variational autoencoder was trained on the dataset to encode samples into a latent space, and the transport cost was based on Euclidean distance. The results showed that transported faces generally retained key features, with some exceptions like gender swaps. The study applied Algorithm 1 on the CelebA dataset to perform unbalanced optimal transport from young faces to aged faces. Faces retained key features, with exceptions like gender swaps. Young faces with higher scaling factors were enriched for males. The model predicts a growth in the prominence of male faces as the population evolves. There was a strong gender imbalance between young and aged populations. In biology, lineage tracing of cells during development or disease progression is of interest. This is a natural application of transport where source and target distributions are unbalanced. In a study using Algorithm 1, unbalanced optimal transport was applied to single-cell gene expression data from two stages of zebrafish embryogenesis. The source population was from a late blastula stage, and the target population was from an early gastrulation stage. Cells with higher scaling factors were found to be enriched for genes associated with differentiation and development of the mesoderm. This experiment demonstrates the potential for meaningful biological discovery through analysis of scaling factors. The experiment using unbalanced optimal transport on zebrafish embryogenesis data showed cells with higher scaling factors enriched for genes related to mesoderm differentiation and development. A stochastic method for unbalanced OT based on the regularized dual formulation of BID7 was presented, which includes a strongly convex regularization term for unconstrained optimization. The dual of the regularized problem is expressed in terms of expectations. The relationship between primal optimizer \u03b3 * and dual optimizer (u * , v * ) is given by DISPLAYFORM5. Rewriting equation (9) in terms of expectations, assuming access to samples from \u00b5, \u03bd, and normalized measures \u03bc, \u03bd, along with normalization constants. Parameterizing u, v with neural networks u \u03b8 , v \u03c6 and optimizing \u03b8, \u03c6 using stochastic gradient descent in Algorithm 2, a generalization of classical OT to unbalanced OT. The dual solution (u * , v * ) learned from Algorithm 2 can reconstruct the primal solution \u03b3 * based on the relation in (10). The dual solution (u*, v*) learned from Algorithm 2 can be used to reconstruct the primal solution \u03b3* which represents the amount of mass transported between points in X and Y. By optimizing \u03b8, \u03c6 with neural networks u\u03b8, v\u03c6 using stochastic gradient descent, an \"averaged\" deterministic mapping from X to Y can be learned. The objectives in (6) and (3) are equivalent when reformulated in terms of \u03b3 instead of (T, \u03be), with a joint measure \u03b3 \u2208 M+(X \u00d7 Y). The formulations are equivalent when the search space is restricted to joint measures specified by (T, \u03be). Lemma 3.3 formalizes this relation. The proof shows that L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) and L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) for any solution (T, \u03be) and \u03b3. The disintegration theorem implies the existence of a family of measurable functions {T x : Z \u2192 Y} x\u2208X. The disintegration theorem implies the existence of a family of measurable functions {T x : Z \u2192 Y} x\u2208X such that \u03b3 y|x is the pushforward measure of \u03bb under T x for all x \u2208 X. Denoting T (x, z) : (x, z) \u2192 T x (z), then by a change of variables, it follows that (T, \u03be) satisfy certain relations. This leads to the conclusion that W c1,c2,\u03c8 (\u00b5, \u03bd) \u2265 L \u03c8 (\u00b5, \u03bd), with implications for optimal entropy-transport and the uniqueness of joint measures specified by any minimizer of L \u03c8 (\u00b5, \u03bd). If c1 satisfies Corollary 3.6 BID27, then the joint measure \u03b3 specified by any minimizer of L\u03c8(\u00b5, \u03bd) is unique. The minimizers of L\u03c8(\u00b5, \u03bd) are restricted to G, ensuring uniqueness of the marginals \u03c0X#\u03b3, \u03c0Y#\u03b3. The product measure generated by the minimizers of L\u03c8(\u00b5, \u03bd) is also unique. For certain cost functions and divergences, L\u03c8 defines a proper metric between positive measures \u00b5 and \u03bd. Based on Lemma 3.3 and standard results on constrained optimization, it can be shown that solutions of the relaxed problem converge to solutions of the original problem. The convergence is proven through the pointwise convergence of the equality constraint and the boundedness of the divergence penalty. This implies the uniqueness of the joint measure and the product measure generated by the minimizers of L\u03c8(\u00b5, \u03bd). In this section, the convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem. It is shown that the sequence of minimizers \u03b3 k is bounded and equally tight, leading to the existence of a weakly convergent subsequence \u03b3. This implies that \u03b3 is a minimizer of W c1,c2,\u03b9= (\u00b5, \u03bd). In this section, the convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem. Lemma B.2 states that for non-negative finite measures P, Q over T \u2282 R d, certain conditions must be met for equality to hold. A simple proof of this result is provided, with a similar result shown in previous studies. The optimal f over the support of Q is obtained when dP dQ belongs to the subdifferential of \u03c8*(f), and the optimal f over the support of P\u22a5 is \u03c8\u221e. The choice of cost functions is discussed in Proposition B.1. The choice of cost functions in Proposition B.1 provides conditions for wellposed problems. It is common to use the Euclidean distance for the cost of transport, c1, and a convex function for the cost of mass adjustment, c2. Different \u03c8-divergences can be used to train generative models to match a generated distribution P to a true data distribution Q. The Jensen's inequality states that the divergence between two probability measures P and Q is minimized when P = Q. However, this does not hold true when P, Q are not probability measures. In such cases, an additional constraint on the entropy function \u03c8 is needed to ensure divergence minimization matches P to Q. The Jensen's inequality states that divergence is minimized when P = Q, but this doesn't apply when P, Q are not probability measures. Additional constraints on the entropy function \u03c8 are required for divergence minimization to match P to Q.\u03c8(s) must attain a unique minimum at s = 1 with \u03c8(1) = 0, \u03c8 \u221e > 0 for P = Q. If not, P = Q when D \u03c8 (P |Q) is minimized. Different examples of \u03c8 for unbalanced OT are provided in Table 1. The choice of activation layers for neural networks mapping to (\u2212\u221e, \u03c8 \u221e ] is crucial. For experiments, fully-connected feedforward networks with 3 hidden layers and ReLU activations were used. The output activation layers included a sigmoid function for mapping pixel brightness to (0, 1) and a softplus function for scaling factor weight mapping to (0, \u221e). Table 1 lists examples of activation layers that can be utilized."
}