{
    "title": "Hk6kPgZA-",
    "content": "Neural networks are vulnerable to adversarial examples, and researchers have proposed various attack and defense mechanisms. A principled approach using distributionally robust optimization guarantees performance under adversarial input perturbations. By incorporating a Lagrangian penalty formulation in a Wasserstein ball, a training procedure is provided that includes worst-case perturbations of training data. This method achieves moderate levels of robustness for smooth losses with minimal computational or statistical cost compared to empirical risk minimization. Statistical guarantees allow for efficient certification of robustness for the population loss, and for imperceptible perturbations, this method matches or surpasses heuristic approaches. In supervised learning, minimizing expected loss over a parameter is crucial for robustness to changes in data distribution. Deep networks in critical systems like self-driving cars are vulnerable to adversarial attacks, leading to misbehavior. Researchers have proposed attack and defense mechanisms, but rigorously identifying defendable attack classes remains a challenge. Our work focuses on developing efficient procedures with rigorous guarantees for small to moderate amounts of robustness in adversarial training. We use distributionally robust optimization to provide provable guarantees on computational and statistical performance, considering a class of distributions around the data-generating distribution. Our approach includes robustness sets with computationally efficient relaxations, even for non-convex loss functions. The choice of P impacts robustness guarantees and computability. We develop robustness sets P with efficient relaxations for non-convex losses. Our adversarial training procedure ensures convergence guarantees similar to non-robust approaches. It certifies performance for the worst-case population loss sup P \u2208P E P [ (\u03b8; Z)]. Our method, implemented in Tensorflow, matches runtimes of other adversarial training procedures. It generalizes to protect against attacks on the test dataset. The formulation of problem FORMULA0 under the Wasserstein metric is intractable, so a Lagrangian relaxation with a penalty parameter is considered. This results in a robust surrogate that allows for adversarial perturbations of the data. The penalty problem is typically solved using the empirical distribution, offering defense against imperceptible adversarial perturbations at minimal computational cost. The robust surrogate function in adversarial training is easy to optimize for smooth losses with a large penalty. Stochastic gradient methods have similar convergence guarantees as non-robust methods. A certificate of robustness is provided for any level of robustness, with an upper bound on worst-case loss. Smooth activations in networks are suggested over ReLU's. Experimental results show state-of-the-art performance on various adversarial attacks. The standard robust-optimization approach minimizes losses for uncertainty sets, underlies recent advances in adversarial training, and uses heuristics like the fast gradient sign method. Adversarial training trains on losses with perturbed data, optimizing the objective of a constrained penalty problem. Model-fitting with these techniques may face convergence issues due to non-concave inner supremum. The current work discusses the challenges of model-fitting with techniques that may not converge due to non-concave inner supremum. Smoothness in deep architectures with ELU's allows for finding Lagrangian worst-case perturbations with low computational cost. Distributionally robust optimization involves choosing P in the robust objective to affect the uncertainty set richness and optimization problem tractability. Various approaches to distributional robustness have been explored, including finite-dimensional parametrizations and non-parametric distances for probability measures. In contrast to f-divergences, Wasserstein distances allow for robustness to unseen data by including distributions with different support. Tractable classes of uncertainty sets and losses have been studied, with different optimization approaches used for f-divergence balls and worst-case regions formed by Wasserstein balls. This work addresses a larger class of losses and costs, providing direct solution methods for a Lagrangian relaxation of the saddle-point problem. The application area includes domain adaptation, with guarantees similar to Lee & Raginsky's work for the empirical minimizer of the robust saddle-point. Our approach in robust optimization leverages the distributionally robust approach to defend against adversarial perturbations. By assuming smoothness of the function and strong convexity of the penalty, we show that the surrogate optimization problem is strongly concave. This provides a computationally efficient and principled approach for robust optimization problems. The approach in robust optimization leverages distributionally robust methods to defend against perturbations. It provides a computationally efficient approach for robust optimization by utilizing duality results and Wasserstein distances to define closeness between distributions. Stochastic gradient descent methods are used to find minimizers for convex cases or approximate stationary points for non-convex cases in relaxed robust problems. The text discusses the Wasserstein form of the robust problem and its Lagrangian relaxation, presenting a duality result for the relaxation. It introduces the robust surrogate and focuses on the Lagrangian penalty problem, developing stochastic gradient-type methods for the relaxed robust problem. The approach leverages distributionally robust methods for robust optimization, emphasizing computational benefits by relaxing strict robustness requirements. The text discusses the computational benefits of relaxing strict robustness requirements in the formulation. Assumptions are made regarding the continuity and convexity of functions to ensure tractable computability. Smoothness assumptions are also required for efficient computation of the robust surrogate. Lemma 1 shows that with certain conditions met, the surrogate remains smooth. The text discusses the benefits of Lagrangian relaxation in a stochastic-gradient approach for a penalty problem. Algorithm 1 converges linearly when the loss is convex in \u03b8 and \u03b3 is large enough. The convergence rate is 1/ \u221a T for a stochastic monotone variational inequality. The text discusses the convergence of the stochastic gradient method in nonconvex optimization problems, guaranteeing convergence to a stationary point of the penalty problem at a rate of 1/ \u221a T. The method achieves rates of convergence comparable to standard smooth non-convex optimization, with the accuracy parameter having a fixed effect on optimization accuracy. The smoothness of the loss function in z is crucial for the convergence guarantee. The smoothness of the loss function in z is essential for the convergence guarantee in nonconvex optimization. Theorem 2 shows that the loss is smooth in z, allowing for computation and certificates of optimality. Replacing ReLU's with sigmoids or ELU's makes distributionally robust optimization tractable for deep learning. The framework can consider adversarial perturbations to feature vectors, with results generalizing to this setting with minor modifications. Our distributionally robust framework is general enough to consider adversarial perturbations to an arbitrary subset of coordinates in Z. Algorithm 1 learns to protect against adversarial perturbations on the training dataset, and these procedures generalize to prevent attacks on the test set. Results hold uniformly over the space of parameters, including the output of the stochastic gradient descent procedure. The first main result gives an upper bound on the population worst-case objective for any level of robustness, which is optimal for the level achieved for the empirical distribution. Our distributionally robust framework considers adversarial perturbations on the training set, with results generalizing to attacks on the test set. The main result provides an upper bound on the worst-case population objective, efficiently computable for any level of robustness. Adversarial perturbations on the training set generalize, guaranteeing robustness similar to solving the population counterpart. The data-dependent upper bound for the worst-case population objective is rigorously established with high probability. Our results show that the empirical worst-case loss provides a certificate of robustness to Wasserstein perturbations. The worst-case population loss is efficiently computable, offering a data-dependent guarantee. The main result establishes an upper bound for the worst-case population performance under adversarial perturbations. The certificate of robustness to Wasserstein perturbations is efficiently computable, offering a data-dependent guarantee. The bounds hold for Lipschitz functions, providing a robustness guarantee scaling linearly with d. The worst-case population objective is certified for any \u03c1 and \u03b8, with the certificate being tightest at the achieved level of robustness \u03c1 n (\u03b8). The certificate of robustness to Wasserstein perturbations is efficiently computable via expression (10). The bounds may be too large for practical use in security-critical applications due to their dependence on covering numbers and dimension. However, Proposition 1's strong duality result still applies to any distribution, allowing for interrogation of possible losses under perturbations for test examples. The empirical distribution on the test set can be used to compute the Monge-map and test loss for bounds on parameter sensitivity. Lemma 1 shows smoothness under certain assumptions, leading to concentration of robustness levels. The generalization rate for adversarial perturbations and natural examples is given by a bound, ensuring positive robustness in the certificate. The text discusses distributionally robust optimization with adversarial training, extending beyond supervised learning. Empirical evaluations compare performance with empirical risk minimization (ERM) and various training methods. Adversarial training literature typically considers attacks with imperceptible perturbations to input features. In distributionally robust optimization, training against weaker adversaries that perturb influential features is considered. The approach involves using squared Euclidean cost for feature vectors and testing against norms p = 2, \u221e. Different methods are used for training and attacks, with theoretical guarantees for large \u03b3 in WRM. Visualizations show the benefits of certified robustness, applied to supervised learning and reinforcement learning problems. In Section 4.3, the Markov decision process used for training differs from that for testing. WRM has theoretical guarantees for large \u03b3 but becomes heuristic for small \u03b3. Comparison with other methods on attacks with large adversarial budgets is done in Appendix A.4. WRM is compared with heuristics trained to defend against different adversaries in Appendix A.5. Synthetic data is generated for the first experiment, training a neural network with different activations and comparing WRM with ERM and FGM. Fair comparisons are made using specific perturbation magnitudes. In Section 4.3, the Markov decision process used for training differs from that for testing. WRM has theoretical guarantees for large \u03b3 but becomes heuristic for small \u03b3. Comparison with other methods on attacks with large adversarial budgets is done in Appendix A.4. WRM is compared with heuristics trained to defend against different adversaries in Appendix A.5. Synthetic data is generated for the first experiment, training a neural network with different activations and comparing WRM with ERM and FGM. Fair comparisons are made using specific perturbation magnitudes. The classification boundaries for different training procedures are illustrated, showing that WRM with ELU's provides a certified level of robustness with an axisymmetric classification boundary. In Figure 2 (a), a certificate of robustness is plotted against worst-case performance for WRM with ELU's. The Lagrangian relaxation is used to evaluate worst-case loss for different values of \u03b3 adv. The certificate is nearly tight near the achieved level of robustness \u03c1 n for WRM and provides a performance guarantee for various \u03c1 values. A neural network classifier is trained on the MNIST dataset with convolutional filters. The neural network classifier on the MNIST dataset consists of convolutional filter layers with ELU activations. Adversarial training techniques are compared, with WRM offering more robustness. All methods achieve high test-set accuracy, but it is important to assess their abilities to combat attacks. Performance of the methods is tested under PGM attacks with respect to 2-and \u221e-norms, showing that WRM outperforms ERM and provides a performance guarantee for worst-case performance. The study compares five methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks, showing that WRM offers the most robustness. Training with Euclidean cost provides defense against \u221e-norm fast gradient attacks. The stability of the loss surface is analyzed, with WRM being the most stable against perturbations. Adversarial-training reduces gradient magnitudes near the nominal input, defending against attacks. The study compares five methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks, showing that WRM offers the most robustness. WRM defends against gradient-based exploits by learning a representation that makes gradients point towards inputs of other classes. The method's defense mechanisms to gradient-based attacks involve creating a more stable loss surface by reducing gradient magnitudes and improving interpretability. In final experiments, distributional robustness in Q-learning is considered. In reinforcement learning, the goal is to maximize cumulative rewards by considering state-action transition probabilities and rewards. Robust MDP's aim to maximize the worst-case realization of rewards under an ambiguity set of state-action transitions. Adversarial state perturbations can be used to incorporate distributional robustness in scenarios with continuous state-spaces. This approach provides robustness to uncertainties in state-action transitions. The adversarial training procedure in reinforcement learning provides robustness to uncertainties in state-action transitions. It is tested in the cart-pole environment, where the goal is to balance a pole on a cart by moving it left or right. The environment caps episode lengths at 400 steps and ends prematurely if the pole falls too far from the vertical or the cart translates too far from its origin. The reward function is based on the angle of the pole from the vertical. The Q-learning algorithm uses a tabular representation with discretized states for the pole angle and its time-derivative, and a binary action space to push the cart left or right. The study compares the performance of two models in the cart-pole environment with perturbations to physical parameters. The robust model outperforms in harder environments and learns more efficiently in the original MDP. Adversarial perturbations may encourage better exploration of the environment. The study shows that robust models outperform in challenging environments and learn more efficiently in the original MDP. Adversarial perturbations may lead to better exploration. A method for guaranteeing distributional robustness with data perturbation is proposed, showing strong statistical guarantees and fast optimization rates for various problems. Smooth networks may be preferable for robustness, with empirical evidence supporting the effectiveness of the approach. The simplicity and wide applicability of the method make it advantageous across different machine-learning scenarios. The study focuses on the simplicity and wide applicability of a method for guaranteeing distributional robustness with data perturbation. The optimization result applies for small values of robustness and a limited class of costs. Statistical guarantees use covering numbers as a measure of model complexity, which can be large for deep networks. This poses a challenge for security-essential contexts. Recent works aim to address this issue by replacing covering number arguments with margin-based bounds. The theoretical guarantees show the possibility of small-perturbation attacks. The work focuses on small-perturbation attacks and building models that guard against such attacks efficiently. In the large-perturbation regime, training certifiably secure systems remains a challenge. Conventional defense heuristics may not be effective in the face of large-perturbation/perceptible-attack settings. Moving beyond current attack and defense models may be necessary for advancements in security research in deep learning. In Figure 7, WRM's \"misclassifications\" are consistently reasonable to the human eye, as gradient-based perturbations transform the original image to other labels. Reasonable misclassifications indicate a learned data representation with interpretable gradients. Visualizing stability over inputs, the smallest WRM perturbation necessary to misclassify a datapoint is illustrated. The performance degradation with a fixed WRM adversary budget higher than that used for training is smooth. Decreasing the penalty parameter \u03b3 results in diminishing gains in achieved robustness. The study illustrates the smallest perturbation needed to misclassify a data point in a model. Comparisons are made between different training methods and the performance differences are less apparent with larger adversarial budgets. The inner supremum is no longer strongly concave for over 10% of the data, indicating a lack of performance guarantees. The approach becomes heuristic for large adversaries. Training methods like FGM, IFGM, and PGM are compared with WRM trained using different cost functions. The study compares different training methods like FGM, IFGM, and PGM with WRM in defending against adversarial attacks. WRM outperforms other methods against imperceptible attacks for both Euclidean and \u221e norms, especially with small perturbations. The study compares different training methods like FGM, IFGM, and PGM with WRM in defending against adversarial attacks. WRM outperforms other methods against imperceptible attacks for both Euclidean and \u221e norms, especially with small perturbations. However, WRM's performance is worse than other methods on attacks with large adversarial budgets, particularly in the case of \u221e-norm attacks. Computational guarantees for WRM do not hold against \u221e-norm adversaries, making optimization with the \u221e-norm challenging. A heuristic algorithm based on proximal algorithms is proposed in Appendix E to solve the inner supremum problem with the \u221e-norm cost function. Comparisons with other adversarial training procedures against \u221e-norm adversaries are shown in FIG0. The study compares different training methods like FGM, IFGM, and PGM with WRM in defending against adversarial attacks. WRM outperforms other methods against imperceptible attacks for both Euclidean and \u221e norms, especially with small perturbations. However, WRM's performance is worse than other methods on attacks with large adversarial budgets, particularly in the case of \u221e-norm attacks. Results comparing the proximal heuristic introduced in Appendix E with other adversarial training procedures against \u221e-norm adversaries are shown in figures. The computation of worst-case perturbations for feedforward neural networks with ReLU activations is proven to be NP-hard. The study at ICLR 2018 addresses the worst-case perturbation problem, proving it to be NP-hard. By imposing a separable structure on U, the optimization problem for feedforward neural networks with ReLU activations is also shown to be NP-hard. The decision reformulation of the problem and its relation to NP-completeness are discussed, along with alternative proofs provided. The optimization problem for feedforward neural networks with ReLU activations is proven to be NP-hard. An alternative proof using convex analysis is provided, showing that the function g(x, z) = \u03b3c(x, z) \u2212 f(x) is a normal integrand. Theorem 5 states that for any \u03c1 > 0, a certain condition holds. The text discusses the application of standard duality results in the space of probability measures, focusing on regular conditional probabilities and measurable mappings. The optimization problem for feedforward neural networks with ReLU activations being NP-hard is also mentioned. The text discusses the application of standard duality results in the space of probability measures, focusing on regular conditional probabilities and measurable mappings. The optimization problem for feedforward neural networks with ReLU activations being NP-hard is also mentioned. Results from Rockafellar & Wets (1998) are used to show the continuity and Lipschitzness of a function z(\u03b8) in \u03b8, leading to the completion of the proof. The text discusses the differentiability of a function f with respect to \u03b8 and its directional differentiability. It mentions the inf-compactness of f and the application of Lemma 3 to show that f is directionally differentiable. The proof is based on the uniqueness of S(\u03b8) and the continuity of g\u03b8 and Lipschitzness of z(\u03b8). The desired result is obtained by utilizing inequalities and gradient steps. The proof utilizes inequality FORMULA7 and gradient steps with DISPLAYFORM0. By a Taylor expansion using the smoothness of the objective F, we establish a bound and show concentration around the population counterpart. The result follows from strong duality in Proposition 1. The proof utilizes inequality FORMULA7 and gradient steps with DISPLAYFORM0 to establish a bound and show concentration around the population counterpart. With probability at least 1 \u2212 e^(-t), the result from strong duality in Proposition 1 is essentially standard BID52. The concentration result (30) is then shown, demonstrating the attainment of P * for all P 0. The proof establishes a bound using inequality FORMULA7 and gradient steps with DISPLAYFORM0 to show concentration around the population counterpart. The concentration result (30) demonstrates the attainment of P * for all P 0, utilizing strong duality in Proposition 1. The transportation mapping T (\u03b8, z) is unique and well-defined under the assumption of strong concavity, leading to the conclusion that P * is attained for all P 0. The proof establishes a bound using inequality FORMULA7 and gradient steps with DISPLAYFORM0 to show concentration around the population counterpart. The transportation mapping T (\u03b8, z) is unique and well-defined under the assumption of strong concavity, leading to the conclusion that P * is attained for all P 0. Then, \u03b3c(z 2 , z) \u2212 \u03b3c(z 1 , z) is analyzed, showing that j = j."
}