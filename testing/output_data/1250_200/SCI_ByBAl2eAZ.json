{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods use noise injection for exploratory behavior. Adding noise to agent's parameters can lead to more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods. Exploration is crucial in deep RL to prevent premature convergence to local optima. Efficient exploration is challenging as it is not guided by the reward function. Various methods have been proposed to address this challenge in high-dimensional and continuous-action environments. Adding temporally-correlated noise or parameter noise can enhance exploration in deep reinforcement learning algorithms. These methods have been shown to increase the variety of behaviors exhibited by the policy. However, existing approaches are limited in their applicability to specific settings or disregard temporal structure and gradient information. This paper explores the effective combination of parameter space noise with off-the-shelf deep RL algorithms like DQN and DDPG. This paper investigates combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to improve exploratory behavior. Results show that parameter noise outperforms traditional action space noise in tasks with sparse rewards. The standard RL framework involves an agent interacting with a fully observable environment modeled as a Markov decision process. The goal is to maximize rewards. The paper explores off-policy reinforcement learning methods using Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG). DQN utilizes a deep neural network to estimate the optimal Q-value function, while DDPG focuses on learning a deterministic policy. Both algorithms aim to maximize the expected return by interacting with the environment and updating their policies accordingly. The Boltzmann policy encourages exploration by sampling noise in the action space. DDPG is an actor-critic algorithm for continuous action spaces, where the actor maximizes the critic's estimated Q-values. TRPO is an on-policy method that updates function approximators according to the currently followed policy. Trust Region Policy Optimization (TRPO) is an extension of traditional policy gradient methods that improves upon REINFORCE by computing an ascent direction that ensures a small change in the policy distribution. It solves a constrained optimization problem using discounted state-visitation frequencies and advantage functions. Policies are represented as parameterized functions, typically neural networks, with structured exploration achieved by sampling from a set of policies with additive Gaussian noise. The perturbed policy is sampled at the beginning of each episode and kept fixed for the entire rollout. State-dependent exploration is crucial in policy optimization. Action space noise and parameter space noise have distinct effects on policy consistency. While Gaussian action noise leads to different actions for the same state, perturbing policy parameters ensures consistency in actions for the same state throughout the rollout. This introduces a dependence between the state and the exploratory action. Perturbing deep neural networks with spherical Gaussian noise ensures consistency in actions for the same state in policy optimization. Layer normalization between perturbed layers allows for a consistent perturbation scale across all layers. Adaptive noise scaling is necessary to pick a suitable scale \u03c3, which may vary over time as parameters become more sensitive to noise. Adaptive noise scaling is proposed to address limitations in parameter space noise in policy optimization. By adjusting the scale of parameter space noise over time based on the variance in action space, a distance measure is defined to determine when to increase or decrease the noise. This approach is applicable to off-policy methods as well, allowing for perturbation of the policy for exploration while training the non-perturbed network on collected data. Parameter space noise can be incorporated in on-policy methods by perturbing the policy for exploration and training the non-perturbed network on collected data. This approach optimizes stochastic policies using likelihood ratios and the re-parametrization trick. The method fixes the value of the noise parameter and scales it adaptively. The section addresses the benefits of incorporating parameter space noise in state-of-the-art RL algorithms and its effectiveness in exploring sparse reward environments compared to evolution strategies. Parameter space noise exploration is compared against evolution strategies for deep policies in sparse reward environments. Reference implementations of DQN and DDPG with adaptive parameter space noise are available online. The added value of parameter space noise over action space noise is measured on high-dimensional discrete-action environments and continuous control tasks using DQN, DDPG, and TRPO. By using parameter perturbation, the network is reparametrized to represent the greedy policy \u03c0 implied by the Q-values. This approach allows for a fair comparison between action space noise and parameter space noise without introducing additional hyperparameters. Perturbing the policy \u03c0 instead of the Q-function leads to more meaningful changes, as it defines an explicit behavioral policy. The policy is trained to exhibit the same behavior as running a greedy DQN, while the Q-network is trained following standard DQN practices. The policy is trained to mimic the behavior of a greedy DQN by maximizing the probability of outputting the greedy action according to the current Q-network. Parameter space noise is compared against regular DQN and two-headed DQN with -greedy exploration. Random actions are sampled for the first 50 thousand timesteps to fill the replay buffer before training. Combining parameter space noise with a bit of action space noise improves performance. Experimental details are provided for 21 games of varying complexity, with learning curves shown for a selection of games. Each agent is trained for 40 M frames, and performance is evaluated with three random seeds. The overall performance of parameter space noise is evaluated by running configurations with three random seeds, showing better results than action space noise on games requiring consistency. However, it struggles in extremely challenging games like Montezuma's Revenge, where more sophisticated exploration methods may be needed. The study also compares against a double-headed version of DQN with -greedy exploration to confirm the improvement in exploration is not due to architecture changes. The study evaluates the performance of parameter space noise compared to action space noise in learning games. Proposed improvements to DQN are mentioned as potentially enhancing results further. Parameter noise is compared with action noise in continuous control environments using DDPG as the RL algorithm. Different noise configurations are tested, with layer normalization found to be important for parameter space noise. In comparing different noise configurations for continuous control tasks, parameter space noise outperforms other schemes, achieving significantly higher returns on HalfCheetah environment. Other exploration methods quickly converge to a local optimum, while parameter space noise behaves similarly initially but still shows superior performance. Exploration schemes quickly converge to a local optimum, with parameter space noise outperforming correlated action space noise. DDPG is capable of learning good policies even without noise, indicating well-shaped reward functions in the environments. Parameter noise helps in escaping local optima and decreasing performance variance between seeds in the Walker2D environment. Parameter noise aids in escaping local optima and decreasing performance variance between seeds. It enables existing RL algorithms to learn on environments with sparse rewards. A toy example with a chain of states is used to evaluate parameter noise on DQN variants. The environment has varying rewards in different states, with increasing difficulty as the chain length grows. Different seeds are trained and evaluated for each chain length to assess performance. The study evaluates parameter noise on DQN variants using a chain of states with varying rewards. Different seeds are trained and evaluated for each chain length to assess performance. The problem is considered solved if one hundred subsequent rollouts achieve the optimal return. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN in this simple environment. In continuous control environments with sparse rewards, parameter space noise may not guarantee optimal exploration compared to action space noise. Specific environments like SparseCartpoleSwingup and SparseDoublePendulum only yield rewards for specific achievements, challenging exploration strategies. In sparse reward environments, DDPG is used to solve tasks like SparseDoublePendulum and SparseCartpoleSwingup. SparseDoublePendulum is relatively easy to solve, while SparseCartpoleSwingup requires parameter space noise for successful policies. SparseHalfCheetah shows non-zero rewards but struggles to learn a successful policy. Parameter space noise can improve exploration behavior in off-the-shelf algorithms like DDPG, but success is not guaranteed in all cases. Evolution strategies also use noise for exploration but lack temporal information. Combining parameter space noise with traditional RL algorithms allows for improved exploration while still utilizing back-propagation for optimization. Comparisons between ES and traditional RL with parameter space noise are made. Comparing ES and traditional RL with parameter space noise directly on 21 ALE games shows DQN outperforming ES on 15 out of 21 Atari games despite being exposed to less data. Parameter space noise combines the exploration properties of ES with the sample efficiency of traditional RL. In the context of deep reinforcement learning, various techniques have been proposed to enhance exploration, but they are complex and computationally expensive. Perturbing policy parameters has been suggested as a method to improve exploration, showing better performance than random exploration in some cases. However, this approach is limited to low-dimensional policies and state spaces. Our method is applied and evaluated for both on and off-policy settings, using high-dimensional policies and environments with large state spaces. It is closely related to evolution strategies and policy optimization techniques. Unlike Bootstrapped DQN, which uses multiple heads for exploration, our approach perturbs network parameters directly, achieving similar, and sometimes superior, exploration behavior. Parameter space noise is proposed as a simpler and more effective alternative to traditional action space noise in deep reinforcement learning algorithms. It has been shown to improve performance in various algorithms such as DQN, DDPG, and TRPO, especially in environments with sparse rewards where action noise fails. This approach perturbs network parameters directly, leading to successful exploration behavior and offering a promising alternative to the current standard of action space noise. Experimental setup for ALE BID3 involves a network architecture with convolutional and hidden layers, ReLUs, layer normalization, parameter space noise, target network updates, Adam optimizer training, replay buffer size, annealing for -greedy baseline, and adaptive noise scaling for maximum KL divergence enforcement. The policy in ALE BID3 is perturbed at the beginning of each episode with adapted standard deviation every 50 timesteps. Layer normalization is included in the fully connected part of the network to prevent getting stuck. Initial data is collected with 50K random actions before training. Observations are down-sampled to 84x84 pixels and converted to grayscale, with a concatenation of 4 subsequent frames provided to the network. Up to 30 noop actions are used at the start of each episode, following the setup described in BID19. The DDPG setup follows a similar network architecture as described in BID18, with 2 hidden layers for both actor and critic. Layer normalization is applied to all layers, and target networks are soft-updated with \u03c4 = 0.001. The critic is trained with a learning rate of 10 \u22123, while the actor uses a learning rate of 10 \u22124. Both actor and critic are updated using the Adam optimizer with batch sizes of 128. The critic is regularized with an L2 penalty of 10 \u22122. The replay buffer holds 100 K state transitions, and observations are normalized. Parameter space noise is adaptively scaled to match action space noise, with \u03c3 = 0.2 for dense environments and \u03c3 = 0.6 for sparse environments. TRPO uses action space noise with \u03c3 = 0.2 for dense environments and \u03c3 = 0.6 for sparse environments. The policy network has 2 hidden layers with 32 tanh units for nonlocomotion tasks and 2 hidden layers with 64 tanh units for locomotion tasks. The Hessian calculation is subsampled with a factor of 0.1, \u03b3 = 0.99, and the batch size per epoch is set to 5 K timesteps. Various environments from OpenAI Gym and rllab are used for different tasks, each with specific reward conditions. SparseDoublePendulum (S \u2282 R 6 , A \u2282 R) rewards agent for reaching upright position. DQN with simple network used for Q-value function approximation. Each agent trained for up to 2 K episodes, varying chain length N. Problem considered solved if optimal return achieved in 100 subsequent trajectories. Environment depicted in Figure 6 for testing exploratory behavior. Adaptive parameter space noise DQN and bootstrapped DQN compared. The curr_chunk discusses the comparison of adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN in a simple and scalable testing environment for exploratory behavior. Various parameters and methods are detailed, including adaptive scaling of parameter space noise and the use of stochastic policy \u03c0 \u03b8 (a|s) with likelihood ratios and the reparametrization trick. The proposed adaption method for re-scaling is also mentioned. The proposed adaption method involves re-scaling parameter space noise over time using a time-varying scale \u03c3 k. This adaptation is based on a simple heuristic that updates \u03c3 k every K timesteps, incorporating a distance measure between non-perturbed and perturbed policies in action space. The proposed adaptation method involves rescaling parameter space noise over time using a time-varying scale \u03c3 k. This idea is based on the Levenberg-Marquardt heuristic and involves a distance measure between non-perturbed and perturbed policies in action space. The choice of distance measure depends on the policy representation, with \u03b1 = 1.01 always used in experiments. For DQN, a probabilistic formulation is used for both non-perturbed and perturbed policies. The proposed method involves rescaling parameter space noise over time using a time-varying scale \u03c3 k based on the Levenberg-Marquardt heuristic. It includes a distance measure between non-perturbed and perturbed policies in action space, using a probabilistic formulation for both policies. The distance measure is defined using the Kullback-Leibler divergence and relates to -greedy action space noise. The proposed method involves rescaling parameter space noise over time using a time-varying scale \u03c3 k based on the Levenberg-Marquardt heuristic. It includes a distance measure between non-perturbed and perturbed policies in action space, using a probabilistic formulation for both policies. The distance measure is defined using the Kullback-Leibler divergence and relates to -greedy action space noise. This distance measure is used to adaptively scale \u03c3 to match the KL divergence between greedy and -greedy policy, resulting in effective action space noise with the same standard deviation as regular Gaussian action space noise. The proposed method involves rescaling parameter space noise over time using a time-varying scale \u03c3 k based on the Levenberg-Marquardt heuristic. It includes a distance measure between non-perturbed and perturbed policies in action space, using a probabilistic formulation for both policies. The distance measure is defined using the Kullback-Leibler divergence and relates to -greedy action space noise. This is computed through the conjugate gradient algorithm, combined with a line search along the noise direction to ensure constraint conformation. Performance results for various exploration approaches are comparable, with adaptive parameter space noise achieving stable performance on InvertedDoublePendulum. Adding parameter space noise aids in learning much more consistently on challenging sparse environments, as shown in the performance results of TRPO with noise scaled according to the parameter curvature. Adaptive parameter space noise achieves stable performance on InvertedDoublePendulum, comparable to other exploration approaches. No noise in either action or parameter space yields similar results, indicating these environments with DDPG are not ideal for exploration testing."
}