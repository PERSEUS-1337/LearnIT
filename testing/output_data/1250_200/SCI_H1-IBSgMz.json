{
    "title": "H1-IBSgMz",
    "content": "Self-normalizing discriminative models approximate the normalized probability of a class without computing the partition function, which is beneficial for computationally-intensive neural network classifiers. Recent studies have shown that language models trained with Noise Contrastive Estimation (NCE) exhibit self-normalization, but the reason was unclear. This study provides a theoretical explanation by viewing NCE as a low-rank matrix approximation. Empirical comparisons with explicit approaches for self-normalizing language models reveal a surprising negative correlation between self-normalization and other factors. The ability of statistical language models to estimate word probabilities is crucial for NLP tasks. RNN language models are preferred but suffer from scalability issues due to softmax computation. Various methods like importance sampling, hierarchical softmax, BlackOut, and NCE have been proposed to address this problem. NCE has been used to train neural LMs with large vocabularies. Self-normalization was proposed as a solution to the high run-time complexity of predicting normalized probabilities at test time in language models. A self-normalized discriminative model is trained to produce near-normalized scores, eliminating the need for costly exact normalization at test time without sacrificing prediction accuracy. Two main approaches were proposed for training self-normalizing models. Two main approaches were proposed to train self-normalizing models: explicit self-normalization using softmax and encouraging the normalization term to be close to one, and using Noise Contrastive Estimation (NCE) with a fixed normalization term Z. Recent studies found that models trained with NCE and fixed Z exhibit self-normalization, but the mechanism behind this behavior remains unexplained. Theoretical analysis by BID0 suggests that explicitly training a model to be self-normalizing on a subset of instances can lead to self-normalization on similar instances. The study provides a theoretical justification for the self-normalization property of Noise Contrastive Estimation (NCE) in language modeling. It shows that NCE's objective aims to find the best low-rank approximation of normalized conditional probabilities without estimating the partition function explicitly. Empirical results suggest that models with better perplexities may have poorer self-normalization properties. The sum of self-normalized scores is negatively correlated with the entropy of the normalized distribution. The NCE algorithm for language modeling transforms parameter learning into a binary classifier training problem. It assumes word sampling from a mixture distribution where noise samples are more frequent than true samples. The distribution has a parametric form with d-dimensional vector representations for words and contexts. The NCE algorithm for language modeling uses a parametric distribution for word sampling, with a normalization term that can be set to 1 during training to simplify computations. The algorithm trains a binary classifier to distinguish between true word-context pairs and noise samples. The NCE algorithm for language modeling simplifies computations by setting Zc=1 at train time, without affecting model performance. However, at test time, log p(w|c) still needs to be normalized by computing Zc over all vocabulary words. An alternative interpretation views NCE as a low-rank matrix approximation, making the normalization factor redundant during training. This self-normalization property was empirically observed in later works. The NCE algorithm simplifies computations by setting Zc=1 at train time, but log p(w|c) still needs to be normalized at test time. NCE can be seen as a low-rank matrix approximation, with the NCE score obtaining its global maximum at the PCE matrix. The function DISPLAYFORM5 achieves its global maximum at the Pointwise Mutual Information (PMI) matrix. The NCE algorithm simplifies computations by setting Zc=1 at train time, but log p(w|c) still needs to be normalized at test time. The function DISPLAYFORM5 achieves its global maximum at the Pointwise Mutual Information (PMI) matrix. The NCE score reaches its peak at the PCE matrix, where the Jensen-Shannon divergence between joint distribution of words and their left-side contexts and the product of their marginal distributions is minimized. The NCE algorithm aims to find the best low-rank approximation of the PCE matrix without the need for normalization factors. Previous studies have used different approaches for setting the normalization factor Z, with some achieving optimal results by setting log(Z) = 9 during training. This alteration can impact the mean input to the sigmoid function in the NCE score. The NCE algorithm aims to find the best low-rank approximation of the PCE matrix without normalization factors. Setting a fixed value for Z can improve training stability, convergence speed, and performance similar to batch normalization. At test time, the model learned by NCE computes conditional probabilities using a normalization factor. The NCE model is self-normalized and finds the best low-rank unnormalized matrix approximation of the PCE matrix. If the matrix is close to the PCE matrix, the NCE model defined by it is approximately self-normalized. The NCE algorithm aims to find the best low-rank approximation of the PCE matrix without normalization factors. If the matrix is close to the PCE matrix, the NCE model defined by it is approximately self-normalized. Theorems 1 and 2 show that the NCE training goal is to make the unnormalized score close to the normalized log probability. The analysis also relates to other language models and their training strategies. The NCE algorithm aims to find the best low-rank approximation of the PCE matrix without normalization factors. BID4 proposed explicitly encouraging self-normalization in training by penalizing deviation from self-normalizing. BID0 provided an efficiently computed approximation by eliminating costly computations. They justified computing Zc only on a subset of the corpus by showing that if a given LM is exactly self-normalized on a dense set of contexts, E| log Zc | is small. This cannot explain why NCE training produces a self-normalized model. The NCE algorithm aims to find a low-rank approximation of the PCE matrix without normalization factors. Importance sampling (IS) is an efficient alternative closely related to NCE, where the normalization factor Zc is canceled out in the objective function. Unlike NCE, the network learned by IS is not self-normalized, requiring explicit computation of the normalization factor at test time. The study investigated the self-normalization properties of NCE language modeling and its explicit self-normalization alternative using LSTM-based language models. The models were initialized with output bias terms to achieve self-normalization at init time. The negative sampling parameter for NCE-LM was set to k = 100 for competitive performance. Other implementation details followed previous work for strong results. The study evaluated NCE language modeling with LSTM-based models, using self-normalization properties. Two datasets, PTB and WIKI, were used for evaluation. Metrics for self-normalization included mean log value and standard deviation of the normalization term. The study evaluated NCE language modeling with LSTM-based models on datasets PTB and WIKI. Metrics for self-normalization included mean log value and standard deviation of the normalization term. Results showed NCE-LM to be approximately self-normalized with low |\u00b5 z | and \u03c3 z values, while SM-LM was far from self-normalized. In line with previous findings, NCE-LM shows self-normalization with low |\u00b5 z | and \u03c3 z values, while SM-LM lacks self-normalization. SM-LM performs slightly better at lower model dimensions, but the gap narrows at d = 650. Surprisingly, NCE-LM's self-normalization quality worsens with higher dimensionality, contrary to expectations. Further investigation is discussed in Section 5.3. Table 2 compares DEV-LM's self-normalization and perplexity with varying \u03b1 values on validation sets. The self-normalization and perplexity performance of DEV-LM for different \u03b1 values on validation sets is compared. Larger \u03b1 values lead to better self-normalization but at the expense of perplexity. The self-normalization worsens with higher model dimensions. A technique to center log(Z) values of self-normalizing models is proposed for test-set evaluation. Table 3 shows results for shifted NCE-LM and DEV-LM models with d = 650, where \u03b1 = 1.0 provides an optimal trade-off between self-normalization and perplexity. The self-normalization and perplexity performance of DEV-LM with \u03b1 = 1.0 is compared to NCE-LM. Both models achieve near perfect self-normalization and similar perplexity values. DEV-LM has a better standard deviation of the normalization term, but NCE-LM's advantage is its training time not growing with vocabulary size. Pearson's correlation between entropy and log(Z) is shown in Table 4. The entropy of predicted distributions by a language model is a measure of uncertainty in word prediction. Low-entropy distributions are focused, while high-entropy ones are more spread out. Pearson's correlation shows a negative relationship between entropy and normalization term log(Z). Larger models exhibit a stronger correlation. Low entropy distributions can have high log(Z) values, deviating from self-normalization goals. The low entropy distributions in language models can lead to high log(Z) values, deviating from self-normalization goals. This observation may explain why larger models have larger variance in their normalization terms. Future research could focus on improving self-normalization algorithms by augmenting NCE's training objective with explicit self-normalization components. Future research could focus on improving self-normalization algorithms by augmenting NCE's training objective with explicit self-normalization components. Insights on the correlation between self-normalization and perplexity performance, as well as the partition function of self-normalized predictions and the entropy of the distribution, could be valuable for enhancing self-normalizing models in future work."
}