{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this, a balanced face image dataset with 7 race groups was created, showing improved accuracy across different races and genders. Commercial computer vision APIs were also compared for balanced accuracy. Existing public face datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. Numerous large-scale face image datasets have fostered research in automated face detection, alignment, recognition, generation, modification, and attribute classification. Despite the abundance of data, existing datasets are biased towards lighter skin, with other races significantly underrepresented. The recent study highlights biases in existing large-scale face databases, with a majority of faces being Caucasian and underrepresentation of other races. Biased data can lead to unfair automated systems and ethical concerns, as seen in criticisms of commercial computer vision systems for their accuracy disparities across different demographics. Existing large-scale face datasets have biases towards Caucasian faces, leading to unfair automated systems. To address this, a new dataset with balanced race composition containing 108,501 facial images from various sources has been proposed. The dataset includes 7 race groups and aims to mitigate race bias in existing datasets. The study shows that current face attribute datasets and models struggle with non-White faces, while the new dataset performs better on novel data. The new dataset aims to address biases in existing face attribute datasets by including more nonWhite faces and differentiating between various racial groups. This expansion enhances the applicability of computer vision methods in analyzing demographics. The goal of face attribute recognition is to classify human attributes from facial appearance, and the new dataset outperforms existing ones, especially across racial groups. The statistics of large-scale face attribute datasets, including a new dataset, show a dominance of the White race. Face attribute recognition is crucial for various computer vision tasks and must perform equally well across gender and race groups to avoid bias incidents like Google Photos mistaking African American faces as Gorillas. Such incidents often lead to service termination or feature removal. Most commercial service providers have stopped providing a race classifier due to concerns about improper training of models and potential negative impacts on users. Face attribute recognition is used in demographic surveys for marketing and social science research to understand human social behaviors. Social scientists use off-the-shelf tools and commercial services to analyze demographic attributes and behaviors of individuals through images. Unfair classification can lead to over-or under-estimation of specific sub-populations, with significant policy implications. The AI and machine learning communities are increasingly focusing on algorithmic fairness and addressing dataset and model biases. In the context of algorithmic fairness and dataset biases, the paper focuses on balanced accuracy in attribute classification, specifically gender classification from facial images. Research in fairness aims to ensure fair outcomes independent of protected attributes like race or gender. Studies have focused on auditing bias in datasets, improving datasets, and designing better algorithms. The paper addresses gender classification from facial images, highlighting biases in commercial systems towards dark-skinned females. It aims to mitigate limitations by collecting diverse face images from non-White race groups, improving generalization performance. The dataset includes Southeast Asian and Middle Eastern races, a first in large-scale in-the-wild face image datasets. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits while ethnicity is based on cultural similarities. Latino is considered a race in this dataset. The collection of diverse face images aims to address biases in gender classification from facial images. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Skin color is used as a proxy for racial or ethnicity grouping in some studies, but it has limitations due to variations in lighting conditions and the multidimensional nature of race. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Skin color is one dimensional and does not provide enough information to differentiate between many groups, such as East Asian and White. Race is a multidimensional concept, and physical race is annotated by human annotators. Skin color, measured by Individual Typology Angle (ITA), is also used to complement race categorization in face datasets sourced from public figures. The dataset aims to minimize selection bias by including diverse faces from the Yahoo YFCC100M dataset, which was not preselected. It is smaller but more balanced on race compared to other datasets. Annotations were made on 7,125 faces randomly sampled from the dataset, and demographic compositions of each country were estimated. After annotating 7,125 randomly sampled faces from the YFCC100M dataset, demographic compositions of each country were estimated. To avoid dominance by the White race, the number of images per country was adjusted. Faces were required to be a minimum of 50 by 50 pixels, with annotations done on images with specific Creative Commons licenses. Race, gender, and age group were annotated using Amazon Mechanical Turk with three workers per image for accuracy. The annotations for face attribute datasets were refined by training a model from initial ground truth annotations and manually verifying them. The race composition of datasets was measured, showing bias towards the White race. Gender balance in datasets ranged from 40%-60% male ratio. Model performance was compared using ResNet-34 architecture trained from each dataset. The study compared model performance using ResNet-34 architecture trained on different datasets with gender ratios ranging from 40%-60% male. Faces were detected using dlib's CNN-based face detector, and an attribute classifier was run on each face. The experiment was conducted in PyTorch, comparing the dataset with UTKFace, LFWA+, and CelebA. FairFace defined 7 race categories but only 4 races were used for comparison. CelebA was only used for gender classification as it lacked race annotations. The study compared model performance using ResNet-34 architecture trained on different datasets with gender ratios ranging from 40%-60% male. Faces were detected using dlib's CNN-based face detector, and an attribute classifier was run on each face. The experiment was conducted in PyTorch, comparing the dataset with UTKFace, LFWA+, and CelebA. FairFace defined 7 race categories but only 4 races were used for comparison. CelebA was only used for gender classification as it lacked race annotations. The models were tested on various datasets to assess generalization performance. The study collected test datasets from diverse sources including geo-tagged Tweets, media photographs from professional outlets, and a protest dataset for evaluating model effectiveness on different races. The authors collected a public image dataset for a protest activity study, sampling 8,000 faces annotated for gender, race, and age. The FairFace model outperformed others in accuracy for race, gender, and age on novel datasets. The FairFace model outperforms other datasets in accuracy for race, gender, and age classification, even with fewer training images. The model also shows more consistent results across different race groups, measured by standard deviations of classification accuracy. The accuracy equality and equalized odds measures are used to assess fair classification for gender classification. The FairFace model achieves the lowest maximum accuracy disparity in gender classification, with less than 1% accuracy discrepancy between male and female, and White and non-White groups. Other models show a strong bias towards males, with the LFWA+ model having the biggest gender performance gap at 32%. Commercial computer vision services also exhibit asymmetric gender biases, likely due to unbalanced training data representation. The dataset characteristics were investigated to measure data diversity, revealing unbalanced representation in training data. The facial embedding used in the study was trained from biased datasets dominated by White faces, leading to loosely separated race groups in the dataset. Different datasets like LFWA+ and UTKFace focus on local clusters, while FairFace contains many non-typical examples. Pairwise distance distributions between faces were examined to measure diversity in the datasets. The study focused on measuring the diversity of faces in different datasets by examining pairwise distance distributions. UTKFace had tightly clustered faces, while LFWA+ showed diverse faces despite majority being white. The face embedding trained on white-oriented data may have influenced the diversity results. Previous studies have shown inconsistencies in classification accuracies across demographic groups in commercial face analytic models. Commercial face analytic models exhibit varying classification accuracies across different demographic groups. The study utilized FairFace images to evaluate gender classification performance of online APIs including Microsoft Face API, Amazon Rekognition, IBM Watson Visual Recognition, and Face++. The dataset used was diverse in terms of race, age, expressions, head orientation, and photographic conditions, providing a robust benchmark for bias measurement. The experiments were conducted on August 13th -16th, 2019, with results indicating different accuracies for each API. Table 6 displays gender classification accuracies of various face analytic APIs, with Amazon Rekognition detecting all faces. Two sets of accuracies are reported, including mis-detections or excluding them. A model trained with the dataset serves as an upper bound for accuracy comparison. Classification accuracy based on skin color is also shown. The study shows gender classification biases favoring males, with dark-skinned females experiencing higher error rates. Skin color alone is not a reliable indicator of bias. Face detection can also introduce gender bias, as seen with Microsoft's model. The paper introduces a balanced face image dataset for gender, race, and age classification, outperforming existing datasets in generalization performance. The dataset proposed in the study achieves better generalization performance for gender, race, and age classification compared to existing datasets. It was derived from the Yahoo YFCC100m dataset and allows for academic and commercial usage. The dataset can be used to train new models and verify balanced accuracy of existing classifiers, addressing concerns about algorithmic fairness in AI systems. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems, improving model accuracy and transparency. The dataset includes the Individual Typology Angle (ITA) for measuring skin color distribution among different races."
}