{
    "title": "rkhlb8lCZ",
    "content": "Wavelet Pooling is introduced as an alternative to traditional neighborhood pooling in Convolutional Neural Networks for image and object classification. This method decomposes features into a second level decomposition, reducing feature dimensions and addressing overfitting. Experimental results show that Wavelet Pooling outperforms or performs comparably with other pooling methods like max, mean, mixed, and stochastic pooling. Convolutional Neural Networks (CNNs) excel in image and object classification due to their layer structures. The algorithm's strength drives constant evaluation and upgrades to enhance accuracy and efficiency. The key components, convolutional and pooling layers, undergo modifications to elevate performance. Pooling, rooted in predecessors like Neocognitron and Cresceptron, subsamples convolutional layer results to reduce spatial dimensions, parameters, increase efficiency, and prevent overfitting. Popular pooling methods include max pooling and average pooling, offering determinism, efficiency, and simplicity, but also have weaknesses. The most popular forms of pooling in Convolutional Neural Networks are max pooling and average pooling, which are deterministic, efficient, and simple but have weaknesses hindering optimal network learning. Other pooling operations like mixed pooling and stochastic pooling use probabilistic approaches to address these issues. However, all pooling operations employ a neighborhood approach to subsampling, similar to nearest neighbor interpolation in image processing. A proposed wavelet pooling algorithm utilizes second-level wavelet decomposition to subsample features, aiming to reduce artifacts and improve feature representation accuracy compared to traditional pooling methods. The proposed pooling method in Convolutional Neural Networks aims to improve feature representation accuracy by utilizing an organic, subband method. It is compared to other pooling methods like max, mean, mixed, and stochastic pooling on benchmark image classification datasets. The paper discusses the background, proposed methods, experimental results, and concludes with a summary. Pooling, also known as subsampling, condenses the output dimensions of the convolutional layer by summarizing regions into one neuron value. Max pooling and average pooling are the two most popular forms of pooling. The two most popular forms of pooling in Convolutional Neural Networks are max pooling and average pooling. Max pooling involves selecting the maximum value of a region for the condensed feature map, while average pooling calculates the average value. Both methods have their limitations, with max pooling potentially erasing details from an image and overfitting training data, and average pooling diluting pertinent details. Researchers have developed probabilistic pooling methods like mixed pooling, which combines max and average pooling by randomly selecting one method during training. This method can be applied in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. Another method, stochastic pooling, improves upon max pooling by randomly sampling from neighborhood regions based on probability values calculated from the activations within the region. The stochastic pooling method selects activations within neighborhood regions based on probability values calculated from the activations. By sampling from a multinomial distribution, the method picks a location within the region with the highest probabilities. This approach avoids the limitations of max and average pooling while utilizing some advantages of max pooling. Additionally, the proposed pooling method uses wavelets to reduce feature map dimensions and minimize artifacts. Our proposed wavelet pooling method reduces feature map dimensions and minimizes artifacts by discarding first-order subbands. It performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT), resulting in detail subbands for image classification. The proposed wavelet pooling method reduces feature map dimensions by performing a 2nd order decomposition using the fast wavelet transform (FWT). This results in detail subbands for image classification, which are reconstructed using the inverse discrete wavelet transform (IDWT) for backpropagation. The proposed wavelet pooling method utilizes the Haar wavelet for 2nd order decomposition, reconstructing image features for backpropagation. Experiments are conducted using MatConvNet, stochastic gradient descent, and GeForce Titan X Pascal GPUs. Different regularization techniques are tested on CIFAR-10 and SHVN datasets, with all pooling methods using a 2x2 window for comparison. The proposed wavelet pooling method uses the Haar wavelet for 2nd order decomposition in image feature reconstruction. Experiments are conducted on various datasets using different pooling methods with a 2x2 window for comparison. The network architecture is based on the MNIST structure with batch normalization. Results show the proposed method outperforms others, with max pooling showing signs of overfitting and average and wavelet pooling demonstrating smoother learning curves. The network structure for CIFAR-10 experiments includes using the full training and test sets from the dataset. Results show that the proposed method has the second highest accuracy, with wavelet pooling resisting overfitting. Different pooling methods are tested with and without dropout layers to observe their effects on learning progression. The network structure for SHVN experiments includes using dropout to observe its effects. The method proposed has the second lowest accuracy, with max pooling slightly overfitting the data. Different pooling methods show varying rates of learning progression. The network structure for KDEF experiments involves using the dataset containing 4,900 images of 35 people displaying seven basic emotions. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions. Errors in the dataset are fixed by mirroring missing or corrupted images and manually cropping them to 762 x 562 dimensions. The data is shuffled, with 3,900 images used for training and 1,000 for testing. Images are resized to 128x128 due to memory and time constraints. Dropout layers are used to regulate the network and prevent overfitting. The proposed method in the KDEF experiments shows the second highest accuracy. The proposed method in the KDEF experiments shows the second highest accuracy. Wavelet pooling is resistant to overfitting but lacks efficiency in computational complexity, serving as a proof-of-concept for potential improvements. In the context of computational efficiency in pooling methods, calculations are done for max pooling, average pooling, mixed pooling, stochastic pooling, and wavelet pooling. Average pooling requires the least computations, followed by mixed pooling and max pooling. Stochastic pooling is the least computationally efficient method. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. Despite this, with proper implementation and improvements to the FTW algorithm, wavelet pooling has the potential to outperform traditional methods in CNNs. It outperforms all others in the MNIST dataset and all but one in the CIFAR-10 and KDEF datasets. Our proposed wavelet pooling method outperforms traditional methods in CNNs on various datasets. It shows improved performance with the addition of dropout and batch normalization for network regularization. Results confirm that no single pooling method is superior, with some performing better depending on the dataset and network structure. Future work could explore varying wavelet basis and upsampling/downsampling factors for better image feature reduction. Improving downsampling factors in wavelet pooling can enhance image feature reduction beyond 2x2 scale. Retaining discarded subbands may boost accuracy and reduce errors during backpropagation. Enhancing the Fast Wavelet Transform (FTW) method can significantly improve computational efficiency. Analyzing the Structural Similarity (SSIM) of wavelet pooling compared to other methods can further validate its effectiveness."
}