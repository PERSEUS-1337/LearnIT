{
    "title": "BJxg7eHYvB",
    "content": "In this paper, a reinforcement learning based algorithm is proposed to reduce GPU memory usage in deep neural networks. Variable swapping and recomputation techniques are used to transfer variables between CPU and GPU memory and trade time for space during forward propagation. A deep Q-network is employed to automatically decide which variables to swap or recompute, resulting in improved performance compared to benchmarks. Limited GPU memory hinders model performance due to the increasing complexity of deep neural networks. Deep neural networks (DNNs) are using deeper structures, leading to higher GPU memory consumption. Deeper networks achieve better accuracy but require more GPU memory. Increasing input batch size can speed up training and improve accuracy, but also demands more GPU memory. Utilizing CPU memory through variable swapping can help manage GPU memory limitations. Variable swapping and recomputation are efficient techniques to manage GPU memory in deep neural networks. Variable swapping allows tensors to be moved in and out of GPU memory, while recomputation uses GPU computing engines to reduce memory usage. By combining swapping with recomputation, we can overlap data transfers with kernel execution, making the process more efficient. Different engines can run parallelly, ensuring that computing and DMA engines are not wasted during training. During data transfers, recomputation is used to avoid wasting computing or DMA engines. It is challenging to determine which variables to swap or recompute due to the complex structures of different DNNs. Existing works use heuristic search algorithms for recomputation or swapping, but they do not consider the time cost or make plans for these processes. Our work proposes a DQN algorithm to automatically make plans for swapping and recomputation in DNNs to reduce memory usage. Users only need to set memory limits without requiring background knowledge on DNNs. Unlike previous methods, our approach considers more information from computational graphs and ensures that variable swapping and recomputation do not compromise network accuracy. Our method utilizes more information from computation graphs and automatically provides plans for recomputation to reduce memory usage in DNNs. Unlike previous methods, we use a DQN algorithm to minimize computation overhead with a GPU memory limit. The algorithm utilizes DQN to minimize computation overhead with a GPU memory limit by optimizing GPU operations in a training iteration. It focuses on offloading, prefetching, removing, recomputing variables, and aims to minimize training time while staying within the memory limit. The algorithm optimizes DNNs and machine learning algorithms by using DQN to find an optimal plan for swapping and recomputing GPU operations within a memory limit. Q-learning is used to choose actions such as swapping, recomputation, or doing nothing. Deep Q-network approximates Q-values to select actions. The algorithm uses DQN to optimize DNNs and machine learning algorithms by approximating Q-values from states with a neural network. GPU operations are introduced, with each operation represented as a node in a graph. Agent states for DQN include information on variables, swapping/recomputing options, attributes, and DNN structures mapped into vectors. The algorithm utilizes DQN to optimize DNNs and machine learning algorithms by approximating Q-values with a neural network. GPU operations are depicted as nodes in a graph, with each node representing an operation. Agent states for DQN encompass details on variables, swapping/recomputing options, attributes, and DNN structures transformed into vectors. The state of each node is represented, and all node states are combined into agent states (graph states). Key notations include node states, weighted edges between nodes, parameter matrix W, and variables offloaded/recomputed in the current state. The process involves updating node states iteratively until reaching the final state. The algorithm uses DQN to optimize DNNs by representing GPU operations as nodes in a graph. Each node includes features like variable size, transfer duration, and action type. Neighbor nodes and edges are added for invariance and uniform node vector length. Graph state combines node states, with actions including offloading variables and doing nothing. This graph feature representation allows training on small graphs and fine-tuning on large ones. The actions on a large graph involve offloading variables, removing variables during forward propagation, and doing nothing. Variable swapping and recomputation each have two phases. Prefetching involves fetching reused variables not in GPU memory. No swap-in or second phase recomputation actions are included. If swapping in a variable does not exceed memory usage, prefetching begins. The agent transitions between nodes representing GPU operations, with actions like doing nothing, removing variables, or offloading. GPU operations may require suspending before prefetching variables. Actions change the agent's state but not the relationships between nodes. Each operation takes one second, offloading takes 1.5 seconds, and the agent moves to the next node after each action. The agent transitions between nodes representing GPU operations, with actions like doing nothing, removing variables, or offloading. If the action is offloading X 0, the agent will be in the 5th node. We need to check the GPU memory usage before offloading. The forward overhead for offloading X 0 is 0.5 seconds. The reward will be negative time for recomputing if X 0 is removed, and negative overhead if X 0 is offloaded. In the backward propagation, offloading X 0 incurs negative overhead due to prefetching. To estimate GPU operation times, we utilize NVIDIA's fast free and malloc functions. While we may not obtain exact weight derivatives, we can approximate GPU operation times. The agent transitions states based on actions, receiving rewards in a simulator environment. Assumptions include estimations for recomputing time and the ability to update DQN. The training environment is simulated to update DQN, with assumptions on recomputing time estimation and parallel variable swapping. Graph states are converted to Q values by concatenating with action node states. An end-to-end DQN is trained using a loss function with a decay factor and rewards for agent states and actions. The training environment simulates DQN updates using a loss function with a decay factor and rewards for agent states and actions. The algorithm updates weights with mini-batches from experience replay dataset and uses a greedy method to choose actions. GPU operations are executed in a time sequence, and memory reduction and usage are evaluated. Variable swapping and recomputation performance are assessed. In this evaluation, the performance of variable swapping and recomputation is tested on various architectures like ResNet, VGG, UNet, Kmeans, and LSTM. The experiments are conducted on CIFAR-10 and Karpathy's char-RNN dataset, as well as a randomly generated k-means dataset. The system used for testing includes a workstation with CPU Intel Xeon E5, NVIDIA GeForce GTX 1080 Ti, and 11 GB RAM. The method is compared with other baselines using the deep learning framework Singa. Our method, tested on Singa, outperforms MXNet-memonger, SuperNeurons, and TFLMS in terms of GPU memory savings and extra computation time. Different approaches like recomputation, variable swapping, and swapping combined with recomputation are compared, showing our method's superiority. Our method outperforms MXNet-memonger, SuperNeurons, and TFLMS in terms of GPU memory savings and extra computation time. SuperNeurons use the LRU algorithm for variable swapping, while TFLMS only uses variable swapping and requires manual setting of swap variables. Our method utilizes more information from computation graphs and provides automatic plans for users. SuperNeurons waste GPU resources for memory saving, resulting in lower GPU utilization during network training compared to our method. Our work is applicable to more general architectures and can work on LSTM functions like CuDNNLSTM, unlike the other methods. Our method outperforms MXNet-memonger, SuperNeurons, and TFLMS in terms of GPU memory savings and extra computation time. It can work on LSTM functions like CuDNNLSTM and supports ResNet with stochastic depth and K-Means. The algorithm allows for a wide range of memory limits, works well on iterative machine learning algorithms, and provides automatic plans for users without requiring expert knowledge. Analyzing different architectures, ResNet and VGG yield similar results, while UNet has a unique structure that requires careful memory management during forward propagation. In this paper, a DQN is proposed to reduce memory usage by devising plans for variable swapping and recomputation. The method can work with different memory limits and provides automatic plans for users without requiring background knowledge on DNN. The approach outperforms MXNet-memonger, SuperNeurons, and TFLMS in terms of GPU memory savings and extra computation time. The method is effective for LSTM functions like CuDNNLSTM and supports ResNet with stochastic depth and K-Means. Our method efficiently reduces memory usage by automatically providing plans for variable swapping and recomputation, without requiring background knowledge on DNN or machine learning algorithms. It is compatible with various network structures such as ResNet, VGG, K-means, SD ResNet, and LSTM, while maintaining network accuracy."
}