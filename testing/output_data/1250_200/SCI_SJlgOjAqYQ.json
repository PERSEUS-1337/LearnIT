{
    "title": "SJlgOjAqYQ",
    "content": "We conducted experiments to test global translation-invariance in deep learning models trained on the MNIST dataset. Both convolutional and capsules neural networks showed poor performance in this aspect, but data augmentation improved their performance. While the capsule network performed better on the MNIST testing dataset, the convolutional neural network generally had better translation-invariance performance. CNNs have achieved state-of-the-art performance in computer vision tasks due to reduced computation cost with weight sharing in convolutional layers and generalization with local invariance in subsampling layers. CNNs need to learn different models for different viewpoints, requiring big data and high costs. Capsule networks aim for 'rate-coded' equivariance by encoding viewpoint-invariant knowledge in weights, not neural activities. They handle different viewpoints robustly by representing visual entities with capsules containing pose, color, lighting, and deformation information. However, it is uncertain if capsule networks can generalize for global translation invariance. Analyzing translation-sensitivity maps for the MNIST dataset is crucial for understanding architectural choices and developing models invariant to viewpoint changes. In this study, a method is introduced to test global translation-invariance in convolutional and capsule neural network models trained on the MNIST dataset. A testing dataset is created by shifting the centre of mass of digit images, allowing for the evaluation of translational translations. Training is done on the MNIST dataset with 60000 samples, and testing is conducted on both MNIST and GTI testing datasets. The GTI dataset covers all possible cases of translational translations with 2520 samples. The GTI dataset is used to test global translation-invariance in CNN models trained on MNIST. The CNN model has nine layers with convolutional and fully connected layers. The GTI dataset allows for evaluation of translational translations and captures tiny differences in models. The accuracy on the GTI testing dataset reflects the model's ability to handle global translational invariance. The CNN model has nine layers with max-pooling and fully connected layers. Dropout is applied to certain layers, and ReLU is used as the activation function. The model is trained on MNIST data and achieves high accuracy on the testing set. However, its performance on global translational invariance is poor, as shown in the results. Images with the digit's center predicted correctly, while those at the corner are assigned to incorrect classes. The CNN model trained on MNIST data struggles with global translational invariance, correctly predicting images with the digit's center but misclassifying those at the corner. Data augmentation by shifting images during training improves performance on the GTI dataset, suggesting 'place-code' equivariance in CNN. CapsNet with a similar architecture is tested on the GTI dataset, achieving 98.05% accuracy. The CapsNet model, with 8.2M parameters, is tested on the GTI dataset using the same architecture as BID9. Training includes Adam optimizer with exponential decay of learning rate. Capsule network shows robustness in viewpoint invariance but struggles with global invariance. Data augmentation in MNIST training improves CapsNet accuracy on GTI dataset. CNN generally performs better on GTI dataset. The CapsNet model struggles with global translational invariance on the GTI dataset, showing lower performance compared to CNN. CapsNet's convolutional layers lack max-pooling and are 'place-coded', potentially hindering its ability to handle translational invariance. Despite this, CapsNet architecture may still have potential advantages over CNN in dealing with global translational invariance due to capsules' ability to learn all viewpoints. CapsNet architecture may have advantages over CNN in handling global translational invariance as capsules can learn all viewpoints. Testing on GTI dataset shows CapsNet's potential, with easy implementation for other computer vision tasks."
}