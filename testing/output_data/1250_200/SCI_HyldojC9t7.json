{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It uses random feature maps to build a kernel specified by the distance measure, improving generalizability over Nearest-Neighbor estimates. D2KE is a versatile approach that can handle complex structured inputs of variable sizes. Our proposed framework excels in classification experiments across various domains such as time series, strings, and histograms for texts and images. It is easier to specify dissimilarity functions for structured inputs like time series, strings, histograms, and graphs, rather than constructing feature representations. There are well-developed dissimilarity measures for complex structured inputs, such as Dynamic Time Warping for time series and Edit Distance for strings. Standard machine learning methods are designed for vector representations, with less focus on structured inputs. The text discusses the limitations of standard machine learning methods for structured inputs and the use of distance-based methods like Nearest-Neighbor Estimation. Research has focused on developing global distance-based machine learning methods to address issues with high variance in estimation from nearest neighbors. This includes treating data similarity matrices as kernel Gram matrices. The text discusses the limitations of standard machine learning methods for structured inputs and the use of distance-based methods like Nearest-Neighbor Estimation. Research has focused on developing global distance-based machine learning methods to address issues with high variance in estimation from nearest neighbors. Among these, approaches include treating data similarity matrices as kernel Gram matrices and using methods like Support Vector Machines or kernel ridge regression. However, most similarity measures do not provide a positive-definite kernel, leading to challenges in defining the empirical risk minimization problem. Efforts have been made to estimate a positive-definite Gram matrix that approximates the similarity matrix, but modifications often result in information loss and inconsistency between training and testing samples. The text introduces a novel framework called D2KE, which constructs a family of positive-definite kernels from a dissimilarity measure on structured inputs. This approach draws from Random Features literature but builds kernels specifically designed for a given distance measure. The kernels generated from D2KE outperform representative-set methods in various application domains. The D2KE framework constructs positive-definite kernels from a given distance measure for structured inputs. These kernels are designed to be Lipschitz-continuous and provide better generalization properties than nearest-neighbor estimation. The framework produces a feature embedding for each instance, improving classification accuracy and computational efficiency in various domains. The methodology constructs a family of PD kernels via Random Features from a given distance measure for structured inputs, providing theoretical and empirical justifications. It generalizes RF methods to complex structured inputs of variable sizes, accelerating kernel machines across domains like time-series, strings, and histograms. Existing approaches for Distance-Based Kernel Learning have limitations, but this method overcomes them by using a generic RF approach. The text discusses various approaches to building positive-definite kernels for structured inputs, such as text and time-series data. These approaches include transforming distance measures, Euclidean embeddings, theoretical foundations for SVM solvers in Krein spaces, and modifying distance functions into kernels. However, some methods result in a diagonal-dominance problem in the kernel Gram matrix. Random feature maps have gained interest for approximating non-linear kernel machines, reducing training and testing times. Various explicit nonlinear random feature maps have been developed for different types of kernels. The Random Fourier Features (RFF) method, which approximates a Gaussian Kernel function, has been extensively studied both theoretically and empirically. Methods to accelerate RFF on high-dimensional input data matrices have been proposed using structured matrices for faster computation and less memory usage. D2KE is a method that computes random features with structured distance metrics for inputs of different sizes, unlike existing methods that only consider vector representations. It constructs a new PD kernel through a random feature map, making it computationally feasible. Another recent work has developed a kernel and algorithm for computing embeddings of single-variable real-valued time-series, but it cannot be applied on discrete data. The method has developed a kernel and algorithm for computing embeddings of single-variable real-valued time-series, but it cannot be applied to discrete structured inputs like strings, histograms, and graphs. In contrast, a unified framework for various structured inputs beyond the limits of BID49 is provided, along with a general theoretical analysis regarding KNN and other distance-based kernel methods. The goal is to estimate a target function from a collection of samples, where the dissimilarity measure between input objects is given instead of a feature representation. The size of structured inputs may vary widely, such as strings with variable lengths or graphs with different sizes. The dissimilarity matrix in BID4 can be found for size-structured inputs, which can vary widely. An ideal feature representation for learning should be compact and result in a simple target function. The dissimilarity measure for learning should satisfy certain properties, including small differences in function values and a small expected distance among samples. Lipschitz Continuity is preferred for the target function with a small Lipschitz-continuity constant. The effective dimension in measuring the space of Multiset is discussed, along with the impact of Lipschitz Continuity on the dissimilarity measure for learning. The covering number N(\u03b4; X, d) is introduced to measure the size of the space implied by a given dissimilarity measure in a structured input space X. The estimation error of a Nearest-Neighbor Estimator is analyzed in relation to these quantities. The effective dimension in measuring the space of Multiset is discussed, along with the impact of Lipschitz Continuity on the dissimilarity measure for learning. The covering number N(\u03b4; X, d) is introduced to measure the size of the space implied by a given dissimilarity measure in a structured input space X. The estimation error of a Nearest-Neighbor Estimator is analyzed in relation to these quantities. Equipped with the concept of effective dimension, a bound on the estimation error of the k-Nearest-Neighbor estimate of f(x) is obtained. In developing an estimator based on a RKHS derived from the distance measure, a family of positive-definite kernels is constructed using a simple but effective approach called D2KE. This approach aims to convert a distance measure into a positive definite kernel by introducing a random structured object \u03c9, a distribution p(\u03c9), and a feature map \u03c6 \u03c9 (x). The kernel is parameterized by both p(\u03c9) and \u03b3, offering a better sample complexity for problems with higher effective dimension. The kernel in Equation (4) is parameterized by p(\u03c9) and \u03b3, and can be interpreted as a soft version of the distance substitution kernel. When \u03b3 \u2192 \u221e, the kernel value is determined by the minimum distance between x and y. Unlike the distance-substitution kernel, this kernel is always positive definite by construction. Random samples are drawn from p(\u03c9) to obtain feature embeddings for solving optimization problems. The kernel in Equation (4) is parameterized by p(\u03c9) and \u03b3, and can be approximated using Random Features (RF) for efficient computation in large-scale settings. RF allows for the direct learning of a target function as a linear function of the RF feature map, by minimizing a domain-specific empirical risk. Additionally, a recent work has explored learning to select a set of random features in a supervised setting, which could be extended to develop a supervised D2KE method. The RF based empirical risk minimization for D2KE kernels is outlined in Algorithm 1, with random feature embeddings computed using a structured distance measure. This approach contrasts traditional RF methods by using an exponent function parameterized by \u03b3. The estimator's statistical performance will be analyzed in Section 5, contrasting it with K-nearest-neighbor methods. The relationship to the representative-set method is discussed, showing a kernel Equation (4) dependency on data distribution. The kernel Equation (4) in RF based empirical risk minimization for D2KE kernels depends on the data distribution. A Random-Feature approximation can be obtained by creating an R-dimensional feature embedding using a part of the training data as samples from p(\u03c9). This approach provides a better generalization error bound even as R approaches infinity, unlike the representative-set method. The choice of p(\u03c9) significantly impacts the kernel's performance, with \"close to uniform\" choices often outperforming p(\u03c9) = p(x). The choice of distribution p(\u03c9) significantly impacts the kernel's performance in RF based empirical risk minimization for D2KE kernels. \"Close to uniform\" choices of p(\u03c9) often outperform p(\u03c9) = p(x) in various domains, as shown in experiments with time-series, string classification, and vector sets. The reasons for the better performance of these distributions are conjectured but require further theoretical exploration in future work. The chosen distributions p(\u03c9) in RF based empirical risk minimization significantly impact kernel performance. Synthetic p(\u03c9) allows for unlimited random features, leading to better kernel approximation. Even with a small number of random features, selected distributions can outperform RSM. The selected p(\u03c9) may capture more relevant semantic information for estimating f(x) under the dissimilarity measure d(x, \u03c9). The framework is analyzed from error decomposition perspectives in RKHS. The RKHS norm constraint minimizes risk, with empirical risk minimizer DISPLAYFORM1 and estimated function f R from random feature approximation. Population and empirical risks denoted as L( f ) and L( f ). Risk decomposition shown in DISPLAYFORM2, discussing function approximation error in smaller function space compared to Lipschitz-continuous functions. Imposing smoothness via RKHS norm constraint and kernel parameter \u03b3 aims for better function approximation. The RKHS norm constraint minimizes risk by approximating the true function well. The estimation error is bounded by a tuning parameter \u03bb, with a dependency of n^-1/2. Random Feature Approximation is discussed for better function approximation. The difficulty of optimizing the kernel in Equation FORMULA12 is highlighted due to the lack of an analytic form. The error from Random Feature Approximation is discussed, focusing on the second term of empirical risk. The approximation error of the kernel is analyzed, with uniform convergence discussed. Proposition 2 provides an approximation error in terms of kernel evaluation, leading to Corollary 1 which guarantees a bound on empirical risk. The Representer theorem states that to minimize the empirical risk, a certain number of Random Features proportional to the effective dimension is needed. Corollary 1 guarantees a bound on empirical risk by considering the closeness of the target function to the population risk minimizer in the RKHS spanned by the D2KE kernel. The proposed method involves minimizing risk in the RKHS spanned by the D2KE kernel, with a bound on empirical risk. It is evaluated in various domains using different dissimilarity measures and data characteristics. Distance measures include Dynamic Time Warping, Edit Distance, Earth Mover's distance, and (Modified) Hausdorff distance for time-series, strings, texts, and images. Computational complexity is addressed by adapting C-MEX programs and Matlab for the distance measures. For experiments in various domains, datasets were selected including time-series, string, text, and image data. Time-series data consisted of multivariate time-series with varying lengths, string data had different alphabet sizes and string lengths, text data had varying document lengths, and image data utilized SIFT descriptors with varying feature vector sizes. Train and test subsets were divided 70/30 for each dataset. The SIFT feature vectors in each image range from 1 to 914. Datasets were split into 70/30 train and test subsets. Comparison was made against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, KSVM, and RSM. Baselines have quadratic complexity, while the proposed method, D2KE, offers a different approach. Our method, D2KE, has complexity linear in both the number of data samples and the length of the sequence. We search for the best parameters through 10-fold cross validation and can use random samples to achieve performance close to an exact kernel. D2KE consistently outperforms or matches baseline methods in classification accuracy while requiring less computation time. Our method, D2KE, outperforms KNN and other methods like DSK_RBF and DSK_ND in terms of performance. The representation induced from a positive-definite kernel makes better use of data than indefinite kernels. D2KE's random object sampling performs significantly better, especially in structured input domains like sequences, time-series, and sets. The framework introduced is useful for structured input domains like sequences, time-series, and sets. It subsumes existing approaches and opens up new directions for creating embeddings based on distance to random objects. The proof involves bounding the magnitude of Hoefding's inequality for a given input pair. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous with Lipschitz constant \u03b3. The framework introduced is useful for structured input domains like sequences, time-series, and sets. It subsumes existing approaches and opens up new directions for creating embeddings based on distance to random objects. The proof involves bounding the magnitude of Hoefding's inequality for a given input pair. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous with Lipschitz constant \u03b3. By optimizing parameters through cross-validation and using different kernels for various methods, experiments were conducted to evaluate performance. Random sampling was utilized to obtain representative data samples for the new method D2KE. The study utilized random sampling for the new method D2KE to achieve performance close to an exact kernel. Linear SVM was used for embedding-based methods, while LIBSVM was used for precomputed dissimilarity kernels. Datasets were collected from various sources, and computations were performed on a DELL system with multithreading to accelerate the process. The study utilized random sampling for the new method D2KE to achieve performance close to an exact kernel. Multithreading with 12 threads was used for distance computations. Gaussian distribution with bandwidth \u03c3 was applied to all datasets. D2KE outperforms KNN and other baselines in classification accuracy for multivariate time-series data. Our method outperforms DSK_RBF and DSK_ND, KSVM, and RSM in utilizing data efficiently. Unlike RSM, our method samples random sequences to denoise and identify patterns in the data, leading to a more abundant feature space. Additionally, our method avoids the computational cost associated with long time-series data. The Levenshtein distance is used as a distance measure to generate random strings for the D2KE method. Results show that D2KE outperforms other distance-based baselines, including DSK_RBF. This performance advantage may be attributed to the Levenshtein distance being a true distance metric. Our method, D2KE, outperforms DSK_RBF on large datasets with better performance and less computation. For text data, we use the earth mover's distance as a measure between documents, generating Bag of Words and word vectors for classification. Results show that D2KE outperforms other baselines on all datasets, with distance-based kernel methods performing better than KNN. D2KE also achieves a significant speedup compared to other methods, thanks to the use of random features. For image data, the modified Hausdorff distance is used as the distance measure between images. The modified Hausdorff distance (MHD) is used as the distance measure between images. SIFT-descriptors are generated using OpenCV, and random images are created from these descriptors. D2KE outperforms other baselines in all cases, with better performance than KNN and RSM. However, the quadratic complexity of some methods makes scaling to large data challenging. D2KE outperforms KNN and RSM, despite challenges in scaling due to the number of images and SIFT descriptor sequences. This highlights D2KE as a strong alternative across various applications."
}