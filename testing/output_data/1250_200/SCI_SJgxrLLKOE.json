{
    "title": "SJgxrLLKOE",
    "content": "Engineered proteins offer the potential to solve problems in biomedicine, energy, and materials science. Creating successful designs is challenging due to the complex relationship between protein sequence and 3D structure, known as the inverse protein folding problem. Generative models for protein sequences conditioned on a graph-structured design target efficiently capture long-range sequence dependencies. This framework improves upon prior models and advances rapid biomolecular design using deep generative models. Computational protein design aims to automate the creation of proteins with specific properties, with significant progress made in the past two decades. In the past two decades, significant progress has been made in computational protein design, including the design of novel 3D folds, enzymes, and complexes. Challenges often arise from the bottom-up nature of current approaches, requiring multiple rounds of trial-and-error. An alternative top-down framework for protein design involves learning a conditional generative model for protein sequences based on a graph-structured target structure. This approach enhances the capture of higher-order, interaction-based dependencies between sequence and structure compared to previous methods. The graph-structured conditioning of a sequence model offers computational efficiency, inductive bias, and representational flexibility by leveraging findings in protein science. The model achieves linear computational scaling by making the graph and self-attention sparse and localized in 3D space. It accommodates both coarse and fine-grained descriptions of structure, demonstrating improved generalization performance in protein sequence modeling. Our model improves generalization performance over recent deep models for protein sequence given structure. Various generative models have been explored for protein engineering and design, including neural models for sequences given 3D structure and deep neural networks modeling conditional distribution of letters in specific positions. In contrast to existing works, our model captures the joint distribution of the full protein sequence while considering long-range interactions from the structure. In parallel to structure-based models, deep generative models for protein sequences in individual families have been developed. Unconditional protein language models have shown promise in transferring protein sequence representations to supervised tasks. Conditional generative modeling aids in adapting to specific parts of structure space, while language models are limited by the smaller number of evolutionary 3D folds. Protein language models can be evaluated with structure-based splitting of sequence data to highlight challenges faced by unconditional language models. In contrast to deep generative models for protein sequences, this work focuses on extending the Transformer model to capture sparse relational information between sequence elements. The model aims to address challenges faced by unconditional language models in assigning high likelihoods to sequences from out-of-training folds. The model extends the Transformer to capture sparse relational information between sequence elements by restricting self-attention to the input graph's sparsity. Protein structure is represented as an attributed graph with node and edge features, accommodating variations in macromolecular design, including 3D considerations. The text discusses the need for a graph representation of fixed backbone coordinates that is invariant to rotations and translations, and locally informative to reconstruct adjacent coordinates. This is achieved by augmenting points with orientations to define a local coordinate system at each point. The text discusses developing invariant and locally informative features by augmenting points with orientations to define a local coordinate system. Spatial edge features are derived from rigid body transformations, decomposed into features for distance, direction, and orientation. Positional embeddings are obtained to encode positional information. Positional embeddings are obtained to encode the role of local structure around nodes, using sinusoidal functions to represent relative positioning. Aggregate edge encoding vectors are created by combining structural and positional encodings. Node features include dihedral angles of the protein backbone and flexible backbone descriptions based on binary edge features. The Structured Transformer model integrates relative positional encodings with binary edge features to capture 3D structure information efficiently. By utilizing local attention for each node's k-nearest neighbors, it avoids quadratic memory and computation costs. This approach allows for deriving global context estimates iteratively, enhancing the model's ability to embed spatial and positional dependencies in the data. Our model extends the standard Transformer by incorporating edge features to capture spatial and positional dependencies, making it suitable for spatially structured settings. The autoregressive decomposition of the joint distribution of the sequence given structure allows for refined node embeddings and predictive modeling of amino acids based on input structure and preceding sequence. The encoder module implements multi-head self-attention components to attend to different subspaces of embeddings. The self-attention component in our model allows each head to attend to a separate subspace of embeddings using learned query, key, and value transformations. The encoder module implements multi-head self-attention components to refine node embeddings and predict amino acids based on input structure and preceding sequence. The decoder module, with augmented relational information, ensures causally consistent access to preceding sequence elements. The model utilizes self-attention to refine node embeddings and predict amino acids. It ensures causally consistent access to preceding sequence elements by concatenating and masking structure. Three layers of self-attention and feedforward modules are stacked for the encoder and decoder. The dataset used for evaluation is based on the CATH hierarchical classification of protein structure. In the CATH 4.2 40% non-redundant protein set, chains up to length 500 were clustered at the fold level into training, validation, and test sets. The splits contained 18025 chains in training, 1637 in validation, and 1911 in test. Models were trained with specific parameters and evaluated based on perplexity of test protein folds. The perplexity per letter of test protein folds was evaluated, with models showing potential usefulness even with high perplexities. There was a significant gap between unconditional language models and models conditioned on structure, with test perplexities ranging from 16-17. Protein language models trained on structure-independent models had test perplexities around 16-17, barely better than null letter frequencies. However, structure-based models, like the Structured Transformer model, achieved considerably lower perplexities of around 7 on the full test set. Local orientation information was found to be important in predicting protein sequences. Comparisons were made with the SPIN2 method, which predicts protein sequence profiles using deep neural networks. Our Structured Transformer model significantly improved upon the perplexities of SPIN2 when evaluated on subsets of the test set. The model utilizes relational 3D structural encodings to design protein sequences based on a graph specification of their structure, achieving improved perplexities over state-of-the-art generative models. This framework suggests the potential for efficiently designing and engineering protein sequences with structurally-guided deep generative models. Our framework proposes efficiently designing protein sequences with structurally-guided deep generative models, emphasizing the importance of modeling sparse long-range dependencies in biological sequences. We acknowledge the MIT MLPDS consortium for their valuable feedback and discussions."
}