{
    "title": "rygZJ2RcF7",
    "content": "Neural networks struggle to generalize transformations outside of their training data. A new technique called neuron editing aims to address this issue by learning how neurons encode transformations in a latent space, allowing for complex transformations with simpler distribution shifts. This technique is showcased in image domain/style transfer and biological applications. Neuron editing technique allows for complex transformations in data with simpler distribution shifts to neuron activations. It is applied in image domain/style transfer and biological applications, such as removing batch artifacts and modeling drug treatment effects. The proposed neural network-based method aims to learn a general edit function for treatments in biology, enabling assessment of treatment generalization beyond measured samples. The proposed neural network-based method aims to learn a general edit function for treatments in biology, enabling assessment of treatment generalization beyond measured samples. Neuron editing, termed for complex transformations in data, is applied in various applications including image domain/style transfer and biological applications like removing batch artifacts and modeling drug treatment effects. The method involves training an autoencoder on the entire population of data to learn an edit function between pre-and post-treatment versions of the data in the latent space with non-linear activations. The autoencoder represents data with variation decomposed into abstract features, allowing accurate data reconstruction. Neuron editing extracts differences between pre-and post-treatment activation distributions to generate synthetic post-treatment data. This technique can be applied to any neural network's latent space but is focused on the autoencoder for its advantages in modeling complex distribution transformations. In this work, the focus is on leveraging the autoencoder's advantages for modeling complex distribution transformations. The non-linear dimensionality reduction helps simplify effects in high-dimensional space. Editing neurons in the internal layer allows for context-dependent modeling. Neurons in the embedding layer are edited to influence treatment outcomes, interacting with data-context-encoding neurons. Editing in a low-dimensional internal layer allows for denoising data and retaining significant dimensions. The assumption is that internal neurons have semantic consistency across data manifolds. Neurons encode features for data manifolds, demonstrating consistency across different data sets. Neuron editing outperforms generative models by closely predicting changes in extrapolated and interpolated data. Comparison with traditional GANs and CycleGAN shows the effectiveness of neuron editing in generating complex variations. Neuron editing is shown to be more effective than traditional GANs and CycleGAN in generating complex variations. The method is motivated by the struggle of GANs in generating plausible individual points and the importance of extrapolation in biological applications. The transformation sought after has properties that produce equivalent distributions when applied to sampled sets representing source, target, and extrapolation distributions. Neuron editing is a more effective approach than traditional GANs and CycleGAN for generating complex variations. The method aims to create a transformation that produces equivalent distributions when applied to source, target, and extrapolation distributions. This transformation is defined in a learned space using an encoder/decoder pair to map data into high-level features. The piecewise linear transformation, NeuronEdit, is applied to extract activations from internal layers of the network for inputs from source and target distributions. Neuron editing introduces a piecewise linear transformation, NeuronEdit, applied to distributions of activations from internal network layers. This transformation operates on source and target distributions, adjusting activation distributions based on percentile differences. The NeuronEdit function exhibits properties similar to a GAN generator, aiming to approximate target distributions while maintaining certain characteristics. The NeuronEdit function, resembling a GAN generator, guarantees consistent editing between source and extrapolation distributions. The transformation is applied to neuron activations from the encoder and cascaded through the decoder without additional training. This approach turns an autoencoder into a generative model by freezing training and exclusively applying transformations during inference. Training a GAN in this setting could exclusively utilize the data in S and T, while neuron editing can model the intrinsic variation in X in an unsupervised manner. GANs are difficult to train due to issues like oscillating optimization dynamics, uninterpretable losses, and mode collapse where the discriminator struggles to detect differences between real and fake examples. Neuron editing avoids the struggles of GANs in detecting differences between real and fake distributions by learning an unsupervised model of the data space with an autoencoder. It isolates the variation in neuron activations that characterize the difference between source and target distributions, similar to word2vec embeddings in natural language processing. Neuron editing is a method that transforms an entire distribution into another distribution, extending word2vec's vector arithmetic. It compares neuron interference to various generating methods like regularized autoencoder, GANs, ResnetGAN, and CycleGAN. The models used different layers and activation functions, with training done using minibatches and the adam optimizer. In a motivational experiment using leaky ReLU activation and minibatches of size 100, a generative model failed to extrapolate a hair color transformation from black to blond on out-of-sample data. This limitation was illustrated in a scenario where the model could not successfully apply the transformation to females with black hair, which were not seen during training. The GAN models struggle to accurately model transformations on out-of-sample data, particularly in changing hair color. Neuron editing in the internal layer of a neural network is shown to be effective in decomposing complex transformations into simpler shifts. This approach avoids artifacts seen in regular GAN models and demonstrates potential in batch correction for addressing data differences. Neuron editing's application in batch correction involves transforming data to address technical artifacts causing batch effects in biological experimental data. By using a source/target pair like Control1/Control2, variations induced by the measurement process can be corrected, allowing for accurate comparisons between samples. The dataset from a mass cytometry experiment measures protein levels in cells from individuals infected with dengue virus. Control1/Control2 are used to correct for technical artifacts, such as artificially low readings in Control1. Neuron editing is applied to transform Sample1 for comparison with raw Sample2, compensating for variations between the samples. The protein InfG in Control1 is a source of variation that needs to be identified and compensated for in Sample1. The GANs used in the study fail to preserve true biological differences, such as higher values of the protein CCR6 in Sample1. This results in all cells being mapped to similar values, leading to loss of important information for later comparisons. The ResnetGAN also does not address this issue, as it aims to produce output similar to the target distribution, which is not desired in this case. Neuron editing successfully removes batch effects and preserves real variation in protein levels, unlike other generative models. The results are accurate globally across all dimensions, as confirmed by a PCA embedding of the data space. Neuron editing removes batch effects and preserves real variation in protein levels. A PCA embedding shows accurate global assessments across all dimensions. In a combinatorial drug experiment on cells from patients with acute lymphoblastic leukemia, a batch effect in IFNg should be corrected while preserving true variation in CCR6. In a combinatorial drug experiment on cells from patients with acute lymphoblastic leukemia, Neuron editing corrects batch effects in IFNg while preserving true biological variation in CCR6. The experiment involves four treatments and measurements from mass cytometry on 41 dimensions. The goal is to predict the effects of applying Das to cells treated with Bez, with a characteristic effect being a decrease in p4EBP1. The effect of applying Das is a decrease in p4EBP1, with no change in pSTATS. Neuron editing accurately models this change without introducing vertical shifts. The regularized autoencoder and three GAN models fail to accurately predict the combination, losing original variability. ResnetGAN also struggles despite residual connections, showing GANs cannot replicate two-dimensional slices of the target data. The GAN objective aims to replicate target data but fails to do so even in two-dimensional slices. Neuron editing accurately predicts transformations across all dimensions, preserving data variation better than GANs. This study addresses a data-transformation problem inspired by biological experiments, introducing a novel approach. Neuron editing is a novel approach that applies treatment effects to the entire dataset using autoencoder latent layers. It mimics transformations by editing internal layer encodings, resulting in realistic changes in image data and predicting synergistic drug effects in biological data. Learning edits in hidden layers allows for complex data transformations in a non-linear space, enhancing interactions with context information during decoding. Learning edits in a hidden layer allows for interactions between the edit and other context information from the dataset during decoding. Future work could involve training parallel encoders with the same decoder or training to generate conditionally."
}