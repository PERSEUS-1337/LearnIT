{
    "title": "rJleN20qK7",
    "content": "In this work, a two-timescale network (TTN) architecture is introduced to enable linear methods to learn values with a nonlinear representation learned at a slower timescale. The approach allows for the use of algorithms developed for the linear setting to provide nonlinear value estimates. Convergence for TTNs is proven, with empirical demonstrations showing the benefits compared to traditional methods like temporal difference learning and Q-learning. Value function approximation heavily relies on the quality of the state representation. Traditional methods use linear function approximators with basis functions, but this can be challenging for high-dimensional observations. Learning representations with neural networks alleviates this burden and allows for scalability to complex observations like images. The Two-Timescale Network (TTN) architecture introduces a nonlinear representation learned at a slower timescale, showing benefits over traditional methods for policy evaluation and control. Linear function approximation for estimating value functions has advantages over nonlinear estimators, enabling least-squares methods and eligibility traces for faster learning. Various algorithms have been developed for linear settings, both for on-policy and off-policy learning, and have been well-explored theoretically. In this work, a strategy is pursued to leverage the benefits of linear methods while learning the representation. Two learning processes run in parallel: one learns nonlinear features using a surrogate loss, and the other estimates the value function linearly. These Two-timescale Networks (TTNs) converge as the features change. In this work, Two-timescale Networks (TTNs) are introduced where one process learns nonlinear features using a surrogate loss, while the other estimates the value function linearly. The separation of the loss for representation and value function enables simpler objectives for driving representation, maintaining the properties of the mean squared projected Bellman error (MSPBE). This approach avoids the complexity of nonlinear MSPBE while still using linear algorithms. Yu and Bertsekas (2009) introduced algorithms for basis adaptation using different losses, such as Bellman error with Monte Carlo samples. BID21 computes a closed form least-squares solution periodically for the last neural network layer, incorporating Bayesian updates. The use of two separate heads for representation and value learning has not been extensively explored. Two-timescale Networks (TTNs) show promise for nonlinear function approximation, combining linear algorithms with the flexibility of nonlinear approximators. The convergence of TTNs is demonstrated, even when using a changing representation. The proof demonstrates the effectiveness of Two-timescale Networks (TTNs) for nonlinear function approximation in control with neural networks. TTNs can exploit benefits of linear value approximations algorithms and are shown to be effective for both low-dimensional and high-dimensional observations. The dynamics of the Markov Decision Process (MDP) are defined by a 3-tuple (S, A, P), with a reward function R and discount function \u03b3. The task involves defining a transition probability function and reward function in a Markov Decision Process (MDP). The agent interacts with the environment by taking actions based on a policy, receiving rewards, next states, and discounts. Policy evaluation aims to compute the value function, which can be approximated using linear or nonlinear functions. The learning problem involves finding parameters to estimate the value function, with a Bellman operator defining a fixed point. Linear value functions can be represented by a matrix with parameters to learn. Linear policy evaluation algorithms estimate a projected fixed point in a Markov Decision Process. For linear function approximation, the projection operator simplifies into a closed form solution involving only the features X. However, for nonlinear function classes, the projection does not have a closed form solution and may be expensive to compute. The projected Bellman error (MSPBE) for nonlinear function classes is complex and expensive to compute. Two-timescale Networks (TTNs) offer a different strategy for nonlinear value function approximation, using concurrent optimization processes for network parameters and value function parameters. Two-timescale Networks (TTNs) utilize concurrent optimization processes for network parameters and value function parameters to approximate the value function. The parameters \u03b8 for the representation change slowly, while the parameters w for the value estimate change quickly. The separation between these processes can be problematic, but selecting a surrogate loss related to the value estimation process helps. The output of the fast part corresponds to the value estimate used by the agent. The slow part of the network minimizes the mean-squared TD error (MSTDE) with its own weights associated with estimating the value function. Stochastic gradient descent on the MSTDE is straightforward, providing worse value estimates than the MSPBE. The MSTDE is a simple surrogate loss for estimating values, providing worse estimates than the MSPBE. Other surrogate losses related to the value function have drawbacks in incremental sampling. The MSRE and MSBE are alternatives, but suffer from delays in updating the representation. The MSTDE is explored as the simplest surrogate loss, while other losses not directly related to the value function could also be defined. The MSTDE is considered a simple surrogate loss for estimating values, with potential drawbacks compared to other losses like MSPBE. Surrogate losses not directly related to the value function, such as predicting the next state and reward, could also be defined. Learning representations through auxiliary tasks or self-supervised tasks have shown success in reinforcement learning. Using rotated images as self-supervised tasks can create a useful representation for the main loss in TTNs. The separation of representation learning does not impact performance. Training TTNs online with stochastic gradient descent and a linear policy evaluation algorithm. Convergence of TTNs requires slow evolution of the network relative to linear prediction weights. See Algorithm 1 in Appendix A for the full procedure. The convergence of TTNs relies on the slow evolution of the network compared to the linear prediction weights. Linear prediction algorithms must converge for any set of features, overcoming technical challenges with linearly dependent features. Stability of iterates is maintained by projecting them to compact, convex sets, ensuring convergence of neural networks with various architectures. The analysis shows that TTNs converge asymptotically to stable equilibria of a projected ODE, capturing the mean dynamics. Results are provided for TD(\u03bb) or GTD2 linear prediction methods, with similar results possible for others. Theorem 1 states convergence to stable equilibria K, with additional results for TD(\u03bb) algorithm under Assumption 4-TD(\u03bb). The TD(\u03bb) algorithm within the TTN setting converges almost surely to the limit w * under Assumption 4-TD(\u03bb). The performance of TTNs is compared to other nonlinear policy evaluation algorithms, exploring the benefits of optimizing MSPBE for value estimates. Experiments in five environments show TTNs may offer a competitive alternative to deep Q-learning in control. The study evaluates the performance of TTNs in various environments, including Puddle World, Acrobot, Cartpole, Catcher, and Puck World. Different observation types are used in Catcher, with results presented in the main body and appendix. Value estimates are assessed using RMSVE, with Cartpole results included only in the appendix. The optimal values for 500 states are obtained using extensive rollouts and RMSVE is computed across these states. The algorithms use specific settings for minimizing mean-squared TD error and network initialization. Results are reported for hyperparameters chosen based on RMSVE over the latter half of a run. Comparisons are made with various algorithms such as nonlinear TD and Adaptive Bases. In comparing TTN to various algorithms like nonlinear TD and Adaptive Bases, TTN performs well, especially in Puddle World where its error is significantly lower than the second best algorithm. Nonlinear GTD also shows good performance across domains, indicating an advantage for theoretically-sound algorithms. The utility of optimizing the MSPBE is highlighted, with TTN benefiting from a second head learning at a faster timescale. In comparing TTN to various algorithms like nonlinear TD and Adaptive Bases, TTN performs well, especially in Puddle World where its error is significantly lower than the second best algorithm. Nonlinear GTD also shows good performance across domains, indicating an advantage for theoretically-sound algorithms. The utility of optimizing the MSPBE is highlighted, with TTN benefiting from a second head learning at a faster timescale. TTNs give flexibility to choose linear policy evaluation algorithms for the fast part, including TD, LSTD, FLSTD, emphatic TD, gradient TD, and their true-online versions for learning the value function. GTD and ETD are newer temporal difference methods with better convergence properties. The true-online variants of linear policy evaluation algorithms, such as TD, LSTD, FLSTD, emphatic TD, and gradient TD, are used for learning the value function in TTNs. GTD and ETD are newer temporal difference methods with improved convergence properties. Least-squares methods are avoided due to quadratic computation, but for TTNs, there is no computational disadvantage to using LSTD methods. FLSTD, which progressively forgets older interactions, may be advantageous when the feature representation changes over time. Incremental versions of least-squares algorithms are used to maintain estimates online in TTNs. Eligibility traces can be used to enhance the performance of these linear algorithms. The use of linear policy evaluation algorithms, such as TD, LSTD, FLSTD, and gradient TD, with eligibility traces, can enhance sample efficiency in TTNs. Nonlinear function approximation poses challenges for deriving eligibility traces for TD, but extending them to nonlinear TD(\u03bb) can be beneficial. Results show that TTNs benefit from using least-squares methods, particularly LSTD, in various domains. Sensitivity analysis reveals that most TTN variants benefit from a nonzero \u03bb value, with high values often being optimal. Least-squares methods, however, perform consistently well across different \u03bb values. Surrogate loss functions were compared in the experiments, showing that different objectives can yield varying results. While the MSTDE was effective in some domains, alternate losses like semi-gradient MSTDE and next state predictions performed better in others. This suggests that there is no universally superior surrogate loss, and choosing the appropriate one can lead to benefits in specific domains. In policy evaluation, different surrogate loss functions were compared, showing varying results. The DQN algorithm utilizes experience replay and a target network to stabilize training. An alternative strategy to target networks for TTN is motivated by fitted Q-iteration, allowing for direct use of FQI. This approach solves for weights on the entire replay buffer, taking advantage of a closed form solution for linear regression towards the Q-values. In policy evaluation, different surrogate loss functions were compared, showing varying results. The DQN algorithm utilizes experience replay and a target network to stabilize training. An alternative strategy to target networks for TTN is motivated by fitted Q-iteration, allowing for direct use of FQI. This approach incorporates a regularization term to prevent significant weight changes between updates, reducing computation to O(nd^2/k) per step. Experimental details differ for control, with hyperparameter tuning done manually for image Catcher runs due to increased computation requirements. TTN outperforms DQN in both non-image and image versions of Catcher, learning faster and achieving higher returns. Both algorithms show signs of catastrophic forgetting, but TTN stabilizes on a better policy. TTNs offer promise in improving sample efficiency in control while maintaining neural network stability. The Two-timescale Networks approach is proposed as a new strategy for policy evaluation with nonlinear function approximation. Two-timescale Networks (TTNs) are introduced as a new strategy for policy evaluation with nonlinear function approximation. TTNs combine slow learning for adapting features and fast learning for a linear value function, proving convergence guarantees for various learning components. The decoupled architecture in TTNs can enhance learning by enabling the use of linear methods like least-squares algorithms and eligibility traces. Stochastic approximation algorithms with traces show significant effects in TTNs, contrasting with nonlinear TD methods. TTNs offer the opportunity to explore linear value function algorithms in complex domains with learned representations. Emphatic algorithms, which have shown improved properties, have not been utilized with neural networks. TTNs also show promise for off-policy learning, where multiple value functions are learned simultaneously to mitigate variance issues. This approach can prevent destabilizing updates in the network, as different objectives can be used for learning. TTNs offer a novel approach to learning with neural networks, avoiding destabilizing updates by using different objectives. Preliminary experiments support this hypothesis. The convergence proof of two-timescale networks is provided in the appendix, along with definitions and notations for the network's operation. The interior of X and \u2202X denote the boundary of X. Let \u03b8 = (\u03b8,w) and \u03a6\u03b8 be the feature matrix corresponding to the feature parameter \u03b8. Define the |S| \u00d7 |S|-matrix P \u03c0 as P \u03c0 s,s a\u2208A \u03c0(s, a)P (s, a, s ), s, s \u2208 S. \u0393 is Frechet differentiable if the Frechet derivative of \u0393 exists at every point in its domain. Assumptions include a pre-determined step-size sequence and an ergodic Markov chain induced by the policy \u03c0. The existence of a unique steady-state distribution d \u03c0 is guaranteed. Given a realization of the transition dynamics of the MDP in the form of a sample trajectory O \u03c0. The algorithm's long-run behavior is analyzed using ODE-based analysis of stochastic recursive algorithms. The ODE determines the asymptotically stable sets, ensuring that the limit points of the stochastic recursion belong to the compact connected internally chain transitive invariant set. The multi-timescale stochastic approximation framework is also considered for a more generalized analysis. The algorithm's behavior is analyzed using ODE-based analysis of stochastic recursive algorithms. There is a unilateral coupling between the neural network and policy evaluation algorithms, where the feature vectors are calibrated by stochastic gradient descent. As a technical requirement, a projected stochastic recursion is considered due to the instability of the iterates. The algorithm's behavior is analyzed using ODE-based analysis of stochastic recursive algorithms with a unilateral coupling between the neural network and policy evaluation algorithms. The feature vectors are calibrated by stochastic gradient descent, and a projected stochastic recursion is considered due to the instability of the iterates. The lemma characterizes the limiting behavior of the iterates generated by the TTN, showing convergence to a set of asymptotically stable equilibria. The lemma characterizes the behavior of the iterates generated by the TTN, showing convergence to stable equilibria. The noise sequence is a martingale-difference sequence, and the iterates converge to zero as t approaches infinity. The lemma discusses the convergence of iterates generated by the TTN to stable equilibria, with the noise sequence being a martingale-difference sequence. The iterates converge to zero as t approaches infinity, with the stochastic recursion tracking the ODE asymptotically. It is noted that determining the constraint set \u0398 without prior knowledge of the ODE's limit set is non-trivial, suggesting starting with an arbitrary convex, compact set \u0398 and gradually expanding. The lemma discusses the convergence of iterates generated by the TTN to stable equilibria, with the noise sequence being a martingale-difference sequence. The iterates converge to zero as t approaches infinity, with the stochastic recursion tracking the ODE asymptotically. Determining the constraint set \u0398 without prior knowledge of the ODE's limit set is non-trivial, suggesting starting with an arbitrary convex, compact set \u0398 and gradually expanding. The hypothesis of the lemma characterizes Lipschitz continuity with respect to the features \u0176\u03b8. Considering the non-projected form of the ODE is encouraged when using the spreading approach, as the limiting flow of the ODE from the projected stochastic recursion is more likely to lie inside a compact, convex set as \u0398 becomes larger. The condition that \u0176\u03b8 is twice continuously differentiable is sufficient to ensure the Lipschitz continuity of \u0393 \u0398 \u03b8. The TD(\u03bb) algorithm with linear function approximation can be applied to estimate the value function using features provided by a neural network. The text discusses the convergence behavior of the TD(\u03bb) algorithm with linear function approximation. It highlights the asynchronous convergence between the feature parameter sequence and the TD(\u03bb) sequence due to different learning rates. The neural network's stochastic gradient descent increment term is smaller compared to the TD(\u03bb) recursion, leading to unique pseudo heterogeneity and multiple perspectives in the convergence process. The text discusses the convergence behavior of the TD(\u03bb) algorithm with linear function approximation. It highlights the quasi-static nature of slower timescale recursion when viewed from the faster timescale, and vice versa. The stochastic sequence generated by the TD(\u03bb) algorithm converges almost surely to a limit within the TTN setting. Similar results apply to other prediction methods like ETD and LSPE. LSTD, despite being computationally expensive, integrates smoothly within the TTN setting. The TD(\u03bb) algorithm with linear function approximation converges almost surely within the TTN setting. LSTD, despite its computational expense, integrates smoothly in this setting. However, applying original GTD2 and TDC algorithms directly is challenging due to the non-singularity condition of feature-specific matrices. The projected GTD2 algorithm is considered in this context. The projected GTD2 algorithm, with parameters \u03b1 t , \u03b2 t , is discussed here. It involves projection operators onto predefined convex, compact subsets W and U in R d. The stability of iterates {w t } and {u t } is ensured through projection. Step-size sequences {\u03b1 t } and {\u03b2 t } are defined, and the algorithm's asymptotic behavior is analyzed under the quasi-stationary assumption of feature vector \u03b8 t. The GTD2 algorithm is analyzed under the quasi-stationary assumption of feature vector \u03b8 t. The algorithm's behavior is described as a multi-timescale stochastic approximation recursion with asynchronous convergence behavior due to different learning rates. The GTD2 algorithm is analyzed as a multi-timescale stochastic recursion with asynchronous convergence behavior. The long-term behavior of the faster time-scale recursion is examined under the quasi-stationary premise. The GTD2 algorithm is analyzed as a multi-timescale stochastic recursion with asynchronous convergence behavior. The long-term behavior of the faster time-scale recursion is examined under the quasi-stationary premise. The recursion equation can be rearranged and iterates are stable. The noise sequence is martingale-difference, and the recursion asymptotically tracks an ODE. The stochastic recursion in GTD2 algorithm asymptotically tracks an ODE, leading to the convergence of w_t to stable equilibria almost surely. A qualitative analysis shows that the stable limit set is the solutions of a linear system inside W. The linear system of equations is consistent and can be viewed as the least squares solution to a specific equation. The GTD2 algorithm's slower time-scale stochastic recursion converges to stable equilibria, denoted as set A_u, inside W. The recursion is managed on a faster timescale relative to the neural network stochastic recursion, maintaining the quasi-stationary condition \u03b8_t \u2261 \u03b8. The GTD2 algorithm's slower time-scale stochastic recursion converges to stable equilibria within set A_u. The recursion operates on a faster timescale compared to the neural network stochastic recursion, maintaining the quasi-stationary condition \u03b8_t \u2261 \u03b8. The equation involving the Frechet derivative and projection operator can be interpreted in terms of stochastic recursive inclusion, with multiple limit points in the TTN setting. This differs from previous assumptions made in the GTD2 algorithm analysis. In the TTN setting, the GTD2 algorithm's behavior is analyzed under a relaxed singularity condition. The stochastic recursion is viewed as a stochastic recursion inclusion and recent results are applied to analyze its limiting behavior. Observations include h3 being a singleton for each u \u2208 U and the martingale-difference noise sequence {Mt+1}t\u2208N. The behavior of the GTD2 algorithm in the TTN setting is analyzed under a relaxed singularity condition. The martingale-difference noise sequence {Mt+1}t\u2208N is considered, with h3 being a singleton for each u \u2208 U. The set-valued map q(u) = Au is shown to be upper-semicontinuous, with conditions established as per Theorem 3.10 of (Ramaswamy and Bhatnagar, 2016). The GTD2 algorithm in the TTN setting is analyzed under a relaxed singularity condition. The asymptotic behavior of the algorithm is characterized, and convergence results are provided for the TD(\u03bb) algorithm within the TTN setting. The TD(\u03bb) algorithm and GTD2 algorithm in the TTN setting converge almost surely to their respective limits. Policy evaluation experiments were conducted on an image-based catcher, showing similar results to non-image experiments. The classic Cartpole environment involves balancing a pole on a cart with a state represented by a vector of 4 numbers. In the classic Cartpole environment, the agent balances a pole on a cart using a 4-number state vector. Two actions are available, rewards are given at each timestep, and the episode ends if the pole dips too low or the cart strays too far. The policy involves applying force based on the pole or cart's movement direction. In Puck World, the agent navigates a 2D space to reach a good puck while avoiding a bad one with an 8-dimensional state. Each action increases the agent's velocity. In Puck World, the agent's policy moves towards the good puck while capping velocity. Actions are chosen based on eligibility towards the good puck and velocity constraints. A penalty is incurred if the agent is near the bad puck. In Puck World, the agent's policy moves towards the good puck while capping velocity. Actions are chosen based on eligibility towards the good puck and velocity constraints. A penalty is incurred if the agent is near the bad puck. The agent picks uniformly at random from all eligible actions. A preliminary experiment is conducted to test TTN's advantage in the off-policy setting with different policies. TTN is compared to off-policy Nonlinear TD with three off-policy algorithms (TD, TDC, and LSTD). The features are learned optimizing the MSTDE on the behavior policy while the values are learned off-policy. The main difference between TTN and Nonlinear TD is that TTN only changes the linear part with off-policy updates. TTN outperforms Nonlinear TD in terms of average error and variance reduction. LSTD version used in policy evaluation experiments with specific inputs and initialization steps. Policies in Puddle World and Catcher are described. Nonlinear TD uses semi-gradient TD update with nonlinear function approximation, known to have theoretical limitations. Nonlinear TD uses semi-gradient TD update with nonlinear function approximation, which has theoretical limitations. Nonlinear GTD, on the other hand, has proven convergence results and is an extension of gradient TD methods to nonlinear function approximation. ABTD and ABBE are two adaptive bases algorithms proposed by Castro et al., optimizing different objectives. ABPBE is omitted due to computational inefficiency. Levine et al.'s algorithm combines DQN with periodic LSTD for control settings. The algorithm Nonlinear TD-Reg is adapted from Levine et al.'s work, using regularization on the last layer's weights for policy evaluation. Refined hyperparameter ranges were tested for experimental runs in different environments. The experiments involved tuning hyperparameters for various algorithms, with different learning rates and trace parameters. Control experiments were conducted with modifications to the Catcher environment. Hyperparameters were adjusted for the Nonlinear TD-Reg algorithm, focusing on regularization for policy evaluation. The hyperparameters for the experiments on the Catcher environment were adjusted for both DQN and TTN algorithms. Key details include setting the discount factor to 0.99, using an \u03b5-greedy policy, initializing a replay buffer, and specifying optimizer settings for each algorithm. The hyperparameters for the experiments on the Catcher environment were adjusted for both DQN and TTN algorithms. For image catcher, manual tuning was done due to long training times. For nonimage catcher, hyperparameter tuning focused on key parameters using a search strategy. Final hyperparameters for nonimage catcher included TTN with \u03b1 slow = 10^-3 and \u03bb reg = 10^-2, and DQN with \u03b1 = 10^-3.75, decaying over 20 thousand steps. LS-DQN had a FQI update every 50,000 steps with a regularization weight of 1. Final hyperparameters for image catcher included TTN with \u03b1 slow = 10^-5 and \u03bb reg = 10^-3, solving for new weights using FQI every 10,000 steps. For image catcher: TTN with \u03b1 slow = 10^-5, \u03bb reg = 10^-3, solve for new weights using FQI every 10,000 steps. Update target network every 10,000 steps. LS-DQN: FQI update every 500,000 steps with regularization weight of 1."
}