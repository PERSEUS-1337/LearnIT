{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is popular for training deep neural networks, where the addition of a momentum term biases the update in the direction of the previous change in parameters. Empirical evidence shows that stochastic momentum improves convergence time, leading to the development of other popular update methods like ADAM and AMSGrad. However, theoretical justification for its use remains an open question. Stochastic momentum improves deep network training by helping SGD escape saddle points faster and find a second order stationary point more quickly. The ideal momentum parameter should be close to 1 according to theoretical analysis, supported by empirical findings. Experimental results further validate the effectiveness of SGD with stochastic momentum in nonconvex optimization and deep learning. SGD with stochastic momentum is a widely adopted algorithm in nonconvex optimization and deep learning, used in various applications such as computer vision, speech recognition, natural language processing, and reinforcement learning. It has been observed to help achieve faster convergence compared to standard SGD. SGD with stochastic momentum has been widely adopted in optimization and deep learning, showing faster convergence compared to standard SGD. The success of momentum has made it a necessary tool for designing new optimization algorithms. Popular variants of adaptive stochastic gradient methods include momentum, with large values like \u03b2 = 0.9 working well in practice. In this paper, a theoretical analysis for SGD with stochastic momentum is provided. The algorithm requires setting the step size parameter \u03b7 and momentum parameter \u03b2. By updating the iterate using stochastic momentum, SGD with stochastic momentum is proven to escape saddle points faster than standard SGD. This highlights the benefits of using stochastic momentum in optimization algorithms. In this paper, the analysis focuses on finding a second-order stationary point for smooth non-convex optimization using SGD with stochastic heavy ball momentum. The algorithm maintains a weighted average of stochastic gradients and updates the current update in the direction of momentum to escape saddle points faster. The goal is to minimize a stochastic nonconvex optimization problem by considering the expectation of stochastic functions. The text discusses the importance of reaching an approximate second-order stationary point in nonconvex optimization problems. It highlights the use of momentum to achieve this goal, emphasizing the benefits of incorporating momentum in the optimization process. The text introduces a condition for the dynamic procedure to produce updates correlated with negative curvature directions of the function. It discusses the stochastic momentum satisfying Correlated Negative Curvature (CNC) and how the dynamics of SGD with heavy ball momentum help in amplifying the escape signal to escape saddle points. The recursive dynamics of SGD with heavy ball momentum amplify the escape signal \u03b3, allowing faster escape from saddle points. Under the CNC assumption and constraints on the momentum parameter \u03b2, properties like APAG, APCG, and GrACE lead to convergence to second-order stationary points in a certain number of iterations. A larger momentum parameter \u03b2 facilitates faster escape from saddle points. Theoretical results show that a larger momentum parameter \u03b2 helps in escaping saddle points faster in optimization and deep learning. Stochastic momentum aids in avoiding saddle points by amplifying the escape signal \u03b3, leading to convergence to second-order stationary points in a certain number of iterations. Stochastic momentum helps in avoiding saddle points by amplifying the escape signal, leading to faster convergence to second-order stationary points in optimization and deep learning. The CNC property in stochastic momentum ensures that the update direction is strongly non-orthogonal to the direction of large negative curvature, aiding in escaping saddle points for faster convergence. In this paper, the study focuses on stochastic momentum and the CNC property, which requires the update direction to be non-orthogonal to the negative curvature direction for faster convergence. The analysis shows that updates escape saddle points due to this property, especially when \u03b2 is close to 1. Momentum accelerates the escape process by maintaining a correlation with the negative curvature direction in successive iterations. The effectiveness of momentum in improving performance is discussed. The study focuses on the benefits of stochastic momentum in accelerating the escape process from saddle points. Momentum can speed up saddle-point escape by a factor of 1 \u2212 \u03b2, but \u03b2 has constraints and cannot be chosen arbitrarily close to 1. Empirical evidence demonstrates the clear advantage of stochastic momentum in solving optimization challenges with significant saddle points. The study examines the use of stochastic momentum to accelerate escape from significant saddle points in optimization tasks. Two stochastic optimization tasks are constructed, each with a notable saddle point. One task involves a non-convex optimization challenge with a specific matrix and gaussian perturbations. Convergence in function value and relative distance to the true model are plotted, showing the progress in escaping saddle points. The study explores the use of stochastic momentum to escape saddle points in optimization tasks. Algorithms are initialized at the same point and use a step size of \u03b7 = 5 \u00d7 10 \u22124. The second objective involves phase retrieval, where one aims to find an unknown vector with limited samples. Empirical findings show stark results in Figure 2. The study investigates the use of stochastic momentum to accelerate convergence in optimization tasks, particularly in phase retrieval. Empirical results show that larger choices of momentum parameter \u03b2 lead to significantly faster convergence. This is the first reported empirical evidence of the speedup provided by stochastic momentum in finding optimal solutions in phase retrieval. The heavy ball method, proposed by Polyak in 1964, is also discussed in the context of optimization algorithms. The heavy ball method, originally proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. Specialized algorithms aim to reach a second-order stationary point by exploiting negative curvature. Specialized algorithms aim to exploit negative curvature to escape saddle points faster and reach a second-order stationary point. Previous works by Ge et al. (2015) and Jin et al. (2017) have shown methods to guarantee gradient descent escapes saddle points. Phase retrieval is nonconvex with strict saddle properties, where each saddle exhibits negative curvature. Our work assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of gradient, allowing for faster escape from saddle points. The algorithm avoids perturbing updates with noise and compares results with related works in Appendix A. The gradient is assumed to be L-Lipschitz and the Hessian to be \u03c1-Lipschitz, ensuring smoothness and convergence. The analysis of stochastic momentum in our work assumes that the gradient is L-Lipschitz and the Hessian is \u03c1-Lipschitz. We also consider properties such as bounded noise in the stochastic gradient and bounded norm of stochastic momentum. Our analysis relies on three key properties of the stochastic momentum dynamic, which we aim to demonstrate empirically in standard problems. SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) and Almost Positively Correlated with Gradient (APCG) with parameter \u03c4. The momentum term must not be significantly misaligned with the gradient for Gradient Alignment or Curvature Exploitation (GrACE) to hold. The momentum term in SGD should not be misaligned with the gradient for GrACE to hold. APCG requires the momentum term to be positively correlated with the gradient in the Mahalanobis norm induced by M t. The PSD matrix Mt measures the local curvature of the function with respect to the trajectory of SGD with momentum. Empirical evidence shows this property holds on natural problems. APCG is necessary in saddle regions with significant iterations. Gradient values are reported when \u2265 0.02, mostly nonnegative. SGD with momentum exhibits APAG and APCG properties in experiments. The analysis focuses on the properties of SGD with momentum in saddle regions. The PSD matrix Mt measures local curvature, showing nonnegative values. GrACE measures alignment between stochastic momentum and gradient, with curvature exploitation. The analysis focuses on the properties of SGD with momentum in saddle regions, where the first term measures alignment between stochastic momentum and gradient, and the second term measures curvature exploitation. The small sum of these terms allows for bounding the function value of the next iterate. Figures 3 and 4 show quantities related to APAG, APCG, and GrACE when solving specific problems using SGD with momentum. The analysis follows a similar template to previous studies. The analysis focuses on the properties of SGD with momentum in saddle regions, structured into three cases. The algorithm analyzed is similar to previous studies, with specific parameters and steps outlined. The analysis focuses on the properties of SGD with momentum in saddle regions, structured into three cases. The algorithm analyzed is similar to previous studies, with specific parameters and steps outlined. On the right-hand side of the equation, a smaller number is multiplied by a larger number, resulting in a slightly worse iteration complexity. The algorithm shows progress in certain cases and reaches a second-order stationary point with high probability. The proof utilizes tools from previous studies but introduces novel momentum analysis. The analysis introduces novel momentum analysis for SGD with momentum, showing progress in reaching second-order stationary points in saddle regions. Theorem 1 states that higher momentum leads to faster convergence by escaping saddle points. The proof details are provided in Subsection 3.2.1. Higher momentum enables faster escape from saddle points. Constraints on \u03b2 ensure it is not too close to 1. The dependency on 1 - \u03b2 in T thred makes the result smaller than previous work, demonstrating faster escape with momentum. In the high momentum regime, Algorithm 2 outperforms CNC-SGD, showing the benefits of higher momentum. Higher momentum facilitates quicker escape from saddle points, as demonstrated in the high momentum regime where Algorithm 2 surpasses CNC-SGD. The analysis focuses on escaping saddle points with SGD momentum, showing that it takes at most T thred iterations to escape a region with small gradient and large negative eigenvalue of the Hessian. The analysis demonstrates that it takes at most T thred iterations to escape a region with a small gradient and large negative eigenvalue of the Hessian. By using a proof by contradiction, it is shown that the function value must decrease by at least F thred in T thred iterations. The analysis shows that the function value must decrease by at least F thred in T thred iterations. The lower bound is larger than the upper bound, leading to a contradiction. Larger momentum helps in escaping saddle points faster. The lower bound of E[t0[wt0+Tthred\u2212wt02]] is obtained by analyzing the recursive dynamics of SGD with momentum. Lemma 2 defines a quadratic approximation for wt0+t\u2212wt0, with key quantities qv,t\u22121, qm,t\u22121, qq,t\u22121, qw,t\u22121, q\u03be,t\u22121 used in the analysis. Lemma 3 shows that the dominant term in the lower bound ensures it is larger than the upper bound, crucial for escaping saddle points. The lower bound of the expected distance is analyzed by considering the recursive dynamics of SGD with momentum. Key quantities from Lemma 2 are used in the analysis to show that the lower bound surpasses the upper bound, crucial for escaping saddle points. Additionally, Lemma 5 provides conditions for the SGD with momentum algorithm to have the APCG property. Lemma 5 states that with certain conditions, SGD with momentum can reach a second-order stationary point faster. A higher momentum value helps in escaping strict saddle points by enlarging the projection to an escape direction. However, ensuring that SGD with momentum possesses these properties is not clear, and further research is needed to identify the necessary conditions. The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases. Recent research has focused on analyzing this method for various optimization problems beyond quadratic functions. Recent research efforts have analyzed the heavy ball method for optimization problems beyond quadratic functions, including stochastic heavy ball momentum and Nesterov's momentum for smooth non-convex objectives. However, the expected gradient norm convergence rate is not better than standard SGD. Some variants of stochastic accelerated algorithms with first-order stationary point guarantees have been proposed, but they do not capture the stochastic heavy ball momentum used in practice. Additionally, a negative result shows that for specific strongly convex and smooth problems, SGD with heavy ball momentum is not effective. The framework by Ghadimi & Lan does not include stochastic heavy ball momentum. Kidambi et al. found that for certain problems, SGD with heavy ball momentum does not achieve the best convergence rate. Specialized algorithms aim to reach a second-order stationary point by exploiting negative curvature explicitly. The work discusses algorithms for escaping saddle points faster, including the use of preconditioning matrices and averaging schemes in stochastic gradient descent. It compares the iteration complexity of different SGD variants and aims to explain why SGD with heavy ball momentum works well in practice. The work discusses algorithms for escaping saddle points faster, including the use of preconditioning matrices and averaging schemes in stochastic gradient descent. It compares the iteration complexity of different SGD variants and aims to explain why SGD with heavy ball momentum works well in practice. Specifically, the analysis focuses on the advantage of stochastic heavy ball momentum and its effect on decreasing the function value. Lemma 6, 7, and 8 discuss the properties of SGD with momentum. Lemma 7 states that under the APAG property, SGD with momentum decreases the function value by a constant, while Lemma 8 bounds the increase of function value of the next iterate. If SGD with momentum has the APAG property, the update step is defined as w t+1 = w t \u2212 \u03b7m t. Lemma 7 states that under the APAG property, SGD with momentum decreases the function value by a constant, while Lemma 8 bounds the increase of function value of the next iterate. If SGD with momentum has the GrACE property, the update step is defined as w t+1 = w t \u2212 \u03b7m t. Lemma 7 and Lemma 8 discuss the properties of SGD with momentum. The update step for SGD with momentum is defined as w t+1 = w t \u2212 \u03b7m t. The update formula for the algorithm is w t0+1 = w t0 \u2212 rm t0 , and w t0+t = w t0+t\u22121 \u2212 \u03b7m t0+t\u22121 , for t > 1. The text also discusses the upper bounding of E t0 [4\u03b7 and the proof of Lemma 2 and Lemma 3. The text discusses the properties of SGD with momentum, including the update formula for the algorithm. It also presents the proof of Lemma 2 and Lemma 3, involving quadratic approximation and conditional expectations. The text presents the proof of Lemma 5, showing that if SGD with momentum has the APCG property, certain inequalities hold. Lemma 5 states that if SGD with momentum has the APCG property, certain inequalities hold. Constraints on parameter \u03b2 are also discussed to ensure it is not too close to 1. To prove Lemma 5 for SGD with momentum having the APCG property, certain inequalities hold with constraints on parameter \u03b2 to avoid it being too close to 1. The analysis involves upper bounding E t0 [ q q,t\u22121 ] using lemmas with specific parameter choices. The upper bounds of various terms in the analysis of SGD with momentum having the APCG property are derived by utilizing the assumption of L-Lipschitz gradient, triangle inequality, and specific parameter choices to avoid \u03b2 being too close to 1. The analysis involves bounding different expressions and terms using specific lemmas and parameter constraints. The text discusses deriving upper bounds for terms in the analysis of SGD with momentum having the APCG property. Specific parameter choices are made to avoid \u03b2 being too close to 1, and various expressions are bounded using lemmas and parameter constraints. The text discusses deriving upper bounds for terms in the analysis of SGD with momentum having the APCG property. Lemmas are used to bound expressions based on specific parameter choices to avoid \u03b2 being too close to 1. The text discusses deriving upper bounds for terms in the analysis of SGD with momentum having the APCG property. It shows that each matrix G j can be written in the form of G j = U D j U, where U is an orthonormal matrix and D j is a diagonal matrix, leading to the conclusion that U is symmetric positive semidefinite as long as each G j is. The proof involves lower bounding 2\u03b7E t0 [ q v,t\u22121 , q w,t\u22121 ] using Lemmas and the APCG property. The text discusses proving by contradiction that the function value must decrease at least F thred in T thred iterations on expectation. By leveraging negative curvature, upper and lower bounds are obtained for the expected distance. The contradiction is achieved by showing that the lower bound is larger than the upper bound, leading to the conclusion that the function value decreases as required. Lemma 13 and Lemma 12 imply that certain conditions are met. By utilizing specific inequalities and conditions, it is shown that a certain inequality holds. Additionally, Lemma 15 defines an event and provides a method for randomly selecting a value with a certain probability. The curr_chunk discusses the selection of a value w based on certain conditions to reach a second order stationary point in a certain number of iterations with high probability. The proof of this is outlined, with a reference to a lemma for further details. The curr_chunk provides a sketch of the proof for selecting a value w to reach a second order stationary point in a certain number of iterations with high probability. The proof utilizes a lemma for guidance, ensuring the conditions are met for the selection process. The proof utilizes Lemma 15 to ensure conditions are met for selecting a value w to reach a second order stationary point in a certain number of iterations with high probability. The proof is based on Lemma 15, ensuring conditions for selecting a value w to reach a second order stationary point in a certain number of iterations with high probability. The algorithm outperforms previous methods by not depending on the variance of stochastic gradient. Algorithm 2 outperforms previous methods by not depending on the variance of stochastic gradient, making it strictly better. A higher momentum can help find a second order stationary point faster."
}