{
    "title": "Byekm0VtwS",
    "content": "The uncertainty in intelligence is crucial for a flexible and powerful brain. Neuromorphic computing chips with uncertainty can mimic the brain, but current neural networks do not consider this uncertainty. A proposed uncertainty adaptation training scheme (UATS) informs neural networks of uncertainty during training, leading to comparable performance on uncertain neuromorphic chips compared to original platforms. The experimental results demonstrate that neural networks can achieve comparable inference performances on uncertain neuromorphic computing chips with the proposed uncertainty adaptation training scheme (UATS). Uncertainty reasoning, involving fuzziness and stochasticity, is essential for human thinking activities and intelligence. Fuzziness allows efficient processing of real-world information by ignoring redundant details, while stochasticity fosters creativity and adaptability in unfamiliar situations. The text discusses the importance of uncertainty reasoning in human intelligence and how it differs from existing artificial intelligence systems. It mentions the use of 8-bit integers for applications and methods like network quantization and Bayesian networks to address issues. Additionally, it highlights the role of neuromorphic computing chips in providing hardware solutions. The text discusses methods like network quantization and Bayesian networks to address issues in artificial intelligence systems. Neuromorphic computing chips provide hardware solutions, with emerging nanotechnology devices and crossbar structures improving efficiency in computing functions. The crossbar structure utilizes Ohm's law and Kirchhoff's law for vector-matrix multiplication, with nanoscale nonvolatile memory devices enhancing storage capability. The computing in memory architecture helps relieve memory bottlenecks. The crossbar structure with nanoscale nonvolatile memory devices provides additional storage capability and computing functions in neuromorphic computing chips. This architecture aims to relieve memory bottlenecks and improve energy and area efficiency for AI applications. Uncertainty is an intrinsic feature that is not fully utilized in these chips. The uncertainty in neuromorphic computing chips arises from analog to digital converters (ADCs) and stochasticity in NVM devices. The VMM result is a summarization of currents, requiring ADC for data transfer. Stochasticity in NVM devices is due to intrinsic physical mechanisms. The stochasticity of NVM devices is caused by random particle movement, leading to varied conductance and different output currents even with the same voltage. This stochasticity is often simulated as a non-ideal factor affecting network performance. A training scheme utilizing this stochasticity is proposed to enhance neuromorphic computing chip performance. Various types of NVM devices exist, including phase change memories, filamentary migrating oxide devices, and ferroelectric tunnel junction synapses. The stochasticity of NVM devices is modeled using a Gaussian distribution to represent different intrinsic physical mechanisms. The mean of the distribution corresponds to the conductance value of the stable state, with variance usually related to the mean. This approach may not fit all device types perfectly but captures the basic characteristics of stability and probability distribution. The conductance of NVM devices is modeled using a Gaussian distribution, where the mean represents the stable state conductance and the variance is typically related to the mean. The standard deviation is assumed to be linearly correlated to the mean, with conductances below a minimum value being cut off. The model of devices stochasticity involves sampling conductance from a Gaussian distribution with mean \u00b5 and variance \u03c3^2. Device fuzziness is a result of the writing process in neuromorphic computing chips for AI applications. Mapping processes determine target conductance based on neural network weights, scaling them to device conductance range. Conductance difference between devices expresses weights, which can be positive or negative. The mapping process scales neural network weights to device conductance range. Conductance difference between devices expresses weights, which can be positive or negative. To achieve higher energy efficiency, lower conductances are preferred. However, device conductance manipulation is affected by stochasticity and measurement inaccuracies. The conductance of the device is affected by stochasticity and measurement inaccuracies. A model using Gaussian distribution is used to describe the fuzziness, with parameters like target conductance and level of devices fuzziness.\u03b2 is assumed to be a constant for simplicity. The uncertainty in neuromorphic computing chips can impact DNN performance, leading to decreased classification accuracy. However, using the uncertainty adaptation training scheme (UATS) can alleviate this issue and even improve accuracy. UATS involves informing neural networks of uncertainty during training to help them learn how to handle it. A stochasticity model is introduced in every feed forward process to address this issue. The uncertainty adaptation training scheme (UATS) informs neural networks of uncertainty during training using stochasticity and fuzziness models. Stochasticity model uses random variable samples for calculations, while fuzziness model replaces weights with random variable samples after every k epochs of training. This helps neural networks learn how to handle uncertainty and improve performance. The training process involves replacing weights with random variable samples to handle uncertainty. The loss function is calculated by averaging the output of multiple FF processes. The uncertainty adaptation training scheme (UATS) was evaluated on various models and datasets, including the MNIST dataset. The uncertainty adaptation training scheme (UATS) was tested on multiple models and datasets, including the MNIST dataset. Two multilayer perceptron (MLP) models and a convolutional neural network (CNN) model were used with specific parameters. The models were trained conventionally and then tested with varying levels of uncertainty using fuzziness and stochasticity models. The uncertainty adaptation training scheme (UATS) was tested on MLP and CNN models with varying levels of uncertainty. Without UATS, uncertainty increased test errors for both models, with CNN (LeNet-5) being most affected. UATS improved model performance by tuning weights and retraining models. The study tested the uncertainty adaptation training scheme (UATS) on MLP and CNN models, showing that UATS improved model performance by tuning weights and retraining models. UATS significantly improved accuracies in both retraining and fine-tuning experiments, with most retraining results outperforming fine-tuning. The study validated the effectiveness of UATS on CIFAR-10 dataset with a ResNet-44 DNN model, achieving lower error rates than ideal cases with proper hyper-parameters. UATS acts as a regularization method, particularly beneficial for deeper neural networks, making training easier. Bayesian networks are useful for uncertain neural networks but require controllable weight distributions, posing challenges for neuromorphic computing chips. The uncertainty in intelligent systems is crucial. Bayesian networks are useful for uncertain neural networks but require controllable weight distributions, which is challenging for neuromorphic computing chips. Various distributions, such as Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli, have been explored to model device stochasticity. These distributions offer insights into the behavior of devices with different characteristics like two stable states or random telegraph noise (RTN). The VMM transforms device distributions into random parameters, reducing the need for random numbers in UATS computations. Methods like sampling weights for inputs or batches and using VMM uncertainty models instead of weights can speed up simulations while maintaining similar results."
}