{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively. This approach preserves semantic content better and follows a natural human language structure. The current text chunk discusses the limitations of the sequential model in image captioning due to its inability to reflect hierarchical structures in natural languages. Despite its effectiveness on benchmarks, this model fails to capture the inherent structures required for diverse and generalized captions. The sequential model in image captioning lacks the ability to reflect hierarchical structures in natural languages, leading to drawbacks such as reliance on n-gram statistics and difficulty in generalization. To address these issues, a new paradigm for image captioning is proposed. The proposed paradigm for image captioning decomposes the extraction of semantics and the construction of syntactically correct captions into two stages. It involves deriving a semantic representation of the image using noun-phrases and recursively composing the caption. The proposed paradigm for image captioning involves recursively composing captions by joining sub-phrases using a connecting module. This approach is not a hand-crafted algorithm and consists of two parametric modular nets for phrase composition and completeness evaluation. It offers several advantages over conventional captioning models. The proposed paradigm for image captioning involves two parametric modular nets for phrase composition and completeness evaluation. It offers advantages over conventional models by preserving semantic content, reflecting natural language structures, increasing caption diversity, and generalizing well to new data. The proposed paradigm for image captioning increases caption diversity and generalizes well to new data by utilizing convolutional and recurrent neural networks for caption generation. The recent works on image captioning utilize convolutional and recurrent neural networks for caption generation, with attention mechanisms to extract relevant image information. Different approaches have been proposed by Vinyals et al, Xu et al, Lu et al, Anderson et al, and Dai et al to improve the captioning process. Recent works on image captioning have introduced various improvements to the caption generation process. Lu et al adjusted attention computation to consider generated text, Anderson et al added an LSTM for better attention control, and Dai et al reformulated latent states as 2D maps to capture semantic information. Other approaches involve directly extracting phrases or semantic words from images, such as predicting frequent training words (Yao et al) or treating noun-phrases as hyper-words (Tan et al). A hierarchical approach was proposed in BID21, where one LSTM decides on phrases to produce and a second-level LSTM generates words for each phrase. Our proposed paradigm in image captioning involves a bottom-up approach, representing the input image with noun-phrases and constructing captions through a recursive composition procedure. This method preserves semantics effectively, requires less data to learn, and results in more diverse captions compared to sequential generation models. The proposed compositional paradigm in image captioning involves a recursive composition procedure that effectively preserves semantics, requires less data to learn, and generates more diverse captions compared to other methods. The procedure extracts noun-phrases from the input image and recursively composes them using a connecting module until a complete caption is formed. The proposed two-stage framework for image captioning involves recursively composing noun-phrases from the input image to generate diverse and semantically rich captions. The two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using CompCap. This approach considers nonsequential dependencies among words and phrases, unlike mainstream models. The framework represents image semantics explicitly with noun-phrases like \"a black cat\" and \"two boys\". In our framework, we represent image semantics explicitly with noun-phrases like \"a black cat\" and \"two boys\". Extracting these noun-phrases is essential for visual understanding tasks, even though more advanced techniques like object detection and attribute recognition can be used. The number of distinct noun-phrases in a dataset is much smaller than the number of images, for example, MS-COCO contains 120K images but only about 3K distinct noun-phrases in the captions. In the study, the number of distinct noun-phrases in a dataset is significantly smaller than the number of images. Noun-phrase extraction is formalized as a multi-label classification problem, with selected noun-phrases treated as classes. Visual features are extracted from images using a Convolutional Neural Network for binary classification of each noun-phrase. The visual features are extracted from images using a Convolutional Neural Network for binary classification of each noun-phrase. The input image is represented using top scoring noun-phrases, pruned through Semantic Non-Maximum Suppression. The caption is constructed recursively with a procedure called CompCap, maintaining a phrase pool and scanning all ordered pairs. CompCap is a recursive compositional procedure used to construct captions by connecting ordered pairs of noun-phrases with a Connecting Module. The procedure also includes an Evaluation Module to determine if the new phrase is a complete caption. The procedure involves selecting pairs with the highest connecting score to form a new phrase. A parametric module may be used for this selection. An Evaluation Module assesses if the new phrase is a complete caption. If not, the pool is updated and the process repeats until a complete caption is obtained or only one phrase remains. The Connecting Module selects a connecting phrase based on left and right phrases and evaluates the connecting score. The Connecting Module (C-Module) selects a connecting phrase based on left and right phrases to evaluate the connecting score. It treats the generation of connecting phrases as a classification problem due to the limited number of distinct connecting phrases in the proposed paradigm. The Connecting Module (C-Module) treats connecting phrases as a classification problem, with a limited number of distinct phrases. It mines connecting sequences from training captions and uses a classifier to output a normalized score. A two-level LSTM model encodes left and right phrases for this purpose. The Connecting Module (C-Module) uses a classifier to output a normalized score based on left and right phrases encoded by a two-level LSTM model. The model controls attention with visual features and evolves the encoded state through low and high-level LSTMs. The encodings of the phrases go through fully-connected layers and a softmax layer to determine connecting scores. The Evaluation Module (E-Module) determines if a phrase is a complete caption by evaluating its score based on the binary classification score obtained in the phrase-from-image stage. Longer phrases produced via the C-Module are scored accordingly. The Evaluation Module (E-Module) evaluates if a phrase is a complete caption based on the binary classification score obtained in the phrase-from-image stage. It can also check for other properties like caption quality using a caption evaluator. The framework can be extended to generate diverse captions using beam search or probabilistic sampling. The framework for generating diverse captions for images can be extended using beam search or probabilistic sampling. This allows for multiple beams in beam search to avoid local minima, or sampling ordered pairs and connecting sequences for diversity. User preferences can also be incorporated by filtering noun phrases or modulating scores. The text chunk discusses how user preferences can be incorporated into the generation of captions for images by filtering noun phrases or modulating scores. Experiments are conducted on MS-COCO and Flickr30k datasets, with specific details on the vocabulary sizes and preprocessing steps. The text chunk describes the preprocessing steps for datasets MS-COCO and Flickr30k, including vocabulary size reduction, caption truncation, and parsing ground-truth captions into trees using NLP toolkit BID31. The modularized training of the connecting module and evaluation module allows for better generalization and less sensitivity to training statistics. Testing involves two forward passes for each module, with 2 or 3 steps typically needed to generate a complete caption. The text chunk discusses the comparison of CompCap with various baseline methods for image captioning, including NIC, AdapAtt, TopDown, and LSTM-A5. Predictions from noun-phrase classifiers are used as additional features for LSTM-A5. The comparison is done to ensure fairness in evaluation. The study compares CompCap with baseline methods for image captioning, using predictions of noun-phrase classifiers as additional features for LSTM-A5. All methods are re-implemented and trained with the same hyperparameters, including using ResNet-152 for image feature extraction. Beam-search of size 3 is used for baselines, while CompCap selects top-scoring noun-phrases for input representation. When testing, parameters are selected for best performance on the validation set to generate captions. CompCap selects top-scoring noun-phrases for input representation. Quality of generated captions is compared on MS-COCO and Flickr30k test sets using various metrics. CompCap performs well under the SPICE metric but lags behind baselines in other metrics. Among all methods, CompCap with predicted noun-phrases performs best under the SPICE metric, correlating well with human judgements. However, it falls short compared to baselines in terms of CIDEr, BLEU-4, ROUGE, and METEOR. These results highlight the differences between sequential and compositional caption generation methods. While SPICE focuses on semantical analysis, other metrics favor frequent training n-grams, which are more common in sequential generation. The compositional approach preserves semantic content better but may include unseen n-grams. An ablation study on the proposed compositional paradigm is also conducted, with the input image represented using groundtruth noun-phrases. CompCap effectively preserves semantic content by using groundtruth noun-phrases from associated captions, leading to improved caption generation. By integrating noun-phrases in a specific order, CompCap shows further improvement in metrics, except for SPICE. This method highlights the importance of semantic understanding in generating better captions. CompCap boosts metrics by selecting connecting phrases and disentangling semantics and syntax. It excels in handling out-of-domain content and requires less data to learn. Studies show improved SPICE and CIDEr scores when trained on MS-COCO/Flickr30k data. CompCap excels in handling out-of-domain content and requires less data to learn. When trained on MS-COCO/Flickr30k data, it shows improved SPICE and CIDEr scores. The ability to generate diverse captions is a key property of CompCap. The distribution of semantics varies across datasets, while syntax remains stable. CompCap can generate diverse captions by varying noun-phrases and order. Diversity is analyzed using five metrics, including novel and unique captions, vocabulary usage, and editing distances. CompCap can generate diverse captions by varying noun-phrases and order. The diversity of captions is quantified using metrics such as vocabulary usage and editing distances. The diversity is measured at both the dataset level and image level, with different methods used to compute diversity at the image level. CompCap obtained the best results in all metrics, suggesting diverse and novel captions. Error analysis revealed that misunderstandings of input visual content were the main cause of errors, which could be addressed with more sophisticated noun-phrase extraction techniques. In this paper, a novel paradigm for image captioning is proposed, where captions are generated in a compositional manner. The approach involves extracting noun-phrases from the input image in the first stage and then assembling them using a recursive compositional procedure in the second stage. This method aims to address errors in captions generated by sequential models by focusing on a more sophisticated noun-phrase extraction technique. Our approach for image captioning involves a two-stage process: extracting noun-phrases from the input image and assembling them into a caption using a recursive compositional procedure. This method aims to improve caption quality by focusing on semantically similar noun-phrases. Our approach involves comparing central nouns in noun-phrases to determine semantic similarity, using encoders to get encodings for each noun-phrase. The normalized euclidean distances of the encodings are computed to measure similarity, ensuring robustness by summing the distances. The approach involves comparing central nouns in noun-phrases for semantic similarity using encoders to obtain encodings. Normalized euclidean distances are computed for the encodings, with the sum indicating similarity. The C-Module contains two independent encoders for ordered pairs to ensure different encodings for phrases in different positions. Comparing C-Modules with shared or independent parameters supports the hypothesis. The study compared C-Modules with shared or independent parameters for encoding phrases in different positions. Results showed that independent parameters led to better performance. Additional hyperparameters for CompCap can be adjusted, such as beam search size for pair and phrase selection. Experimental curves in FIG6 indicate minor influence on CompCap performance."
}