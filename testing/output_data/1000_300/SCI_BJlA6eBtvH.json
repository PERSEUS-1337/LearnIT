{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a major challenge for neural networks in dynamic data environments. A Differentiable Hebbian Consolidation model addresses this issue by combining rapid learning plasticity with fixed parameters, allowing for retention of learned representations over time. Task-specific synaptic consolidation methods can be integrated to maintain important weights for each target task. Our method integrates task-specific synaptic consolidation methods to retain learned representations in the softmax output layer for a longer timescale. It outperforms comparable baselines on various benchmarks by reducing forgetting, including an imbalanced variant of Permuted MNIST. This model requires no additional hyperparameters and showcases adaptability in dynamic environments, a key aspect of human intelligence challenging to embed in artificial intelligence. Recent advances in machine learning have shown improvements in solving complex tasks through extensive training on large datasets. However, ML models used in real-world deployment face non-stationarity, leading to performance degradation when trained with new data. This issue, known as catastrophic forgetting, poses a challenge for deep neural networks tasked with continual learning. In continual learning, deep neural networks face catastrophic forgetting, hindering their ability to adapt to new tasks without losing performance on previous ones. This challenge arises from concept drift, imbalanced class distributions, and non-iid training data in real-world applications. In real-world applications requiring continual learning, ML systems face challenges such as concept drift, imbalanced class distributions, and non-iid training data. This leads to the \"stability-plasticity dilemma\" where the model must balance plasticity to integrate new knowledge and stability to preserve existing knowledge. Biological neural networks also rely on synaptic plasticity for learning and memory. In biological neural networks, synaptic plasticity is crucial for learning and memory. Two theories explain continual learning in humans: synaptic consolidation preserves important synapses, while the complementary learning system theory suggests high-level structural extraction. Recent work on differentiable plasticity has shown that neural networks can be trained end-to-end through backpropagation and stochastic gradient descent to optimize slow weights for long-term memory, as well as fast weights for task-specific updates. These fast weights leverage Hebbian learning rules and represent the amount of plasticity in each synaptic connection. Recent work on differentiable plasticity has demonstrated the use of slow weights for long-term memory and fast weights for task-specific updates in neural networks. Fast weights act as short-term memory, allowing for reactivation of long-term memory traces. Networks with learned plasticity have shown superior performance compared to those with uniform plasticity. Approaches have been proposed to address catastrophic forgetting by adjusting synapse plasticity dynamically based on memory retention importance. In the task-incremental continual learning setting, a Differentiable Hebbian Consolidation model is developed to adapt quickly to changing environments and consolidate previous knowledge by adjusting synapse plasticity. The model modifies the traditional softmax layer by augmenting the slow weights in the final fully-connected layer with plastic weights using Differentiable Hebbian Plasticity. This approach demonstrates flexibility when combined with task-specific methods. The model proposed in the current chunk combines Differentiable Hebbian Plasticity with task-specific synaptic consolidation approaches to address catastrophic forgetting. It leverages Hebbian plasticity, synaptic consolidation, and CLS theory to adapt quickly to new data while preserving previous knowledge. The method is tested on benchmark problems like Permuted MNIST and Split MNIST. The proposed model combines Differentiable Hebbian Plasticity with task-specific synaptic consolidation to address catastrophic forgetting. It leverages compressed episodic memories in the softmax layer to remember previous knowledge. Testing on benchmark problems like Permuted MNIST and Split MNIST shows the effectiveness of the method. The theory of Hebbian learning suggests that learning and memory are attributed to weight plasticity in neural networks. Hebbian learning theory emphasizes synaptic plasticity and memory retention through the modification of synaptic strength. Recent advancements in meta-learning incorporate fast weights for efficient one-shot and few-shot learning. Recent literature has demonstrated the use of fast weights in neural networks for one-shot and few-shot learning. Munkhdalai & Trischler (2018) introduced a model with fast weights in FC layers before the softmax, implemented with non-trainable Hebbian learning-based associative memory. Rae et al. (2018) proposed a Hebbian Softmax layer that enhances learning of rare classes by combining Hebbian learning and SGD updates. Miconi et al. (2018) suggested differentiable plasticity, optimizing synaptic plasticity with SGD for each connection. The use of fast weights in neural networks for one-shot and few-shot learning has been demonstrated in recent literature. This approach optimizes synaptic plasticity with SGD for each connection, mainly shown on recurrent neural networks for pattern memorization tasks and maze exploration. The work also addresses the challenge of overcoming catastrophic forgetting by augmenting slow weights in the FC layer with fast weights implemented using DHP. The parameters of the softmax output layer are updated to achieve fast learning and preserve knowledge over time. The work leverages two strategies to overcome catastrophic forgetting: Task-specific Synaptic Consolidation dynamically adjusts synaptic strengths to retain memories, and CLS Theory uses a dual memory system. Notable works inspired by synaptic consolidation include those by Kirkpatrick et al., Zenke et al., and Aljundi et al. The work leverages task-specific synaptic consolidation to overcome catastrophic forgetting by dynamically adjusting synaptic strengths. Regularization strategies inspired by this approach estimate the importance of each parameter or synapse, updating network parameters to prevent changes to important parameters of previously learned tasks. Regularization strategies like Elastic Weight Consolidation (EWC) add a regularizer to the loss function to adjust plasticity and prevent changes to important parameters of previously learned tasks. EWC computes the importance of each parameter using the diagonal of an approximated Fisher information matrix. An online variant of EWC was proposed to improve scalability. Our work draws inspiration from CLS theory, a computational framework representing memories with a dual memory system via the neocortex and hippocampus. Online methods like Synaptic Intelligence (SI) and Memory Aware Synapses (MAS) measure parameter importance differently than EWC, ensuring scalability and efficiency in computing \u2126 k. Our work is inspired by CLS theory, which represents memories using a dual memory system. Various approaches based on CLS principles involve pseudo-rehearsal, exact or episodic replay, and generative replay. Exact replay methods store data from previous tasks for later replay, while generative replay methods use a separate model to generate images for replay. In contrast to iCaRL, our work focuses on rehearsal and regularization using an external memory to store exemplar patterns from old task data. Our work is primarily interested in neuroplasticity techniques inspired by CLS theory for alleviating catastrophic forgetting. Previous research has shown how synaptic connections can be composed of fixed and fast-changing weights, similar to properties of CLS theory. Recent studies have explored replacing soft attention mechanisms with fast weights in RNNs and augmenting slow weights in the FC layer with fast weights. Recent research has focused on using neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting in continual learning. Methods include replacing soft attention mechanisms with fast weights in RNNs, incorporating Hebbian Softmax layers, augmenting slow weights in the FC layer with fast weights matrix, and exploring differentiable plasticity and neuromodulated differentiable plasticity. These approaches are designed for rapid learning on simple tasks or meta-learning over a distribution of tasks. The goal of recent research is to address catastrophic forgetting in continual learning by using neuroplasticity techniques. Methods include incorporating Hebbian Softmax layers and exploring differentiable plasticity. The focus is on metalearning a local learning rule for fast weights in order to improve continual learning setups. In the context of continual learning, the goal is to metalearn a local learning rule for fast weights using Hebbian plastic components in the softmax layer. This involves adjusting the magnitude of the Hebb with a scaling parameter alpha and accumulating mean hidden activations for each target label in the mini-batch. In continual learning, the goal is to metalearn a local learning rule for fast weights using Hebbian plastic components in the softmax layer. The scaling parameter alpha adjusts the magnitude of the Hebb, accumulating mean hidden activations for each target label in the mini-batch. The network parameters are optimized by gradient descent. The \u03b7 parameter acts as a learning rate and decay term for Hebbian traces in the network. Parameters are optimized by gradient descent during training on different tasks in continual learning. Weight connections in standard neural networks have fixed weights, equivalent to setting plasticity coefficients \u03b1 = 0. Our model utilizes Hebbian weight updates and hidden activations to make predictions during test time. By accumulating activations directly into the softmax output layer weights, we achieve better initial representations and retain learned deep representations for longer periods. This approach allows for fast learning and prevents activations from different classes from competing for space. The DHP Softmax method improves test accuracy by enabling fast learning and preventing interference between classes. It allows for selective consolidation into a stable component to protect old memories, while scaling easily with increasing tasks. The advantage of DHP Softmax over external memory is its simplicity in implementation, requiring no additional space or computation. The advantage of DHP Softmax is its simplicity in implementation, scaling easily with increasing tasks. Hebbian update for class c = 6 involves averaging hidden activations into one vector, reflecting individual episodic memory. The plastic component learns rapidly, performing sparse parameter updates to store memory traces without interference. The DHP Softmax method improves learning of rare classes and speeds up binding of class labels to deep representations without introducing additional hyperparameters. It utilizes Hebbian synaptic consolidation to update synaptic importance parameters of the network. A sample implementation using PyTorch is provided in Appendix B. Following existing regularization strategies like EWC, Online EWC, SI, and MAS, the loss L(\u03b8) is regularized and synaptic importance parameters are updated online. The quadratic loss for Hebbian Synaptic Consolidation is updated, focusing on the weights of connections between neurons. Task-specific consolidation approaches are adapted without computing synaptic importance parameters on the plastic component of the network. During training the first task, the synaptic importance parameter is set to 0 for all methods except SI. The plastic component in our model helps prevent catastrophic forgetting by adjusting the connections' plasticity. Compared to other methods like Online EWC, SI, and MAS, our approach adds slow weights to increase the DNN's capacity. Our approach increases DNN capacity with plastic weights and slow weights in the softmax layer to prevent forgetting in sequential task learning. Tested on various benchmarks, including Imbalanced Permuted MNIST, model evaluated based on average classification accuracy on all tasks as a function of n. Our approach increases DNN capacity with plastic weights and slow weights in the softmax layer to prevent forgetting in sequential task learning. The model's memory retention and flexibility are evaluated based on test performance on the first task and the most recent one. Forgetting is measured using the backward transfer metric, which indicates the impact of learning new tasks on previous tasks. Different task-specific consolidation methods are compared by training neural networks with Online EWC, SI, and MAS on all tasks sequentially. In a benchmark study, neural networks were trained with Online EWC, SI, and MAS consolidation methods on sequential tasks. The input distribution changes between tasks, leading to concept drift. The Permuted MNIST benchmark used a multi-layered perceptron network with two hidden layers. In Permuted MNIST and Imbalanced Permuted MNIST benchmarks, a multi-layered perceptron network with two hidden layers was used. The plastic component's \u03b7 value was set to 0.001 without much tuning effort. Comparing the network with DHP Softmax to a vanilla MLP network (Finetune) showed DHP Softmax's ability to reduce catastrophic forgetting. Task-specific consolidation methods were also compared with and without DHP Softmax. The network with DHP Softmax showed improvement in alleviating catastrophic forgetting compared to the baseline network. Performance was compared with and without DHP Softmax using task-specific consolidation methods, showing higher test accuracy with DHP Softmax. An ablation study examined network parameters and Hebb traces for interpretability. The behaviour of the proposed model during training on the Permuted MNIST benchmark is analyzed. The synaptic connections become more plastic to acquire new information quickly in the initial tasks, but then decay to prevent interference between learned representations. The Hebb trace maintains a memory of synapses contributing to recent activity without runaway positive feedback. Plasticity coefficients grow within each task, indicating the network leverages the structure in the plastic component. The network leverages the structure in the plastic component when learning new tasks. Imbalanced Permuted MNIST problem introduces class imbalance and concept drift, making predictive performance challenging. The DHP Softmax model achieves 80.85% test accuracy after learning 10 tasks with imbalanced class distributions, showing a 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer, resulting in an average test accuracy of 88.80% with MAS, outperforming all other methods. The DHP Softmax model with MAS achieves a 0.04 decrease in BWT, resulting in an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. It outperforms all other methods across all tasks on the split MNIST dataset. The initial \u03b7 value of 0.001 was used for cross-entropy loss. DHP Softmax alone achieved 98.23% test performance, a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and increased average test accuracy across all tasks, especially T 5. Continual learning was performed on 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets were used for continual learning. The CNN architecture was similar to previous studies, with an initial \u03b7 parameter of 0.0001. Training was done with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. Results showed that DHP Softmax plus MAS improved test accuracy by 2.14% over MAS alone. SI with DHP Softmax outperformed other methods with an average test performance of 81.75% and a BWT of -0.04. Adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. This approach resulted in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax achieved an average test performance of 81.75% and a BWT of -0.04 after learning all five tasks. The addition of compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters improves performance in continual learning environments. The \u03b1 parameter in the plastic component automatically adjusts the plastic connections, allowing the model to generalize across experiences. The neural network with DHP Softmax shows significant improvement compared to traditional softmax layers, without introducing additional hyperparameters. The neural network with DHP Softmax, which does not introduce extra hyperparameters, showed improvement across benchmarks compared to traditional softmax layers. The model's flexibility allows for Hebbian Synaptic Consolidation using EWC, SI, or MAS to mitigate catastrophic forgetting after learning multiple tasks sequentially. DHP Softmax combined with SI outperforms other consolidation methods. Consolidation methods like EWC, SI, and MAS are used to improve a model's ability to alleviate catastrophic forgetting after learning multiple tasks sequentially. DHP Softmax combined with SI performs best on Split MNIST and 5-Vision Datasets Mixture. Combining DHP Softmax with MAS leads to superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. MAS computes synaptic importance parameters based on Hebb's rule, prioritizing highly correlated connections. The model consistently shows lower negative BWT and higher average test accuracy across all benchmarks. Hebbian plasticity enables neural networks to learn continually and remember distant memories, reducing catastrophic forgetting in dynamic environments. Continual synaptic plasticity aids in learning from limited labeled data and adapting at long timescales. This work aims to explore gradient descent optimized Hebbian consolidation for learning and memory in DNNs to enable continual learning on sequential tasks. This work explores gradient descent optimized Hebbian consolidation for continual learning in neural networks. The model is trained on a sequence of tasks with task-specific loss to prevent catastrophic forgetting. After training on a sequence of tasks with task-specific loss to prevent forgetting, the model learns an approximated mapping to the true underlying function. The learned mapping maps new inputs to target outputs for all tasks learned so far, even if the classes in each task are different. Experiments were conducted on Nvidia Titan V or Nvidia RTX 2080 Ti, training with mini-batches of size 64 using plain SGD with a learning rate of 0.01 for at least 10 epochs, with early stopping implemented. For training on a sequence of tasks, the network uses mini-batches of size 64 and plain SGD with a learning rate of 0.01. Training continues for at least 10 epochs with early stopping if validation error does not improve for 5 epochs. Hyperparameters for task-specific consolidation methods are set accordingly. For task-specific consolidation methods, hyperparameters were optimized through grid search using different values for \u03bb in Online EWC, SI, and MAS. In the Imbalanced Permuted MNIST problem, training samples were artificially removed based on random probabilities for each class and task. In the Imbalanced Permuted MNIST problem, training samples were artificially removed from each class in the original MNIST dataset based on random probabilities for each task. The distribution of classes in each imbalanced dataset for tasks 1 to 10 is shown in Table 2. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. In SI, the damping parameter \u03be was set to 0.1. The best hyperparameters were found similarly to the Permuted MNIST benchmark. For Split MNIST experiments, the regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. Grid search was performed to find the best hyperparameter combination for each method. For Split MNIST experiments, hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. Grid search was conducted to optimize hyperparameter combinations for each method on a 5 task binary classification sequence. The Vision Datasets Mixture benchmark consists of 5 tasks: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The notMNIST dataset includes font glyphs for letters 'A' to 'J'. FashionMNIST has 10 clothing categories with 60,000 training and 10,000 testing grayscale images. SVHN contains digits '0' to '9' with 73,257 training and 26,032 testing color images. FashionMNIST, SVHN, and CIFAR-10 datasets consist of grayscale and color images of various categories for training and testing. The CNN architecture includes 2 convolutional layers with LeakyReLU nonlinearities and max-pooling operations, followed by an FC layer before the final softmax output layer. A multi-headed approach was used for classification. The CNN architecture includes 2 convolutional layers with LeakyReLU nonlinearities, max-pooling operations, and an FC layer before the final softmax output layer. A multi-headed approach was used for classification, with separate \u03b7 parameters for each connection in the final output layer. This improved stability of optimization and convergence to optimal test performance. The CNN architecture includes 2 convolutional layers with LeakyReLU nonlinearities, max-pooling operations, and an FC layer before the final softmax output layer. A multi-headed approach was used for classification, with separate \u03b7 parameters for each connection in the final output layer to improve stability of optimization and convergence to optimal test performance. Different hyperparameters were tested for task-specific consolidation methods, including \u03bb values for Online EWC, SI, and MAS, as well as a random search for the best hyperparameter combination for each method. In a sensitivity analysis, the effect of the Hebb decay term \u03b7 on average test performance after learning tasks sequentially was examined. Low values of \u03b7 were found to alleviate catastrophic forgetting best. This analysis was also performed for the Split task. Setting low values of \u03b7 led to the best performance in alleviating catastrophic forgetting. A sensitivity analysis was also done for the Split MNIST problem. Table 4 shows the average test accuracy for MNIST-variant benchmarks. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to the final output layer of a neural network through plastic connections. The implementation is simple and utilizes popular techniques. The DHP Softmax model, implemented in PyTorch, incorporates compressed episodic memory through plastic connections in a neural network. It outperforms Finetune in class-incremental learning tasks on CIFAR-10 and CIFAR-100 datasets. Test accuracies show DHP Softmax performing as well or better than training from scratch. DHP Softmax outperforms Finetune in class-incremental learning tasks on CIFAR-10 and CIFAR-100 datasets. Test accuracies show DHP Softmax performing as well or better than training from scratch or SI."
}