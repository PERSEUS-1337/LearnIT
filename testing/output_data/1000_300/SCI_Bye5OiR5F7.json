{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experimental results show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves. The adversarial game involves an optimization problem over an implicit generative model for the generator. The generator aims to recreate the density distribution from the real source to fool the discriminator. The Wasserstein distance is used as a discrepancy measure between densities in learning generative models. The Wasserstein distance, also known as Earth Mover's distance, is used as a discrepancy measure between densities in learning generative models. It has been utilized to define the loss function for generative models like Wasserstein GAN. Optimal transport can introduce structures for optimization, such as the Wasserstein steepest descent flow. This paper derives the Wasserstein steepest descent flow for deep generative models in GANs using the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. The Fisher-Rao natural gradient, induced by the KL divergence, is often advantageous in learning problems compared to the Euclidean gradient. However, in GANs, the low dimensional support sets make the Fisher-Rao natural gradient problematic. To address this, the gradient operator induced by the Wasserstein-2 metric is proposed. The proximal operator for GAN generators is computed using the squared constrained Wasserstein-2 distance as regularization, which can be approximated by a neural network. The constrained Wasserstein-2 metric has a simple structure in implicit generative models. Regularization in GANs involves the squared constrained Wasserstein-2 distance, approximated by a neural network. The metric is generalized with a relaxed proximal operator for simplified computation. This method serves as a drop-in regularizer for generator updates, demonstrated to be effective in various GAN experiments. The Wasserstein natural gradient is introduced in Algorithm 1 for GAN optimization. Section 3 demonstrates the method's effectiveness in various GAN experiments, while Section 4 reviews related work on optimal transport and its proximal operator. Optimal transportation defines distance functions between probability densities, with W 2 representing the Wasserstein-2 distance. The paper focuses on extending the classic theory of the Wasserstein-2 distance to cover parameterized density models. It introduces the constrained Wasserstein-2 metric function for parameter space \u0398, where the density path is constrained within a parametrized model. The constrained Wasserstein-2 metric function for parameter space \u0398 involves infimum among feasible Borel potential functions and continuous parameter paths. It differs from the full density set Wasserstein-2 distance and can be used for steepest descent optimization schemes based on Riemannian structures like the Fisher natural gradient. The constrained Wasserstein-2 metric allows for a Riemannian metric structure, leading to the Wasserstein natural gradient. The Wasserstein gradient operator for a loss function F on parameter space \u0398 is defined, with the steepest descent flow determined by the Wasserstein Riemannian metric. The steepest descent flow in the constrained Wasserstein metric is determined by the natural gradient operator W \u03b8. The gradient descent iteration uses the Euclidean gradient operator \u2207 \u03b8 with the Wasserstein Riemannian metric matrix G. The computation of G(\u03b8) \u22121 can be challenging, leading to the use of the proximal operator in the Jordan-Kinderlehrer-Otto scheme. This scheme incorporates a regularization term in the parameter update to approximate the Wasserstein distance locally. The Semi-Backward Euler method is a first-order scheme for the gradient flow of a loss function in a parameterized setting. It is easier to approximate than the forward Euler method and simpler than the backward Euler method, as it does not require computing and inverting G(\u03b8) and involves a more tractable constrained optimization over \u03a6. The Semi-Backward Euler method in implicit generative models simplifies the computation by using a neural network to approximate variable \u03a6. This allows for a simpler formulation of the constrained Wasserstein-2 metric. In this case, the update in Proposition 3 simplifies the computation by using a neural network to approximate variable \u03a6. The constrained Wasserstein-2 metric in implicit generative models is reformulated to define the relaxed Wasserstein metric and introduces a simple algorithm for the proximal operator on generators. The constrained Wasserstein metric requires the derivative of the generator g w.r.t. \u03b8 to be a gradient vector field of \u03a6 w.r.t x. The gradient constraint is satisfied when the sample space is 1 dimensional. The derivative of the generator g w.r.t. \u03b8 must be a gradient vector field of \u03a6 w.r.t x. The gradient constraint is satisfied when the sample space is 1 dimensional. Fitting this constraint is challenging due to computational difficulties. To simplify computations, a relaxed Wasserstein metric is considered on the parameter space. This leads to an approximation of the relaxed Wasserstein proximal operator based on a new metric. The update in high dimensional sample spaces regularizes the generator by the expectation of squared differences. A toy example illustrates the effectiveness of the Wasserstein proximal operator in GANs with a family of distributions and a given ratio. In a toy example illustrating the effectiveness of the Wasserstein proximal operator in GANs, a family of distributions with a given ratio is considered. The proximal regularization for a loss function is defined, and various statistical distance functions between parameters are checked. The Wasserstein-2 and Euclidean distances are found to be effective in measuring differences in probability models. The Wasserstein-2 and Euclidean distances are effective in measuring differences in probability models. The Euclidean distance does not depend on the model's structure, while the constrained Wasserstein-2 metric does. Numerical experiments show that Wasserstein-2 proximal outperforms Euclidean proximal for the Wasserstein-1 loss function. The Relaxed Wasserstein Proximal (RWP) algorithm outperforms Euclidean proximal for the Wasserstein-1 loss function in numerical experiments. It provides better speed and stability in training GANs by applying regularization on the generator, which is a novel approach compared to traditional methods focusing on regularizing the discriminator. The Relaxed Wasserstein Proximal (RWP) algorithm improves training speed and stability in GANs by regularizing the generator. It introduces hyperparameters for updating the generator and discriminator, tested on various GAN types using CIFAR-10 and CelebA datasets with DCGAN architecture. The Relaxed Wasserstein Proximal (RWP) algorithm improves training speed and stability in GANs by regularizing the generator. It introduces hyperparameters for updating the generator and discriminator, tested on various GAN types using CIFAR-10 and CelebA datasets with DCGAN architecture. Wassersteing Proximal regularization is applied to Standard GANs, WGAN-GP, and DRAGAN using the CIFAR-10 and CelebA datasets. The Fr\u00e9chet Inception Distance (FID) is used to measure the quality of generated samples, with specific hyperparameter choices outlined in the study. The Relaxed Wasserstein Proximal regularization improves training speed and stability in GANs by regularizing the generator. It shows improved convergence speed and lower FID for all GAN types, with a 20% enhancement in sample quality for DRAGAN. Similar results are observed for the CelebA dataset. Multiple generator iterations may hinder Standard GANs on CelebA initially, requiring algorithm restarts. A 20% improvement in sample quality is seen for the CelebA dataset, with results shown in FIG2. Multiple generator iterations may initially hinder Standard GANs on CelebA, but restarting the algorithm leads to successful runs. The defect may be rectified with a more stable loss function like WGAN-GP. The effect of multiple generator updates compared to discriminator updates is examined, with RWP updating the generator multiple times before the discriminator. Omitting regularization in WGAN-GP results in high variance in FID. The use of RWP regularization in GAN training improves stability and lowers FID compared to omitting regularization. Latent space walks demonstrate that RWP does not cause the GAN to memorize. RWP also improves speed and achieves a lower FID, even after multiple generator iterations. RWP regularization improves speed and achieves lower FID in GAN training. Multiple generator iterations may cause initial learning to fail, but successful runs show convergence with RWP. An experiment with 10 generator iterations per outer-iteration demonstrates the effectiveness of RWP in achieving convergence and lower FID compared to training without RWP. The Semi-Backward Euler method on the CIFAR-10 dataset shows comparable training to the standard WGAN-GP method. The Semi-Backward Euler (SBE) method on the CIFAR-10 dataset shows comparable training to the standard WGAN-GP method. The experiment was averaged over 5 runs, with RWP regularization improving speed and achieving lower FID in GAN training. The training of SBE involves approximating three functions: the usual discriminator, generator, and potential function \u03a6 p. The algorithm and hyperparameter settings are detailed in the appendix. The Semi-Backward Euler method aligns with norm WGAN-GP in terms of generator iterations. In Section G, optimization attempts were made on three networks in FIG3. The Semi-Backward Euler method showed comparable performance to norm WGAN-GP. Many studies use the Wasserstein distance as the loss function in optimal transport and GANs due to its statistical properties and ability to compare probability distributions. The Wasserstein distance is a statistical estimator used in optimal transport and GANs, specifically in Wasserstein GAN. It compares probability distributions supported on lower dimensional sets and requires the discriminator to satisfy the 1-Lipschitz condition. The Wasserstein-2 metric provides a metric tensor structure, forming an infinite dimensional Riemannian manifold called the density manifold. The gradient flow in the density manifold connects with transport-related partial differential equations, such as the Fokker-Planck equation. Learning communities explore leveraging gradient flow structure in probability space for stochastic gradient descent and studying nonparametric models like the Stein gradient descent method. The gradient flow structure in probability space is studied for stochastic gradient descent, including nonparametric models like the Stein gradient descent method. Wasserstein gradient flow can be constrained on parameter space, with a focus on Gaussian families and elliptical distributions. The constrained Wasserstein gradient is applied to implicit generative models, focusing on regularizing the generator. This approach improves convergence speeds and minimizes FID in Wasserstein GAN. The proposed method utilizes the gradient flow of Wasserstein-1 distance on parameter space to achieve faster convergence speeds and obtain a better minimizer in terms of FID. The variational formulation introduces a Riemannian structure in density space, considering smooth and strictly positive probability densities. The elliptic operator identifies the function modulo additive constants with the tangent vector of the space of densities. The tangent space of P + is defined by \u03a6 \u2208 F and \u03c1 \u2208 P +, with V \u03a6 \u2208 T \u03c1 P +. The elliptic operator \u2207\u00b7(\u03c1\u2207) identifies \u03a6 with the tangent vector V \u03a6 of densities. The inner product g W endows P + with a Riemannian metric tensor. The Wasserstein gradient operator in (P +, g W) is given for a loss function F : P + \u2192 R. The gradient flow satisfies analytical results on the Wasserstein-2 gradient flow. Wasserstein-2 metric and gradient operator are constrained on statistical models defined by (\u0398, R n, \u03c1). The paper discusses the Wasserstein-2 metric and gradient operator constrained on statistical models defined by a parameterization function \u03c1. A Riemannian metric is defined on the statistical manifold, and the constrained Wasserstein gradient operator in parameter space is studied. The associated metric tensor is smooth and positive definite, forming a smooth Riemannian manifold. The proof of Theorem 1 involves the constrained Wassertein gradient operator in parameter space on a smooth and positive definite Riemannian manifold. The derivation of the proposed semi-backward method is presented in the proof of Proposition 3, along with the proof of a claim regarding geodesic paths. The derivation of the proposed semi-backward method is presented in the proof of Proposition 3, along with the proof of a claim regarding geodesic paths. The method is a consistent numerical method known as the Semi-backward method. Additionally, Proposition 4 is proven in another source and is included here for completeness. The Semi-backward method is a consistent numerical method in time. Proposition 4 is proven in another source and is included here for completeness. The implicit model is defined by a push-forward relation. The probability density transition equation satisfies the constrained continuity equation. The equation is proven by showing equality for any function in a certain space. The Semi-backward method is a consistent numerical method in time. Proposition 4 is proven in another source and is included here for completeness. The implicit model is defined by a push-forward relation. The probability density transition equation satisfies the constrained continuity equation. The equation is proven by showing equality for any function in a certain space. Operators with respect to x \u2208 R n are computed explicitly for the Wasserstein and Euclidean proximal operators. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 are specified. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 include batch sizes of 64 for all experiments. Specific settings for CIFAR-10 with WGAN-GP, Standard and DRAGAN, and aligned and cropped CelebA with Standard are detailed, such as optimizer types, learning rates, latent space dimensions, and generator iterations. The experiments used a latent space dimension of 100, h = 0.2, and = 5 generator iterations with the Adam optimizer for both the generator and discriminator. The Relaxed Wasserstein Proximal algorithm was showcased on Standard GANs with specific parameters and steps outlined for optimization. The algorithm for training GANs with Relaxed Wasserstein Proximal involves updating the discriminator and performing Adam gradient descent multiple times. The key difference from standard GAN training is the inclusion of specific terms and the number of generator iterations. Samples generated using this method on CelebA and CIFAR-10 datasets had FID scores of 17.105 and 38.3 respectively. In FIG4, samples from a Standard GAN with RWP regularization on CelebA had an FID of 17.105. In FIG5, samples from WGAN-GP with RWP on CIFAR-10 had an FID of 38.3. Latent space walk BID34 showed smooth transitions, indicating no memorization in GANs with RWP regularization. Specific hyperparameters for SBE on WGAN-GP included a batch size of 64, DCGAN architecture, MLP for \u03a6 p, layer normalization, and Adam optimizer with specific parameters. The study utilized a one-hidden-layer fully connected network for the potential \u03a6 p, with layer normalization. The Adam optimizer was used with specific parameters for the generator, discriminator, and potential \u03a6 p. The latent space dimension was set to 100. The discriminator was updated 5 times, the generator once, and the potential 5 times in each outer-iteration loop."
}