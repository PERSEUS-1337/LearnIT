{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are used for control policies in reinforcement and imitation learning. A new technique called Quantized Bottleneck Insertion helps to create finite representations of RNN memory vectors and observation features. This allows for better analysis and understanding of RNN behavior. Results on synthetic environments and Atari games show that the finite representations can be surprisingly small, with as few as 3 memory states and 10 observations for a perfect Pong policy. This approach also improves interpretability of deep reinforcement learning and imitation learning. Representations in deep reinforcement learning and imitation learning are often difficult to understand, especially when using recurrent neural networks (RNNs) due to their internal memory encoding. A new technique called Quantized Bottleneck Insertion creates finite representations of RNN memory vectors and observation features, leading to improved interpretability. Surprisingly, these finite representations can be as small as 3 memory states and 10 observations for a perfect Pong policy, enhancing the analysis and understanding of RNN behavior. In this paper, the focus is on understanding and explaining RNN policies by creating more compact memory representations. The challenge lies in interpreting the high-dimensional continuous memory vectors used by RNNs. The goal is to quantize the memory and observation representation to capture discrete concepts, which could enhance explainability. Manipulating and analyzing the quantized system can help in understanding the memory usage in RNN policies. Our main contribution is introducing a method to transform an RNN policy with continuous memory and observations into a finite-state representation called a Moore Machine. This is achieved through the insertion of Quantized Bottleneck Networks (QBNs), which are auto-encoders with quantized latent representations. The QBNs encode memory states and observation vectors encountered during RNN operation, replacing the \"wires\" that propagate memory. The method introduces Quantized Bottleneck Networks (QBNs) to encode memory states and observation vectors in a trained RNN policy, creating a Moore Machine Network (MMN) with quantized memory and observations. The MMN can be used directly or fine-tuned for improved accuracy, with \"straight through\" gradient estimators proving effective in training. Experiments in synthetic domains and grammar learning problems demonstrate the effectiveness of this approach. The approach using Quantized Bottleneck Networks (QBNs) in RNN policies is effective, demonstrated through experiments in synthetic domains and benchmark grammar learning problems. The method accurately extracts ground-truth Moore Machine Networks (MMNs) to provide insights into RNN memory usage. Experiments on 6 Atari games show near-equivalent MMNs can be extracted, revealing insights into memory usage not obvious from observing RNN policies. Our work focuses on learning finite-memory representations of continuous RNN policies, which is a novel approach compared to prior work on extracting Finite State Machines from recurrent networks. This includes efforts to understand RNN memory usage in various games and the relationship to learning finite-state representations. Our approach involves inserting discrete elements into recurrent neural networks to preserve behavior while allowing for a finite state characterization, unlike previous methods that produce separate FSM approximations. Our approach involves directly inserting discrete elements into recurrent neural networks to preserve behavior and enable fine-tuning and visualization. This method, known as QBN insertion, transforms pre-trained recurrent policies into a finite representation, extending prior work on learning FSMs and MMNs. Our work extends the approach of learning MMNs and introduces QBN insertion to transform pre-trained recurrent policies into a finite representation. Unlike prior work on fully binary networks, we focus on learning discrete representations of memory and observations for interpretability rather than efficiency in reinforcement learning. Recurrent neural networks (RNNs) are commonly used in reinforcement learning to represent policies with internal memory. An RNN processes observations and outputs actions based on a continuous-valued hidden state. The RNN extracts observation features, outputs actions according to a policy, and transitions to a new state using a transition function implemented via gating networks like LSTMs or GRUs. In reinforcement learning, recurrent neural networks (RNNs) are used to represent policies with internal memory. The RNN processes observations, outputs actions based on a hidden state, and transitions to a new state using gating networks like LSTMs or GRUs. The goal is to extract compact quantized representations of the hidden states and observation features using Moore Machines and their deep network counterparts. A Moore Machine is a finite state machine where states are labeled by output values corresponding to actions. It is described by hidden states, observations, actions, a transition function, and a policy mapping hidden states to actions. A Moore Machine Network is a Moore Machine where the transition function and policy are represented by deep networks, suitable for continuous or unbounded observations like images. A Moore Machine Network (MMN) is a Moore Machine where the transition function and policy are represented by deep networks. MMNs provide a mapping from continuous observations to a finite discrete observation space. They use quantized state and observation representations with a restricted memory composed of k-level activation units. MMNs can be viewed as traditional RNNs with environmental observations transformed to a k-level representation. The MMN is a traditional RNN with a restricted memory composed of k-level activation units. Learning MMNs from scratch can be challenging for non-trivial problems, such as training highperforming MMNs for Atari games. A new approach involves learning quantized bottleneck networks (QBNs) after training an RNN, to embed continuous observations. The new approach for training highperforming MMNs for Atari games involves learning quantized bottleneck networks (QBNs) after training an RNN. QBNs embed continuous observation features and hidden states into a k-level quantized representation, which are then inserted into the original recurrent net with minimal behavior changes. The resulting network consumes quantized features and maintains quantized state, effectively creating an MMN. The QBN is a type of autoencoder with a constrained k-level latent representation, aiming to discretize a continuous space. It involves a multilayer encoder mapping inputs to a quantized encoding, using 3-level quantization. The output nodes can use tanh activation, but adjustments may be needed due to gradient issues. In our case, we use 3-level quantization with the activation function \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x) to support 3-valued quantization. The quantize function in the QBN makes b(x) non-differentiable, but the straight-through estimator is effective for backpropagation. The straight-through estimator is effective for backpropagation in dealing with gradients between the decoder and encoder in the QBN. It treats the quantize function as the identity function during back-propagation, allowing the last layer of E to produce a k-level encoding. Training a QBN as an autoencoder using L2 reconstruction error for a given input x is effective. Running a recurrent policy in the target environment can produce a large set of training sequences for observation, observation feature, and hidden state at time t. The approach involves training two QBNs, b f and b h, on sets of observed features and states respectively. If low reconstruction error is achieved, the latent \"bottlenecks\" of the QBNs can be viewed as high-quality encodings. These QBNs are then inserted into the original RNN as \"wires\" to propagate input to output with some noise. The QBNs b f and b h are inserted into the original RNN to create an MMN, providing a quantized representation of features and states. While the MMN may not behave identically to the RNN due to imperfect reconstruction, fine-tuning can help mitigate performance degradation. The resulting MMN may not behave identically to the original RNN, but its performance is often close initially. Fine-tuning on the original RNN data can help mitigate degradation, aiming for the MMN to match the RNN's softmax distribution. Visualization tools can be used to understand the memory and feature bits, but solving the full interpretation problem is not within the scope of this work. One way to gain insight is to use the MMN to create a Moore Machine over atomic state and observation spaces. By analyzing this machine, one can understand the role of different machine states and their relationships. To create the Moore Machine, the learned MMN is used to generate a dataset of consecutive pairs of quantized states, quantized features, and selected actions. The state-space of the Moore Machine corresponds to distinct quantized states, while the observation-space consists of unique quantized feature vectors. The Moore Machine is created using the learned MMN to analyze machine states and their relationships. The transition function is constructed from data to capture transitions, and minimization techniques are applied to reduce the number of states and observations. Experiments aim to extract MMNs from RNNs without loss in performance. In experiments, techniques are used to minimize the number of states and observations in the equivalent Moore Machine BID19. The study aims to extract MMNs from RNNs without performance loss and explores the interpretability of recurrent policies. Two domains are considered: Mode Counter synthetic environment and benchmark grammar learning problems. The Mode Counter Environments (MCEs) allow for varying memory requirements and types of memory usage in grammar learning problems. MCEs are a type of Partially Observable Markov Decision Process with multiple modes, each requiring different memory levels for the agent to infer and take the correct action. The Mode Counter Environments (MCEs) test the agent's ability to infer the mode and take the correct action using a combination of observations and memory. Different parameterizations place varying requirements on memory usage for optimal performance. Three MCE instances test memory and observation usage in different ways: 1) Amnesia, where optimal actions can be selected based only on current observations without needing memory, and 2) Blind, where observations do not provide information about optimal actions. The Mode Counter Environments (MCEs) test the agent's ability to infer the mode and take the correct action using a combination of observations and memory. Different parameterizations place varying requirements on memory usage for optimal performance. Three MCE instances test memory and observation usage in different ways: 1) Amnesia, where optimal actions can be selected based only on current observations without needing memory, 2) Blind, where observations do not provide information about optimal actions, and 3) Tracker, where memory and observations are both required to select optimal actions. The recurrent architecture used in the Mode Counter Environments (MCEs) consists of a feed-forward layer, a GRU layer, and a fully connected softmax layer for action distribution. Imitation learning is used for training, achieving 100% accuracy on the imitation dataset. The observation and hidden-state architectures in MMN training vary in bottleneck units. The encoder includes tanh nodes feeding into quantized bottleneck nodes, while the decoder is not specified. In experiments, bottleneck units B f and B h are varied. Encoders have 1 feed-forward layer of tanh nodes, with 4 times the size of the bottleneck. Training in MCE environments was fast compared to RNN training. QBNs with sizes of B f \u2208 {4, 8} and B h \u2208 {4, 8} were trained and embedded into RNNs to create discrete MMNs. Performance was measured before and after fine tuning, with most cases not requiring fine tuning. After varying bottleneck units B f and B h, QBNs with sizes of B f \u2208 {4, 8} and B h \u2208 {4, 8} were embedded into RNNs to create discrete MMNs. Fine-tuning was mostly unnecessary as agents achieved optimal performance immediately after bottleneck insertion due to low reconstruction error. In cases where fine-tuning was needed, it resulted in perfect MMN performance except for one exception which yielded 98% accuracy. Interestingly, inserting one bottleneck at a time resulted in perfect performance, indicating that combined error accumulation of both bottlenecks reduced performance. Moore Machine Extraction also showed the number of states and observations before and after minimization. After embedding QBNs into RNNs, Moore Machine Extraction revealed the number of states and observations before and after minimization. Typically, there were more states and observations before minimization, indicating non-minimal representations. However, after minimization, exact minimal machines were obtained for each MCE domain, showing that MMNs learned via QBN insertions were optimal in most cases. The ground truth minimal machines found were equivalent to the true minimal machines. The MMNs learned via QBN insertions were found to be equivalent to the true minimal machines in most cases, as shown in the Appendix. The exception occurred when the MMN did not achieve perfect accuracy. Analyzing these machines provides insights into memory usage, such as the machine for Blind having only one observation symbol, making its transitions independent of input observations. In contrast, the machine for Amnesia shows that each observation symbol leads to the same state, determining the action solely based on the current observation. In policy learning problems, the focus is on training RNNs for grammars with two actions 'accept' and 'reject'. Each episode involves a random string from the grammar, with a reward given for choosing the correct action. The RNNs are one-layer GRUs with 10 hidden units and a softmax layer with 2 nodes. Training is done using imitation learning with an Adam optimizer and learning rate of 0.001. The training dataset has an equal number of accept/reject instances. The RNNs used in policy learning have 10 hidden units and a softmax layer with 2 nodes for accept/reject actions. Training involves imitation learning with an Adam optimizer and learning rate of 0.001. The test results show high accuracy, except for grammar #6. MMN training does not require a bottleneck encoder due to the finite alphabet nature of the problem. Bottlenecks for hidden memory state are learned and inserted into the RNNs to create MMNs. The RNNs used in policy learning have 10 hidden units and a softmax layer with 2 nodes for accept/reject actions. Training involves imitation learning with an Adam optimizer and learning rate of 0.001. MMN training does not require a bottleneck encoder due to the finite alphabet nature of the problem. Bottlenecks for hidden memory state are learned and inserted into the RNNs to create MMNs with B h \u2208 {8, 16}. Results show a reduction in MM's state-space after minimization while maintaining performance. Tomita grammars are used to evaluate language accuracy. After minimizing the RNNs used in policy learning, the MM's state-space is reduced while maintaining performance. The technique is applied to RNNs learned for six Atari games, where the input observations are more complex. The minimized machines are equivalent to the minimal machines known for the Tomita grammars. The input observations for Atari are more complex, making it unclear if similar results can be expected. There are efforts to understand Atari agents, but none aim to extract finite state representations for Atari policies. The Atari agents have the same recurrent architecture with specific preprocessing steps and network layers. The model has 4 convolutional layers with specific parameters and activation functions. It includes a GRU layer, a fully connected layer, and uses the A3C RL algorithm for training. The performance was evaluated on six games. The architecture was adjusted for QBN experiments to match observation features. For the QBN experiments in Atari domains, the encoder and decoder architecture was adjusted to match observation features. Training data was generated using noisy rollouts to increase diversity. Bottlenecks were trained for B_h \u2208 {64, 128} and B_f \u2208 {100, 400} to handle the complexity of Atari games. The bottlenecks were trained for B_h \u2208 {64, 128} and B_f \u2208 {100, 400} to handle the complexity of Atari games. Each bottleneck was trained to saturation of training loss and inserted into the RNN to create an MMN for each game. The MMNs showed promising performance, achieving identical or close scores to the RNN after fine-tuning for games like Pong, Freeway, Bowling, and Boxing. This demonstrates the ability to learn a discrete representation of input and memory for complex games. After fine-tuning, MMNs achieved similar scores to RNNs in games like Pong, Freeway, Bowling, and Boxing, showing the ability to learn complex game representations. However, in Breakout and Space Invaders, MMNs had lower scores due to poor reconstruction in certain game parts. For example, in Breakout, the MMN failed to press the fire-button after clearing the first board. The drop in performance of MMNs in Breakout and Space Invaders was attributed to poor reconstruction in specific game segments. For instance, in Breakout, the MMN failed to press the fire-button after clearing the first board, resulting in lower scores. This highlights the need for more intelligent training approaches to capture crucial information in rare but significant states. Minimizing MMNs led to a drastic reduction in the number of states and observations, making them easier to analyze manually. After minimizing MMNs, the number of states and observations drastically reduces, sometimes to just one state and observation. This simplification allows for manual analysis, but understanding complex policies may still be challenging. In Atari games like Pong, MM only has three states and transitions to the same state regardless of the current state, simplifying the mapping of observations to actions without the need for memory. In Pong, the Markov Model has 10 observation symbols, each transitioning to the same state regardless of the current state. This simplifies the mapping of observations to actions without requiring memory. In contrast, Bowling and Freeway have only one observation symbol in the minimal MM, ignoring input images for action selection. Freeway's policy always takes the Up action, showing a simple open-loop controller behavior. The MM extraction approach provides insight into the policy structures of Atari games like Bowling, Breakout, Space Invaders, and Boxing. It reveals open-loop structures and the use of memory and observations in decision-making. Further semantic analysis and visualization are needed for a comprehensive understanding of the policies. Our approach extracts finite state Moore Machines from RNN policies to better understand memory use. By training Quantized Bottleneck Networks to produce binary encodings of memory and input features, we insert bottlenecks into the RNN to create a discrete Moore machine for analysis. This allows for a deeper understanding of policy structures in Atari games like Bowling, Breakout, Space Invaders, and Boxing. Our approach involves extracting finite state Moore Machines from RNN policies by training Quantized Bottleneck Networks to produce binary encodings of memory and input features. This discrete Moore machine allows for a deeper understanding of policy structures in Atari games and accurately extracts ground truth in known environments. The learned MMNs maintain similar performance to the original RNN policies and provide insights into memory usage. The study focuses on extracting finite state Moore Machines from RNN policies to understand policy structures in Atari games. It highlights the small number of memory states and observations required and identifies cases where memory was not used significantly. Future work includes developing tools for attaching meaning to observations and analyzing finite-state machine structures for further insight. The study analyzes formal properties of policies represented by a Markov Chain Environment (MCE). The MCE is defined by mode number, transition function, mode lifespan mapping, and count set. The hidden state consists of the current mode and count of time-steps in that mode. Mode transitions occur based on lifespan and transition distribution. Observations are continuous-valued and based on the current state. The agent receives continuous-valued observations based on the current state. Observations determine the mode when the mode count is in C, otherwise, they are uninformative. The agent must remember the current mode and use memory to keep track of how long the mode has been active for optimal performance. Experiments are conducted with different MCE instances, including Amnesia with random initial mode and transition distributions. The experiments involve three MCE instances: Amnesia, Blind, and Tracker. Amnesia tests reactive policies without memory, Blind tests memory usage for deterministic mode sequences, and Tracker examines the extraction of MMN for recurrent policies. The experiments involve three MCE instances: Amnesia, Blind, and Tracker. Tracker is similar to Amnesia but allows for larger \u2206(m) values. It requires an optimal policy to pay attention to observations and use memory to keep track of the current mode and mode count. This can result in challenging problems as the number of modes and their life-spans increase."
}