{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. This operator can be integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM is applied to mobile service traffic and air quality forecasting using point-cloud streams. Results show accurate long-term predictions, outperforming other neural network models. Point-cloud stream forecasting predicts future values from geospatial point-cloud data sources like mobile network antennas and air quality sensors. CloudLSTM is utilized for forecasting mobile service traffic and air quality using point-cloud streams, outperforming other neural network models. Point-cloud stream forecasting predicts future values from geospatial data sources like mobile network antennas and air quality sensors. PointCNN is a model proposed for forecasting over point-cloud data streams, leveraging spatial-local correlations of point clouds without pre-processing. It aims to handle scattered point-cloud data, unlike RNN models that are limited to grid-structural data. The proposed PointCNN leverages spatial-local correlations of point clouds, regardless of input order. CloudLSTM architecture is introduced for precise forecasting over point-cloud streams, combining Seq2seq learning and attention mechanisms. The proposed PointCNN leverages spatial-local correlations of point clouds for precise forecasting over point-cloud streams, emphasizing order invariance and key properties for ideal forecasting models. The stream forecasting model for point-cloud applications should embrace five key properties: order invariance, information intactness, interaction among points, robustness to transformations, and location variance. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of CloudLSTM, satisfying key properties such as robustness to transformations and location variance. DConv generalizes convolution on point-clouds, computing weighted summations over point-clouds while inheriting properties of ordinary convolution on grids. The Dynamic Point Cloud Convolution (DConv) operator, a core module of CloudLSTM, performs weighted summations over point-clouds, inheriting properties of ordinary convolution on grids. DConv operates on point-clouds, ensuring information intactness by outputting the same number of elements as input. The DConv operator in CloudLSTM performs weighted summations over point-clouds, maintaining information integrity by outputting the same number of elements as input. Q K n is a subset of points in S i, including the K nearest points to p n in Euclidean space. Learnable weights W are 5D tensors shared across different anchor points. The DConv operator in CloudLSTM utilizes learnable weights shared across anchor points to aggregate coordinate features for exploiting spatial correlations. The weights are defined as 5D tensors and are used to calculate scalar weights for input and output channels, as well as nearest neighbors. Sigmoid function limits predicted coordinates to (0, 1) to prevent outliers. The DConv operator in CloudLSTM uses learnable weights shared across anchor points to aggregate coordinate features for exploiting spatial correlations. The coordinates of raw point-clouds are normalized to (0, 1) before feeding them to the model for improved transformation robustness. The K nearest points can vary for each channel at each location, reflecting different types of measurements in the pointcloud dataset. The DConv operator in CloudLSTM utilizes learnable weights to aggregate coordinate features for capturing spatial correlations in different datasets. Spatial correlations vary between measurements due to human mobility, impacting data consumption of each app or air quality indicator. CloudLSTM does not fix the K nearest neighbors across channels, allowing each channel to find the best neighbor set, improving forecasting performance. The DConv operator weights its K nearest neighbors across all features to produce values and coordinates in the next layer. The DConv operator in CloudLSTM utilizes learnable weights to aggregate coordinate features for capturing spatial correlations in different datasets. It weights its K nearest neighbors across all features to produce values and coordinates in the next layer, improving forecasting performance. The operator is symmetric and operates on every point in a set, capturing local dependencies and improving robustness to global transformations. The DConv operator in CloudLSTM captures local dependencies and improves robustness to global transformations by operating over neighboring point sets and normalizing coordinate features. It learns the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step, essential for spatiotemporal forecasting neural models. The DConv operator in CloudLSTM captures local dependencies and improves robustness to global transformations by operating over neighboring point sets and normalizing coordinate features. It learns the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step. DConv can be efficiently implemented using simple 2D convolution, parallelized easily in existing deep learning frameworks. It builds upon PointCNN and Deformable Convolution, introducing variations tailored to pointcloud structural data. The DConv operator in CloudLSTM improves robustness to global transformations by capturing local dependencies in point clouds. It ensures order invariance without extra complexity or information loss, aligning weights based on distance rankings. DConv can be seen as a version of DefCNN for point clouds, deforming input maps instead of weighted filters. The DConv operator in CloudLSTM improves robustness to global transformations by capturing local dependencies in point clouds. It deforms input maps and aligns weights based on distance rankings. CloudLSTM is formulated similarly to ConvLSTM, with input, forget, and output gates, memory cell, hidden states, and learnable weight and bias tensors. The CloudLSTM model includes input, forget, and output gates, memory cell, hidden states, and learnable weight and bias tensors. It combines CloudLSTM with Seq2seq learning and soft attention mechanism for forecasting, proven effective in spatiotemporal modeling on grid-structural data. The Seq2seq CloudLSTM model incorporates an encoder and decoder with CloudLSTMs, utilizing the soft attention mechanism for forecasting. Data is processed by CloudCNN layers before generating predictions. In this study, a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is employed for forecasting. The data is processed by Point Cloud Convolutional layers, similar to word embedding in NLP tasks. Additionally, new models like Convolutional Point-cloud RNN and Convolutional Point-cloud GRU are explored, without employing the attention mechanism used in CloudLSTM. The study compares the performance of CloudRNN and CloudGRU with CloudLSTM in forecasting mobile service demands and air quality indicators. 12 baseline deep learning models are also compared using TensorFlow and TensorLayer libraries. In this study, 12 baseline deep learning models are implemented using TensorFlow and TensorLayer libraries. The models are trained on a computing cluster with two NVIDIA Tesla K40M GPUs and optimized using the Adam optimizer. Experiments are conducted on spatiotemporal point-cloud stream forecasting tasks over 2D geospatial environments. Coordinate features are omitted in the final output due to fixed locations in the data sources. Mobile Traffic Forecasting experiment conducted on large-scale multi-service datasets collected in two European metropolitan areas over 85 days. Data includes traffic volume from devices associated with 792 and 260 antennas in the cities, expressed in Megabytes aggregated over 5-minute intervals. Antennas are non-uniformly distributed, forming 2D point clouds over space. Coordinate features are excluded in the final output. The dataset consists of 260 antennas in two cities, forming 2D point clouds over urban regions. Traffic volume is measured in Megabytes at each antenna, aggregated over 5-minute intervals for 38 mobile services. Air quality forecasting is done using a public dataset with six air quality indicators collected by 437 monitoring stations in China. The dataset includes 8,760 snapshots for each of the two city clusters in China, collected from 437 air quality monitoring stations over a year. The data is used for experiments after filling missing values with linear interpolation. Measurements are transformed into input channels for models, with coordinate features normalized to (0, 1) range. For baseline models requiring grid input, point-clouds are transformed into grids using the Hungarian algorithm. The dataset consists of 8,760 snapshots from air quality monitoring stations in China, with missing values filled using linear interpolation. Coordinate features are normalized to (0, 1) range, and point-clouds are transformed into grids for baseline models. CloudLSTM is compared with PointCNN, CloudCNN, and PointLSTM for feature extraction from point-clouds. The CloudLSTM model is compared with various baseline models like MLP, CNN, LSTM, and ConvLSTM for accuracy using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Additionally, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are also considered for evaluating the model's performance on mobile traffic snapshots. The CloudLSTM model is evaluated for accuracy using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for mobile traffic prediction. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are used to assess the fidelity of forecasts. Neural networks forecast city-scale mobile traffic consumption over a time horizon of 30 minutes, with an extension to 3 hours for long-term performance evaluation. The CloudLSTM model, along with other RNN-based models, is evaluated for long-term performance in air quality forecasting. Different numbers of prediction steps are tested, and the influence of neighboring points and attention mechanism is investigated. RNN-based architectures show superior performance compared to CNN-based models and MLP. The CloudLSTM, CloudRNN, and CloudGRU variants outperform other architectures in air quality forecasting, showing better performance with lower MAE/RMSE and higher PSNR/SSIM. CloudLSTM performs better than CloudGRU, which outperforms CloudRNN. The forecasting performance of CloudLSTM is not significantly affected by the number of neighbors (K), suggesting the use of a small K to reduce model complexity. The attention mechanism improves forecasting performance by capturing better features over geospatial point-clouds. The CloudLSTM's forecasting performance is not greatly impacted by the number of neighbors (K), indicating the use of a small K to simplify the model. The attention mechanism enhances forecasting by capturing dependencies between input sequences and vectors in decoders. Long-term forecasting evaluation shows reliable performance with minimal MAE growth over prediction steps for most models in city 1. In city 1, long-term forecasting shows reliable performance with minimal MAE growth for most models. In city 2, a low K may affect CloudLSTM's long-term performance. CloudLSTMs outperform ConvLSTM in air quality forecasting by up to 12.2% in MAE and 8.8% in RMSE. Lower K values improve prediction performance, and CloudCNN consistently performs well. The CloudLSTM models outperform ConvLSTM in air quality forecasting by up to 12.2% in MAE and 8.8% in RMSE. Lower K values improve prediction performance, and CloudCNN consistently performs well as a feature extractor over point-clouds. Performance evaluations for long-term forecasting up to 72 future time steps are conducted on RNN-based models. The study conducted experiments using strict variable-controlling methodology to analyze the performance of various models like LSTM, ConvLSTM, PredRNN++, PointLSTM, and CloudLSTM. The D-Conv was found to significantly contribute to performance improvements. Comparisons between CloudRNN, CloudGRU, and CloudLSTM showed CloudLSTM outperformed the others. The introduction of CloudLSTM, a neural model for spatiotemporal forecasting tailored to pointcloud data streams, was highlighted. CloudLSTM is a dedicated neural model for spatiotemporal forecasting on pointcloud data streams. It utilizes the DConv operator for convolution over point-clouds, predicting values and coordinates of each point while adapting to changing spatial correlations. DConv can be combined with various RNN models and attention mechanisms efficiently. The DConv operator is efficiently implemented using a standard 2D convolution operator for spatiotemporal forecasting on point-cloud data streams. It transforms input and output tensors into 3D tensors and performs convolution in multiple steps, including reshaping and applying the sigmoid function. The DConv operator efficiently implements a standard 2D convolution for spatiotemporal forecasting on point-cloud data streams. It transforms tensors into 3D tensors, reshapes the output, and applies the sigmoid function for optimized convolution operations. The complexity of DConv is analyzed by separating the operation into finding neighboring sets and performing weighting computations. The complexity of the DConv operator for spatiotemporal forecasting on point-cloud data streams involves finding K nearest neighbors and computing features of the output points. This introduces extra complexity by searching for neighbors, but the overall complexity remains manageable even with higher dimensional point clouds. DConv introduces complexity by searching for K nearest neighbors for each point, but remains manageable even with higher dimensional point clouds. Normalizing coordinates enables transformation invariance to shifting and scaling. CloudLSTM is combined with an attention mechanism for encoder and decoder states. In (Luong et al., 2015), the encoder and decoder states are denoted as H j en and H i de. The context tensor for the encoder state i is represented with a score function e i,j. Our proposal was compared against baseline models like MLP, CNN, 3D-CNN, DefCNN, LSTM, ConvLSTM, and PredRNN++. The study discusses LSTM as an advanced RNN for time series forecasting, with ConvLSTM as a baseline model and PredRNN++ as the state-of-the-art architecture for spatiotemporal forecasting on grid-structural data. CloudRNN and CloudGRU share a similar Seq2seq architecture with CloudLSTM, but without the attention mechanism. The detailed configuration and number of parameters for each model are shown in Table 3. In this study, various models were considered for time series forecasting, including ConvLSTM, PredRNN++, and PointLSTM. The models used 2 layers, with 3x3 filters for image applications. The PredRNN++ has a different structure compared to other Seq2seq models. Different configurations of CloudLSTM with varying receptive fields were evaluated, along with an attention mechanism. All architectures were optimized using the MSE loss function. The study evaluated different models for time series forecasting, including CloudLSTM with attention mechanism. The models were optimized using the MSE loss function and evaluated using MAE, RMSE, PSNR, and SSIM metrics. The study evaluated various time series forecasting models, including CloudLSTM with an attention mechanism, using MSE loss function and metrics like MAE, RMSE, PSNR, and SSIM. Antenna locations in two cities were anonymized for data collection via deep packet inspection at the packet gateway. Traffic classifiers were used to associate flows with services, with details withheld for confidentiality reasons. The study collected anonymized mobile service traffic data from antenna locations in two cities under supervision of privacy agencies. The dataset used for analysis includes 38 different services and does not contain personal subscriber information. Due to confidentiality agreements, the raw data cannot be publicly shared. The study collected anonymized mobile service traffic data from antenna locations in two cities, comprising 38 different services. The analysis revealed that streaming is the dominant type of traffic, accounting for almost half of the total consumption. Other services like web, cloud, social media, and chat also consume significant fractions of mobile traffic. The study analyzed mobile traffic data, showing streaming as the main type of traffic, followed by web, cloud, social media, and chat services. Gaming only contributes 0.5% of the demand. The air quality dataset includes information from 43 cities in China, with 2,891,393 records from 437 monitoring stations over a year. Stations are divided into two clusters, A with 274 stations and B with 163. Missing data was filled using linear interpolation. The dataset can be accessed at https://www.microsoft.com/en-us/research/project/urban-air/. The dataset from the study on mobile traffic and air quality includes missing data filled through linear interpolation. The performance of Attention CloudLSTMs in forecasting accuracy is evaluated for individual mobile services, showing higher prediction errors for services with higher traffic volume due to more frequent fluctuations. The MAE evaluation is presented on a service and category basis in Fig. 8, with similar performance observed over both cities. The MAE for long-term air quality forecasting on city clusters using RNN-based models shows higher prediction errors for services with higher traffic volume due to more frequent fluctuations. Larger K values in CloudLSTM improve robustness, with slower MAE growth over time. Visualizing hidden features provides insights into mobile traffic forecasting. The CloudLSTM model shows slower MAE growth over time with K = 9, consistent with mobile traffic forecasting results. Hidden features visualization in CloudLSTM provides insights into the knowledge learned by the model, with scatter distributions shown in Fig. 10 for encoders and decoders. Input data snapshots are from City 2 with 260 antennas/points. Figure 11 displays NO2 forecasting examples in City Cluster A generated by all RNN-based models. In Fig. 10, scatter subplots show hidden features in CloudLSTM for City 2. Fig. 11 and 12 display NO2 forecasting examples in City Clusters A and B by RNN-based models, highlighting the superior performance of CloudLSTMs in capturing trends and delivering long-term visual fidelity. The proposed architectures, particularly Attention CloudLSTMs, show better prediction capabilities in capturing trends in point-cloud streams and maintaining high long-term visual fidelity compared to other architectures. The use of DConv with Sigmoid functions helps regulate coordinate features of points, allowing them to move closer together for improved computation. CloudLSTM, with stacked DConv structures via LSTM, enhances representability and refines the positions of input points at each time step, enabling each point to move to its optimal position. The CloudLSTM model, with stacked DConv structures via LSTM, enhances representability and refines the positions of input points at each time step. It demonstrates strong performance in forecasting with outlier points, as shown in the air quality dataset using DBSCAN to identify outliers. The model achieves the lowest prediction error compared to other models, including CloudCNN. Our CloudLSTM model maintains low prediction errors even with outlier points, outperforming other models like CloudCNN. The CloudCNN with DConv operator shows the best forecasting performance among CNN-based models. Experiments with controlled scenarios show the robustness of CloudLSTM to outliers. Moving outliers away from the center by different distances on both x and y axes, the CloudLSTM model maintains strong forecasting performance regardless of outlier proximity. Compared to PointLSTM, CloudLSTM shows superior results, proving its robustness to outliers. CloudLSTM performs well in forecasting both inliers and outliers, outperforming PointLSTM. Comparisons with simple baselines like MLPs and LSTMs show that CloudLSTM significantly outperforms models relying on k-nearest neighbors for forecasting. The number of neighbors K impacts the receptive field of each model. Our CloudLSTM outperforms MLPs and LSTMs in forecasting by incorporating local spatial dependencies through DConv kernels and merging global spatial dependency via stacks of time steps and layers. The number of neighbors K does not significantly affect the performance of each baseline model. Additionally, seasonal information in the mobile traffic series can be further utilized to enhance forecasting accuracy. The CloudLSTM model outperforms MLPs and LSTMs in forecasting by incorporating local spatial dependencies through DConv kernels and merging global spatial dependency via stacks of time steps and layers. Seasonal information in the mobile traffic series can be utilized to improve forecasting performance, but directly feeding the model with data spanning multiple days is impractical due to the large number of data points involved. To capture seasonal information more efficiently, 30 minute-long sequences are concatenated with a sub-sampled 7-day window. To efficiently capture seasonal information, 30-minute sequences are concatenated with a sub-sampled 7-day window, forming an input with length 90. Experiments on a subset of the mobile traffic dataset show improved forecasting performance with seasonal information. However, this increases model complexity, prompting future work on more efficient ways to incorporate seasonal information. The model learns periodic information to reduce prediction errors, but concatenation increases input length and model complexity. Future work aims to fuse seasonal information more efficiently with minimal complexity increase."
}