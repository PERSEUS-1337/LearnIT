{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. NPs efficiently fit observed data with linear complexity and can learn a wide family of conditional distributions. However, they suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data they condition on. To address this issue, attention is incorporated into NPs, allowing each input location to attend to relevant context points for prediction, resulting in improved accuracy, faster training, and an expanded range of functions that can be learned. Incorporating attention into Neural Processes (NPs) improves prediction accuracy, speeds up training, and expands the range of functions that can be modeled. Regression tasks can be approached by modeling the distribution of output given input using a deterministic function or by computing a distribution over functions. The latter approach allows for reasoning about multiple functions consistent with the data. Bayesian machine learning involves using training data to compute a distribution over functions for making predictions. Gaussian Processes (GPs) and Neural Processes (NPs) offer efficient methods for modeling regression functions. NPs can predict the distribution of target outputs based on context input-output pairs, allowing for flexible modeling of data generated from a stochastic process. NPs and GPs have different training regimes. Neural Processes (NPs) can model data generated from a stochastic process based on context input-output pairs. Unlike Gaussian Processes (GPs), NPs are trained on samples from multiple realizations of a stochastic process, leading to different training regimes. However, NPs tend to underfit the context set, resulting in inaccurate predictive means and overestimated variances. Neural Processes (NPs) underfit the context set, leading to inaccurate predictive means and overestimated variances. The encoder aggregates context to a latent summary, causing a bottleneck in the mean-aggregation step. Increasing dimensionality may help address this issue. The encoder bottleneck in Neural Processes (NPs) makes it hard for the decoder to learn which context points are relevant for target predictions. Drawing inspiration from GPs, NPs implement differentiable attention to attend to relevant context points for accurate predictions. Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by using differentiable attention to attend to relevant contexts for accurate predictions. ANPs show enhanced expressiveness, better reconstruction of contexts, and faster training compared to NPs. They are able to model a wider range of functions and are more efficient in terms of both iterations and wall clock time. The Neural Process (NP) is a regression model that maps input to output. It defines conditional distributions based on observed contexts and targets, with permutation invariance. The model uses a deterministic function to aggregate context pairs into a finite representation, typically passed through an MLP and aggregated by mean. The Neural Process (NP) model includes a deterministic function that aggregates context pairs into a finite representation with permutation invariance. The likelihood is modelled by a Gaussian factorised across targets, with a global latent variable z accounting for uncertainty in predictions. The Neural Process (NP) model incorporates a global latent variable z to account for uncertainty in predictions. The model includes a latent path modelled by a factorised Gaussian parametrised by s, with q(z|s) as the prior on z. The encoder and decoder components, q, r, s, are used to model different realisations of the data generating stochastic process. The model can be defined using just the deterministic path, just the latent path, or both, with the most expressive model achieved by using both paths. The Neural Process (NP) model incorporates a global latent variable to handle prediction uncertainty. By using both deterministic and latent paths, the model achieves expressiveness and allows for attention incorporation. Parameters are learned through maximizing ELBO for a subset of contexts and targets, encouraging context and target summaries to be close. Random selection of contexts and targets during training helps the NP learn effectively. Neural Processes (NPs) learn a wide range of conditional distributions by randomly selecting contexts and targets at each training iteration. They offer scalability, flexibility, and permutation invariance but lack consistency in contexts. The distribution of targets may not match when generated with different context sets. Neural Processes (NPs) offer scalability, flexibility, and permutation invariance in learning conditional distributions. However, they lack consistency in contexts, as the distribution of targets may vary based on the order of generation. Maximum-likelihood learning minimizes the KL divergence between the conditional distributions of the data-generating process and the NPs. Attention mechanisms compute weights for key-value pairs to allow queries to attend to specific information. An attention mechanism computes weights for key-value pairs to allow queries to attend to specific information. The permutation invariance property of attention is crucial in its application to Neural Processes (NPs). Differentiable addressing mechanisms have been successfully applied in Deep Learning, such as handwriting generation, recognition, and neural machine translation. Self-attention has been used for expressive sequence-to-sequence mappings in natural language processing and image modeling. The paper discusses the use of self-attention for sequence-to-sequence mappings in natural language processing and image modeling. Different attention mechanisms, such as locality-based and dot-product attention, are explained using key-value pairs and queries. Dot-product attention allows for efficient computation of query values with matrix multiplications and softmax. The multihead attention architecture extends dot-product attention by linearly transforming keys, values, and queries for each head. This allows the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. Self-attention is applied to compute representations of context points, and cross-attention is used for the target input to attend to these context representations. The attentive neural processes use self-attention to compute representations of context points, allowing the target input to predict the target output by attending to these context representations. The self-attention helps model interactions between context points and encode relations, with higher order interactions achieved by stacking self-attention mechanisms. The model uses self-attention to capture relations between context points, allowing each query to focus on relevant context points for prediction. The latent path preserves global dependencies for target predictions. The latent path in the model preserves global dependencies for target predictions, with z giving rise to correlations in the marginal distribution of the target predictions y T. The decoder remains the same, with the shared context representation r C replaced by the query-specific representation r *. ANP is trained using the same loss as the original NP, with added expressivity. The ANP model introduces attention mechanism to improve accuracy, but at the cost of increased computational complexity. Despite the higher complexity, training time for ANPs remains comparable to NPs and ANPs learn significantly faster. The ANP model introduces attention mechanism to improve accuracy and learns significantly faster than NPs. Training time remains comparable despite increased computational complexity. The (A)NP learns a stochastic process and should be trained on multiple functions. The (A)NPs are trained on synthetic GP data generated from a Gaussian Process with varying hyperparameters. The number of contexts and targets are randomly chosen at each iteration, and cross-attention is used in the deterministic path for 1D function regression. The (A)NPs are trained on synthetic GP data with varying hyperparameters. Cross-attention is used in the deterministic path for 1D function regression, showing rapid decrease in reconstruction error and lower values at convergence compared to NP, especially for dot product and multihead attention. Learning is fast despite added computational cost. Laplace and dot-product ANP have similar computation times to NP, while multihead ANP takes around the same time for the same value of d. The (A)NPs are trained on synthetic GP data with varying hyperparameters, showing rapid decrease in reconstruction error and lower values at convergence compared to NP. Computation times of Laplace and dot-product ANP are similar to NP, while multihead ANP takes around twice the time. Raising the bottleneck size in NPs helps achieve better reconstructions, but there is a limit to improvement. ANPs offer significant benefits over simply increasing the bottleneck size in NPs. In FIG2 (right) the learned conditional distribution is visualized for a qualitative comparison of attention mechanisms. The predictive mean of the NP underfits the context, while dot-product attention accurately predicts almost all context points. Laplace attention, being parameter-free, shows similar behavior. Keys and queries are parameterized representations of x-values for dot-product attention. The dot-product attention outperforms Laplace attention due to computed similarities in a learned representation space. Multiple heads in multihead attention help smooth out interpolations and increase predictive uncertainty away from contexts like a GP. The dot-product attention outperforms Laplace attention in a learned representation space. Multiple heads in multihead attention help smooth out interpolations and increase predictive uncertainty away from contexts. The (A)NP is more expressive than the NP, learning a wider range of functions. Trained (A)NPs tackle a toy Bayesian Optimization problem, sampling entire functions and providing accurate context reconstructions. See Appendix C for details and analysis of results. The task involves finding the minimum of test functions from a GP prior. The experiment demonstrates the ability to sample entire functions from the (A)NP and achieve accurate context reconstructions. Image data regression is performed on 2D functions, treating images as generated from a stochastic process. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. See Appendix C for more details and results analysis. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Results of three different models are shown on both datasets: NP, Multihead ANP, and Stacked Multihead ANP. Predictions of the full image with varying numbers of random context pixels are generated. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Results of three different models are shown on both datasets: NP, Multihead ANP, and Stacked Multihead ANP. Predictions of the full image with varying numbers of random context pixels are generated. The NP gives reasonable predictions with diversity for fewer contexts, but the reconstructions of the whole image are not accurate compared to Stacked Multihead ANP. Attention helps achieve crisper inpaintings, enhancing the ANP's ability to model less smooth 2D functions. The diversity in faces and digits obtained with different values of z is apparent in the different samples, providing evidence for the claim that z can model global structure of the image. The diversity in faces and digits obtained with different values of z is apparent in the samples, supporting the claim that z can model the global structure of the image. The model generalizes well even with limited context points, showing improved context reconstruction error and NLL for target points with multihead crossattention. Stacked self-attention enhances crispness and global coherence in the predictions. In FIG7, each head of Multihead ANP for CelebA is visualized, showing different roles for each head in attending to pixels. The cyan head focuses only on the target pixel, while other heads look at nearby regions or specific areas of the image. The use of stacked self-attention improves crispness and global coherence in the predictions. The Multihead ANP for CelebA visualizes different roles for each head in attending to pixels. The heads focus on nearby regions or specific areas of the image, with one head exploiting symmetry. The model can map images from one resolution to another by predicting pixel intensities in a continuous space. The Multihead ANP for CelebA visualizes different roles for each head in attending to pixels, with one head exploiting symmetry. The model can predict pixel intensities in a continuous space, allowing for mapping images from one resolution to a higher resolution. ANPs may provide accurate reconstructions for reliable mappings between different resolutions, as shown in FIG6. The ANP is capable of mapping low resolutions to realistic 32 \u00d7 32 target outputs with some diversity for different values of z. The ANP, trained on 32 \u00d7 32 images, can map low resolutions to realistic 32 \u00d7 32 outputs with diversity. It can also map to higher resolutions like 256 \u00d7 256, producing sharper images with internal representations of features like eyes. The ANP can map low resolutions to realistic 32 \u00d7 32 outputs with diversity and higher resolutions like 256 \u00d7 256, producing sharper images with internal representations of features like eyes. It learns to fill in the eye even when the original image is too coarse to separate the iris from the sclera, showcasing its flexibility in modeling a wide family of conditional image applications. The ANP is not meant to replace state-of-the-art algorithms for image inpainting or super-resolution. Instead, it showcases flexibility in modeling various conditional distributions. Attention in NPs is similar to GP kernels, measuring similarity between points in the same domain. The use of attention in an embedding space is related to Deep Kernel Learning, where a GP is applied to learned data representations. In an embedding space, attention is used similarly to GP kernels to measure similarity between points. Learning in a GP framework involves maximizing the marginal likelihood, but the training regimes of GPs and NPs differ. Predictive uncertainties in GPs heavily rely on kernel choice, while NPs learn uncertainties directly. Comparing the methods directly is challenging due to these differences. Variational Implicit Processes (VIP) BID19 are related to Neural Processes (NPs), where VIP approximates the process and its posterior with a Gaussian Process (GP). Meta-Learning (A)NPs are models designed for few-shot learning tasks. The process and its posterior are approximated by a Gaussian Process in a stochastic process using a finite dimensional z. Meta-Learning (A)NPs focus on few-shot learning tasks, with works using attention for tasks in Meta-RL such as continuous control and visual navigation. Few-shot density estimation using attention has also been explored extensively in various works. Attention has been used for tasks in Meta-RL such as continuous control and visual navigation. Few-shot density estimation using attention has also been explored extensively in numerous works. The Neural Statistician and the Variational Homoencoder have a similar permutation invariant encoder but use local latents on top of a global latent. Multitask learning has also been tackled in the GP literature by various works. Generative Query Networks are models for spatial prediction. In the GP literature, multitask learning has been addressed without attention mechanisms. Generative Query Networks are models for spatial prediction, with a special case of Neural Processes where attention is applied to resolve underfitting issues. This approach improves prediction accuracy, training speed, and expands the range of functions that can be handled. ANPs augment NPs with attention to address underfitting issues, improving prediction accuracy, training speed, and expanding the range of functions that can be modeled. Future work includes incorporating cross-attention into the latent path, training ANPs on text data, and exploring connections with the Image Transformer (ImT) BID21 for image applications. The Image Transformer (ImT) BID21 has connections with ANPs, using self-attention to predict pixel blocks. By replacing the MLP in the ANP decoder with self-attention, a model similar to ImT can be created. ANPs will be equipped with self-attention in the decoder to explore their expressiveness further. Target ordering and grouping will be crucial in this setup. In this setup, targets affect each other's predictions, making target ordering and grouping important. Architectural details of NP and Multihead ANP models for regression experiments are shown. Latent path outputs parameterize distributions, and experiments use multihead cross-attention. The text discusses the parameterization of distributions in regression experiments using multihead cross-attention. Self-attention and cross-attention modules are compared, with the former being stacked in 2 layers for the Stacked Multihead ANP in 2D Image regression experiments. In the 2D Image regression experiments, 2 layers of self-attention are stacked for Stacked Multihead ANP. Stacking more layers did not show significant improvements. Parameters for the squared exponential kernel include a length scale of 0.6 and kernel scale of 1. Batch size is 16, and Adam Optimiser BID14 with a fixed learning rate of 5e-5 is used. In the 2D Image regression experiments, Stacked Multihead ANP uses 2 layers of self-attention. Parameters for the squared exponential kernel include a length scale of 0.6 and kernel scale of 1. A batch size of 16 is used with Adam Optimiser BID14 at a fixed learning rate of 5e-5. 16 random hyperparameter values are sampled to draw curves from GPs. The predictions of Multihead ANP are closer to the oracle GP than NP, but still underestimate predictive variance. Variational inference used in learning ANP tends to underestimate predictive variance, requiring further investigation for resolution. The conditional distributions for fixed and random kernel hyperparameters in dot-product attention show non-smooth behavior, resembling a nearest neighbor predictor. The KL term in NP loss differs between training on fixed and random kernel hyperparameter GP data. The KL term in NP loss varies between training on fixed and random kernel hyperparameter GP data. In the fixed hyperparameter case, the KL quickly goes to 0, indicating the model finds the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, there is added variation in the data, leading to a non-zero KL as the model uses latents to model uncertainty in the stochastic process. The model uses latents to model variation in the stochastic process and tackles the Bayesian optimization problem by considering all previous function evaluations as context points. Thompson sampling is used to act according to the minimal predicted value. Results show consistent performance in predicting the true minimum. Thompson sampling is used to draw a simple function from the surrogate model and act based on its predicted minimal value. Results show that the simple regret is smallest for a NP with multihead attention, approaching the oracle GP. The cumulative regret decreases most rapidly for multihead, indicating efficient use of previous function evaluations. The lower cumulative regret compared to the oracle GP is due to under-exploration caused by smaller uncertainties of ANP away from the context. The study uses Thompson sampling to select functions from a surrogate model, with results showing that a NP with multihead attention has the smallest simple regret and efficient use of previous evaluations. The uncertainties of ANP away from the context lead to under-exploration. The experiments involve selecting random pixels as targets and contexts, with rescaled x and y values. The architecture used is a stacked self-attention model without Dropout or positional embeddings, with specific hyperparameters for MNIST and CelebA datasets. The study utilizes a stacked self-attention architecture without Dropout or positional embeddings for both MNIST and CelebA datasets. It uses one sample of q(z|s C ) to estimate the loss and visually shows that the NP overestimates predictive variance. The NP with attention reduces uncertainty significantly as the number of contexts increases, with Stacked Multihead ANP improving results over Multihead ANP. The study shows that the NP with attention reduces uncertainty as the number of contexts increases, with Stacked Multihead ANP improving results significantly over Multihead ANP. The different roles of heads in predicting targets are highlighted, with all heads becoming useful when the context is disjoint from the target. Visualizations in FIG0 demonstrate the effectiveness of multihead attention in NP. Visualisation of pixels attended by each head of multihead attention in the NP for target prediction, with different colours representing each head and the target pixel marked by a cross."
}