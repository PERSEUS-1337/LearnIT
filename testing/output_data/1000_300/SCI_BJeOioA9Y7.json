{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The teachers and student can have different structures and be trained on different tasks. The student becomes independent of the teachers after training, outperforming fine-tuning and other knowledge exchange methods in various learning tasks. Research communities have developed numerous deep net architectures for various tasks, with some trained from scratch and others fine-tuned using structurally similar networks. In reinforcement learning, approaches like progressive neural nets, PathNet, 'Growing a Brain,' and Actor-mimic involve utilizing multiple teachers or pre-training models on source tasks. PathNet BID6, 'Growing a Brain' BID30, Actor-mimic BID20, and Knowledge distillation BID9 are techniques for learning new tasks. However, these methods have limitations such as computational intensity and parameter constraints. For example, progressive neural net models BID23 have limitations due to the number of teachers they can handle. The limitations of current techniques for learning new tasks, such as PathNet BID6, 'Growing a Brain' BID30, and Actor-mimic BID20, include computational intensity and parameter constraints. Progressive neural net models have restrictions on the number of teachers they can handle. To address these issues, a new approach called knowledge flow has been developed, allowing for the transfer of knowledge from multiple teachers to a student during training. This method ensures the student becomes independent by the end of training, regardless of the number of teachers used, and maintains a constant size for the resulting student network. This framework is versatile and can be applied to various tasks, from reinforcement learning to fully-supervised training. Our framework allows for knowledge transfer from multiple teachers to a student during training, ensuring independence and maintaining a constant network size. It is applicable to various tasks, including reinforcement learning and fully-supervised training. The goal of reinforcement learning is to find a policy that maximizes the expected future reward from each state. The paper follows the asynchronous advantage actor-critic (A3C) formulation to optimize policy parameters for maximizing future rewards in reinforcement learning. The policy mapping and value function are modeled using deep nets with parameters \u03b8 \u03c0 and \u03b8 v. The optimization involves a loss function based on negative log-likelihood and negative entropy regularizer, aiming to maximize the empirical k-step return. The optimization process in reinforcement learning involves using a negative entropy regularizer and squared loss to maximize the expected return. Warm-start techniques can be applied instead of optimizing from scratch, and a proposed method called knowledge flow addresses shortcomings in the existing approach. Knowledge flow is a framework that transfers knowledge from multiple deep nets (teachers) to a deep net being trained (student) to improve learning efficiency. The student initially relies heavily on teacher one's weights, which gradually decrease as training progresses. This process is illustrated using example deep nets in a figure. The student's parameters are randomly initialized, while the teachers' parameters are fixed and obtained from pre-trained models. The student becomes independent as knowledge from multiple teachers is transferred to improve learning efficiency. Teacher parameters are fixed and obtained from pre-trained models, while student parameters are randomly initialized. Teacher representations are added to the student net through a trainable matrix and weight scaling. The student model is enhanced by incorporating teacher representations through a trainable matrix and weight scaling. The normalized weights determine which representations to trust at each layer, allowing the student to benefit from the teachers' knowledge. Ultimately, the student should perform well on the target task independently of the teachers. The student model relies on teacher knowledge initially but becomes more independent as training progresses. Two additional loss functions encourage the student to master tasks on its own, ensuring it no longer depends on teachers by the end of training. The dependency loss captures how much a student relies on teachers, while the second loss ensures the student's behavior remains stable when teacher influence decreases. These modifications, along with additional loss terms, lead to different program transformations for supervised and reinforcement learning tasks. The parameters \u03b8 and \u03b8 old are used to control the strength of the transformations. The tilde ('\u00b7') denotes dependence on probability and policy distributions. Parameters \u03b8 and \u03b8 old control teacher influence strength. \u03bb 1 and \u03bb 2 regulate teacher influence decrease. Initially, a low \u03bb 1 allows student reliance on teachers, gradually increasing independence. Proposed method reduces negative transfer effects by decreasing teacher layer weight. Despite differing objectives, students may benefit from teacher's low-level knowledge. The proposed method quickly decreases the weight for teacher layers to reduce negative transfer effects. Despite differences, students could benefit from the low-level representation of teachers. The modification of deep nets and the use of loss functions dep and KL help decrease the influence of teachers in the student model. The candidate set for each layer in the student model includes the teachers' layers to be considered. A normalized weight is introduced to decide which representation to trust at every layer of the student net. The candidate set for each layer in the student model includes the teachers' layers to be considered. A normalized weight is introduced to decide which representation to trust at every layer of the student net. The combined intermediate representation of layer j for the student model is obtained using a maximal number of introduced matrices Q in the framework. In practice, a student's layer is not linked to every layer of a teacher network. The results of state-of-the-art methods A3C, PPO, and ACKTR are also discussed. In practice, it is recommended to link one teacher layer to one or two student layers, introducing matrices Q for this purpose. Additional trainable parameters Q and w are introduced in the framework to help the student learn faster, but they are not part of the resulting student network. In the framework, additional parameters Q and w are introduced to help the student learn faster. These parameters function as auxiliary knobs that gradually decrease the influence of teachers during training. By minimizing the dependence cost, the student is encouraged to become more independent, with weights for the student's layers increasing. Ultimately, in the final stage of training, the student no longer relies on Q, w, or any transformed representations from teachers. The dependence cost is introduced as the negative log probability to encourage the student to become more independent by increasing weights for its layers. Minimizing this cost leads to the student being less reliant on teacher transformations. To prevent a fast decrease in teacher influence that could degrade performance, a Kullback-Leibler regularizer is used in supervised learning. In supervised and reinforcement learning tasks, a Kullback-Leibler regularizer is used to prevent rapid changes in the student model's output distribution. Knowledge flow is evaluated using only the student model to avoid influence from teacher nets. In reinforcement learning with Atari games, the agent learns to predict actions based on rewards and input images from the environment. The agent learns to predict actions based on rewards and input images from the environment using a fully forward architecture with three hidden layers. The model includes a convolutional layer, a fully connected layer, and outputs for action probabilities and value function estimation. The hyper-parameter settings are similar to previous work, except for the learning rate. The third hidden layer includes a softmax output for action probabilities and a scalar output for value function estimation. Hyper-parameter settings are similar to previous work, with the exception of using Adam with shared statistics instead of RMSProp. The learning rate is set to 10^-4 and gradually decreased to zero. \u03bb1 and \u03bb2 are selected using a method from BID23, with experiments repeated 25 times and the top three results reported. The learning rate gradually decreased to zero during training. Experiments were repeated 25 times with different random seeds and \u03bb values. Results from the top three runs were reported. Evaluation was done by playing each game for 30 episodes, following the 'no-op' procedure. Comparison with PathNet and PNN frameworks was conducted, with state-of-the-art results on Atari games included for reference. The comparison between state-of-the-art transfer reinforcement learning frameworks shows that our transfer framework outperforms PathNet and PNN in various experiments. With one teacher, our student model achieves higher scores in 11 out of 14 experiments compared to PathNet. Additionally, with two teachers, our student model with 0.7M parameters outperforms PNN with 16M parameters in five out of seven experiments. The results demonstrate effective knowledge transfer from teachers to the student, with performance improvement when the number of teachers increases. Training curves for the experiments are shown in FIG1, with the curve representing the average of the top three runs out of 25. In our framework, increasing the number of teachers from one to two significantly improves student performance across all experiments. Training curves in FIG1 show our approach performing well. Different environment/teacher settings were tested, summarized in TAB2, with our implementation achieving better scores than PathNet and progressive neural network. In our framework, knowledge flow with expert teachers outperforms fine-tuning on a non-expert teacher. The student model in knowledge flow can learn from multiple teachers, avoiding negative impacts from insufficiently pretrained teachers. The student model benefits from knowledge flow by learning from multiple teachers, avoiding negative impacts from insufficiently pretrained teachers. This approach outperforms fine-tuning, as shown in training curves. The student can benefit from intermediate representations of teachers, even if input/output spaces differ. For example, in the experiment with Chopper Command and Space Invaders as teachers, the student achieves scores ten times larger than learning without a teacher or fine-tuning. The student model benefits from learning from teachers, achieving scores ten times larger than learning without a teacher. Various image classification benchmarks are used for supervised learning, with parameters determined using validation sets. Evaluation metrics include reporting top-1 error rate on test sets. The study reported average results from three runs using different random seeds on CIFAR-10 and CIFAR-100 datasets. Training and test sets for both datasets contain 50,000 and 10,000 images respectively. Densenet (depth 100, growth rate 24) was used as a baseline model. Teachers were trained on CIFAR-10, CIFAR-100, and SVHN datasets, and the student model was trained using different teacher combinations. Fine-tuning from CIFAR-100 improved performance by 4% over the baseline on the CIFAR-10 task. Knowledge flow improves performance by 13% over the baseline model when using a combination of good and inadequate teachers for the CIFAR-10 target task. Fine-tuning from CIFAR-100 expert outperforms the baseline, while fine-tuning from SVHN expert performs worse. Similar results are observed on the CIFAR-100 dataset. Additional details on knowledge flow properties are provided in the appendix, contrasting with other 'knowledge' transfer techniques. In contrast to previous methods like PathNet and Progressive Net, our approach ensures independence of the student during training, addressing limitations in existing techniques. Our method utilizes lateral connections similar to BID23 but maximizes student capacity utilization. Our method ensures independence of the student during training, unlike BID23. Distral combines distill & transfer learning for joint training of multiple tasks, sharing a distilled policy for common behavior. Knowledge flow focuses on a single task transfer of information, while multi-task learning boosts performance by sharing information from different tasks. In multi-task learning, information from different tasks is shared to improve performance. Knowledge flow leverages information from multiple teachers to help a student learn a new task. Various related works include actor-mimic, learning without forgetting, and policy distillation. A general knowledge flow approach allows training a deep net from multiple teachers, showing improvements in reinforcement and supervised learning compared to training from scratch. The general knowledge flow approach allows training a deep net from multiple teachers, showing improvements in reinforcement and supervised learning compared to training from scratch. Experiments were conducted on MNIST, MNIST with digit '3' missing, CIFAR-100, and ImageNet using teacher and student models with different parameters. Future plans include learning when to use which teacher and how to actively swap teachers during student training. The teacher model is an MLP with two hidden layers of 1200 hidden units, and the student model is an MLP with two hidden layers of 800 hidden units. For CIFAR-100, the teacher model is from Chen FORMULA2, while for ImageNet, the teacher model is a 50-layer ResNet BID8 and the student model is an 18-layer ResNet. The distilled student model outperforms KD due to benefiting from both the output layer behavior and intermediate layer representations of the teacher. The 'EMNIST Letters' dataset consists of 28x28 pixel images showing handwritten letters with 26 balanced classes of lower and upper case letters. The 'EMNIST Letters' dataset contains 28x28 pixel images of handwritten letters with 26 balanced classes. The training and test sets have 124,800 and 20,800 images respectively. The 'EMNIST Digits' dataset has 10 balanced classes with 240,000 training and 40,000 test images. Teachers were trained on different datasets, and the student model's performance was compared to baseline and state-of-the-art results on EMNIST. In the study, the student model was trained with various teachers on EMNIST Letters dataset. Results showed improved performance compared to baseline and fine-tuning methods. The STL-10 dataset, with 10 classes, was used for training. Teachers were trained on CIFAR-10 and CIFAR-100 datasets. In the experiment, 5,000 labeled images were used for training on the STL-10 dataset. Teachers were trained on CIFAR-10 and CIFAR-100 datasets. Results showed that pretraining on CIFAR-10 and CIFAR-100 reduced test errors by more than 10%. Student model training in their framework further reduced test error by 3%. In our framework, using weights pretrained on CIFAR-10 and CIFAR-100 reduced test errors by over 10%. Student model training further decreased test error by 3%. Results are based on training only on labeled data, making direct comparisons with other approaches challenging. We also compared to Distral BID26 in multi-task reinforcement learning on Atari games with three tasks. In the experiments on Atari games, Distral is suboptimal for learning a multi-task agent due to its inability to decrease teacher influence when the target task differs significantly from the source tasks. In contrast, the framework presented can reduce a teacher's influence and mitigate negative transfer, as shown in the results summarized in TAB5. Our framework can decrease a teacher's influence to reduce negative transfer, as demonstrated in the C10 experiment with C100 and SVHN experts as teachers. The C100 teacher had a higher normalized weight than the SVHN teacher, confirming its relevance to C10. Ablation studies with untrained teachers showed worse performance, validating the benefit of learning from knowledgeable teachers. Our experiments verify that learning with untrained teachers results in worse performance compared to learning with knowledgeable teachers. Knowledge flow achieves higher rewards than training with untrained teachers in different environments and teacher-student settings. The KL term prevents drastic changes in the student's output distribution when the teachers' influence decreases. The KL term prevents drastic changes in the student's output distribution when teachers' influence decreases. Ablation study shows that without the KL term, rewards drop significantly, while with the KL term, performance remains stable. Training with the KL term achieves higher rewards compared to training without it. Training with the KL term achieves higher rewards compared to training without it. Using different architectures for the teacher and student models, with the teacher model having 3 convolutional layers and the student model having 2 convolutional layers. Both models have fully connected layers followed by output layers for actions and values. In experiments, teachers with different architectures from the student model were used, with 3 convolutional layers for teachers and 2 for the student. The target task was KungFu Master, with teachers being experts in Seaquest and Riverraid. Results showed similar performance between teachers with different and same architectures. Learning with teachers of different architectures achieved higher rewards compared to learning with teachers of the same architecture in the target task of KungFu Master. Using an average network for parameter updates also showed improved performance in the experiment. In an experiment, using an exponential average to compute \u03b8 old results in similar performance as using a single model. For instance, in the Boxing task with a Riverraid expert teacher, using an average network achieves an average reward of 96.2 compared to 96.0 with a single network. Other techniques for knowledge transfer include fine-tuning, progressive neural nets, PathNet, and 'Growing a Brain'. In contrast to PathNet BID6, our approach involves multiple teacher nets trained for transfer learning and lifelong learning. Progressive Net BID23 leverages transfer and avoids catastrophic forgetting by introducing lateral connections. Our method introduces scaling with normalized weights to ensure independence of the student during training, addressing a limitation in Progressive Net BID23. Distral, a combination of 'distill & transfer learning', involves joint training of multiple tasks with a shared policy to encourage consistency between different tasks. BID26 explores joint training of multiple tasks with a shared policy to promote consistency. Unlike Distral, which focuses on multi-task learning, knowledge flow transfers information from multiple teachers to help a student learn a new task. Knowledge distillation BID9 involves distilling information from a larger deep net into a smaller one, allowing knowledge transfer between different sources. Our technique enables knowledge transfer between different domains, allowing an agent to learn multiple tasks simultaneously. It utilizes a combination of feature regression and cross entropy loss to encourage the student to produce similar actions and representations. Additionally, Learning without forgetting BID13 allows adding a new task to a deep net without losing original capabilities, using only data from the new task. Our proposed technique involves leveraging a teacher's initial representation during training to prevent forgetting of original capabilities when adding a new task to a deep neural network. This approach only uses data from the new task while retaining old capabilities by recording the old network's output on the new data. This method differs from others by explicitly transferring knowledge from teacher networks."
}