{
    "title": "BkecJjCEuN",
    "content": "The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask and self-supervised learning. Training an end-to-end audio feature extractor based on WaveNet, the study demonstrates improved performance in supervised classification tasks by up to 6% through simultaneous training with self-supervised tasks. Incorporating data augmentation further enhances the multitask setting. Incorporating self-supervised tasks and data augmentation into training can significantly boost the performance of supervised classification tasks by up to 6%. Deep neural networks are crucial for modeling auditory data, but the scarcity of labeled training data poses a challenge. Leveraging unlabeled data for unsupervised learning shows promise in addressing this issue. Incorporating self-supervised tasks and data augmentation into training can significantly boost the performance of supervised classification tasks. The scarcity of labeled training data poses a challenge, but leveraging unlabeled data for unsupervised learning shows promise in addressing this issue. This problem is exacerbated when training directly on the acoustic waveform, where input is high-dimensional and noisy. The aim is to develop a technique that enables models to generalize better by incorporating auxiliary self-supervised auditory tasks during model training. The main contributions are the successful identification of appropriate self-supervised audio-related tasks and demonstrating that they can be trained jointly with supervised tasks to improve performance. WaveNet is used as a general feature extractor capable of providing rich audio representations using raw waveform data as input. By learning multi-scale hierarchical representations from raw audio, WaveNet-based models can adapt effectively. WaveNet is utilized as a feature extractor for rich audio representations from raw waveform data. The framework explores supervised classification tasks like audio tagging, speaker identification, and speech command recognition. Leveraging unlabeled data and self-supervised tasks improves performance and can be used for pre-training and transfer learning. Self-supervised tasks can enhance performance through transfer learning. Models trained for multiple tasks may uncover underlying structures, leading to better single-task performance with less data. Multitask learning aims to create a general-purpose representation. Other approaches exist for shared representations in models. Self-supervised learning in audio domain is underexplored. An end-to-end audio processing network was implemented to leverage self-supervision for common representation discovery. The implementation of an end-to-end audio processing network utilizes self-supervision for common representation discovery. The network is modeled after the WaveNet architecture and consists of trunk and head networks trained jointly for various experiments. The WaveNet trunk includes 3 blocks of 6 dilation stacks with 64 convolutional units per module. The WaveNet trunk consists of 3 blocks of 6 dilation stacks with 64 convolutional units per module, yielding an effective receptive field length of 190 samples. It was tested on audio tagging, speaker identification, and speech command recognition tasks using labeled and unlabeled datasets. The audio tagging task was trained on the FSDKaggle2018 dataset. The audio tagging task is trained on the FSDKaggle2018 dataset, which contains 11,073 audio files. Each audio segment is cropped to 2 seconds and padded with zeros if needed. The WaveNet trunk produces embeddings that are averaged across time to create a single output vector for the entire sequence. This output feeds into a fully-connected layer with 512 units and ReLU nonlinearity, followed by a softmax output layer for classification. Training is done by minimizing cross entropy between the softmax outputs and classification labels. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The architecture includes a global average pooling layer and a 2-layer neural network for classification. The speech command recognition task involves cropping audio clips to 2 seconds, normalizing them, and applying a pre-emphasis filter. The dataset consists of 65,000 utterances of 30 short words in one-second WAVE format files, with 12 categories including words like yes, no, up, down, left, right, on, off, stop, go, and others classified as unknown or silence. The recognition head features a stack of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The speech command recognition task involves 10 words and unknown/silence classification. The recognition head consists of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. Three convolution layers have widths of 100, 50, and 25 with strides of 16, 8, and 4. Self-supervised tasks include next-step prediction, noise reduction, and upsampling, trained on Librispeech dataset BID15. The auxiliary tasks in the multitask framework were trained on unlabeled data from the Librispeech dataset BID15. They share a common head architecture with convolutional layers and a regression-type loss function. The focus was on using waveform inputs for audio processing tasks instead of spectrograms to allow for a broader range of tasks. The focus is on using waveform inputs for audio processing tasks to broaden the range of tasks that can be performed. Multitask learning is emphasized for improved performance compared to single task baseline models trained on raw audio. Closing the performance gap between models trained on spectral representations and those trained on waveforms is a future research direction. Multitask learning improves performance compared to single task models trained on raw audio. Including additional unsupervised tasks without more data boosts audio tagging task scores. Incorporating larger unlabeled datasets further enhances performance metrics. Including versions of Librispeech into our training regimen improved performance metrics, with a MAP@3 increase of up to .056 with an additional 500 hours of unlabeled data. Swapping the audio tagging task with speech command classification or speaker identification also showed improvement with increasing amounts of unlabeled data. Speech command classification increased from 93.05% to 93.78%, while speaker identification performance peaked at 75.22% on the VoxCeleb dataset. These results demonstrate that multitask learning can enhance supervised task performance without additional labeled data. Multitask learning improved performance without additional labeled data. Comparing it to data augmentation techniques, pitch shifting showed a MAP@3 increase of .066, while noise augmentation had a smaller increase of .024. The gains from noisy data augmentation were similar to those from self-supervised training. Training with pitch-shift augmentation showed a MAP@3 increase of .066, while noise augmentation resulted in a smaller increase of .024. Combining pitch-shift augmentation with self-supervised tasks yielded the highest performance increase of .089, indicating complementary methods for improving label efficiency. Transfer learning from self-supervised tasks trained on unlabeled data was considered to enhance multitask learning approach. Transfer learning from self-supervised tasks trained on unlabeled data was found to be more effective than simultaneously training all tasks together. By pre-training self-supervised tasks on unlabeled data and then fine-tuning with a small amount of labeled data, performance gains were achieved in supervised tasks using a WaveNet-based model on raw audio waveforms. By jointly training supervised and self-supervised tasks on raw audio waveforms, performance gains scale with the quantity of unlabeled data. This approach can supplement existing data augmentation schemes and generalize to various audio tasks. Further development directions include exploring the limit of auxiliary tasks for model benefit and setting bounds on performance improvement. The study explores the benefits of multitasking models in processing raw audio waveforms, focusing on the representation formed during tasks like audio forecasting, noise removal, and upsampling. The WaveNet architecture is chosen for its ability to handle high temporal resolution tasks, enabling the model to be sufficient for various auditory tasks. The study focuses on the benefits of multitasking models using the WaveNet architecture for processing raw audio waveforms with high temporal resolution. WaveNet models employ causal dilated convolutions for parallel processing, making them faster to train compared to RNNs. The model consists of a task-agnostic trunk following the WaveNet structure, with task-specific heads for different tasks. The WaveNet model utilizes stacked dilated causal convolutions in its structure, with task-specific heads for different tasks. The trunk consists of N blocks, each containing S dilated causal convolution layers with increasing dilation factors. Each layer in the trunk involves a \"residual atom\" computation with \"Filter\" and \"Gate\" operations. The WaveNet model utilizes stacked dilated causal convolutions in its structure, with N blocks and S layers in each block. Each layer computes a hidden state vector and layer output using various operations. The total effective receptive field of the trunk is 1+N(2S-1), with a total receptive field of 190 for the chosen configuration. The trunk of the WaveNet model has a total receptive field of 190, equivalent to 12 milliseconds of audio sampled at 16kHz. Each task-specific head processes input data through the shared trunk independently. Tasks have their own objective functions, optimizers, and learning rates. Supervised tasks are primary, while self-supervised tasks are auxiliary. \"Audio tagging\" is used as the primary supervised classification task in the experiments. In the experiments, customized learning rates and annealing schedules were used for supervised and self-supervised tasks. The primary task was \"audio tagging\" while auxiliary tasks included \"next-step prediction\", \"noise reduction\", and \"upsampling\". The head architectures were designed to be simple to allow the shared trunk to learn representations for multiple audio tasks. The next-step prediction task involves predicting the next value in a sequence of audio frames. The next-step prediction task involves predicting the next value in a sequence of audio frames using a 2-layer stack of 1 \u00d7 1 convolutional layers with ReLU nonlinearities. The head takes \u03c4 frames of data from the trunk and produces an output representing the model's prediction for the next audio frame. The next-step prediction task involves using a 2-layer stack of 1 \u00d7 1 convolutional layers to predict the next audio frame. The head treats this as a regression problem, computing the mean squared error between predicted and actual values as the loss function. The original WaveNet implementation treated this task as a classification problem, but treating it as regression worked better in multitask situations. In defining the noise reduction task, noise is treated as an additive random process on top of the true signal. The model is trained to predict the clean sample given a window of noisy samples. The noise reduction head has a structure similar to the next-step head and is trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. Our noise reduction head, with a structure similar to the next-step head, is trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. The smooth L1 loss is preferred for stability in denoising tasks over mean squared error. Additionally, an unsupervised upsampling task can be created by downsampling the audio source, with the downsampled signal as input and the original source as the target. This upsampling task is akin to the \"super-resolution\" task in computer vision. The upsampling task involves downsampling the original audio to 4 kHz and repeating every time-point 4 times to mimic a 16 kHz sample rate. The network infers high frequency information lost during the transform, using a structure similar to next-step prediction and noise reduction tasks. A smooth L1 loss function is used to compare the estimated upsampled audio with the original. The study used an upsampling head with a structure similar to previous tasks, employing a smooth L1 loss function to compare the estimated upsampled audio with the original. The model was trained on raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets, with all code written in PyTorch. Inputs were cropped to two seconds, downsampled to 16 kHz, and normalized. Noise-reduction task involved adding noise from ChiME3 datasets at random SNR levels. The study involved scaling inputs to [-1, 1] and adding noise from ChiME3 datasets for a noise-reduction task. Hyperparameter search was done for the main task, varying the number of blocks and layers per block. The network's performance was not significantly affected by architecture specifications. Additionally, a search was conducted for the depth and width of each auxiliary task head. The study focused on hyperparameter search for the main task, exploring different block and layer configurations. The network's performance was found to be largely unaffected by architecture specifications. Additionally, a search was conducted for the depth and width of each auxiliary task head, with a final choice of hyperparameters made based on performance on both main and auxiliary tasks. The model was jointly trained on all tasks simultaneously using a uniform weighting strategy for the loss function calculation. In the study, the model was trained on multiple tasks simultaneously with a uniform weighting strategy for the loss function calculation. The Adam optimizer was used with specific parameters, and the learning rate was decayed every 5 epochs. A batch size of 48 was utilized, and separate forward propagation was required for noise reduction and upsampling tasks. Important model parameters can be found in TAB3."
}