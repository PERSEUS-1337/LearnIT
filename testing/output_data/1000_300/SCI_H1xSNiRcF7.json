{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models for learning from uncertain supervision and inferring soft-inclusions among concepts. The Box Lattice model showed promising results in modeling soft-inclusions through high-dimensional hyperrectangles but faced optimization challenges. This work presents a novel hierarchical embedding model inspired by the Box Lattice model. In this work, a novel hierarchical embedding model is introduced, inspired by the Box Lattice model. The model uses Gaussian convolutions over boxes to improve optimization in the disjoint case and maintain desirable properties. Performance improvements are demonstrated on various tasks, especially in sparse data scenarios. The text discusses hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. Embedding methods, such as Word2Vec, have been key in machine learning, converting semantic problems into geometric ones. Recent interest lies in structured or geometric representations, associating objects with complex geometric structures like density functions in Gaussian embeddings. The text discusses structured or geometric representations for modeling transitive relations and complex joint probability distributions. Geometric objects like density functions, convex cones, and hyperrectangles are used to express asymmetry, entailment, and ordering more naturally than simple points in a vector space. The focus is on the probabilistic Box Lattice model for its empirical performance and ability to model negative correlations. The text discusses the Box Embeddings (BE) model as a generalization of order embeddings, focusing on its ability to model complex joint probability distributions. However, the \"hard edges\" of boxes in BE present challenges for gradient-based optimization, especially with sparse data. The text discusses challenges with gradient-based optimization in the Box Embeddings (BE) model due to disjoint boxes with overlap in ground truth. To address this, a new model is proposed based on smoothing the hard edges of boxes into density functions using Gaussian convolution. This approach shows superiority in modeling transitive relations on WordNet and Flickr caption entailment. The new model proposes smoothing hard edges of boxes into density functions using Gaussian convolution, demonstrating superiority in modeling transitive relations on WordNet and Flickr caption entailment. It extends the box embedding model and shows improvements in the pseudosparse regime. The probabilistic extension of the box lattice model by BID9 and the deterministic box embedding model by BID22 were discussed. Another hyperrectangle-based generalization of order embeddings known as box embeddings was also introduced. Additionally, hyperbolic space models for learning hierarchical embeddings were mentioned, which optimize an energy function and are nonprobabilistic. Our approach involves smoothing the energy landscape of hierarchical embedding models using Gaussian convolution, a technique common in mollified optimization. This method is increasingly being used in machine learning models such as Mollifying Networks, diffusion-trained networks, and noisy activation functions. Our focus is on embedding orderings and transitive relations, a subset of knowledge graph embedding, with a probabilistic approach. Our focus is on embedding orderings and transitive relations in knowledge graph embedding. We seek to learn an embedding model mapping concepts to subsets of event space, suited for transitive relations and fuzzy concepts. We introduce methods for representing ontologies as geometric objects using order theory and vector and box lattices. FIG0 illustrates these representations. A non-strict partially ordered set (poset) is defined by a set P and a binary relation. In order theory, posets generalize totally ordered sets to allow incomparable elements. Lattices are posets with unique least upper and greatest lower bounds. Bounded lattices have additional top and bottom elements. Lattices have join (\u2228) and meet (\u2227) operations. A lattice is a poset with unique least upper and greatest lower bounds, equipped with join (\u2228) and meet (\u2227) operations. Bounded lattices have additional top and bottom elements. The extended real numbers and sets partially ordered by inclusion form bounded lattices. The dual lattice can be obtained by swapping \u2227 and \u2228 operations. A semilattice has either a meet or join operation, but not both. In the context of lattices, the dual lattice can be obtained by swapping \u2227 and \u2228 operations. A vector lattice, also known as a Riesz space, is a vector space endowed with a lattice structure. The standard choice of partial order for the vector lattice R n is to use the product order from the underlying real numbers. The Order Embeddings of BID20 represent partial orders as vectors using the reverse product order, creating a lattice structure. Objects are embedded as vectors that become more specific as they move away from the origin. A toy example in FIG0 illustrates this concept in a two-dimensional space. The box lattice introduced by Vilnis et al. in the probabilistic extension of BID9 represents concepts in a knowledge graph using axis-aligned hyperrectangles. The lattice structure includes least upper bounds and greatest lower bounds, with operations for scalar coordinates. The lattice meet and join operations define the intersection and combination of boxes, respectively. The lattice meet is the largest box contained within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are associated with the volume of boxes under a suitable probability measure. The probability p(x) is determined by the interval boundaries of the associated box for event x. The use of the uniform measure requires the boxes to be constrained to the unit hypercube. When using gradient-based optimization to learn box embeddings, an issue arises when two concepts are incorrectly labeled as disjoint, leading to zero gradient signal flow due to the meet being zero. This problem is especially problematic in sparse lattices where most boxes have little to no intersection. The authors propose a surrogate function to optimize in cases of disjoint intervals in sparse lattices, aiming to avoid gradient sparsity and improve model quality. The authors introduce a new approach to address gradient sparsity in sparse lattices by relaxing the assumption of \"hard edges\" in standard box embeddings. They rewrite the joint probability measure of intervals as an integral of indicator functions, aiming to improve optimization and preserve geometric intuition. The authors propose a new approach to address gradient sparsity in sparse lattices by replacing indicator functions with functions of infinite support using kernel smoothing. This method aims to improve optimization and preserve geometric intuition. The authors introduce a new method for addressing gradient sparsity in sparse lattices by using kernel smoothing to replace indicator functions with functions of infinite support. This approach aims to enhance optimization and maintain geometric intuition. The method is demonstrated in one dimension, showing the evaluation of lattice elements with smoothed indicators using a closed form solution. In the zero-temperature limit, the formula from the approximation of \u03a6 by a logistic sigmoid is recovered. However, multiplication of Gaussian-smoothed indicators does not give a valid meet operation on a function lattice, violating the idempotency requirement. This complicates applications that train on conditional probabilities. In the case of indicator functions, violating the idempotency requirement complicates applications that train on conditional probabilities. A modification of equation 3 can retain smooth optimization properties while ensuring p(x \u2227 x) = p(x). The hinge function m h satisfies a specific identity, but not the softplus function. However, a similar functional form equation is true for both the hinge function and the softplus. The softplus function, unlike the hinge function, satisfies an equation with a similar form. In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for overlapping intervals, while equation 3 is not. This leads to defining probabilities using equation 7. In the context of defining probabilities using equation 7, the text discusses the normalization of the softplus function for the interval case. Two approaches to normalization are mentioned, one involving unconstrained learning and division by measured sizes, and the other involving projection onto a unit hypercube and normalization by a specific function. The final probability is calculated as a product over dimensions. The final probability p(x) is calculated by projecting onto the unit hypercube and normalizing by m soft. This function, similar to the Gaussian model, is not a valid probability measure on the entire joint space of events. However, it retains the inductive bias of the original box model and satisfies the necessary condition that p(x, x) = p(x). Comparing different functions in FIG2, the softplus overlap shows better behavior for highly disjoint boxes than the Gaussian model while preserving the meet property. The softplus overlap function in FIG2 outperforms the Gaussian model for highly disjoint boxes while maintaining the meet property. Experiments on the WordNet hypernym prediction task show that the smoothed box model performs nearly as well as the original box lattice in terms of test accuracy. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, requiring less hyper-parameter tuning. Further experiments are conducted using different numbers of positive and negative examples from the WordNet mammal subset to confirm the performance in the sparse regime. The training data for the WordNet mammal subset includes 1,176 positive examples, with 209 positive examples in the dev/test sets. Negative examples are generated randomly. The Smoothed Box model outperforms OE and Box models on imbalanced data, important for real-world entailment graph learning. The Smoothed Box model outperforms OE and Box models on imbalanced data, which is crucial for real-world entailment graph learning. Experimental results on the WordNet mammal subset and Flickr entailment dataset are provided, showing superior performance of the Smoothed Box model. The Smoothed Box model, using the unit cube constraint and softplus function, shows improved performance on unseen captions. Additionally, the model is applied to a market-basket task using the MovieLens dataset to predict user preferences for movies. In the MovieLens-20M dataset, pairs of user-movie ratings higher than 4 points are collected to analyze preferences. A subset of movies with over 100 user ratings is selected, resulting in 8545 movies. Conditional probability P(A|B) is calculated and compared with various baseline models. Different embedding methods are used for target and conditioned movies, with additional parameters for the \"imply\" relation in the complex bilinear model. Evaluation metrics include KL divergence, Pearson correlation, and Spearman correlation. Our approach involves smoothing the energy and optimization landscape of probabilistic box embeddings, improving performance in Spearman correlation, a key metric for recommendation tasks. The smoothed model outperforms the original box lattice and other baselines, with a focus on robustness to initialization conditions. This model is easier to train due to fewer hyper-parameters. Appendix C presents an approach to smoothing the energy and optimization landscape of probabilistic box embeddings, which has shown to be effective in handling sparse data and poor initialization. The model requires fewer hyper-parameters, making it easier to train and has surpassed current state-of-the-art results on various datasets. The research acknowledges the ongoing challenges in learning problems posed by complex embedding structures and aims to explore function lattices and constraint-based approaches further. The research aims to explore function lattices and constraint-based approaches to learning in order to address challenges posed by complex embedding structures. The Gaussian overlap formula is evaluated for lattice elements x and y using smoothed indicators f and g, with a focus on the integral of the product of two functions. The approach presented in Appendix C smooths the energy and optimization landscape of probabilistic box embeddings, showing effectiveness in handling sparse data and poor initialization. The research explores function lattices and constraint-based learning to tackle challenges of complex embedding structures. The integral of the product of two functions is evaluated using the Gaussian overlap formula. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. Additional experiments test the robustness of the smoothed box model to initialization, focusing on the overlap of disjoint boxes. The research explores function lattices and constraint-based learning to tackle challenges of complex embedding structures. The integral of the product of two functions is evaluated using the Gaussian overlap formula. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. Additional experiments test the robustness of the smoothed box model to initialization, focusing on the overlap of disjoint boxes. The results show that the smoothed model is not affected by disjoint initialization, while the original box model's performance degrades significantly. The smoothed model shows resilience to disjoint initialization, unlike the original box model. Methodology and hyperparameter selection details are provided for experiments, with code available for reproduction. WordNet experiments involve evaluating the model on the development set and selecting the best parameters for optimal performance. The best development model is used to score the test set, with baseline models trained using parameters of BID22. Negative examples are generated randomly based on the ratio for each batch of positive examples. A parameter sweep is done for all models to choose the best result for each model. The experimental setup uses a single-layer LSTM architecture that reads captions and produces box embeddings. The model is trained for a large fixed number of epochs and tested on the development data at each epoch. Hyperparameters were determined on the development set. The model is trained for a large fixed number of epochs and tested on the development data at each epoch. Hyperparameters were determined on the development set. The best development model is used to report test set score. Optimization is stopped if the best development set score fails to improve after 200 steps."
}