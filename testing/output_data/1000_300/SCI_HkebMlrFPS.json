{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this study introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning and outperform strong baselines on benchmark datasets. The study introduces multi-mode codebook embeddings for phrases and sentences, which outperform strong baselines on benchmark datasets for unsupervised phrase similarity, sentence similarity, hypernym detection, and extractive summarization. These embeddings provide a more interpretable semantic representation compared to single-mode embeddings used in many NLP models. The study introduces multi-mode codebook embeddings for phrases and sentences, which outperform strong baselines on benchmark datasets for unsupervised phrase similarity, sentence similarity, hypernym detection, and extractive summarization. These embeddings represent each target word as multiple points or regions in a distributional semantic space by clustering words appearing beside the target word. The multi-mode representation of real property is illustrated as an example, where it can refer to real estate or a true characteristic in philosophic discussions. Previous approaches cluster neighboring words to discover senses, unlike topic modeling like LDA which clusters all words in the corpus. Extending multi-mode representations to phrases or sentences faces efficiency challenges due to the large number of unique phrases and sentences in a corpus. Extending multi-mode representations to phrases or sentences is challenging due to the large number of unique sequences in a corpus. The number of parameters for clustering-based approaches is significant, making it difficult to estimate and store. Our compositional model aims to predict cluster centers' embeddings from the sequence, unlike previous work that focused on finding clustering centers for observed co-occurring words. The previous work focused on finding clustering centers for observed co-occurring words, while our compositional model aims to predict cluster centers' embeddings from the sequence of words in the target phrase. This approach helps overcome the challenge of sparseness in co-occurring statistics and reduces the number of parameters needed to learn the compositional meaning of each sequence. In this work, a neural encoder and decoder are used to compress redundant parameters in local clustering problems. Instead of clustering co-occurring words at test time, a mapping between target sequences and cluster centers is learned during training. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. The proposed model uses a neural network to generate cluster centers in a specific order by matching predicted cluster centers with observed word embeddings using a coefficient matrix. This approach improves unsupervised phrase similarity tasks and can measure asymmetric relations like hypernymy without supervision. The model's multimode representation outperforms single-mode alternatives in sentence representation. The model proposed uses a neural network to generate cluster centers in a specific order, improving unsupervised phrase similarity tasks and measuring asymmetric relations like hypernymy without supervision. The multimode representation outperforms single-mode alternatives in sentence representation, as demonstrated in an extractive summarization experiment. The training setup, objective function, and architecture of the prediction mode are described in subsequent sections. The model utilizes a neural network to create cluster centers in a specific order, enhancing unsupervised phrase similarity tasks and measuring asymmetric relations like hypernymy. It outperforms single-mode alternatives in sentence representation, as shown in an extractive summarization experiment. The training setup, objective function, and architecture of the prediction mode are detailed in subsequent sections. The model represents each sentence as multiple codebook embeddings predicted by a sequence to embeddings model, encouraging the generation of codebook embeddings that can reconstruct co-occurring words while avoiding common topics that occur with every sentence. The model uses a neural network to create cluster centers for unsupervised phrase similarity tasks and asymmetric relations. It outperforms single-mode alternatives in sentence representation. The training setup, objective function, and architecture of the prediction mode are detailed. The model represents each sentence as multiple codebook embeddings predicted by a sequence to embeddings model. The goal is to cluster words that could possibly occur beside the sequence, rather than actual co-occurring words in the training corpus. The model focuses on predicting co-occurring words by learning from similar sequences, viewing them as sets rather than sequences. It considers word order in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in a pre-trained word embedding space, arranged into a matrix where each column is a normalized word embedding. The embeddings of co-occurring words are arranged into a matrix in a pre-trained word embedding space. Predicted cluster centers of the input sequence are represented as a matrix. The number of clusters is fixed to simplify the prediction model design. The reconstruction loss of k-means clustering in the word embedding space is calculated based on word cluster assignments. In the experimental section, the reconstruction loss of k-means clustering in the word embedding space is discussed. Non-negative sparse coding relaxes constraints by allowing positive coefficients, leading to diverse cluster centers generation in neural networks. This approach contrasts with k-means clustering, which tends to collapse to fewer modes. The models using a NNSC loss generate diverse cluster centers, while kmeans loss collapses to fewer modes. NNSC loss is smoother and easier to optimize for neural networks. The reconstruction error is defined with a hyper-parameter controlling sparsity. Multiple outputs and estimating permutation between prediction and ground truth words are computationally expensive. The proposed loss function is efficient as it minimizes L2 distance in a pre-trained embedding space and estimates M Ot using convex optimization. This allows for end-to-end training of the neural network while preventing it from predicting the same global topics regardless of input. Our method extends Word2Vec to encode compositional meaning and decode multiple embeddings using a neural network architecture similar to seq2seq models. The encoder maps input sequences to contextualized embeddings with similar co-occurring word distribution, while the decoder differs from typical seq2seq models. The encoder maps input sequences to contextualized embeddings with similar co-occurring word distribution. The decoder does not make discrete decisions and outputs a sequence of embeddings. Codebook embeddings are passed through different linear layers to capture different aspects. Removing attention on contextualized word embeddings from the encoder significantly impacts the model. The encoder maps input sequences to contextualized embeddings with similar word distribution. Codebook embeddings are passed through linear layers to capture different aspects. Attention on contextualized word embeddings from the encoder significantly impacts the model's performance. The framework is flexible, allowing for the use of different architectures and input features. The framework is flexible, allowing for the use of different architectures and input features. Codebook embeddings capture semantic facets of a phrase or sentence well, improving performances in unsupervised semantic tasks. The cased version of pre-trained GloVe embeddings is used for evaluation. Our model utilizes pre-trained GloVe embeddings for sentence and phrase representation, trained on Wikipedia 2016 with stop words removed. Noun phrases are considered in experiments, with boundaries extracted using POS tags. The models do not require additional resources like PPDB and are compared to baselines using raw text and sentence/phrase boundaries. This approach is practical for domains with limited resources. Our models do not require additional resources like PPDB and are compared to baselines using raw text and sentence/phrase boundaries. The models are trained with GloVe embeddings and limited by computational resources, resulting in underfitting after a week. Comparing to BERT, our models struggle to preserve syntax information and produce effective pretrained embeddings for downstream tasks. BERT is a strong baseline for evaluating phrase similarity, using a word piece model to address out-of-vocabulary issues. Semeval 2013 task 5(a) English and Turney 2012 are standard benchmarks for this evaluation. BiRD and WikiSRS are recent datasets with ground truth. The Semeval 2013 task involves distinguishing similar phrase pairs from dissimilar ones. Turney's approach aims to identify the most similar unigram to a query bigram. BiRD and WikiSRS datasets measure phrase relatedness and similarity. The model evaluates two scoring functions for phrase similarity, using contextualized word embeddings from a transformer encoder. The model evaluates two scoring functions for phrase similarity, using contextualized word embeddings from a transformer encoder. Our method computes the cosine similarity between phrase embeddings and compares performance with 5 baselines including GloVe Avg and Word2Vec Avg. Our method evaluates phrase similarity using contextualized word embeddings from a transformer encoder and compares performance with 5 baselines. Our models significantly outperform all baselines in 4 datasets, with strong performances in Turney verifying that our encoder incorporates word order information in producing phrase embeddings. Our models outperform baselines in 4 datasets, with strong performances in Turney confirming that our encoder incorporates word order information. Non-linearly composing word embeddings improves prediction of co-occurring word embeddings. Performance of Ours (K=1) is slightly better than Ours (K=10), supporting the finding that multi-mode embeddings may not always improve performance. Performance of Ours (K=10) remains strong compared to baselines, indicating similarity performance is not sensitive to the number of clusters. The performance of Ours (K=10) remains strong compared to baselines in sentence similarity tasks, indicating that similarity performance is not sensitive to the number of clusters. The STS benchmark is widely used for sentence similarity evaluation, where models predict semantic similarity scores between sentence pairs. Additionally, a benchmark called STSB Low compares performances on datasets with lower similarity scores. In addition to BERT CLS, BERT Avg, and GloVe Avg, our method is compared with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). A weighting method proposed by Arora et al. (2017) is applied in the STS benchmark, with \u03b1 set to 10^-4. Post-processing method from Arora et al. (2017) is used to remove the first principal component, denoted as GloVe SIF. The GloVe SIF method involves post-processing to remove the first principal component estimated from the training distribution. This method is compared with GloVe Prob_avg, which does not involve post-processing. The strong performance of average embedding suggests considering word embeddings in addition to sentence embeddings for measuring sentence similarity. Multi-facet embeddings allow for estimating word importance in predicting co-occurring words. The multi-facet embeddings enable the estimation of word importance by computing cosine similarity between words and predicted codebook embeddings. This importance weighting is then multiplied with original weighting vectors to generate results for different methods like Our Avg, Our Prob_avg, and Our SIF. The comparison of results is shown in Table 3, including Ours SC which matches topics between sets. The proposed method utilizes multi-facet embeddings to estimate word importance, which is then multiplied with original weighting vectors to generate results for different methods. Comparisons in Table 3 show that Ours SC outperforms WMD and BERT Avg, especially in STSB Low. The benefits of multi-mode representation are demonstrated by significantly better scores in Ours (K=10) compared to Ours (K=1). Additionally, using attention weighting boosts performance, particularly in STSB Low, without relying on the generalization assumption of the training distribution. The proposed method utilizes multi-facet embeddings to estimate word importance and generate results for different methods. A variant using a bi-LSTM as the encoder and a LSTM as the decoder performs worse than the transformer alternative. The model is applied to HypeNet for hypernymy detection, showing better performance than ST Cos. The approach ignores the order of co-occurring words in the NNSC loss. The hypernymy detection dataset is based on the assumption that co-occurring words of a phrase are less related to some of its hyponyms. The predicted codebook embeddings of a hyponym often reconstruct the embeddings of its hypernym better. An asymmetric scoring function is defined for detecting hypernyms, showing better performance compared to baselines. Our model for extractive summarization outperforms baselines by providing asymmetric similarity measurements. The objective is to discover a summary with normalized embeddings that reconstruct the word distribution in the document. The summary consists of sentences with embeddings optimized using a greedy selection method. This method allows for evaluating the embeddings of sentences and generating multiple codebook embeddings. Our model for extractive summarization outperforms baselines by providing asymmetric similarity measurements. The method allows for evaluating the embeddings of sentences and generating multiple codebook embeddings to represent each sentence in the document. Comparisons are made with alternative ways of modeling sentence aspects, such as average word embeddings and embeddings of all words in the sentences. The method denoted as W Emb normalizes the gain of the reconstruction loss by the sentence length. The method denoted as W Emb normalizes the gain of the reconstruction loss by the sentence length. Comparisons are made with alternative ways of modeling sentence aspects, such as average word embeddings and embeddings of all words in the sentences. The results on the testing set of CNN/Daily Mail are compared using F1 of ROUGE in Table 5. All methods choose 3 sentences by following the setting in Zheng & Lapata (2019). Unsup, No Order means the methods do not use the sentence position information in the documents. In evaluating unsupervised sentence embeddings, methods that do not rely on sentence position information are compared. Larger cluster numbers (K) lead to better results, with K=100 performing best after selecting 3 sentences. This suggests that a larger K is preferred in this context. Our method allows for setting a large cluster number K to improve performance. Topic modeling has been widely studied and applied for its interpretability and flexibility. Neural networks can be used to discover coherent topics, but our focus is on efficiently finding different sets of topics on small word subsets. Sparse coding on word embeddings is utilized for modeling multiple topics. Our goal is to efficiently discover different sets of topics on small word subsets by using sparse coding on word embedding space and parameterizing word embeddings with neural networks. Representing words as single or multiple regions in Gaussian embeddings helps capture asymmetric relations. Challenges include designing a neural decoder for a set rather than a sequence and addressing the dependency between elements. One challenge is designing a neural decoder for sets rather than sequences, focusing on measuring distances between ground truth and predicted sets. Different loss options like Chamfer distance are used, with the goal of reconstructing sets using fewer bases. Achieving permutation invariance for neural networks involves removing elements. Our goal is to efficiently predict clustering centers that reconstruct observed instances, overcoming efficiency challenges in learning multi-mode representations for long sequences. A neural encoder models compositional meaning. In this work, the authors address the challenges of learning multi-mode representations for long sequences like phrases or sentences. They use a neural encoder to model the compositional meaning and a neural decoder to predict codebook embeddings as the representation. The proposed models outperform BERT, skip-thoughts, and GloVe in unsupervised benchmarks. The proposed models can predict interpretable clustering centers for sequences, outperforming BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings excel with complex input sequences, while single-facet embeddings perform well with simpler inputs. Future work aims to create a model generating multi-facet embeddings for both phrases and sentences, evaluating it for supervised or semi-supervised tasks, and applying it to other unsupervised learning tasks like graph embedding or recommendation. The method involves using a simple model with pre-trained embeddings for supervised or semi-supervised tasks. The model is kept simple to converge quickly, without fine-tuning hyperparameters. The transformer architecture is similar to BERT, with a sparsity penalty weight of 0.4. Parameters like sentence size and co-occurring words are limited to enhance efficiency. The coefficient matrix \u03bb is set to 0.4. Maximal sentence size is 50, ignoring longer sentences. Maximal co-occurring words set to 30. Number of dimensions in transformers is 300. Different numbers of transformer layers for sentence and phrase representation. Dropout on attention varies. Window size d t is 5. Code will be released for more hyper-parameter details. Architecture and hyperparameters are determined by the model. The number of codebook embeddings K is determined by the performance of training data, with larger K needing longer training time. Skip-thoughts have a hidden embedding size of 600 and were retrained for 2 weeks. The model architecture and hyperparameters are determined by the validation loss of the self-supervised co-occurring word reconstruction task. The model requires longer training time, with 1 week being insufficient for convergence. Skip-thoughts have a hidden embedding size of 600 and were retrained for 2 weeks. Comparing with BERT Large, it performs better in similarity tasks but worse in hypernym detection. Increasing model size may improve performance, suggesting a potential future direction. BERT performs better in similarity tasks compared to hypernym detection. Increasing the model size may enhance performance, but the method discussed is still superior, especially in phrase similarity tasks. The hypothesis is that BERT's training method may not be optimal for short sequences like phrases. Comparisons with other baselines show that selecting shorter sentences may lead to poor performance. The performance of different unsupervised summarization methods is compared based on sentence length. The method Ours (K=100) outperforms W Emb (GloVe) and Sent Emb (GloVe) when summaries have similar length. W Emb (*) tends to outperform Sent Emb (*) when comparing summaries of similar length, but this comparison may not be fair as W Emb (*) can select more sentences. Preventing the selection of many short sentences may improve performance. In comparing unsupervised summarization methods based on sentence length, Ours (K=100) performs better than W Emb (GloVe) and Sent Emb (GloVe) for summaries of similar length. The figure suggests that Ours (K=100) is optimal for summaries under 50 words, while W Emb (BERT) is better for longer summaries. Combining our method with BERT may offer promising results for improved performance. The mixed results suggest that combining our method with BERT in a way might be a promising direction to get the best performance in this task. Visualizing predicted embeddings from 10 randomly selected sentences in the validation set shows the format of the file similar to Table 1, with embeddings visualized by the nearest five neighbors in a GloVe embedding space and their cosine similarities to the vector."
}