{
    "title": "rylwJxrYDS",
    "content": "We propose vq-wav2vec to learn discrete representations of audio segments through a self-supervised context prediction task. The algorithm uses clustering to quantize dense representations, enabling the application of NLP algorithms. Experiments show BERT pre-training achieves state-of-the-art results on TIMIT phoneme classification and WSJ speech recognition. Learning discrete speech representations has gained recent interest, with approaches including autoencoding and continuous speech representation learning. In this paper, the vq-wav2vec encoder learns discrete representations of speech through a context prediction task, allowing the application of NLP algorithms. The algorithm quantizes raw audio into dense representations and aggregates them into context representations for training acoustic models. This approach combines research on continuous speech representations and autoregressive models, leading to improved transcription outputs. The vq-wav2vec encoder learns discrete representations of speech through context prediction, enabling the application of NLP algorithms. Acoustic models are trained by quantizing raw audio with vq-wav2vec and using BERT on the discretized sequence. The discretization algorithm, vq-wav2vec, utilizes wav2vec loss and architecture to learn fixed-length audio segments. Gumbel-Softmax and online k-means clustering are used to choose discrete variables. BERT representations outperform log-mel filterbank inputs and dense wav2vec in experiments. The BERT model outperforms log-mel filterbank inputs and dense wav2vec representations on TIMIT and WSJ benchmarks. Discretization of audio allows for the application of NLP algorithms to speech data, such as using a sequence to sequence model for speech recognition over discrete audio tokens. WAV2VEC learns audio representations through a self-supervised context-prediction task, producing a representation for each time step. The WAV2VEC model learns audio representations through a self-supervised context-prediction task, using convolutional neural networks to produce representations for each time step at a rate of 100 Hz. The model is trained to distinguish future samples from distractors by minimizing contrastive loss, optimizing the loss function over different step sizes. The WAV2VEC model uses a self-supervised context-prediction task to learn audio representations. It optimizes the loss function over different step sizes and utilizes a step-specific affine transformation. BERT is a pre-training approach for NLP tasks that employs a transformer encoder model for text representation. Our approach, vq-wav2vec, learns vector quantized representations of audio data using a future time-step prediction task. It utilizes masked language modeling and next sentence prediction for training, following similar architectural choices as wav2vec. The model consists of two convolutional networks for feature extraction and aggregation, along with a quantization module to build discrete representations. The vq-wav2vec model learns vector quantized representations of audio data by mapping raw speech segments to dense features using an encoder network. These dense representations are then quantized into discrete indices and reconstructed. The quantization module utilizes Gumbel-Softmax and online k-means clustering techniques for efficient representation learning. Multiple vector quantizations are performed to prevent mode collapse. The Gumbel-Softmax enables selecting discrete codebook variables in a differentiable way. A linear layer followed by ReLU and another linear output logits for the Gumbel-Softmax. During training, probabilities for choosing variables are determined by uniform samples. The vector quantization approach of van den Oord et al. (2017) is an alternative. The Gumbel-Softmax allows for selecting codebook variables in a differentiable manner using uniform samples. The vector quantization method by van den Oord et al. (2017) offers an alternative approach. The final loss includes additional terms with a stop gradient operator and a hyper-parameter \u03b3. The final loss in the vector quantization method includes terms for future prediction, moving codebook vectors closer to encoder output, and ensuring encoder outputs are close to a centroid. Mode collapse issues have been addressed in the past by re-initializing codewords or adding regularizers to the loss function. The text describes a strategy to independently quantize partitions of a feature vector z, similar to product quantization, to address mode collapse issues in vector quantization. This approach results in larger dictionaries and improved downstream performance. The text discusses quantizing partitions of a feature vector to address mode collapse in vector quantization. It explains how to represent the full feature vector using indices and different VQ approaches for groups. Sharing codebook variables across groups yields competitive results. Training a vq-wav2vec model allows for discretizing audio data for algorithms requiring discrete inputs, such as BERT pre-training. Training a vq-wav2vec model allows for discretizing audio data, making it applicable to algorithms like BERT pre-training. By masking spans of consecutive discretized speech tokens during BERT training, we aim to improve speech recognition. To improve speech recognition, BERT training is modified by masking spans of consecutive discretized speech tokens. This method makes masked token prediction harder and enhances accuracy compared to masking individual tokens. Models are pre-trained on the full 960h of Librispeech and evaluated on TIMIT and Wall Street Journal datasets. The study evaluates models on TIMIT and Wall Street Journal datasets using ablations on a 100h subset with 36M tokens. Different phonemes are considered for TIMIT, while acoustic models are trained on 31 graphemes for WSJ. The models used have 34 \u00d7 10 6 parameters with specific encoder and aggregator configurations. The aggregator in the model consists of 12 layers with 512 channels, using skip connections between blocks. Training involves predicting 8 steps into the future with a context prediction loss, warming up for 500 steps, and using a cosine schedule for learning rate annealing. The batch size is 10, with random cropping of 150,000 frames for each example. The model is trained on 8 GPUs with a batch size of 10 and random cropping of 150,000 frames. A smaller model is used for experiments on the 100h Librispeech subset, trained for 40k updates. Gumbel-Softmax Models are utilized with 2 groups and 320 latents per group, with temperature annealing from 2 to 0.5 over 70% of updates. The linear layer in the model projects encoder features into 640 logits. Gumbel-Softmax generates one-hot vectors for each of the 2 groups. The temperature is annealed from 2 to 0.5 over 70% of updates. After training on 960h of Librispeech, 13.5k unique codeword combinations are obtained. k-means models with 2 groups and 320 variables per group are used. vq-wav2vec on full Librispeech yields 23k unique codewords. BERT base models have 12 layers, model dimension 768, inner dimension 3072, and 12 attention heads. The learning rate is warmed up over the first 10,000 updates. In den Oord et al. (2017), \u03b3 = 0.25 is a robust choice for balancing the VQ auxiliary loss. BERT base models have 12 layers, model dimension 768, inner dimension 3072, and 12 attention heads (Devlin et al., 2018). The learning rate is warmed up over the first 10,000 updates to a peak value of 1 \u00d7 10 \u22125, and then linearly decayed over a total of 250k updates. Training is done on 128 GPUs with a batch size of 3072 tokens per GPU, totaling 393k tokens. Each token represents 10ms of audio data. For ablations, a smaller setup with model dimension 512, FFN size 2048, 8 attention heads, and dropout 0.05 is used. Models are trained for 250k updates with a batch size of 2 examples per GPU. Wav2letter is used as the acoustic model (Collobert et al., 2016) and trained for 1k epochs on 8 GPUs for both TIMIT and WSJ using the auto segmentation criterion. The text discusses training a wav2letter acoustic model on WSJ using vq-wav2vec and BERT representations instead of log-mel filterbanks. Various language models are evaluated, including a 4-gram KenLM model and a character-based convolutional model. The models are tuned following a specific protocol and compared to results from the literature. The study compares wav2letter acoustic models trained on WSJ using vq-wav2vec and BERT representations instead of log-mel filterbanks. Different language models are evaluated, with vq-wav2vec combined with BERT achieving a new state-of-the-art WER of 2.34 on nov92. Table 1 demonstrates that vq-wav2vec combined with BERT training achieves a new state-of-the-art WER of 2.34 on nov92. Gains are most significant without using a language model, which is the fastest setting. The use of Gumbel-Softmax with vq-wav2vec only employs 13.5k distinct codewords for audio signal representation, enabling training of BERT models with a small vocabulary. Comparison between Gumbel-Softmax and k-means for vector quantization is conducted, including training models with varying numbers of codewords to assess performance. Table 3 presents TIMIT phoneme recognition results in terms of phoneme error rate (PER) for various models, including vq-wav2vec with BERT small. Gumbel-Softmax and k-means clustering show comparable performance, with Gumbel-Softmax outperforming k-means without BERT. However, these differences diminish with BERT. Table 4 displays Librispeech results for a standard sequence to sequence model without BERT pre-training, showcasing the gap to wav2vec. The study compares Gumbel-Softmax and k-means clustering performance in different setups, showing that Gumbel-Softmax is more accurate without BERT, while k-means performs better with a 4-gram LM setup. The large codeword model reduces the gap to the original wav2vec model. In TIMIT phoneme recognition, vq-wav2vec and BERT achieve a new state of the art with a 21% error reduction. Additionally, training a standard sequence to sequence model for speech recognition after discretizing audio shows promising results. In preliminary experiments, an off-the-shelf Big Transformer was trained on the vq-wav2vec Gumbel-Softmax discretized Librispeech corpus and evaluated on the Librispeech dev/test sets using a 4k BPE output vocabulary. Results show promise, although not as good as the state of the art due to the lack of data augmentation. The study also investigates the compression capabilities of vq-wav2vec by training models with different numbers of groups and variables to vary the codebook size and measure accuracy on TIMIT phoneme recognition without BERT training. The study explores the compression capabilities of vq-wav2vec by training models with varying numbers of groups and variables to measure accuracy on TIMIT phoneme recognition without BERT training. Compression is measured using bitrate and accuracy tradeoff on the phoneme recognition task, with experiments conducted on different group sizes and variables. The study evaluates the compression capabilities of vq-wav2vec by training models on the 100h clean Librispeech subset. Baselines include various lossy compression algorithms on TIMIT audio data. Results show that acoustic models on vq-wav2vec perform best across different bitrate settings. Masking entire spans of tokens yields better results than individual tokens. BERT training on discretized audio data is robust to masking large parts of the input. Table 5a demonstrates that masking entire spans of tokens outperforms individual tokens. BERT training on discretized audio data is robust to masking large parts of the input, as shown in Table 5b. The vq-wav2vec algorithm quantizes unlabeled audio data, improving performance on WSJ and TIMIT benchmarks by leveraging BERT pre-training. Future work includes exploring algorithms requiring discrete inputs for audio data and finetuning the pre-trained model to output transcriptions. Additionally, investigating the relationship between variables and groups shows that multiple groups are beneficial compared to a single group with a large. The study explores the benefits of multiple groups compared to a single group with a large number of variables in training vq-wav2vec models on Libri100. Results on the TIMIT dev set show that with a single group and many variables, only a small number of codewords survive."
}