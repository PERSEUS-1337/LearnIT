{
    "title": "rygVV205KQ",
    "content": "In this work, imitation learning is used to tackle challenges in high-dimensional sparse reward tasks for reinforcement learning agents. Adversarial imitation is shown to be effective in learning a representation of the world from pixels and exploring efficiently with rare reward signals. The adversary, acting as the reward function, can be small with just 128 parameters and trained using a basic GAN formulation. This approach overcomes limitations of other imitation methods, requiring only video demonstrations and sparse rewards to solve complex tasks like robot manipulation. The proposed agent can solve a challenging robot manipulation task of block stacking from only video demonstrations and sparse reward, outperforming non-imitating agents. It learns faster than competing approaches with hand-crafted reward functions and GAIL baselines. Additionally, a new adversarial goal recognizer allows the agent to learn stacking purely from imitation in some cases. In this work, it is shown that GAIL can handle high-dimensional pixel observations with a single-layer discriminator network when provided with the right features. The efficiency of GAIL can be improved by using a D4PG agent, which utilizes a replay buffer to store past experiences. Various types of features, such as self-supervised embeddings and random projections through a deep residual network, can be successfully used with a tiny, single-layer adversary. In addition to reducing the dependency on hand-crafted rewards, a proposed approach using GAIL and D4PG agents can solve a challenging robotic block stacking task from pixels with only demonstrations and sparse binary rewards. This approach outperforms previous imitation methods that relied on dense staged rewards and true state information. The stack completion task was achieved using a 6-DoF Jaco robot arm agent that learns block stacking from demonstration videos and sparse rewards. The approach outperformed previous methods by learning faster with no hand-crafted rewards. Additionally, an adversary-based early termination method improved task performance and learning speed, while an auxiliary goal recognizer adversary allowed the agent to achieve 55% stacking success from video imitation only. The actor processes improve task performance and learning speed by creating a natural curriculum for the agent. An agent learns without task rewards using an auxiliary goal recognizer adversary, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark BID33 show reasons for improvement in the agent, with random projections and value network features playing key roles. The DDPG algorithm involves training an agent to maximize rewards by finding a policy that guides actions based on state transitions. Actor and critic neural networks are used to represent the policy and action-value function, with new transitions added to a replay buffer for training. The goal is to match 1-step returns by minimizing the difference between predicted and actual rewards. The DDPG algorithm involves training an agent to maximize rewards by finding a policy that guides actions based on state transitions. In D4PG, improvements are made to the basic DDPG agent. GAIL involves learning a reward function by training a discriminator network to distinguish between agent and expert transitions. In GAIL, a reward function is learned by training a discriminator network to distinguish between agent and expert transitions. The GAIL objective involves the agent policy, expert policy, and an entropy regularizer. To utilize all training data, a D4PG agent is used for off-policy training with experience replay. The reward function is jointly trained with modified equations for the discriminator. The reward function in GAIL involves training a discriminator to distinguish between agent and expert transitions. It interpolates imitation reward and sparse task reward, using the sigmoid of the logits to bound the imitation reward between 0 and 1. The discriminator does not use actions in its evaluation, only access to videos of the expert. The sigmoid function bounds the imitation reward between 0 and 1, allowing for intuitive early termination of episodes in the actor process. Multiple CPU actor processes run in parallel, receiving updated parameters every 100 steps, while a single GPU learner process is used. Early termination occurs when the discriminator score falls below a threshold \u03b2 to prevent the agent from deviating too far from expert trajectories. In practice, a threshold \u03b2 is set at 0.1 to stop episodes when the discriminator score is below it. The type of network used in the discriminator is a critical design choice, as too much capacity can make it easy to distinguish agent from expert, while too little capacity may not capture the differences. Expert demonstrations provide valuable data for feature learning. The discriminator architecture is crucial in distinguishing between agent and expert. Expert demonstrations offer valuable data for feature learning, covering the necessary regions of the state space. Access to expert actions is not assumed, ruling out behavior cloning for feature learning. Learning features by predicting in pixel space is avoided due to high-resolution images and the need to encourage long-term structure learning for imitation. Contrastive predictive coding (CPC) is a representation learning technique that maps observations into a latent space for long-term predictions. It uses a probabilistic contrastive loss with negative sampling, allowing joint training of the encoder and autoregressive model without the need for a decoder. This approach moves beyond hand-crafted dense staged rewards to sparse rewards and can even eliminate task rewards entirely. Contrastive predictive coding (CPC) is a representation learning technique that uses a probabilistic contrastive loss with negative sampling to train an encoder and autoregressive model jointly. It suggests replacing task rewards with a neural network goal recognizer or a discriminator to detect goal states, addressing issues of adversarial exploitation by the agent. To address adversarial exploitation, one approach is to replace sparse task rewards with a secondary goal discriminator network that detects goal states based on expert demonstrations. This modified reward function enhances the agent's performance beyond imitation learning methods like GAIL. By training a second discriminator to recognize goal states, agents can surpass demonstrators by learning to reach goals faster. Environments include a Kinova Jaco arm with 9 degrees of freedom and hand-crafted reward functions. Demonstrations are collected using a SpaceNavigator 3D motion controller. The Kinova Jaco arm with 9 degrees of freedom uses velocity commands for continuous velocities in the range of [-1, 1] at 20Hz. Demonstrations are collected using a SpaceNavigator 3D motion controller, with 500 episodes gathered for each task. Another 500 trajectories are collected for validation, and a dataset of 30 \"non-expert\" trajectories is used for diagnostics. The second environment involves a 2D walker from the DeepMind control suite BID33, where demonstrations are collected to match a target velocity. The second environment involves a 2D walker from the DeepMind control suite BID33. Demonstrations are collected to match a target velocity using 64x64 pixel observations of 200 expert demonstrations. The imitation method is compared to D4PG and GAIL agents, showing favorable results with a tiny adversary. The CPC model predicts future observations based on frame distances. The CPC model predicts future observations well for expert sequences but not for non-expert ones. Conditioning on k-step predictions improves performance on stacking tasks when the discriminator uses CPC embeddings. D4PG with sparse rewards struggles due to exploration complexity, while imitation methods learn quickly with superior performance using sparse rewards. The agent using value network features takes off more quickly than with CPC features, reaching comparable performance in the end. The agent using value network features learns quickly with superior performance despite sparse rewards. GAIL with CPC features reaches comparable performance in the end. GAIL from pixels performs poorly, while GAIL with tiny adversaries on random projections has limited success. The discriminator network has 128 parameters with CPC features and 2048-dimensional with value network features. Norm clipping in the critic optimizer may explain why GAIL value features work while pixel features do not. A new agent \"GAIL -pixels + clip\" with norm clipping does not succeed in Jaco. The addition of norm clipping in the critic optimizer does not lead to success in the new agent \"GAIL -pixels + clip\" in Jaco or Walker2D. Using CPC features for temporal predictions and input to the discriminator is explored, showing visualizations of learned features. Ablation experiments on Jaco stacking reveal that adding layers to the discriminator network does not improve performance, and early termination is detrimental. In ablation experiments on Jaco stacking, adding layers to the discriminator network does not improve performance, and early termination negatively impacts results. The study also investigates the use of a tiny discriminator versus a deeper network for imitation learning, showing that a small discriminator on a meaningful representation yields better performance. The study found that adding layers to the discriminator network in Jaco stacking did not improve performance. Early termination had a negative impact on results. A small discriminator on a meaningful representation yielded better performance in imitation learning. In a study on imitation learning, the agent's performance improves with more episodes, showing better imitation of the expert. Data efficiency is demonstrated with as few as 60 expert demonstrations. Results on the planar walker indicate success with the proposed method using value network features and random projections. Videos of the trained agent are available for reference. The proposed method using value network features and random projections successfully trained the agent to run on the planar walker. Videos of the trained agent are available in the supplementary materials. Results show that two out of five runs were able to learn without any task reward, with the best agent achieving a 55% success rate. The agent demonstrated more efficient stacking compared to the human operator, completing the task in under 2 seconds. The best agent achieved a 55% success rate without task rewards. The agent stacked more efficiently than the human operator, taking under 2 seconds. Leveraging expert demonstrations to improve agent performance has a long history in robotics. Our task differs as we only have access to pixel observations, not states and actions. Imitation learning in deep reinforcement learning aims to replicate expert behaviors from single demonstrations using pixel observations. This approach, known as supervised imitation or behavioral cloning, leverages deep networks to excel in solving tasks without access to states and actions. In supervised imitation learning, behaviors are replicated from single demonstrations using deep networks. One-shot imitation extends this by inferring behaviors from demonstrations and replicating them on new instances. Different approaches include using attention mechanisms or gradient-based meta learning. The focus is on the agent learning through interaction with the environment rather than supervised learning. Our approach differs from behavioral cloning as we focus on the agent learning through interaction with the environment rather than supervised learning. Behavioral cloning can lead to failures when the agent encounters states not seen in expert trajectories, requiring many demonstrations and limiting generalization. Instead of behavior cloning, inverse reinforcement learning (IRL) is proposed, where a reward function is learned from demonstrations and then optimized using reinforcement learning. In contrast to behavioral cloning, inverse reinforcement learning (IRL) focuses on learning a reward function from demonstrations and optimizing it through reinforcement learning. Various methods like deep Q-Learning from demonstration (DQfD) and deterministic policy gradients from demonstration (DPGfD) have been developed to train agents using expert trajectories and adversarial learning techniques like GAIL have been applied to imitation problems. GAIL applies adversarial learning to imitation problems, with challenges in high-dimensional input spaces. Minimal adversaries help solve sparse reward tasks. Other work focuses on learning compact representations for imitation without expert actions. Both BID30 and BID3 use self-supervised features from third person observations to bridge the gap between different views. They track a single expert trajectory, while we aim to generalize all possible initializations of a challenging task using GAIL. Our approach incorporates static and dynamic features to train block stacking agents successfully from sparse rewards on pixels. Videos of our agents can be found on an anonymized website. Our approach incorporates static and dynamic features to train block stacking agents successfully from sparse rewards on pixels. The stacking accuracy is approximately 15%. The model consists of an encoder mapping observations to a latent representation. Videos of our learned agents can be viewed on an anonymized website. The model includes an encoder mapping observations to a latent representation and an autoregressive model summarizing past latents into a context vector. By optimizing the loss function, the mutual information between the context and target is maximized, resulting in compact representations. The model optimizes L CPC to maximize mutual information between z t+k and c t, linearly embedding shared variables into compact representations for predicting future steps. The approach involves training a CPC expert model, then training an agent using CPC future predictions without pixel space prediction. Reward functions are modified from BID37, with five defined stages and rewards for each episode lasting 500 time steps. The model optimizes L CPC to maximize mutual information between z t+k and c t, linearly embedding shared variables into compact representations for predicting future steps. Actor and critic share a residual network with twenty convolutional layers and use independent three-layer fully connected networks. The model utilizes Distributional Q functions with a categorical representation of Z. Bootstrap targets are computed with N-step returns using a fixed atoms bounded between V min and V max. The model uses bootstrap targets computed with N-step returns and a categorical representation of Z. The loss function for training distributional value functions involves cross entropy, and distributed prioritized experience replay is used for stability and learning efficiency."
}