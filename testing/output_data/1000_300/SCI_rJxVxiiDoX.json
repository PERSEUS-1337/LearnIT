{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. By combining quantization-aware training and weight matrix factorization, we can reduce model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training is used to optimize weights for low-power keyword spotting models. By training wake word models with quantization-aware training, successful training of 8 bit and 4 bit quantized models is achieved. This approach helps mitigate performance degradation caused by quantization in keyword spotting systems. The paper discusses training a small-footprint, low-power keyword spotting system using quantization-aware training. Successful training of 8 bit and 4 bit quantized models is achieved, mitigating performance degradation. The keyword 'Alexa' is chosen for experiments using a 500 hrs far-field corpus for training and a 100 hrs dataset for evaluation. The study focuses on training a keyword spotting system using quantization-aware training with the keyword 'Alexa'. A 500 hrs far-field corpus is used for training and a 100 hrs dataset for evaluation. The models are evaluated using end-to-end DET curves and AUC. Training is done in 3 stages with a small ASR DNN pre-trained in the 1st stage. The performance of 'naively quantized' models is shown in a table, with an observed AUC improvement in quantized models. In another 20 epochs, the performance of 'naively quantized' models is shown in TAB0, with an observed AUC improvement in quantized models. DET curves for full-precision, quantized, and quantization-aware trained models are compared, with 16 bit and 8 bit quantized-models not significantly different from the full-precision model."
}