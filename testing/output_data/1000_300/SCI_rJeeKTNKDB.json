{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive and outperforms previous baselines in multiple molecular optimization tasks. Our model significantly outperforms previous state-of-the-art baselines in molecular optimization tasks. The task involves modifying compounds to improve their biochemical properties, similar to machine translation. The model is trained on a corpus of molecular pairs to translate input molecular graphs into better forms, facing challenges due to the vast candidate space and complex molecular properties. Success relies on the inductive biases in the encoder-decoder architecture for generating molecular graphs. The previous paragraph discusses a model that outperforms previous baselines in molecular optimization tasks by utilizing inductive biases in the encoder-decoder architecture. The current paragraph focuses on a previous approach that used a junction tree encoder-decoder for generating molecular graphs but had limitations in how the tree and graph encoding were carried out separately. The current paragraph introduces a multi-resolution, hierarchically coupled encoder-decoder for graph generation in molecular optimization tasks. The approach involves predicting substructure components and their attachments in a non-autoregressive manner to model strong dependencies between successive attachments and substructure choices. The encoder-decoder model for graph generation in molecular optimization tasks involves predicting substructure components and their attachments in a non-autoregressive manner. The decoding process efficiently decomposes each generation step into smaller hierarchical steps to avoid combinatorial explosion. The method is extended to handle conditional translation based on desired criteria. Our decoding process efficiently decomposes each generation step into smaller hierarchical steps to avoid combinatorial explosion. The method is extended to handle conditional translation based on desired criteria, allowing different combinations of criteria at test time. The model interleaves tree and graph decoding steps to avoid generating invalid junction trees and predicts consistent local substructure attachments during training. Our new autoregressive decoder improves molecule optimization tasks by outperforming previous methods, yielding significant improvements on QED and DRD2 tasks. It runs faster during decoding and benefits from hierarchical decoding and multi-resolution encoding. Additionally, conditional translation is successful with our model. During decoding, our model is 6.3 times faster than previous methods. Ablation studies confirm the benefits of hierarchical decoding and multi-resolution encoding. Conditional translation can generalize even with minimal training data. Various methods have been used for molecular graph generation, including generating molecules based on SMILES strings and developing generative models for outputting adjacency matrices and node labels simultaneously. Various methods have been used for molecular graph generation, including generative models that output adjacency matrices and node labels simultaneously. Some approaches decode molecules sequentially node by node, while others adopt node-by-node methods in the context of reinforcement learning. A hypergraph grammar based method for molecule generation has also been developed. Jin et al. (2018) generated molecules based on substructures using a two-stage procedure. Our method differs from previous approaches by jointly predicting substructures and their attachments with an autoregressive decoder, unlike the two-stage procedure used by Jin et al. (2018) for molecule generation. Graph neural networks have been extensively studied for graph encoding, but our method focuses on graph encoders for molecules. Our method represents molecules as hierarchical graphs, different from previous approaches. It is related to graph encoders for molecules and learns to represent graphs in a hierarchical manner. Defferrard et al. (2016) and Ying et al. (2018) used graph coarsening algorithms to create multiple graph hierarchy layers for molecule representation. Gao & Ji (2019) proposed learning graph hierarchy during encoding. Unlike previous methods focusing on single vector representation, this approach encodes molecules into multiple sets of vectors for graph generation. Our focus is on graph generation, where a molecule is encoded into multiple sets of vectors for dynamic aggregation in each generation step. The task is to learn a function that maps a molecule into another with better chemical properties using an encoder-decoder with neural attention. The decoder adds new substructures and predicts their attachment to the current graph in two steps. This hierarchical generation requires a matching encoder design. The attachment prediction in graph generation involves two steps: predicting attaching points in the new substructure and their corresponding attaching points in the current graph. A hierarchical graph representation of molecules includes substructure, attachment, and atom layers. The model encodes nodes into substructure, attachment, and atom vectors for decoding. The encoder is tailored for the decoder to support hierarchical generation. The model encodes nodes in a graph into substructure, attachment, and atom vectors for decoding. Substructures are defined as subgraphs of molecules, including rings and bonds. The vocabulary of substructures is constructed from the training set. The paper discusses substructures in molecular graphs, including rings and bonds, with a vocabulary constructed from the training set. A substructure tree is created to show how substructures are connected in the molecule, and a graph decoder generates a molecule by expanding its substructure tree in a depth-first order. The graph decoder generates a molecule by incrementally expanding its substructure tree in a depth-first order. It predicts new substructures and how they should be attached to the graph, moving to the new substructure and repeating the process. Topological prediction determines if a new substructure will be attached, with substructure prediction creating a new substructure if the probability is greater than 0.5. The model predicts new substructures and their attachment to the graph. If the probability is over 0.5, a new substructure is created with a parent assigned. The attachment between substructures is defined by atom pairs, which are predicted in two steps. The model predicts atom pairs for attaching new substructures in two steps: 1) Predicting attaching atoms to form a vocabulary for each substructure, and 2) Finding corresponding atoms in the substructure. The predictions are based on learned atom representations and give an autoregressive factorization of the distribution over the next substructure and its attachment. The model predicts atom pairs for attaching new substructures based on learned atom representations. The predictions follow an autoregressive factorization process, with the attachment enumeration being tractable due to small substructure sizes. The encoder represents molecules hierarchically to support the decoding process. The encoder represents molecules hierarchically with a graph structure to support the decoding process. The hierarchical graph consists of three components: atom layer, attachment layer, and necessary information for attachment prediction. The attachment layer provides information for attachment prediction, with each node representing a specific attachment configuration of a substructure. The substructure layer, similar to the substructure tree, is crucial for substructure prediction during decoding. Edges connect atoms and substructures between layers to propagate information effectively. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used for encoding, denoted as MPN \u03c8(\u00b7) with parameter \u03c8. The atom layer MPN encodes the atom layer of HX using embedding vectors for atoms and bonds in X. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used for encoding, denoted as MPN \u03c8(\u00b7) with parameter \u03c8. The atom layer MPN encodes the atom layer of HX using embedding vectors for atoms and bonds in X. The attachment layer MPN processes node and edge features to compute substructure representations. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. During decoding, the hierarchical MPN architecture is used to encode the hierarchical graph H G at each step. This results in substructure vectors and atom vectors for future nodes and edges. During decoding, the hierarchical MPN architecture encodes the hierarchical graph H G to generate diverse outputs for molecule Y. The model uses variational translation with an additional input z to indicate the intended mode of translation, sampled from a Gaussian prior during testing. Training is done using variational inference. The model extends the method to a variational translation model with an additional input z, sampled from a Gaussian prior during testing. Training is done using variational inference, where z is sampled from the posterior to compute structural changes from molecule X to Y. The latent code z is passed to the decoder to reconstruct output Y, following a standard conditional VAE training objective. The model extends the method to handle conditional translation by allowing users to specify desired criteria during testing. This additional input, g X,Y, is used to compute \u00b5 X,Y and \u03c3 X,Y during variational inference, augmenting the latent code as [z, g X,Y] for the decoder to reconstruct the output Y. This approach enables users to control the outcome by specifying their criteria in g X,Y. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with an additional input g X,Y to augment the latent code for the decoder. Users can specify criteria in g X,Y during testing to control the outcome. A novel conditional optimization task is constructed where desired criteria are fed as input to the translation process. Molecular similarity between input X and output Y must be above a certain threshold sim(X, Y) \u2265 \u03b4 at test time to prevent arbitrary compound translation. To prevent arbitrary compound translation, molecular similarity between input X and output Y must be above a certain threshold sim(X, Y) \u2265 \u03b4 at test time. The model is trained on four different tasks, including LogP Optimization where the model needs to translate input X into output Y such that logP(Y) > logP(X), with a similarity constraint of sim(X, Y) \u2265 0.4. The model is trained to translate input X into output Y with a similarity constraint of sim(X, Y) \u2265 0.4. Two similarity thresholds are experimented with (\u03b4 = 0.4, 0.6). Different criteria for improving properties of Y after translation are encoded as a vector g. Evaluation metrics include translation accuracy and diversity, with each test molecule translated 20 times with different latent codes. Our method (HierG2G) is compared against baselines including GCPN. Evaluation metrics involve translation accuracy and diversity, with each molecule translated 20 times using different latent codes. The final translation is selected based on property improvement and similarity constraints. Translation success rate is reported for other tasks, with diversity measured by the average pairwise Tanimoto distance between successfully translated compounds. HierG2G is compared against baselines such as GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. A direct comparison is made with an atom-based translation model (AtomG2G). Baselines include different methods for molecule generation, with Seq2Seq and JTNN using sequence-to-sequence and graph-to-graph architectures, respectively. CG-VAE decodes molecules atom by atom and optimizes properties in the latent space. AtomG2G makes three predictions in each generation for comparison. HierG2G is compared against baselines such as GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. A direct comparison is made with an atom-based translation model (AtomG2G), which predicts atom types and bond types in each generation step. Our model achieves state-of-the-art results on four translation tasks, outperforming JTNN in translation accuracy. Our model outperforms JTNN and AtomG2G in translation accuracy and output diversity on four tasks. It runs 6.3 times faster than JTNN during decoding and shows over 10% improvement on the DRD2 task compared to AtomG2G. The hierarchical model demonstrates its advantage in this comparison with other translation methods. Our hierarchical model outperforms other translation methods like Seq2Seq, JTNN, and AtomG2G in both accuracy and diversity. Despite low success rates on certain criteria, training with different pairs improves performance. Ablation studies on QED and DRD2 tasks show the importance of architecture choices. The hierarchical model outperforms other translation methods in accuracy and diversity. Ablation studies on QED and DRD2 tasks reveal the impact of architecture choices, such as structure-based decoding and hierarchy levels in the encoder and decoder. The second experiment explores reducing hierarchies in the encoder and decoder MPN, impacting translation accuracy. Removing substructure and attachment layers degrades performance significantly. Replacing LSTM MPN with GRU MPN results in decreased translation performance, but still outperforms JTNN. The hierarchical graph-to-graph translation model developed in this paper showcases superior performance. Our method, using LSTM MPN architecture, outperforms JTNN despite a decrease in translation performance. The hierarchical graph-to-graph translation model generates molecular graphs using chemical substructures, learning coherent multi-resolution representations. Experimental results demonstrate superior performance over previous models. The message passing network MPN \u03c8 over graph H is defined, and an attention layer with bilinear function is utilized. Additional figures illustrate AtomG2G decoding. Algorithm 3 LSTM MPN with T message passing iterations simultaneously processes edges in graph H. An attention layer with a bilinear function is used in the AtomG2G architecture for atom prediction and bond prediction. AtomG2G is a molecule translation method that represents molecules as molecular graphs, not hierarchical graphs with substructures. AtomG2G is an atom-based translation method that adds new atoms to the queue Q. Training set sizes and substructure vocabulary sizes for datasets are listed in Table 3. Multi-property optimization combines QED and DRD2 datasets. Hyperparameters for HierG2G include hidden layer dimension of 270, embedding layer dimension of 200, latent code dimension of 8, and KL regularization weight of 0.3. The training and test set is provided in the supplementary material. Hyperparameters for HierG2G and AtomG2G models include hidden layer and embedding dimensions, latent code dimension, KL regularization weight, and number of message passing iterations. CG-VAE models were used for molecule generation and property prediction tasks. For experiments, CG-VAE models were trained to generate molecules and predict properties. Three models were created for logP, QED, and DRD2 tasks. At test time, compounds were translated using gradient ascent over latent representations to maximize property scores. A low KL regularization weight (\u03bb KL = 0.005) was found necessary for meaningful results. Ablation studies were conducted to analyze the impact of different parameters. In ablation studies, experiments were conducted to modify the decoder and reduce hierarchies in the encoder and decoder MPN. Different models were tested, including a two-layer model and a one-layer model, to make topological and substructure predictions. In experiments, a two-layer model and a one-layer model were tested for topological and substructure predictions. The two-layer model represents molecules as cX = cGX \u222a cAX, using hidden vector hAk. The one-layer model represents molecules as cX = cGX, using atom vectors v\u2208Skhv for predictions. The hidden layer dimension is adjusted to match the original model size."
}