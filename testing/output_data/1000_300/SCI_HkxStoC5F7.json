{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning. It extends existing probabilistic interpretations of meta-learning and introduces \\Versa{}, which uses a flexible amortization network for few-shot learning, replacing optimization at test time with forward passes through inference networks. \\Versa{} is evaluated on benchmark datasets, achieving state-of-the-art results for few-shot learning with arbitrary numbers of shots and classes. The approach is demonstrated through a challenging ShapeNet view reconstruction task, showcasing its ability to rapidly adapt to new datasets at test time. Despite recent advances in meta-learning, there is a lack of general purpose methods for flexible, data-efficient learning, highlighting the need for a unifying view to improve existing frameworks. In this paper, a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) is developed to understand and improve existing methods. The framework covers a broad class of meta-learning methods, including gradient-based, metric-based, amortized MAP inference, and conditional probability modeling. It leverages shared statistical structure between tasks via hierarchical elements. The framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) covers various meta-learning methods, leveraging shared statistical structure between tasks and enabling fast learning through amortization. A new method called VERSA is proposed, which substitutes optimization procedures at test time with forward passes through inference networks, resulting in faster test-time performance. The proposed method VERSA substitutes optimization procedures at test time with forward passes through inference networks, resulting in faster test-time performance. It employs a flexible amortization network for few-shot learning datasets, handling arbitrary numbers of shots and classes at train and test time. VERSA sets new state-of-the-art results on standard benchmarks and performs well in settings with different test conditions. The framework presented includes a multi-task probabilistic model and a method for meta-learning probabilistic inference. It utilizes a standard multi-task directed graphical model with shared parameters for all tasks and task-specific parameters. The joint probability of outputs and task-specific parameters for T tasks is given by the inputs and global parameters. The framework involves a multi-task probabilistic model with shared and task-specific parameters. The goal is to meta-learn fast approximations to the posterior predictive distribution for unseen tasks. Point estimates are used for shared parameters, while distributional estimates are used for task-specific parameters in few-shot learning. The framework involves a multi-task probabilistic model with shared and task-specific parameters. Distributional estimates are used for task-specific parameters in few-shot learning. The probabilistic solution to few-shot learning comprises forming the posterior distribution over task-specific parameters and computing the posterior predictive. Emphasis is on quickly performing these steps at test time. The framework approximates the posterior predictive distribution efficiently. The framework involves a multi-task probabilistic model with shared and task-specific parameters for few-shot learning. It approximates the posterior predictive distribution efficiently using an amortized distribution. This enables fast predictions at test time by learning a feed-forward inference network. The framework involves a multi-task probabilistic model for few-shot learning, using an amortized distribution for efficient posterior predictive distribution approximation. The training method focuses on meta-learning the approximate posterior predictive distribution to minimize KL-divergence between true and approximate distributions. The goal is to find parameters that best approximate the posterior predictive distribution in an average KL sense. The training method involves meta-learning the approximate posterior predictive distribution to minimize KL-divergence. By selecting a task, sampling training data, forming the posterior predictive, and computing log-density, accurate prediction support is achieved through amortized inference. This process provides an unbiased estimate of the objective for optimization. The training method involves meta-learning the approximate posterior predictive distribution by simulating approximate Bayesian held-out log-likelihood evaluation. The procedure focuses on minimizing KL-divergence between the posterior predictive distribution and the approximate inference procedure. The objective function directly targets the posterior predictive distribution, different from standard variational inference. The training procedure involves end-to-end stochastic training with inputs and shared parameters. The training procedure involves end-to-end stochastic training with inputs and shared parameters, optimizing the objective to maximize predictive performance. Episodic train/test splits are used at meta-train time, with the integral over \u03c8 approximated using Monte Carlo samples. The approach developed is Meta-Learning Probabilistic Inference for Prediction (ML-PIP), which uses episodic train/test splits and approximates the integral over \u03c8 with Monte Carlo samples. The learning objective implicitly learns the prior distribution over parameters through q \u03c6 (\u03c8|D, \u03b8). This framework unifies existing approaches and supports versatile learning for rapid and flexible inferences. The ML-PIP framework supports versatile learning by enabling rapid and flexible inferences through the use of deep neural networks. It allows for test-time inference with simple computations and supports various tasks without the need for retraining. Design choices are discussed to retain flexibility, including inference with sets as inputs. In the ML-PIP framework, design choices are made to retain flexibility, such as inference with sets as inputs. The amortization network processes sets of variable size using permutation-invariant instance-pooling operations. For few-shot image classification, a parameterization inspired by early work and recent extensions to deep learning is utilized. The ML-PIP framework utilizes a feature extractor neural network shared across tasks, feeding into task-specific linear classifiers. Amortization is proposed to model the distribution over weight matrices in a context-independent manner, allowing for metalearning without specifying the number of few-shot classes ahead of time. Amortization is proposed to specify weight vectors in a context-independent manner, depending only on examples from each class. The amortization network operates on extracted features to reduce the number of learned parameters. End-to-end training is employed, backpropagating to the generator through the inference network. In our implementation, end-to-end training is used, backpropagating to \u03b8 through the inference network. The classification matrix \u03c8 is constructed by performing C feed-forward passes through the inference network. The context independent inference assumption is approximated and justified theoretically and empirically in Appendix B. This approximation addresses the limitations of a naive amortization approach. Density Ratio Estimation BID36 BID49 shows that full approximate posterior distributions closely match context independent counterparts, addressing limitations of naive amortization. VERSA for Few-Shot Image Reconstruction involves multi-output regression for view reconstruction from limited training images. View reconstruction is a learning task involving a generative model that infers object appearance from different angles based on a small set of observed views. The model uses a latent vector as input and parameters of the generator network to produce images at specified orientations. The generator network uses global parameters \u03b8 and task-specific parameters \u03c8 (t) for image generation. A Gaussian likelihood is used in pixel space with a sigmoid activation. An amortization network processes image representations and view orientations before instance-pooling. ML-PIP unifies various meta-learning approaches as approximate inference in hierarchical models. In this section, ML-PIP unifies different meta-learning approaches as approximate inference in hierarchical models, connecting gradient and metric-based variants, amortized MAP inference, and conditional modeling. The task-specific parameters are represented by point estimates, comparing previous approaches to VERSA.Gradient-Based Meta-Learning. The curr_chunk discusses the concept of semi-amortized inference in meta-learning, specifically focusing on the use of gradient ascent for training loss optimization. It highlights the recovery of Model-agnostic meta-learning perspective and the role of predictive KL in test-train splits. Additionally, it mentions the potential use of multiple gradient steps in RNN for parameter computation. The curr_chunk discusses VERSA, a method for few-shot learning that avoids back-propagation during training and simplifies inference by treating both local and global parameters. It also introduces amortized point estimates for task-specific parameters in neural networks. The curr_chunk introduces amortized point estimates for task-specific parameters in neural networks, focusing on predicting weights of classes from activations to support online learning and transfer tasks. This method utilizes hyper-networks to amortize learning about weights, contrasting with VERSA which uses a more flexible amortization function for few-shot learning. The curr_chunk discusses the use of hyper-networks to support online learning with few-shot classes and transfer tasks. It introduces the ML-PIP framework for amortizing learning about weights and highlights the flexibility of VERSA in supporting multi-task learning. The amortization network plays a key role in model specification for task-specific parameters. The task-specific parameters are used in predictive display. The amortization network computes \u03c8 * (D, \u03b8) as part of the model specification. ML-PIP training is equivalent to training a conditional model via maximum likelihood estimation. Comparison to Variational Inference (VI) shows differences in optimization and regularization methods. VERSA significantly improves over standard VI. In contrast to ML-PIP, VERSA does not use meta train/test splits and incorporates KL for regularization. It outperforms standard VI in few-shot classification and is evaluated on various tasks including toy experiments, Omniglot, miniImageNet datasets, and ShapeNet objects. The training procedure investigates amortized posterior inference and approximate inference. In Section 5.3, VERSA's performance on a one-shot view reconstruction task with ShapeNet objects is examined. An experiment is conducted to investigate the approximate inference during training. Data is generated from a Gaussian distribution with varying means across tasks, with T = 250 tasks in two experiments, N \u2208 {5, 10} train observations, and M = 15 test observations. The inference network q \u03c6 (\u03c8|D) is introduced for amortizing inference, with learnable parameters trained using Adam BID25. The model is trained to convergence with mini-batches of tasks, and the posterior q \u03c6 (\u03c8|D) is inferred with the learned amortization parameters. VERSAs performance is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets. The model is trained with Adam BID25 using mini-batches of tasks and infers the posterior q \u03c6 (\u03c8|D) with learned amortization parameters. The true posterior over \u03c8 is Gaussian and can be computed analytically. VERSA accurately recovers posterior distributions over \u03c8 despite minimizing predictive KL divergence in data space. The approximate posterior distributions for unseen test sets are shown in Fig. 4. VERSAs performance is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets. The model follows specific implementations and experimental protocols for each dataset, accurately recovering posterior distributions despite minimizing predictive KL divergence. Training is episodic, with examples used for training inputs and evaluating the objective function. VERSAs performance is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets. Training is episodic, with examples used for training inputs and evaluating the objective function. The results show VERSA achieving new state-of-the-art performance of 67.37%. The quality of the learning algorithm can be assessed separately from the power of the underlying discriminative model. VERSA achieves new state-of-the-art results on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks using a convolution-based network architecture. It is competitive on other benchmarks as well. VERS improves performance on Omniglot 20 way -5-shot benchmark by adapting only the weights of the top-level classifier. Comparison to standard and amortized VI shows substantial improvement over amortized VI, with non-amortized VI further enhancing performance. VERS improves performance on Omniglot 20 way -5-shot benchmark by adapting only the weights of the top-level classifier. Non-amortized VI enhances performance substantially compared to amortized VI, but VERSA outperforms both methods and allows for flexibility in varying the number of classes and shots between training and testing. VERSATILITY. VERSA allows for flexibility in varying the number of classes and shots between training and testing, demonstrating high accuracy and robustness. It outperforms other methods like MAML in both speed and accuracy, making it efficient for evaluation tasks. The VERSA model outperforms MAML in speed and accuracy, showing a 5\u00d7 speed advantage and 4.26% better accuracy. The dataset used consists of 37,108 objects from 12 object categories, with 36 views generated for each object. VERSA is evaluated against a conditional variational autoencoder (C-VAE) with similar architectures. The VERSA model generates 36 views of objects and is compared to a C-VAE model. VERSA produces sharper images with more detail compared to C-VAE, capturing correct object orientation. ML-PIP is a probabilistic framework for meta-learning that unifies a broad class of models. VERSA outperforms C-VAE in image generation, showing superior results in quantitative metrics like mean squared error and structural similarity index. Increasing the number of shots improves VERSA's performance. ML-PIP is a probabilistic framework for meta-learning that unifies various methods. VERSA, a few-shot learning algorithm, achieves state-of-the-art performance without gradient-based optimization at test time. It outperforms Prototypical Networks in tasks like 1-shot view reconstruction. Prototypical Networks perform better when trained on higher \"way\" than that of testing, achieving 68.20 \u00b1 0.66% accuracy when trained on 20-way classification and tested on 5-way. The new inference framework in Section 2 is based on Bayesian decision theory, providing a recipe for making predictions for unknown test variables by combining information from observed training data and a loss function. The text discusses Bayesian decision theory (BDT) and a loss function for predicting unknown test variables. It introduces a stochastic variational objective for meta-learning probabilistic inference grounded in Bayesian inference and decision theory. Distributional BDT is generalized to return a full predictive distribution over the test variable. The text introduces amortized variational training, where a predictive distribution is optimized to minimize expected loss over tasks by learning shared variational parameters. This allows for quick predictions at test time using a training dataset as an argument. Amortized variational training optimizes a predictive distribution to minimize expected loss over tasks by learning shared variational parameters. This enables quick predictions at test time using a training dataset as an argument. The optimal variational parameters are found by minimizing the expected distributional loss across tasks, without requiring computation of the true predictive distribution. This approach emphasizes the meta-learning aspect of inferring predictive distributions from training tasks. Amortized variational training optimizes predictive distribution by learning shared variational parameters to minimize expected loss over tasks. The log-loss function is employed, emphasizing the meta-learning aspect of inferring predictive distributions from training tasks. The optimal q \u03c6 is the closest member to the true predictive distribution p(\u1ef9|D) in a KL sense. Exploration of alternative scoring rules and task-specific losses is left for future work. The wake-sleep algorithm BID19 explores alternative scoring rules and task-specific losses for future work. It discusses the approximation of the predictive distribution and the theoretical and empirical justifications for the context-independent approximation. Density ratio estimation BID36 BID49 is used to justify the optimal softmax classifier in terms of conditional densities. The optimal softmax classifier can be expressed using conditional densities, with estimators constructed independently for each class. The context-independent assumption is evaluated through a simple experiment to examine if weights may be context-independent. In a simple experiment, the context-independent assumption is tested by evaluating if weights can be context-independent. Fifty tasks are randomly generated from a dataset, and free-form variational inference is performed on the weights for each task. The goal is to see if the distribution of weights for a specific class remains similar regardless of other classes in the task. The experiment focuses on 5-way classification in the MNIST dataset. The experiment focuses on 5-way classification in the MNIST dataset. The model achieves 99% accuracy on test examples for the tasks. The weights cluster according to class in a 2-dimensional space, with some overlap between classes. The weights cluster according to class in a 2-dimensional space, with some overlap. For tasks containing both class '1' and '2', class '2' weights are located away from their cluster, suggesting independence of class-weights from the task. If the model lacks capacity to assign class weights properly, one weight may move to an 'empty' region. A VI-based objective is derived for the probabilistic model, with \"amortized\" VI parameterized by a neural network. The procedure involves moving a class weight to an empty region in the space. A VI-based objective is derived for the probabilistic model, with \"amortized\" VI parameterized by a neural network. The objective function remains the same for both \"amortized\" and \"non-amortized\" VI. An evidence lower bound (ELBO) is used for a single task, and a stochastic estimator is derived to optimize it. The curr_chunk discusses few-shot classification experiments using the Omniglot dataset. It includes details on image preprocessing, training procedures, and dataset splits. The objective function used differs from the one in the previous paragraph in terms of the absence of a KL term for prior distribution and the lack of distinction between training and test data within a task. The training procedure for few-shot classification on the Omniglot dataset involves resizing images to 28x28 pixels, augmenting character classes with rotations, and splitting data into training, validation, and test sets. Training is done episodically with random selection of classes and character instances. Validation is used to monitor learning progress, and final evaluation is done on randomly selected tasks. The validation set is used to monitor learning progress and select the best model for testing. Final evaluation is done on 600 randomly selected tasks from the test set. The models are trained using the Adam optimizer with a constant learning rate and specific iterations for different scenarios. The miniImageNet dataset consists of 60,000 color images divided into 100 classes. The 20-way -1-shot model is trained for 100,000 iterations using a Gaussian form for q with 10 \u03c8 samples. The miniImageNet dataset has 60,000 color images divided into 100 classes, with 84 \u00d7 84 pixel dimensions. Training is episodic like Omniglot, using the Adam optimizer. Different models are trained with specific iterations and learning rates. The neural network architectures for the feature extractor \u03b8, amortization network \u03c6, and linear classifier \u03c8 are detailed in TAB0 to D.4. The feature extraction network architecture for the 5-way -1-shot model includes a shared network with the amortization network to reduce learned parameters. The amortization network provides Gaussian parameters for the weight distributions of the linear classifier. Sampling from the weight distributions utilizes the local-reparameterization trick. The feature extraction network has specific layers and output sizes for image processing. The feature extraction network architecture for the 5-way -1-shot model includes sharing the network with the amortization network to reduce learned parameters. Specific layers and output sizes are used for image processing in the network. The miniImageNet Shared Feature Extraction Network uses normalization and dropout with a keep probability of 0.5. It consists of multiple convolutional layers with varying sizes and pooling operations. The ShapeNetCore v2 BID5 database is used for experimentation with 3D objects. ShapeNetCore v2 BID5 is a database of 3D objects with 55 categories. 12 largest categories are used for experiments. A dataset of 37,108 objects is created by combining categories. Objects are randomly shuffled and split into 70% for training, 10% for validation, and 20% for testing. Each object generates 36, 128x128 pixel views converted to grayscale and resized to 32x32 pixels. The model is trained episodically with batches of tasks. The model is trained episodically with batches of tasks. Each training iteration involves selecting an object randomly from the training set and training on a single view. The remaining views are used to evaluate the objective function. The system generates 36 views of the object using an amortization network. Network architectures for the encoder, amortization, and generator networks are described in Tables E.2 to E.4. Training is done using the Adam optimizer with a constant learning rate of 0.0001 and 24 tasks per batch for 500,000 iterations. The network architectures for the encoder, amortization, and generator networks are described in Tables E.2 to E.4. Training involves using the Adam BID25 optimizer with a learning rate of 0.0001, 24 tasks per batch, and 500,000 training iterations. The ShapeNet Encoder Network (\u03c6) has specific layers and output sizes for image processing."
}