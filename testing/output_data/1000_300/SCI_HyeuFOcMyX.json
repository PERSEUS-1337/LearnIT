{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance. By manipulating the planner codes, translations with different structures can be achieved. This approach highlights the importance of planning in ensuring grammatical and logical correctness in speech. In neural machine translation, structural planning is essential for generating accurate translations. By manipulating planner codes, different syntactic structures can be achieved. Unlike humans, NMT models do not have a planning phase when generating sentences, leading to uncertainty in structural information until concrete words are produced. This highlights the importance of planning in ensuring grammatical and logical correctness in machine translation tasks. In neural machine translation, structural planning is crucial for accurate translations. By inserting planner codes at the beginning of output sentences, the model can plan the sentence structure before decoding actual words. This helps address uncertainty in word prediction and ensures grammatical correctness in the translation process. In neural machine translation, structural planning is essential for accurate translations. Planner codes are inserted at the beginning of output sentences to disambiguate uncertain information about sentence structure. This helps regulate the search space and improve the effectiveness of beam search. The codes are learned through a discretization bottleneck in an end-to-end network that reconstructs the syntactic structure using both the input sentence and the planner codes. In this work, simplified POS tags are used for structural annotation S Y. Planner codes are learned through a discretization bottleneck in an end-to-end network. The codes are merged with target sentences in training data, improving translation performance. The structure of output sentences can be controlled by manipulating planner codes. Structural annotation is extracted to describe the \"big picture\" of sentences, reducing uncertainty in decoding phase. In this work, a code learning model is used to obtain planner codes for sentence generation. Structural annotations are extracted to provide a high-level overview of the sentence, reducing uncertainty in the decoding phase. The annotations simplify POS tags of the target sentence by focusing on key elements like nouns and verbs. This process helps in controlling the structure of output sentences and improving translation performance. The code learning model extracts structural annotations to simplify POS tags for sentence generation, reducing uncertainty in decoding. The model computes discrete codes based on POS tags and uses Gumbel-Softmax trick for approximated one-hot vectors. The code learning model utilizes a backward LSTM to encode tag sequences, followed by discretization into one-hot vectors using Gumbel-Softmax trick. Information from input and vectors is combined to initialize a decoder LSTM for sequential tag prediction. The model architecture includes a context input X to the decoder, optimized with crossentropy loss. Planner codes C can be obtained for all target sentences after training. The code learning model, depicted in Fig. 2, is a sequence auto-encoder with an extra context input X to the decoder. Parameters are optimized with crossentropy loss. After training, planner codes C are obtained for all target sentences. The training data is then modified to include (X, C Y ; Y) pairs. A regular NMT model is trained using the modified dataset, with beam search used during decoding. Planner codes are searched before emitting real words, and removed during evaluation. Recent methods aim to improve syntactic correctness of translations, with BID19 restricting the search space of the NMT decoder. Recent methods aim to improve the syntactic correctness of translations by incorporating target-side syntactic structures explicitly. Various approaches have been proposed, such as BID19 restricting the search space of the NMT decoder and BID2 using a multi-task approach to parse a dependency tree. Aharoni and Goldberg (2017) trained a NMT model to generate linearized constituent parse trees instead of predicting words. Additionally, BID20 proposed a model to generate words and parse actions simultaneously, conditioning word prediction and action prediction on each other. However, none of these methods plan the structure before decoding sentences. Goldberg (2017) trained a NMT model to generate linearized constituent parse trees. BID20 proposed a model to generate words and parse actions simultaneously, conditioning word prediction and action prediction on each other. Some works also learn discrete codes for different purposes, like compressing word embeddings and breaking down dependencies among words. The models were evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks. Tokenization was done using Kytea for Japanese texts and moses toolkit for other languages, with bytepair encoding used in the code learning model. Goldberg (2017) trained a NMT model to generate linearized constituent parse trees. BID20 proposed a model to generate words and parse actions simultaneously, conditioning word prediction and action prediction on each other. Some works also learn discrete codes for different purposes, like compressing word embeddings and breaking down dependencies among words. The models were evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks. Tokenization was done using Kytea for Japanese texts and moses toolkit for other languages, with bytepair encoding used in the code learning model. The model is trained using Nesterov's accelerated gradient (NAG) for maximum 50 epochs with a learning rate of 0.25. Different settings of code length N and the number of code types K are tested, with a clear trade-off between S Y accuracy and C Y accuracy observed. The NMT model shows a trade-off between source sentence accuracy (S Y) and code accuracy (C Y). Using 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with Key-Value Attention, a balanced trade-off is achieved. The model also utilizes residual connections, dropout with a rate of 0.2, and the NAG optimizer for training. The NMT model utilizes residual connections and dropout with a rate of 0.2 for training. The NAG optimizer is used with a learning rate of 0.25, annealed by a factor of 10 if no improvement is seen in 20K iterations. Conditioning word prediction on generated planner codes improves translation performance. Greedy search on JaEn dataset results in lower BLEU score compared to baseline. Beam search on planner codes followed by greedy search does not significantly change results. The NMT model uses residual connections and dropout during training. Greedy search on JaEn dataset yields lower BLEU score than baseline. Beam search on planner codes, followed by greedy search, does not show significant improvement. It is suggested to explore multiple candidates with diverse structures simultaneously for better results. Manual selection of planner codes can also be considered. Table 3 shows candidate translations generated by the model based on different planner codes in a Ja-En task. Manipulating the codes results in translations with varied structures, indicating the method's potential for producing diverse paraphrased translations. The distribution of assigned planner codes for English sentences in the ASPEC Ja-En dataset is also illustrated in Figure 3, highlighting the potential for sampling diverse translations. The proposed method allows for diverse translations by manipulating planner codes in the ASPEC Ja-En dataset. The distribution of codes learned for English sentences shows a skewed distribution, indicating room for improvement. Predicting structural annotations directly may lead to error propagation in word generation. In this paper, a planning phase is added to neural machine translation to generate planner codes that control the output sentence structure. An end-to-end neural network with a discretization bottleneck is designed to predict simplified POS tags of target sentences. The method improves translation performance and allows for sampling translations with different structures. The planning phase helps remove uncertainty in the decoding algorithm. The addition of a planning phase in neural machine translation improves translation performance by generating planner codes that control sentence structure. This framework can be extended to plan other latent factors like sentiment or topic."
}