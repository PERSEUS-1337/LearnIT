{
    "title": "S1HlA-ZAZ",
    "content": "We introduce an end-to-end trained memory system inspired by Kanerva's sparse distributed memory. The memory has a robust distributed reading and writing mechanism, analytically tractable for optimal compression. Formulated as a hierarchical conditional generative model, it combines top-down memory and bottom-up perception to produce observation codes. Empirical results show improved generative models on Omniglot and CIFAR datasets compared to DNC. Our memory model has greater capacity and is easier to train than DNC variants. Our memory model, inspired by Kanerva's sparse distributed memory, has greater capacity and is easier to train compared to the Differentiable Neural Computer (DNC) and its variants. The basic problem of efficiently using memory in neural networks remains open, with models like DNC collapsing reading and writing into single slots. This leads to information not being shared across memory slots, requiring additional slots for new inputs. Matching Networks and Neural Episodic Controller directly store data embeddings, increasing memory volume with the number of inputs. The Neural Statistician BID7 summarizes datasets by averaging embeddings, resulting in small statistics but potentially losing information. Associative memory architectures like the Hopfield Net BID14 store patterns in low-energy states, providing insight into efficient memory design. The Hopfield Net BID14 stores patterns in low-energy states, while the Boltzmann Machine BID1 introduces latent variables but has slow reading and writing mechanisms. Kanerva's sparse distributed memory model BID15 resolves this issue by allowing fast reads and writes and dissociating capacity from input dimensionality. A conditional generative memory model inspired by Kanerva's sparse distributed memory is presented in this paper. The paper presents a conditional generative memory model inspired by Kanerva's sparse distributed memory. It introduces learnable addresses and reparametrised latent variables to optimize memory writing operation. The hierarchical generative model adapts quickly to new data, providing top-down knowledge in addition to bottom-up perception. Our proposal introduces a memory system that adapts quickly to new data, enriching priors in VAE-like models through an adaptive memory. It offers effective online distributed writing for compression and storage of complex data, extending the VAE with a memory store for the prior distribution. The VAE's generative model involves a latent variable z with prior and conditional distributions. The intractable posterior is approximated by an inference model. Parameters are represented by \u03b8 for the generative model and \u03c6 for the inference model. Training aims to maximize log-likelihood by optimizing \u03b8 and \u03c6 for a variational lower-bound. The dataset consists of iid samples. The objective is to achieve a negative term in the likelihood. The objective of training a VAE is to maximize its log-likelihood by optimizing \u03b8 and \u03c6 for a variational lower-bound. This includes a negative reconstruction loss term and a regularizer term. The model introduces the concept of an exchangeable episode and aims to maximize expected conditional log-likelihood. The conditional log-likelihood BID5 is maximized by utilizing the conditional independence of x t given memory M. The joint distribution of p(X, M) is factorized into p(X) and posterior p(M |X), interpreting computing p(M |X) as writing X into memory. This approach formulates memory-based generative models by maximizing the mutual information I(X; M) between memory and the episode to store. The joint distribution of the generative model can be factorized as the memory M is a random matrix with a matrix variate Gaussian distribution. The distribution is equivalent to the multivariate Gaussian distribution of vectorized M. Independence is assumed between the columns but not the rows of M. The memory matrix M follows a multivariate Gaussian distribution, with independence between columns but not rows. Addresses A are optimised through back-propagation, with a normalisation constraint. The addressing variable yt computes weights for memory access, with a prior distribution of isotropic Gaussian. The addressing variable yt computes weights for memory access using an isotropic Gaussian prior. A learned projection transforms yt into a key vector, and weights across memory rows are computed. The code zt generates samples of xt through a conditional distribution with a memory-dependent prior. The projection is implemented as a multi-layer perception to better suit addressing. The distribution p \u03b8 (x t |z t ) is tied for all t \u2208 {1 . . . T }. The memory-dependent prior for z t results in a richer marginal distribution, dependent on memory and the addressing variable. In the hierarchical model, M captures global statistics for an episode, while local latent variables y t and z t capture local statistics for data x t within an episode. The reading inference model factorizes the approximated posterior distribution using conditional independence. The reading inference model factorizes the approximated posterior distribution using conditional independence, balancing the trade-off between preserving old information and writing new information through Bayes' rule. The trade-off between preserving old information and writing new information is balanced optimally through Bayes' rule. Memory writing is interpreted as inference, computing the posterior distribution of memory. Batch inference involves directly computing the posterior, while online inference sequentially accumulates evidence. The posterior distribution of memory is approximated using one sample of y t, x t. The addressing variable and code posterior distributions are parameterized. The posterior of memory is analytically tractable in the linear Gaussian model. Parameters R and U are updated based on the prediction error and cross-covariance between observations and memory. The covariance for z is also considered in the update process. The update rule for memory in the linear Gaussian model involves prediction error, cross-covariance, noise variance, and covariance for z. Prior parameters are trained through back-propagation to learn the dataset structure, while the posterior adapts to features in the data. The main cost of the update rule is inverting \u03a3 z, with a complexity of O(T^3). On-line updating can reduce per-step cost by using one sample at a time. The update rule for memory in the linear Gaussian model involves inverting \u03a3 z with a complexity of O(T^3). On-line updating can reduce per-step cost by using one sample at a time, and intermediate updates can be done with mini-batches. The storage and multiplication of the memory's row-covariance matrix U is another major cost, with a complexity of O(K^2). Although reducing this covariance to diagonal can lower the cost to O(K), it is found to be useful for coordinating memory accessing. To reduce the cost of the linear Gaussian model, diagonalizing the covariance can lower it to O(K). Training the model involves optimizing a variational lower-bound of the conditional likelihood. Sampling from q\u03c6(yt, zt|xt, M) approximates the inner expectation efficiently. Using a mean-field approximation for memory improves computational efficiency. Investigating low-rank approximation of U is a potential future direction for balancing cost and performance. To improve efficiency, a mean-field approximation is used for memory, replacing memory samples. Analytical tractability of the Gaussian distribution allows for distribution-based reading and writing operations. The model features iterative sampling with feedback loops for multiple iterations. Kanerva's sparse distributed memory demonstrates dynamics that enhance learning. The iterative sampling mechanism in Kanerva's sparse distributed memory improves memory reading by feeding back output as input for multiple iterations, reducing errors and converging to stored memories. This iterative process is also present in the model discussed, enhancing denoising and sampling performance. Using knowledge about memory in reading suggests using a parameterised model with memory input, although this can be costly. To improve memory reading, a parameterised model with memory input is suggested. Training with the whole matrix M can be costly, but loopy belief-propagation can efficiently approximate intractable posteriors. Iterative sampling with the model is likely to converge to the true posterior q \u03c6 (y t |x t , M ). Future research will aim to better understand this process. To improve memory reading, a parameterised model with memory input is suggested. Sampling with the model is likely to converge to the true posterior q \u03c6 (y t |x t , M ). Details of the model implementation are in Appendix C. The model uses encoder and decoder models to evaluate the improvements of an adaptive memory. The experiments use the same model architecture with variations in filters, memory size, and code size. The Adam optimiser was used for training with minimal tuning. The variational lower bound (eq. 12) L divided by the length of episode T is reported for comparison with existing models. The Omniglot dataset was initially used for testing the model. In experiments with model BID16, the variational lower bound is reported for comparison with existing models using the Omniglot dataset. A 64 \u00d7 100 memory M and a smaller 64 \u00d7 50 address matrix A are used. 32 images are randomly sampled to form an \"episode\" without class labels. The model is optimized using Adam with a learning rate of 1 \u00d7 10 \u22124. In experiments with model BID16, the variational lower bound is reported for comparison with existing models using the Omniglot dataset. A 64 \u00d7 100 memory M and a smaller 64 \u00d7 50 address matrix A are used. 32 images are randomly sampled to form an \"episode\" without class labels. The model is optimized using Adam with a learning rate of 1 \u00d7 10 \u22124. In a worst-case scenario, images in an episode have little redundant information for compression. The model is tested with the CIFAR dataset, which contains more information than a binary omniglot pattern. Label information is discarded for unsupervised testing. Convolutional coders with 32 features at each layer, a code size of 200, and a 128 \u00d7 200 memory with a 128 \u00d7 50 address matrix are used for CIFAR. Training process of the model is compared with a baseline VAE model using the same encoder. The training process of the model with a Kanerva Machine is compared to a baseline VAE model using the same encoder and decoder. The model shows a modest increase in parameters, with stable training and lower negative variational lower-bound. The model learns to use memory efficiently, as seen in the dip in KL-divergence. Our model with a Kanerva Machine achieved significantly lower negative variational lower-bound compared to a VAE trained on the Omniglot dataset. The model showed better reconstruction and KL-divergence, with the KL-divergence sharply decreasing around the 2000th step, indicating efficient memory utilization. The top-down prior from memory provided most of the information for the code, resulting in a rich prior at the cost of additional KL-divergence for y t. Training curves were similar to CIFAR training. At the end of training, the VAE reached a negative log-likelihood (NLL) of \u2264 112.7, worse than state-of-the-art unconditioned generation but comparable to IWAE training results. The reduction in KL-divergence, rather than reconstruction loss, was crucial for improving sample quality, as observed in experiments with Omniglot and CIFAR datasets. The Kanerva Machine achieved a conditional NLL of 68.3, showcasing the power of incorporating an adaptive memory into generative models. The weights were well distributed over the memory, demonstrating how patterns were superimposed on others during reconstruction. The weights in the memory were well distributed, showing patterns superimposed during reconstruction. The text discusses denoising through iterative reading and generalizing \"one-shot\" generation to batches of images with many classes. In this section, the text discusses how samples are shaped by conditioning data using trained models tested with episodes from 2, 4, or 12 classes. The sample quality improved in consecutive iterations, reflecting the conditioning patterns' statistics. This approach does not apply to VAEs, as shown by feeding back output from VAEs into the next iteration, where sample quality did not improve. The comparison of samples from CIFAR dataset shows that VAEs do not improve sample quality in consecutive iterations, unlike the Kanerva Machine which maintains clear local structures. The model was also tested on recovering original images from corrupted inputs, demonstrating its generalization ability. The VAE model was tested on recovering original images from corrupted inputs, showing generalization ability. The model's structure allows interpretability of internal representations in memory, with meaningful linear interpolations between address weights. The VAE model allows for interpretability of internal representations in memory through linear interpolations between address weights. Comparisons with DNC and Kanerva Machine show differences in training curves and test variational lower-bounds. The section compares the VAE model with DNC and Kanerva Machine in terms of training curves and test variational lower-bounds. The DNC model reached a test loss close to 100 but was sensitive to hyper-parameters and random initialization. Only 2 out of 6 instances with the best hyper-parameter configuration reached this level. The Kanerva Machine, unlike the DNC, is robust to hyper-parameters and trained fastest with batch size 16 and learning rate 1 \u00d7 10 \u22124, converging below 70 test loss with all configurations. This makes it significantly easier to train due to principled reading and writing operations that do not rely on model parameters. The Kanerva Machine is a novel memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory. It generalizes well to larger episodes and maintains an advantage over the DNC in terms of variational lower-bound. The model can exploit redundancy in episodes with fewer classes, resulting in lower reconstruction losses. The Kanerva Machine is a memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model. It removes the assumption of a uniform data distribution by training a generative model to learn the observed data distribution. This allows for retrieving unseen patterns through sampling from memory. Probabilistic interpretations of Kanerva's model have been explored in previous works. Our model generalizes Kanerva's memory model to continuous, non-uniform data with Bayesian inference. It integrates with deep neural networks, unlike other models that do not update memory following learning. Our model extends Kanerva's memory model to continuous data using Bayesian inference and deep neural networks. Unlike other models, our model quickly adapts to new data and stores information in a compressed form by leveraging statistical regularity in images. Our model efficiently stores information in a compressed form by leveraging statistical regularity in images through the encoder, learned addresses, and Bayes' rule for memory updates. It employs an exact Bayes' update-rule without compromising neural networks' flexibility. The combination of classical statistical models and neural networks shows promising results for novel memory models in machine learning. Kanerva's memory model combines classical statistical models and neural networks for novel memory models in machine learning. It involves fixed addresses A pointing to modifiable memory M, both of size K \u00d7 D. Inputs are uniform random vectors compared through Hamming distance. The memory model involves fixed addresses A pointing to modifiable memory M, both of size K \u00d7 D. Inputs are compared through Hamming distance, with addresses selected based on a threshold. Patterns are stored and read from memory using binary weight vectors. The reading process can be iterated multiple times for output. Kanerva's model involves summing addresses to pass a threshold and produce a readout. The process can be iterated by feeding back the output as input. When K and D are large enough, a small portion of addresses will always be selected, making operations sparse and distributed. Stored vectors can be retrieved correctly even if an address's content is overwritten. Kanerva also showed that a significantly corrupted query can still be discovered through iterative reading. However, the model's application is limited by the assumption of a uniform and binary data distribution, which is rarely true in real-world data. The model architecture includes a convolutional encoder converting input images into embedding vectors. It consists of 3 blocks with convolutional layers and ResNet blocks, with varying filter sizes depending on the dataset. The model architecture includes a convolutional encoder with 4 \u00d7 4 filter and stride 2, followed by ResNet blocks. The convolutional layers have 16 or 32 filters. The output is linearly projected to a 2C dimensional vector. Adding noise to the input helps stabilize training. Gaussian noise with zero mean and standard deviation of 0.2 is used. Different likelihood functions are used for different datasets. The added noise, Gaussian with zero mean and standard deviation of 0.2, is used consistently. Bernoulli likelihood is used for Omniglot dataset, and Gaussian likelihood for CIFAR. To prevent likelihood collapsing, uniform noise U(0, 1/256) is added to CIFAR images during training. The differentiable neural computer (DNC) is wrapped with the same interface as Kanerva memory for fair comparison. DNC receives addressing variable y_t and z_t during reading and writing stages, with z_t sampled from q\u03c6(z_t | x_t). During writing, y_t and z_t are concatenated as input into the memory controller. During writing, the DNC discards the read-out and only keeps its state as memory. Reading involves discarding the state at each step to prevent it from storing new information. A 2-layer MLP with 200 hidden neurons and ReLU nonlinearity is used as the controller instead of LSTM to avoid interference with DNC's external memory. The issue of controllers bypassing memory output is addressed by removing this output to ensure proper functioning. The DNC BID10 BID24 ensures proper functioning by removing controller output that bypasses memory, focusing on memory performance. Test loss is compared between models using full and diagonal covariance matrices, with the former showing faster decrease in loss. The 8 models described in section 4 were trained on machines with similar setups. Models using full covariance matrices were slightly slower per-iteration but had a faster decrease in test loss. The bottom-up stream in the model compensates for memory by sampling z t from p \u03b8 (z t |y t , M ) for the decoder p \u03b8 (x t |z t ). The negative variational lower bound, reconstruction loss, and total KL-divergence during CIFAR training show similar patterns to those in Fig. 2. The small difference in KL-divergence significantly affects sample quality. Training is ongoing, and the Kanerva Machine is showing advantages over the VAE. The training of Omniglot models shows similarities to Fig. 2. The difference in KL-divergence impacts sample quality significantly. The advantage of the Kanerva Machine over VAE is increasing. Linear Gaussian model is defined by Eq. 6. The joint distribution and posterior distribution are derived using Gaussian conditional formula. Update rules are obtained from matrix variate Gaussian distribution properties. The model described works well for writing to memory using samples from q \u03c6 (z t |x t ). The update rule in equations 9 to 11 is derived from properties of matrix variate Gaussian distribution. An alternative method is described that fully utilizes the analytic tractability of the Gaussian distribution for reading and writing to memory. The parameters of the memory are denoted by \u03c8 = {R, U, V}."
}