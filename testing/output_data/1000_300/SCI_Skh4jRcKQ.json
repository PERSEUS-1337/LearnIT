{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified in learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. The unusual \"coarse gradient\" provided by the STE-modified chain rule is crucial in minimizing the training loss. The study focuses on learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. The use of the straight-through estimator (STE) in the backward pass is crucial for minimizing the training loss. Proper selection of STE leads to a positive correlation between the expected coarse gradient and the population gradient, aiding in minimizing the population loss. However, a poor choice of STE can result in instability during training, as demonstrated in CIFAR-10 experiments. The study also highlights the success of deep neural networks in various machine learning applications. Recent efforts have been made to train coarsely quantized DNNs to achieve memory savings and energy efficiency during inference. This approach aims to maintain performance while reducing the memory storage and computational requirements of deep neural networks. Recent efforts have been made to train coarsely quantized DNNs to achieve memory savings and energy efficiency during inference, while maintaining performance. Weight quantization of DNNs has been extensively studied in the literature, and training fully quantized DNNs poses a challenging optimization problem. The gradient in training activation quantized DNNs is almost everywhere zero, making standard back-propagation inapplicable. Training activation quantized DNNs poses a challenge due to the gradient being almost everywhere zero. To address this, a non-trivial search direction is constructed by modifying the chain rule, using the straight-through estimator (STE). Alternative approaches include stochastic neurons and the target propagation algorithm for learning binary activated networks. The straight-through estimator (STE) is a method based on the perceptron algorithm for training binary activated networks. It uses a modified chain rule to calculate gradients and has been extended to train multi-layer networks with binary activations. The straight-through estimator (STE) is a method used to train binary activated networks, extended to multi-layer networks with binary activations. Various derivatives of activation functions have been proposed in the literature for training deep neural networks with constrained weights and activations. The straight-through estimator (STE) is used to train binary activated networks, extended to multi-layer networks with binary activations. Recent studies have explored different proxies for quantized ReLU activations, including derivatives of vanilla ReLU and clipped ReLU. Limited theoretical understanding exists for training DNN with stair-case activations using STE. Various scenarios where certain layers are not ideal for back-propagation have been discussed, such as leaky ReLU activation and implicit weighted nonlocal Laplacian layers. In a recent study by Wang et al. (2018) and Athalye et al. (2018), an implicit weighted nonlocal Laplacian layer was proposed to improve the generalization accuracy of DNN classifiers. Athalye et al. (2018) introduced a backward pass differentiable approximation to circumvent adversarial defenses, breaking defenses at ICLR 2018. The \"gradient\" of the loss function w.r.t. weight variables through the STE-modified chain rule is referred to as coarse gradient, which is not the standard gradient descent. In a recent study, the coarse gradient is discussed, which is not the standard gradient of the loss function. The focus is on understanding the choice of Sign Thresholding Estimator (STE) in training quantized ReLU nets. Different STEs are considered for learning a two-linear-layer network with binary activation and Gaussian data. The model of population loss minimization is adopted to analyze the optimization perspective. The study focuses on the Sign Thresholding Estimator (STE) for training quantized ReLU nets, proving that proper choices of STE lead to descent training algorithms. The negative expected coarse gradients of vanilla and clipped ReLUs are descent directions for minimizing population loss, while the identity STE can lead to instability near local minima. Empirical performance of the three STEs is examined on MNIST and CIFAR-10 classifications with general quantized ReLU. The study examines the empirical performances of three Sign Thresholding Estimators (STEs) on MNIST and CIFAR-10 classifications with general quantized ReLU. While vanilla and clipped ReLUs work well on shallow networks, clipped ReLU STE is preferred for deeper networks. Training with identity or ReLU STE can be unstable at good minima, leading to higher training loss and decreased generalization accuracy. This suggests that poor STEs generate coarse gradients incompatible with the energy landscape. The study explores the performance of Sign Thresholding Estimators (STEs) on MNIST and CIFAR-10 classifications with general quantized ReLU. While vanilla and clipped ReLUs are effective for shallow networks, clipped ReLU STE is preferred for deeper networks due to instability issues with identity or ReLU STEs. Poor STEs generate coarse gradients that are incompatible with the energy landscape, impacting training loss and generalization accuracy. The study examines the role of quantized activation function monotonicity in coarse gradient descent. Sign Thresholding Estimators (STEs) exploit this property, with clipped ReLU STE preferred for deeper networks to avoid instability issues. Empirical comparisons of STEs in 2-bit and 4-bit activation quantization reveal instability phenomena in CIFAR experiments. Technical proofs and figures are deferred to the appendix due to space limitations. The study discusses algorithms related to poor STEs observed in CIFAR experiments, with technical proofs and figures deferred to the appendix due to space constraints. Key notations and model details are provided, including the use of trainable weights in linear layers and activation functions on input vectors. The curr_chunk discusses the structure of the first and second linear layers, the activation function used, and the generation of labels in a convolutional layer. It also mentions the assumption of Gaussian distribution for the entries of Z. The activation function used is a binary function instead of ReLU. The curr_chunk discusses the Gaussian distribution assumption for Z entries in a learning task, focusing on population loss minimization and gradient calculations. The idea of replacing a zero component with a non-trivial function in training is introduced. The curr_chunk introduces the concept of replacing a zero component with a non-trivial function in training using the STE \u00b5. This method is applied to train a two-linear-layer CNN with binary activation, resulting in a coarse gradient descent algorithm. Preliminaries about the landscape of the population loss function f (v, w) are also discussed. The curr_chunk discusses the population loss function f(v, w) and its partial gradients with respect to v and w. It also mentions the conditions for local minimizers in the model. The text discusses the conditions for local minimizers in the model, stating that stationary points can only be saddle points and non-differentiable points are global minimizers. It also proves that the population gradient is Lipschitz continuous on bounded domains. The text discusses the conditions for local minimizers in the model, stating that stationary points can only be saddle points and non-differentiable points are global minimizers. It also proves that the population gradient is Lipschitz continuous on bounded domains. The main results focus on the behaviors of the coarse gradient descent algorithm using different activation functions, showing convergence to critical points with ReLU derivatives but not with the identity function. The text proves that Algorithm 1 using ReLU derivatives converges to a critical point, while the identity function does not. With a small learning rate, the objective sequence decreases monotonically, converging to a saddle point or local minimizer. The convergence guarantee is established for coarse gradient descent under certain assumptions. The convergence properties of Algorithm 1 with the identity function \u00b5(x) = x do not hold near local minimizers satisfying \u03b8(w, w*) = \u03c0. The empirical loss descends along the negative coarse gradient direction with few data samples, gaining monotonicity and smoothness as sample size increases. This explains the effectiveness of STE with large datasets in deep learning. The same results hold for a rotation-invariant distribution of input data. The proof for the main results assumes input data rows are i.i.d. following a rotation-invariant distribution. Mathematical analysis is sketched for different sample sizes, showing plots of empirical loss moving by negative coarse gradient direction. The key observation is that the coarse partial gradient has non-negative correlation with the population partial gradient, forming a descent direction. The key observation in Lemma 5 is that the coarse partial gradient has non-negative correlation with the population partial gradient, forming a descent direction for minimizing the population loss. Lemma 5 also provides conditions for the inner product between expected coarse and population gradients. Additionally, the significance of the estimate (12) in guaranteeing the descent property of Algorithm 1 is highlighted. The coarse gradient descent behaves similarly to gradient descent on f(v, w). The estimate (12) guarantees the descent property of Algorithm 1 by ensuring monotonically decreasing energy until convergence. When Algorithm 1 converges using ReLU STE, it reaches a critical point of the population loss function. For clipped ReLU STE, similar results are obtained. The coarse partial gradient using clipped ReLU STE generally has a positive correlation. The coarse gradient using clipped ReLU STE has positive correlation with the true gradient and vanishes only at critical points. When Algorithm 1 converges, the coarse and true gradients vanish simultaneously, occurring at saddle points. Lemma 8 states that Algorithm 1 converges when the coarse and true gradients vanish simultaneously at saddle points. However, Lemma 9 and Lemma 10 show that the coarse gradient derived from the identity function may not vanish at local minima, potentially preventing Algorithm 1 from converging. The coarse gradient descent may not converge near spurious minimizers if the inner product between expected coarse and true gradients does not vanish. The training loss increases and instability arises as the iterates approach the minimizer, leading to different empirical performances of vanilla and clipped ReLUs on deeper nets. In this section, the performances of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations are compared. The instability issue of using an improper STE in training algorithms is also reported. The resolution \u03b1 for quantized ReLU must be carefully chosen to maintain accuracy, following a modified batch normalization approach. In experiments, weights are kept float and resolution \u03b1 for quantized ReLU is crucial for accuracy. A modified batch normalization layer is used without scale and shift, following a unit Gaussian distribution. \u03b1 is pre-computed using Lloyd's algorithm on simulated data and fixed during training. Batch normalization is added to each activation layer in LeNet-5. The quantization approach used is HWGQ (Cai et al., 2017), with a focus on uniformity. In experiments, weights are kept float and resolution \u03b1 for quantized ReLU is crucial for accuracy. A modified batch normalization layer is used without scale and shift, following a unit Gaussian distribution. \u03b1 is pre-computed using Lloyd's algorithm on simulated data and fixed during training. We add batch normalization to each activation layer in LeNet-5. The optimizer used is stochastic gradient descent with momentum = 0.9. Training consists of 50 epochs for LeNet-5 on MNIST, and 200 epochs for VGG-11 and ResNet-20 on CIFAR-10. Parameters are initialized with those from their pre-trained full-precision counterparts. The schedule of the learning rate is specified in the appendix. Experimental results are summarized in Table 1, showing training losses and validation accuracies. Among the three STEs, the derivative of clipped ReLU performs the best, followed by vanilla ReLU. The learning rate is specified in TAB2 in the appendix. Experimental results in Table 1 show training losses and validation accuracies for three different STEs. Clipped ReLU performs best overall, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on the shallow LeNet-5 network. The phenomenon of being repelled from a good minimum is observed on ResNet-20 with 4-bit activations using the identity STE, demonstrating an instability issue as predicted in Theorem 1. Coarse gradient descent algorithms using vanilla and clipped ReLUs converge to minima neighborhoods with validation accuracies. The experimental results in Table 1 demonstrate the instability issue with 4-bit activations using the identity Straight-Through Estimator (STE). Training with the identity STE leads to lower validation accuracies compared to using clipped ReLU. Despite the choice of STE, the landscape of the loss function remains consistent. Training with a tiny learning rate initially results in a significant increase in training loss and validation error within the first 20 epochs. Switching to a normal learning rate schedule at epoch 20 and running 200 additional epochs helps stabilize the training process. The training process with 4-bit activations using the identity Straight-Through Estimator (STE) initially shows instability, leading to increased training loss and validation error within the first 20 epochs. Switching to a normal learning rate schedule at epoch 20 and running 200 additional epochs helps stabilize the training. The use of identity STE results in a worse minimum due to coarse gradients that do not vanish at good minima, while ReLU STE also performs poorly on ResNet-20 due to instability at good minima. The coarse gradient descent using the identity Straight-Through Estimator (STE) on ResNet-20 with 4-bit activations is repelled from good minima. The learning rate is set to 10^-5 until epoch 20. Theoretical justification for STE as a descent training algorithm is provided. Three STEs were considered for learning a two-linear-layer CNN with binary activation. Negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing population loss, while identity STE generates incompatible coarse gradients. Instability issue was confirmed in CIFAR experiments for improper STE choices. The identity Straight-Through Estimator (STE) generates coarse gradients incompatible with the energy landscape, leading to instability in CIFAR experiments. Further research aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. Lemma 11 states properties of Gaussian random vectors with nonzero vectors and angles. The proof involves various identities and assumptions, leading to the conclusion of the lemma. Lemma 12 discusses Gaussian random vectors with nonzero vectors and angles. The proof involves identities and inequalities, leading to the conclusion of the lemma. The second lemma discusses population loss and partial gradients of a function with specific conditions on the vectors involved. The proof involves various calculations and inequalities to establish the results. Lemma 2 discusses partial gradients of a function under specific conditions on the vectors involved. The proof involves calculations and inequalities to establish the results, showing that stationary points are saddle points. The objective function is rewritten and its Hessian matrix is shown to be indefinite, indicating saddle points. Perturbed objective values are compared to show that for small non-zero changes, the objective value increases. Additionally, a Lipschitz constant is introduced to establish results for differentiable points. The text discusses the proof of Lemma 4, which involves expected partial gradients and Lipschitz constants for differentiable points. The expected coarse gradient is also considered in the context of the objective function's Hessian matrix indicating saddle points. Lemma 4 is proven by showing the inner product between expected coarse and true gradients. If certain conditions are met, a constant A_relu exists. Lemma 6 shows that under certain conditions, specific expressions for v and \u03b8(w, w*) are satisfied. Lemma 7 provides further insights into the inner product between expected coarse and true gradients with additional conditions. Lemma 7 delves into the inner product between expected coarse and true gradients under certain conditions. The proof involves calculations and the use of Cauchy-Schwarz inequality. Lemma 8 is similar to Lemma 6, with q(\u03b8, w) non-negative and equal to 0 only at \u03b8 = 0, \u03c0. Lemma 9 states that the expected coarse partial gradient w.r.t. w is \u00b5(x) = x. Lemma 10 discusses the inner product between expected coarse and true gradients when w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0)."
}