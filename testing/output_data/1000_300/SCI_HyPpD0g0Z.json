{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features that do not change across domains and \"orthogonal\" features that can vary across domains. It is important to focus on the \"conditionally invariant\" features to guard against future adversarial domain shifts. The domain itself is assumed to be a latent variable in this approach. When training deep neural networks for image classification, it is crucial to focus on \"conditionally invariant\" features that do not change across domains to guard against future adversarial shifts. The domain is considered a latent variable, making it impossible to directly observe distributional changes in features across different domains. Data augmentation involves generating multiple images from an original image, with an ID variable linking them. Only a small fraction of images need to have an ID variable for this method to work effectively. In a causal framework, the ID variable is added to the model to treat the domain as a latent variable. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network using a graph Laplacian improves performance in settings with changing domains. This approach links to interpretability, fairness, and transfer learning in deep neural networks. Deep neural networks have excelled in prediction tasks like visual object recognition. Issues can arise when learned representations rely on dependencies that vanish in test distributions, leading to domain shifts and degraded performance. An example is the \"Russian tank legend\" where sampling biases in training data were not replicated in the real world. The \"Russian tank legend\" is an example of sampling biases in training data that did not replicate in the real world. The machine learning system was trained to distinguish between Russian and American tanks from photos, with high accuracy due to image quality differences. This highlights the importance of large sample sizes in deep learning to mitigate the effects of hidden confounding factors. Deep learning requires large sample sizes to mitigate the effects of confounding factors and achieve invariance to known factors like translation and rotation. Adversarial examples, which are imperceptibly perturbed inputs misclassified by ML models, highlight the difference between human and artificial cognition. Humans can achieve invariance with just a few rotated examples of the same object. Humans can achieve invariance with just a few rotated examples of the same object, questioning if we can mimic this ability in deep neural networks. Biases in training datasets can lead to unfair discrimination in ML algorithms, as seen in Google's photo app tagging non-white people as \"gorillas\" due to biased training examples. In response to biases in training datasets leading to unfair discrimination in ML algorithms, a proposed solution is counterfactual regularization (CORE) to control latent features extracted by estimators. The goal is for classifiers to use only core features related to the target of interest in a stable manner. Counterfactual regularization (CORE) aims to create an estimator that is invariant to style features, making it robust against domain shifts. It utilizes knowledge about grouping instances related to the same object, reducing the need for data augmentation and improving predictive performance in small sample size settings. The manuscript discusses the introduction of counterfactual regularization (CORE) and its application in improving predictive performance in small sample size settings. It includes examples of how CORE can reduce the need for data augmentation and introduces the concept of counterfactual observations in the logistic regression setting. The CelebA dataset is used to classify whether a person wears glasses, utilizing grouping information to ensure consistent predictions for images of the same person. The text discusses the use of counterfactual observations in logistic regression to improve predictive performance in small sample size settings. Grouping information is utilized to ensure consistent predictions for images of the same person. The training set includes 10 identities with approximately 30 images each. Examples from the CelebA dataset and augmented MNIST dataset are shown, highlighting counterfactual examples. The text discusses the use of counterfactual observations in logistic regression to improve predictive performance in small sample size settings. Grouping information is utilized to ensure consistent predictions for images of the same person. The comparison involves a subsampled CelebA dataset and an augmented MNIST dataset. Exploiting the group structure reduces the average test error by approximately 32%. This method also makes data augmentation more efficient by creating additional samples through modifying original inputs. The estimator in CORE uses grouping information to enforce invariance with respect to style features, such as rotation, in data augmentation. This approach results in more efficient training with fewer augmented samples compared to standard methods. The CORE estimator enforces invariance with style features like rotation in data augmentation, reducing test error from 32.86% to 16.33%. Similar works include BID14 and Domain-Adversarial Neural Networks (DANN), but CORE requires grouped observations while others rely on unlabeled data. The BID13 approach, inspired by BID5, focuses on learning a representation without origin information using adversarial training. In contrast, BID14 identifies conditionally independent features by adjusting variable transformations to minimize squared MMD. The BID14 model adjusts variable transformations to identify conditionally independent features by minimizing squared MMD distance between distributions in different domains. In contrast to our approach, BID14 explicitly observes the domain identifier, while we treat it as latent. Our method penalizes the classifier for using latent features outside the set of conditionally independent features, aligning with causal modeling objectives and guarding against domain shifts. Causal modeling aims to address transfer learning and adversarial domain shifts by ensuring valid predictions under interventions on predictor variables. However, applying causal inference to guard against style feature shifts in image classification poses challenges due to the anti-causal nature of the task and the need to focus on specific interventions. Various approaches have been proposed to leverage causal motivations for deep learning or use deep learning for causal inference, focusing on cause-effect inference and finding causal relations between variables. The challenge lies in guarding against style feature shifts in image classification, which requires specific interventions and considerations not addressed in existing methods. The Neural Causation Coefficient (NCC) is proposed for cause-effect inference between random variables X and Y, distinguishing object features from context features. Generative neural networks are used to identify causal relations and orient graph edges. Bahadori et al. (2017) introduce a regularizer combining penalties with estimated causal probabilities for features. Bahadori et al. (2017) propose a regularizer combining penalties with estimated causal probabilities for features in causal generative models. Besserve et al. (2017) connect GANs with causal generative models using a group theoretic framework. Kocaoglu et al. (2017) suggest causal implicit generative models for sampling from conditional and interventional distributions. BID29 use deep latent variable models to estimate individual treatment effects, while BID21 leverage causal reasoning for fairness considerations in machine learning. The use of deep latent variable models and proxy variables to estimate individual treatment effects is proposed. Causal reasoning is leveraged to characterize fairness considerations in machine learning, deriving causal nondiscrimination criteria. Algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, showing structural similarity to style features. Distinguishing between core and style features is akin to disentangling factors of variation, which has garnered interest in generative modeling. Estimating disentangled factors of variation in generative modeling has gained interest. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce dependence on specified object transforms in the latent representation. The goal is to learn a representation that excludes certain features, such as location, image quality, posture, brightness, background, and contextual information. In Matsuo et al. (2017), the approach involves a data generating process with unobserved noise and a confounding situation where style features differ based on class. Bouchacourt et al. (2017) also work on separating style and content in a variational autoencoder framework. The goal is to solve a classification task directly without explicitly estimating latent factors. In a classification task, the goal is to separate style and content without estimating latent factors explicitly. The prediction is based on a function with parameters corresponding to weights in a DNN. The prediction in image processing is made using a function with parameters representing weights in a DNN. The goal is to minimize the expected loss by penalized empirical risk minimization, choosing weights that minimize the empirical loss with a penalty term. The weights or parameters are optimized to minimize the empirical loss with a penalty term, such as a ridge penalty. The structural model includes latent variables like the domain variable D and ID variable, which groups observations. The prediction is anti-causal, with the class label Y causing changes in the image X through latent variables. The class label Y is causal for the image X, mediated by core features X ci and style features X \u22a5. Interventions \u2206 can affect style features but not core features. The distribution of X ci |Y is constant across domains, while X \u22a5 |Y can vary. The style features X \u22a5 and Y are confounded by latent domain D, while core features X ci are conditionally independent of D|Y. In contrast, the core features satisfy X ci \u22a5 \u22a5 D|Y, while the style variable includes various context-dependent factors. The style intervention variable \u2206 influences both the latent style X \u22a5 and the image X. The prediction under the style intervention \u2206 = \u03b4 is denoted as f \u03b8 (X(\u2206 = \u03b4)). In this work, the causal graph is used to explain domain adaptation, transfer learning, and guarding against adversarial examples. The intervention \u2206 is assumed to be within a ball in q-norm around the origin. Imperceptible changes in X can lead to misclassification, so the goal is to devise a classification in this graph. The goal is to devise a classification in a causal graph that minimizes adversarial loss by considering interventions within a ball in q-norm around the origin. Adversarial domain shifts involve strong interventions on style features that can lead to misclassification. The adversarial loss under large style interventions is characterized by distinguishing between core and style features. The goal is to protect against shifts in test data distributions by considering interventions on style features. Causal inference faces the challenge of never observing counterfactuals directly. The classical problem of causal inference is the inability to observe counterfactuals directly, where treatment effects cannot be simultaneously observed. Counterfactuals involve changing the treatment while keeping other variables constant. In this context, counterfactuals refer to situations where the class label and ID are constant but the style intervention value changes. In causal inference, counterfactuals involve changing the treatment while keeping other variables constant. The style intervention \u2206 plays a role similar to treatment T in medical examples, allowing for different conditions to be observed for the same object (Y, ID). For example, \u2206 could represent variables determining different images of a person wearing glasses, including background, posture, and image quality. In causal inference, counterfactuals involve changing the treatment while keeping other variables constant. The style intervention \u2206 allows for different conditions to be observed for the same object (Y, ID). It corresponds to all variables determining different images of a person wearing glasses, including background, posture, and image quality. The 'treatment effect' of \u2206 occurs in the space of style features X \u22a5, not in the 'conditionally invariant' space X ci. The goal is to penalize any change in classification under different style interventions \u2206 but constant class and identity (Y, ID). The pooled estimator treats all examples identically by summing over the loss with a ridge penalty. The adversarial loss of the pooled estimator may be infinite. The pooled estimator with a cross-validated ridge penalty may have an infinite adversarial loss. Conditions (i) and (ii) ensure that the estimator works well in terms of adversarial loss if certain relationships between variables are not deterministic. The pooled estimator will work well in terms of adversarial loss if certain relationships between variables are absent. To minimize the adversarial loss, the invariant parameter space must ensure that the predictor is constant for all core features. The challenge lies in inferring the invariant space from data. The challenge is inferring the unknown invariant space I from data using empirical risk minimization with a regularization constant \u03c4 to approximate the optimal invariant parameter vector. The regularization constant \u03c4 is used to control variations in estimated predictions for class labels across counterfactuals of an image. The true invariant space I is a subset of the empirically invariant subspace I n. The graph Laplacian regularization penalizes the sum of variances \u03c3 2 i (\u03b8). The graph Laplacian regularization penalizes the sum of variances \u03c32i(\u03b8) in the sample space induced by the identifier variable ID. It is crucial to define the graph in terms of the ID variable to guard against adversarial domain shifts, as shown in experiments. Other regularizations do not perform as well in this aspect. The graph Laplacian regularization penalizes variances induced by the identifier variable ID to guard against adversarial domain shifts. In a one-layer network for binary classification, the CORE estimator outperforms the pooled estimator in handling confounded training data and changing style features in test. The CORE estimator outperforms the pooled estimator in handling confounded training data and changing style features in test distributions. Various experiments were conducted to assess the performance, including classifying elephants and horses with controlled confounding levels. Additional experimental results and implementation details are provided for reference. A TensorFlow BID0 implementation of CORE will be available, along with code for reproducing experiments. Performance is not very sensitive to the choice of penalty \u03bb. Stickmen images are used as an example, with Y representing {adult, child} and X ci as height. Age and movement are dependent due to a hidden common cause D. The data generating process in the training dataset shows a dependence between age and movement, influenced by a hidden common cause. The model's prediction accuracy may fail when presented with new data that does not follow the same patterns observed in the training set. Test sets with interventions show that the dependence between age and movement vanishes, leading to different associations between large movements and children or adults. In test sets 2 and 3, interventions remove the dependence between age and movement. Movements are heavier in test set 3 than in test set 2. Misclassification rates for CORE and the pooled estimator are shown in FIG0. CORE outperforms the pooled estimator with as few as 50 counterfactual observations. Including more counterfactual examples would not improve the pooled estimator's performance. The pooled estimator uses movement as a predictor for age, while CORE does not due to counterfactual regularization. Including more counterfactual examples would not improve the pooled estimator's performance. The CelebA dataset is used to classify whether a person in an image is wearing eyeglasses, with image quality being lower for images showing a person wearing glasses. The strength of the image quality intervention is determined by sampling new image quality as a percentage of the original image's quality from a Gaussian distribution. The image quality intervention in the study involves mimicking lower image quality for people wearing glasses. Counterfactual observations are only available for individuals wearing glasses. Different counterfactual observation settings are discussed, with misclassification rates compared between CORE and the pooled estimator on various test sets. In this study, counterfactual observations are discussed for different settings, with c = 5000 and m = 20000. Misclassification rates for CORE and the pooled estimator are compared on different test sets, where the pooled estimator outperforms CORE on test set 1. However, the pooled estimator struggles on test sets 2-4 due to its reliance on image quality as a predictor for the target. The study compares the performance of CORE and the pooled estimator on different test sets. The pooled estimator struggles on test sets 2-4 due to using image quality as a predictor, while CORE's predictive performance is not affected by changing image quality distributions. The experiment also assesses whether CORE can exclude \"color\" from its learned representation. The study evaluates if CORE can exclude \"color\" from its learned representation by including counterfactual examples of different colors. Using the AwA2 dataset, grayscale images are added for elephants to test this. Results show CORE outperforms the pooled estimator on various test sets, with no impact from changing image quality distributions. The study compares the performance of CORE and the pooled estimator on different test sets with varying color distributions. The pooled estimator struggles on test sets 2 and 3 due to its reliance on the color \"gray\" in the training set, while CORE is less affected by changing color distributions. The study shows that the predictive accuracy of the pooled estimator is influenced by the color \"gray\" in the training set, while CORE is less affected by changing color distributions. Adding grayscale images helps CORE achieve color invariance, which could be important for fairness considerations if color was a protected attribute. The study introduces counterfactual regularization (CORE) to achieve robustness in image analysis by distinguishing core and style features, ensuring fairness by not including \"color\" as a protected attribute in its learned representation. The CORE estimator exploits instances of the same object in training data to achieve invariance of classification performance with respect to adversarial interventions. The CORE estimator aims to achieve invariance of classification performance by exploiting instances of the same object in the training data. It can handle adversarial interventions on style features like image quality, fashion type, color, and body posture. The regularization of CORE can achieve the same classification performance as data augmentation approaches with fewer instances, even when style features are unknown. The regularization of CORE penalizes features that vary strongly between instances of the same object in the training data, avoiding the need for known style features. Using larger models like Inception or ResNet architectures may not guard against interventions on implicit style features. Assessing the benefits of CORE for training Inception-style models in terms of sample efficiency and generalization performance is a potential future direction, especially with the use of video data. An interesting future direction could involve using video data to leverage temporal information for grouping and counterfactual regularization, potentially aiding in debiasing word embeddings. The core features X ci are crucial in the linear structural equation model for image classification. The text discusses the linear relationship between style features and image data using a matrix W. It introduces core features X ci and latent variables in a logistic regression model for predicting Y from X. The training process involves estimating \u03b8 with logistic loss and testing with expected losses on test data. The text discusses logistic loss for training and testing with expected losses on test data, including standard and adversarial interventions. The formulation relies on assumptions regarding the distribution of \u2206 and the matrix W's rank. Assuming \u2206 is sampled from a distribution with positive density on an -ball in 2-norm around the origin, matrix W has full rank, and the number of counterfactual examples is at least as large as the dimension of style variables. Sampling process involves collecting independent samples and selecting random values for counterfactual examples. The pooled estimator has infinite adversarial loss for the CORE estimator, with an equivalent result for misclassification loss. The oracle estimator is constrained to be orthogonal to the column space of W. The pooled estimator has infinite adversarial loss for the CORE estimator, with an equivalent result for misclassification loss. To show that W t\u03b8pool = 0 with probability 1, let \u03b8 * be the oracle estimator constrained to be orthogonal to W. By contradiction, if W t\u03b8pool = 0, then the constraint W t \u03b8 = 0 becomes non-active, implying \u03b8 pool = \u03b8 * and the directional derivative of the training loss should vanish at \u03b8 * for all \u03b4 in the column space of W. The derivative of L n (\u03b8) in direction of \u03b4 is proportional to x i,j \u2208 R p. The oracle estimator \u03b8 * remains the same under true training data and counterfactual training data. The derivative g(\u03b4) can be written as the difference between two formulas. Since \u03b4 is in the column-space of W, there exists u such that \u03b4 = W u. The eigenvalues of W t W are all positive, and r i (\u03b8 * ) is not affected by the interventions \u2206 i,j. The model assumptions state that \u03b4 can be written as W u, and the eigenvalues of W t W are positive. The estimator \u03b8 * remains the same regardless of the training data used. The interventions \u2206 i,j are drawn from a continuous distribution, leading to the left hand side of the equation having a continuous distribution. The probability of the left hand side not being 0 is 1, completing the first part of the proof. With probability 1, \u03b8 core = \u03b8 * as defined in equation (6). The first part of the proof is completed by showing that the left hand side of the equation has a non-zero probability. The second part demonstrates that with probability 1, \u03b8 core = \u03b8 * as defined in equation (6). This is achieved by showing that the estimator remains unchanged when using data without interventions as training data. The estimator remains unchanged when using data without interventions as training data. The proof is completed by showing that \u03b8 core = \u03b8 * with probability 1. The CelebA dataset is used to classify gender based on images. The CelebA dataset is used to classify gender based on images, creating a confounding by including mostly images of men wearing glasses. Counterfactuals are used to assess results, with test sets showing different associations between gender and glasses. In the CelebA dataset, the association between gender and glasses is flipped in test set 2. The study compares training a four-layer CNN end-to-end versus using Inception V3 features and retraining the softmax layer. Results show that as c increases, the performance difference between CORE and the pooled estimator becomes smaller. The pooled estimator performs worse on test set 2 as m becomes larger, indicating a larger exploitation of X \u22a5. In the CelebA dataset, the study analyzes a confounded setting where X \u22a5 represents brightness and the data generating process is illustrated. The pooled estimator performs worse on test set 2 as m increases, indicating a greater exploitation of X \u22a5. The study analyzes a confounded setting where X \u22a5 represents brightness. Test sets show different interventions on image brightness, with the pooled estimator outperforming CORE on test set 1 but struggling on test sets 2 and 4. The study compares the performance of the pooled estimator and CORE in handling changing brightness distributions in test sets. The pooled estimator struggles on test sets 2 and 4 due to using image brightness as a predictor, while CORE is less affected by changing brightness distributions. Different interventions on image brightness are explored, including using a different image of the same person as a counterfactual (CF setting 2) and using an image of a different person as a baseline (CF setting 3). The study compares the performance of the pooled estimator and CORE in handling changing brightness distributions in test sets. Different interventions on image brightness are explored, including using a different image of the same person as a counterfactual (CF setting 2) and using an image of a different person as a baseline (CF setting 3). Counterfactual setting 1 works best as it allows for explicit control over the brightness variation. Counterfactual setting 2 presents challenges as different images of the same person can vary in multiple factors. Grouping images of different persons can still improve predictive performance to some extent. The study explores different interventions on image brightness, including using different images of the same person as a counterfactual and grouping images of different persons to improve predictive performance. The misclassification rates of CORE on the dataset are shown, with results not very sensitive to the penalty choice. Varying the number of identities in the training data set affects the sample sizes and misclassification rates for the test set. The study examines interventions on image brightness, using different images as counterfactuals to improve predictive performance. Total sample sizes range from 321 to 4386, with misclassification rates for the test set shown. CORE enhances predictive performance, especially with small sample sizes, mitigating confounding factors. As sample sizes increase, CORE and pooled estimator performance becomes comparable. Additional results vary the number of augmented training examples for different sample sizes. The experiment introduced in \u00a72.2 explores the impact of varying the number of augmented training examples on misclassification rates. Results show that CORE makes data augmentation more efficient, with lower misclassification rates on rotated digits. Additionally, results for different numbers of counterfactual examples are shown in FIG0 10b, indicating that the CORE estimator's performance is not sensitive to the number of examples. The CORE estimator's performance is not sensitive to the number of counterfactual examples, as shown in FIG0 10b. The pooled estimator fails to achieve good predictive performance on test sets 2 and 3. Counterfactual setting 1 works best, with small differences between settings 2 and 3. There is a large performance difference between \u00b5 = 40 and \u00b5 = 50. The performance of the CORE estimator is not affected by the number of counterfactual examples. The pooled estimator shows poor predictive performance on test sets 2 and 3. Counterfactual setting 1 performs the best, with minor differences between settings 2 and 3. There is a significant performance gap between \u00b5 = 40 and \u00b5 = 50. The image quality may not be predictive enough for the target with \u00b5 = 50. The models were implemented in TensorFlow, with different network architectures and training procedures. The study compares the CORE and pooled estimators using the same network architecture and training procedure but with different loss functions. The models are trained five times to assess variance, with training data shuffled in each epoch to ensure mini batches contain counterfactual observations. Mini batch size is set to 120, making optimization more challenging for small c values."
}