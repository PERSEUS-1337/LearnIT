{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this, a balanced face image dataset with 108,501 images representing 7 race groups was created. The model trained on this dataset showed improved accuracy on novel datasets across different race and gender groups. The model trained on a balanced face image dataset showed improved accuracy on novel datasets across different race and gender groups. Various large-scale face image datasets have fostered research and development in automated face detection, alignment, recognition, and generation. Various research and development efforts have been focused on automated face detection, alignment, recognition, generation, modification, and attribute classification. These systems have been applied in various fields such as security, medicine, education, and social sciences. However, existing public face datasets are biased towards Caucasian faces, with other races like Latino faces being underrepresented. Existing large scale face databases are biased towards lighter skin faces, particularly Caucasian, compared to darker faces, such as Black. This bias can lead to unfairness in automated systems and raise ethical concerns about model accuracy across different demographic groups. Commercial computer vision systems have been criticized for their asymmetric accuracy in gender recognition across sub-demographics. Several commercial computer vision systems have been criticized for their asymmetric accuracy in gender recognition across sub-demographics. Biases in training data can lead to better performance on male and light faces. To address race bias in existing face datasets, a novel dataset with balanced race composition containing 108,501 facial images has been proposed. A novel face dataset with balanced race composition containing 108,501 facial images from various sources. The dataset includes 7 race groups and aims to mitigate race bias in existing face datasets. The dataset performs better on novel data, especially on nonWhite faces, showing improved generalization across racial groups. The new face attribute dataset includes Latino, Middle Eastern, East Asian, and Southeast Asian faces, improving generalization across racial groups. This dataset expands the applicability of computer vision methods to fields analyzing different demographics using image data. The new face attribute dataset includes diverse racial groups, enhancing the applicability of computer vision methods. Existing datasets are predominantly White, highlighting the importance of ensuring equal performance across different gender and race groups in face attribute recognition. Computer vision tasks like face verification and person re-identification must perform well across different gender and race groups to maintain trust. Incidents of racial bias, such as Google Photos mistaking African American faces for Gorillas, have led to service termination or feature removal. Most commercial providers have stopped offering a race classifier due to these issues. Face attribute recognition is used for demographic surveys in marketing and social science research to understand human social behaviors. Social scientists use images to infer demographic attributes and analyze behaviors, such as demographic analyses of social media users. Unfair classification can lead to over or underestimation of specific sub-populations, resulting in significant costs. In the context of demographic surveys, unfair classification in social media user analyses can have significant costs. AI and machine learning communities are increasingly focusing on algorithmic fairness and dataset biases. Research in fairness aims at producing fair outcomes regardless of protected attributes like race and gender. Research in fairness focuses on ensuring fair outcomes regardless of protected attributes like race and gender. Studies in algorithmic fairness involve auditing bias in datasets, improving datasets, and designing better algorithms. One specific task is balanced gender classification from facial images, with concerns raised about bias in commercial gender classification systems. The main task discussed is balanced gender classification from facial images, with concerns about bias in commercial systems. The paper aims to mitigate existing limitations and biases by collecting more diverse face images from non-White race groups, improving generalization performance. The paper aims to address biases in existing databases by collecting diverse face images from non-White race groups, improving generalization performance. The dataset includes Southeast Asian and Middle Eastern races, filling a gap in representation. The dataset defines 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits, while ethnicity is based on cultural similarities. Latino is considered a race based on facial appearance. Subgroups like Middle Eastern, East Asian, Southeast Asian, and Indian were further divided as they look distinct. Few examples for Hawaiian and Pacific Islanders and Native Americans were found and discarded during data collection. In the dataset, 7 race groups were defined based on physical traits. Subgroups like Middle Eastern, East Asian, Southeast Asian, and Indian were further divided as they look distinct. Few examples for Hawaiian and Pacific Islanders and Native Americans were discarded during data collection. The experiments in the paper were based on 7 race classification. The bias in dataset measurement can be based on skin color or race, with limitations in using skin color due to illumination and light conditions. The Pilot Parliaments Benchmark dataset used profile photographs of government officials in controlled lighting. Skin color variations are significant within groups, making it difficult to differentiate races based on brightness alone. Race is multidimensional while skin color is one-dimensional. Skin color does not effectively differentiate between groups like East Asian and White. Race was annotated by human judges, with skin color measured by Individual Typology Angle (ITA) as a complementary measure. Many face datasets sourced from public figures like politicians and celebrities may be biased due to factors like age and attractiveness. Images are often taken in limited situations by professional photographers, leading to quality bias. Some datasets are collected via web search using specific keywords, potentially limiting the diversity of faces included. Our goal is to minimize selection bias in face datasets collected from public figures and maximize diversity. We started with the Yahoo YFCC100M dataset and incrementally increased our dataset size to ensure balance in race representation. The dataset constructed is smaller but more balanced on race compared to the Diversity in Faces dataset. Faces were detected and annotated from the YFCC100M dataset, with demographic compositions of each country considered to adjust the number of images. The dataset excluded U.S. and European countries to prevent dominance by the White race. The minimum face size detected was set at 50 by 50 pixels. The dataset excluded the U.S. and European countries in the later stage of data collection after sampling enough White faces. Faces with a minimum size of 50 by 50 pixels were used, allowing for recognizable attributes. Images with \"Attribution\" and \"Share Alike\" Creative Commons licenses were utilized. Amazon Mechanical Turk was used for annotations, with three workers assigned to each image. Ground-truth values were determined if two or three workers agreed, with images discarded if there was no consensus. These annotations were considered noisy and further refined. After refining annotations through model training, the dataset's race composition was analyzed. Most face attribute datasets, especially those focusing on celebrities or politicians, showed bias towards the White race. Additionally, race labels were manually annotated for 3,000 random samples from datasets without race annotations, revealing skewed distributions. Most existing face attribute datasets, particularly those featuring celebrities or politicians, exhibit bias towards the White race. Gender balance is relatively more even, ranging from 40%-60% male ratio. Model performance comparison was conducted using ResNet-34 architecture trained on each dataset with ADAM optimization. Face detection was done using dlib's CNN-based face detector, and the attribute classifier was run on each face in PyTorch. The dataset was compared with UTKFace, LFWA+, and CelebA datasets. The experiment compared the dataset with UTKFace, LFWA+, and CelebA datasets. CelebA was used only for gender classification as it lacks race annotations. FairFace has 7 race categories but only 4 were used for comparison. Models trained on these datasets were used for cross-dataset classifications. Using models trained from datasets like UTKFace, LFWA+, and FairFace, cross-dataset classifications were performed by alternating training and test sets. FairFace, with 7 races, was adjusted to be compatible with other datasets by merging racial groups. CelebA, lacking race annotations, was included for gender classification. Results for race, gender, and age classification across subpopulations are shown in Tables 2 and 3. The model performed best on LFWA+ due to its diversity and generalizability. Generalization performance was tested on three novel datasets. The study tested model generalization performance on three novel datasets collected from different sources than the training data. The test datasets included geo-tagged Tweets from four countries and media photographs from online outlets. The study collected data from four countries (France, Iraq, Philippines, and Venezuela) by sampling 5,000 faces. They also used photographs from 500 online media outlets and a protest dataset with diverse race and gender groups. The authors collected a dataset for a protest activity study from Google Image search using keywords like \"Venezuela protest\" and \"football game\". The dataset includes diverse race and gender groups engaging in various activities in different countries. 8,000 faces were randomly sampled and annotated for gender, race, and age. The FairFace model outperformed other models in gender, race, and age classification accuracy on external validation datasets. The FairFace model outperforms other models in gender, race, and age classification accuracy on novel datasets, even with smaller training sets. It also shows more consistent results across different race groups compared to other datasets. The FairFace model demonstrates superior performance in gender, race, and age classification accuracy on new datasets, with consistent results across various race groups. The model's maximum accuracy disparity is the lowest among different classifiers. The FairFace model achieves the lowest maximum accuracy disparity in gender classification, with less than 1% accuracy discrepancy between male and female, and White and non-White groups. Other models show a strong bias towards males, with the LFWA+ model having the biggest gender performance gap at 32%. The study found that the FairFace model had the lowest gender classification accuracy disparity, with less than 1% difference between male and female groups. The research also highlighted the unbalanced representation in training data as a likely cause of gender biases in computer vision services. Additionally, the dataset analysis showed that the faces in FairFace were well spread in the 2D space, indicating data diversity. The facial embedding used in the study was based on ResNet-34 from dlib, trained from biased datasets like FaceScrub and VGG-Face. FairFace dataset showed well-spread faces with loosely separated race groups, possibly due to non-typical examples. Comparison with LFWA+ and UTKFace datasets revealed differences in face diversity. Pairwise distance distributions were examined to measure the diversity of faces in the datasets. The study compared the diversity of faces in different datasets using pairwise distance distributions. UTKFace had tightly clustered faces, while LFWA+ showed diverse faces despite mostly white examples. The face embedding trained on similar datasets may have influenced the results. The study used the FairFace dataset to test gender classification APIs, comparing them to previous work on politicians' faces. The dataset is diverse in race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement. The study utilized the FairFace dataset, which is diverse in race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement. The dataset included 7,476 random samples with an equal representation of faces from each race, gender, and age group, excluding children under 20 due to ambiguity in gender determination. The experiments were conducted on August 13th -16th, 2019. The study compared gender classification APIs from Microsoft, Face++, IBM, and FairFace. Table 6 displays gender classification accuracies of tested APIs, detecting faces and classifying gender. Not all 7,476 faces were detected by APIs except Amazon Rekognition. Detection rates are reported in Table 8. Accuracies are reported with and without mis-detections. A model trained with the dataset is included for comparison. The gender classifiers tested showed a bias towards males, consistent with previous reports. Dark-skinned females had higher error rates, but there were exceptions, such as Indians being classified more accurately by some APIs. Skin color alone is not a sufficient factor for accurate classification. The study found that skin color alone is not a reliable indicator of model bias in classification. Additionally, face detection can introduce gender bias, with Microsoft's model failing to detect many male faces. The paper proposes a new face image dataset balanced on race, gender, and age, which outperforms existing datasets in generalization classification performance for gender, race, and age. Our dataset, derived from the Yahoo YFCC100m dataset, achieves better generalization classification performance for gender, race, and age on novel image datasets from Twitter, online newspapers, and web search. It ensures balanced accuracy across race groups and can be used for training new models and verifying existing classifiers for algorithmic fairness in AI systems. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems, improving transparency and acceptance in society. The dataset includes a distribution of different races measured by Individual Typology Angle (ITA)."
}