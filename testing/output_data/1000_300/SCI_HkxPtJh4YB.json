{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family defined over permutation matrices. The method is justified by the Sinkhorn approximation of the permanent, addressing the problem of marginal inference for binary matrices representing permutations. The distribution over permutation matrices is defined using the Frobenius matrix inner product. Marginal inference, computing the matrix of expectations, is known to be intractable due to the computation of the permanent. Sinkhorn variational marginal inference offers an efficient solution by approximating the matrix of expectations using the Sinkhorn operator applied to the parameter matrix. The Sinkhorn operator is applied to the parameter matrix to compute the distribution over permutation matrices. The Sinkhorn approximation is shown to produce the best results for probabilistic inference of neural identity in C.elegans. The relation between marginal inference and the normalizing constant is discussed, with the dual function log Z L playing a key role in achieving the supremum. The dual function log Z L plays a key role in achieving the supremum for marginal inference and computation of the permanent Z L. The approximation of \u03c1 is based on replacing the intractable dual function with a component-wise entropy. The approximation of the normalizing constant Z L is achieved by replacing the intractable dual function with a component-wise entropy, resulting in the Sinkhorn permanent perm S (L). Bounds for this approximation are provided, with a comparison to a heuristic approach proposed independently. The Bethe variational inference method is also discussed as a general framework. The Bethe variational inference method provides a general rationale for obtaining variational approximations in graphical models, approximating the dual function A*(\u00b5) as if the underlying Markov random field had a tree structure. This approximation has been successfully applied to permutations, with the corresponding approximate marginal B(L) computed through belief propagation and enjoying better theoretical guarantees than the Sinkhorn approximation. The Bethe approximation computes approximate marginal B(L) through belief propagation, with better theoretical guarantees than the Sinkhorn approximation. Computational differences exist, with Sinkhorn algorithms requiring row and column normalization, while Bethe approximation involves more complex message computations. In practice, Bethe approximation produces better permanent approximations, as shown in Fig 1 (b) for n = 8. The Bethe approximation produces better permanent approximations in practice, as confirmed by theoretical predictions. Comparisons with ground truth show that the Sinkhorn approximation often yields qualitatively better marginals by putting more mass on non-zero entries. Additionally, the Sinkhorn approximation scales better for moderate n, with faster iteration times compared to Bethe approximation. The Sinkhorn iteration is faster than Bethe approximation, with each iteration taking only 0.0027 seconds. Comparison of approximations using submatrices from the C.elegans dataset shows differences in log permanent and mean absolute errors. Sampling-based methods can also be used for marginal inference. Recent advances in neurotechnology have enabled whole brain imaging of the C.elegans worm, a species with a stereotypical nervous system. The number of neurons (roughly 300) and their connections remain unchanged from animal to animal. However, a technical challenge must be addressed: identifying neurons from volumetric images before studying how brain activity relates to behavior. Sampling-based methods, including sophisticated samplers, can be used for marginal inference, but practical limitations exist. The technical challenge of identifying neurons from volumetric images in the C.elegans worm brain is being addressed using a probabilistic neural identification methodology. This approach aims to assign canonical labels to observed neurons, providing uncertainty estimates for model predictions. This process is crucial for studying how brain activity relates to behavior in the worm. The methodology aims to estimate the matrix of marginal probabilities for identifying observed neurons with canonical identities. These probabilities offer uncertainty estimates for model predictions, based on a gaussian model for each canonical neuron. The likelihood of observing data is determined by a permutation, inducing a posterior distribution over probabilities. In the context of NeuroPAL, a flat prior is assumed over P, inducing a posterior distribution over probabilities for identifying neurons. A downstream task involves computing approximate probabilistic neural identifies \u03c1, where a human labels neurons with uncertain model estimates to improve identification accuracy. Annotations lead to model updates and increased accuracy, with the goal of reaching high accuracy with few annotations. The uncertainty resolves and model updates increase identification accuracy for neurons. Human annotations improve accuracy with few annotations needed. Different approximation methods are compared, including Sinkhorn and Bethe, against baselines like random selection and naive scaling of likelihood matrix rows. The 'ground truth' protocol, where incorrect model predictions are labeled, is also considered. The Sinkhorn and Bethe approximations are compared against baselines like random selection and naive scaling of likelihood matrix rows. Results show that Sinkhorn is slightly better, providing more accurate estimates of low probability marginals. MCMC does not outperform the baseline, suggesting lack of convergence. Sinkhorn approximation may offer faster, simpler, and more accurate approximate marginals than Bethe. The Sinkhorn approximation is a sensible alternative to sampling and may provide faster, simpler, and more accurate approximate marginals than the Bethe approximation. The relation between the quality of permanent approximation and corresponding marginals needs further analysis. The (log) Sinkhorn approximation of the permanent of L can be obtained by evaluating S(L) in the problem it solves. The dataset used is described in Yemini et al. The permanent of L, perm S (L), is evaluated in the problem it solves. The dataset used consists of ten NeuroPAL worm heads with human labels and log-likelihood matrices. Sinkhorn and Bethe approximations were used with 200 iterations each. MCMC sampler method from Diaconis (2009) was used with 100 chains of length 1000. The MCMC sampler method described in Diaconis (2009) was used with 100 chains of length 1000, ensuring convergence. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. An efficient log-space implementation of the message passing algorithm from Vontobel (2013) was utilized. Submatrices of size n were randomly drawn from ten available log likelihood C.elegans matrices. Error bars were omitted. The parameter eps is introduced for numerical stability. 1000 submatrices of size n were randomly drawn from the ten available log likelihood C.elegans matrices in Pontobel (2019). Error bars were too small to be noticed."
}