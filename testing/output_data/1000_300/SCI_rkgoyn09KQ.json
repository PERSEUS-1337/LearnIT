{
    "title": "rkgoyn09KQ",
    "content": "In this work, a neural autoregressive topic model is combined with a LSTM-based language model to address challenges in probabilistic topic modeling. The LSTM-LM captures word order in local collocation patterns, while the TM learns latent representations from entire documents. The LSTM-LM models language characteristics like syntax and semantics, while the TM uncovers thematic structures in document collections. The two models are complementary. The ctx-DocNADE model combines a topic model and a language model to learn word meanings in a unified probabilistic framework. It addresses challenges in topic modeling with limited context or small training corpora by incorporating external knowledge through a LSTM-based language model, improving word-topic mapping on short-text datasets. The novel neural autoregressive topic model variants with neural language models and embeddings priors outperform state-of-the-art generative topic models in generalization, interpretability, and applicability over long-text. Probabilistic topic models like LDA, RSM, and DocNADE are commonly used to extract topics from text collections and predict word probabilities for each topic. These models learn latent document representations for NLP tasks but ignore word order and semantic information. Extending these models to incorporate word order and language structure is a key task. Traditional topic models like LDA, RSM, and DocNADE do not consider word order and language structure, focusing only on \"bag-of-words\" statistics. This limitation becomes evident when analyzing sentences with the same unigram statistics but different topics. For example, the phrase \"market falls\" preceding the word \"bear\" in a sentence suggests a topic related to stock market trading. Ignoring language structure, such as syntax and semantics, hinders the accurate assignment of words to topics. Recent research has shown that deep contextualized LSTM-based language models can capture different language concepts in a layer-wise fashion, with the lowest layer capturing language syntax and the topmost layer capturing semantics. However, LSTM-LMs do not capture semantics at a document level. Recent research has shown that deep contextualized LSTM-based language models capture language syntax in the lowest layer and semantics in the topmost layer. However, LSTM-LMs do not capture semantics at a document level. Other studies have integrated latent topics with neural language models to improve global dependencies, but struggle with long term dependencies and language concepts. DocNADE variants focus on learning word occurrences across documents at a coarse granularity. The proposed neural topic model, named as LSTM-LM, integrates language structure into neural autoregressive topic models to account for word ordering, language concepts, and long-range dependencies. This allows for accurate word prediction by considering global and local contexts modeled via DocNADE and LSTM-LM, respectively. The proposed neural topic model, ctx-DocNADE, combines joint word and latent topic learning in a unified neural autoregressive framework to incorporate language structure and word order for accurate word prediction. It offers learning complementary semantics by capturing global and local contexts, making it a powerful approach for long texts and corpora with many documents. The curr_chunk discusses the challenges of learning from contextual information in settings with short texts and few documents due to limited word co-occurrences, significant word non-overlap, and a small training corpus. It highlights the effectiveness of distributional word representations like word embeddings in capturing semantic and syntactic relatedness in words for NLP tasks. The curr_chunk discusses the use of word embeddings in NLP tasks, showing how they capture semantic relatedness between words in short text fragments. Previous work has integrated word embeddings into topic models like LDA and DMM to improve information extraction from short texts. However, some approaches, like DocNADE, overlook language structure such as word ordering and syntax. The curr_chunk discusses incorporating distributed compositional priors in DocNADE by using pre-trained word embeddings via LSTM-LM to enhance topic learning and textual representations. This approach combines complementary learning and external knowledge to improve topic representation coherence. By combining complementary learning and external knowledge, the ctx-DocNADEe model integrates pre-trained word embeddings and LSTM-LM to enhance topic representation coherence. The approach improves textual representations' generalizability, interpretability, and applicability, outperforming state-of-the-art generative topic models on various datasets. Our approach, textTOvec, outperforms generative topic models on diverse datasets, showing gains in topic coherence, precision, and F1 for text classification. The proposed modeling generates contextualized topic vectors for short-text and long-text documents. The code is available at https://github.com/pgcool/textTOvec. Generative models like RBM and its variants are based on estimating complex dependencies in multidimensional data. NADE BID13 decomposes the joint distribution of binary observations into autoregressive conditional distributions using a feed-forward network, allowing for tractable gradients of the data negative log-likelihood. DocNADE BID12 models collections of documents as bags of words, focusing on learning word representations reflecting document topics. DocNADE BID12 models collections of documents as orderless bags of words, learning word representations reflecting document topics. It disregards language structure and semantic features, focusing on underlying topics only. The model computes autoregressive conditional probabilities for word observations using a feed-forward neural network. The DocNADE model represents documents as bags of words, with a weight matrix connecting hidden to output layers. It computes autoregressive conditional probabilities for word observations using a neural network. Two extensions are proposed: ctx-DocNADE introduces language structure via LSTM-LM, while ctx-DocNADEe incorporates external knowledge through pre-trained word embeddings. The ctx-DocNADE and ctx-DocNADEe models extend the DocNADE model by incorporating language structure via LSTM-LM and external knowledge through pre-trained word embeddings. These models consider word ordering, syntactical and semantic structures, long and short term dependencies, and external knowledge to overcome limitations of BoW-based representations. Each document is modeled as a sequence of multinomial observations, with context taken into account for each word. The ctx-DocNADE and ctx-DocNADEe models enhance the DocNADE model by incorporating language structure through LSTM-LM and pre-trained word embeddings. They consider word ordering, syntactical and semantic structures, long and short term dependencies, and external knowledge to improve BoW-based representations. The conditional probability of a word in ctx-DocNADE is influenced by hidden vectors and weight matrices, allowing for global and local influences in the unified network. The embedding layer in the LSTM component of the unified network is randomly initialized, extending DocNADE to consider word ordering and language concepts. The second version incorporates distributional priors by initializing the LSTM embedding layer with a pre-trained matrix E and weight matrix W. Algorithm 1 and Table 1 compare log p(v) for a document v in Doc-NADE, ctx-DocNADE, and ctx-DocNADEe settings, showing the impact of tied weights in the matrix W in the DocNADE component. The matrix W is a model parameter, while E is a static prior. Algorithm 1 and Table 1 display log p(v) for a document v in three settings: Doc-NADE, ctx-DocNADE, and ctx-DocNADEe. In DocNADE, tied weights in matrix W reduce computational complexity to O(HD). LSTM is used to extract hidden vectors h LM i in ctx-DocNADE or ctx-DocNADEe, with computational complexity of O(HD + N). The models can generate a textTOvec representation, and ctx-DeepDNEe extends DocNADE and LSTM to a deep, multiple hidden layer architecture. The trained models in the LSTM network BID10 BID27 can extract a textTOvec representation. Deep feed-forward neural networks can be extended with multiple hidden layers for improved performance. The conditional probability is computed using the last hidden layer. State-of-the-art comparison is done for IR and classification F1 for short texts. The last layer computes p(v i = w|v <i ) in the LSTM network. The study compares IR and classification F1 for short texts using different modeling approaches. Evaluation measures include generalization, topic coherence, text retrieval, and categorization on various datasets. Baselines are used for comparison on four tasks. Data statistics are shown in TAB1 with examples from 20NewsGroups and Reuters21578. The appendices contain data description and example texts. TAB1 displays data statistics for 20NewsGroups and Reuters21578. Evaluation of proposed models ctx-DocNADE and ctx-DocNADEe is compared with various baselines including word and document representations, LDA based BoW TMs, neural BoW TMs, pre-trained word embeddings, and jointly trained topic and language models. Neural BoW TMs like DocNADE and NTM BID2, along with other TMs such as Gauss-LDA and glove-DMM, were used in the experimental setup. DocNADE was trained on both reduced vocabulary (RV) and full vocabulary (FV) settings for evaluation tasks. The models were run over 200 topics to assess the quality of learned representations. The study evaluated various models like ctx-DocNADE and ctx-DeepDNEe using glove embeddings of 200 dimensions over 200 topics. Pre-training was done for ctx-DocNADEs to enhance learning. Experimental setup details and hyperparameters can be found in the appendices. Evaluation included log-probabilities for test documents and average held-out perplexity per word. The study evaluated generative performance of topic models using pre-trained word embeddings. The log-probabilities for test documents were estimated to compute average held-out perplexity per word. The optimal mixture coefficient was determined based on validation set, with ctx-DocNADE achieving lower perplexity than baseline DocNADE on AGnewstitle and 20NS 4 datasets. The study compared the generative performance of topic models using pre-trained word embeddings. Results showed that ctx-DocNADE with \u03bb = 0.01 achieved lower perplexity than baseline DocNADE on AGnewstitle and 20NS 4 datasets. Topic coherence was assessed using a coherence measure proposed by BID25, with higher scores indicating more coherent topics. Gensim module was used to estimate coherence for 200 topics, showing higher coherence in ctx-DocNADE compared to DocNADE (.772 vs .755), suggesting that contextual information and language structure help in generating more coherent topics. The study compared the generative performance of topic models using pre-trained word embeddings. Results showed that ctx-DocNADE with \u03bb = 0.01 achieved lower perplexity than baseline DocNADE on AGnewstitle and 20NS 4 datasets. Higher coherence in ctx-DocNADE compared to DocNADE (.772 vs .755) suggests that contextual information and language structure help in generating more coherent topics. Embeddings in ctx-DocNADEe boost topic coherence, leading to a 4.6% gain on average over 11 datasets. Proposed models outperform baselines methods glove-DMM and glove-LDA. Additional comparisons were made with other approaches combining topic and language models. Our proposed models, ctx-DocNADE and ctx-DocNADEe, focus on improving topic models for textual representations by incorporating language concepts and external knowledge via neural language models. We compare our models to other approaches like TDLM and Topic-RNN, which focus on improving language models using topic models. The performance of our models is quantitatively evaluated in terms of topic coherence on the BNC dataset, following the experimental setup of TCNLM. Our proposed models, ctx-DocNADE and ctx-DocNADEe, outperform other approaches in terms of topic coherence on the BNC dataset. The sliding window size and mixture weight \u03bb play a crucial role in the topic modeling process. The inclusion of word embeddings results in more coherent topics. The relevance of the LM component for topic coherence is illustrated by BID32. Including word embeddings in ctx-DocNADEe leads to more coherent topics compared to the baseline DocNADE. However, ctx-DocNADEe does not show improvements in topic coherence over ctx-DocNADE. In a qualitative analysis, ctx-DocNADE captures topics with verbs in the past participle. The comparison of model performance is limited to topic coherence due to the unlabeled BNC dataset. In a document retrieval task using short-text and long-text documents with label information, test documents are treated as queries to retrieve closest documents in the training set based on cosine similarity. Precision scores are averaged over multiple labels for each query, with RSM and DocNADE outperforming LDA. The comparison is limited to topic coherence due to the unlabeled BNC dataset. The introduction of pre-trained embeddings and contextual information improves performance on the information retrieval task, especially for short texts. Topic modeling without pre-processing and filtering certain words also shows improved precision. The proposed extensions, specifically ctx-DocNADEe, report a 7.1% gain in IR precision on average over multiple datasets. The proposed extensions, ctx-DocNADEe and ctx-DeepDNEe, show significant improvements in IR precision over baseline settings. They outperform previous models like TDLM and ProdLDA, with gains of 7.1% and 6.5% in precision, respectively. Additionally, text categorization results show the quality of the textTovec representations. Our proposed models, ctx-DocNADEe and ctx-DeepDNEe, outperform previous models like TDLM and ProdLDA in text categorization. They utilize glove embeddings and show a significant gain in F1 score compared to DocNADE(RV), with an overall improvement of 4.4%. The ctx-DocNADEe shows a gain in F1 score compared to DocNADE(RV) on short and long texts. In terms of classification accuracy on the 20NS dataset, ctx-DocNADEe outperforms NTM and SCHOLAR. Meaningful topics are captured in the topic extraction process. The ctx-DocNADEe model outperforms DocNADE(RV) in F1 score for short and long texts. It also surpasses NTM and SCHOLAR in classification accuracy on the 20NS dataset. Meaningful topics are extracted, with an example showing a topic related to computers. The model retrieves top 3 texts for a query from the TMNtitle dataset, with ctx-DocNADEe showing no unigram overlap with the query. The ctx-DocNADEe model retrieves top 3 texts for a query with no unigram overlap. The query is related to \"emerging economies move ahead nuclear plans\" and the retrievals include topics like IMF sign lifting Japan yen, Japan recovery, and Japan banks billion nuclear operator sources. Additionally, representations learned at different fractions are shown. The ctx-DocNADEe model demonstrates improvements in quality of representations learned at different fractions of training data, showing gains in both IR and classification tasks. The proposed models, ctx-DocNADE and ctx-DocNADEe, outperform DocNADE, with significant precision and F1 score increases at smaller fractions of the datasets. The ctxDocNADEe model shows improvements in precision and F1 scores compared to DocNADE at smaller fractions of the datasets. This work combines neural autoregressive topic models with neural language models to better estimate word probabilities in context. In a probabilistic framework, a neural autoregressive topic model (DocNADE) and a neural language model (LSTM-LM) are combined to introduce language concepts in each autoregressive step. This allows for learning a latent representation from the entire document while considering local collocation patterns. External knowledge is incorporated through word embeddings, resulting in improved performance over state-of-the-art generative topic models on various metrics across 15 datasets. Instructors with tertiary education and experience in equipment operation and maintenance are required to deliver clear instructions. Maintenance staff must be available 24/7 for on-call maintenance of the Signalling System. Standard applies to various cables including LAN and Fibre Optic cables. The Contractor is responsible for on-call maintenance of the Signalling System throughout the year. Standard applies to all cables including LAN and Fibre Optic cables. Asset labels must be installed on equipment as per Engineer's specifications. Stations with Interlocking capability can be switched to \"Auto-Turnaround Operation\" for automatic train routing. The Signalling System maintenance includes switching stations with Interlocking capability to \"Auto-Turnaround Operation\" for automatic train routing. Document retrieval for Generalization task involves using class labels to check retrieved documents. For document retrieval in the Generalization task, Doc2Vec models were trained on 12 datasets using gensim. Models used distributed bag of words, with 1000 iterations, a window size of 5, and a vector size of 500. A logistic regression classifier was trained on the document vectors to predict class labels, with a one-vs-all approach for multilabel datasets. The models were evaluated using accuracy and macro-averaged F1 score on the test set. In the Generalization task, document vectors were used to predict class labels for multilabel datasets. Models were trained with a liblinear solver using L2 regularization, and accuracy and macro-averaged F1 score were computed on the test set. LFTM was used to train glove-DMM and glove-LDA models with specific hyperparameters. Classification was performed using relative topic proportions as input, and topic coherence was evaluated using NPMI with 20 topics. The experimental results show that DocNADE outperforms SCHOLAR in generating representations for tasks like information retrieval and classification, but lags behind in interpretability. SCHOLAR BID3 generates more coherent topics than DocNADE, but performs worse in other tasks. This suggests a potential direction for future research."
}