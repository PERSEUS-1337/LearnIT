{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. This is important because on-policy evaluation is costly and has negative effects. A major challenge in off-policy learning is developing counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over conventional baseline algorithms. Off-policy learning is crucial for evaluating deterministic policies using historical data, as on-policy evaluation is costly and has negative impacts. Off-policy evaluation is essential for assessing deterministic policies using historical data, as on-policy evaluation can be expensive and risky. Utilizing methods like Q learning, doubly robust estimator, and self-normalized approaches can enable safe exploration of policy hypotheses before implementation. Extensive research has been conducted on off-policy learning in reinforcement learning and contextual bandits to improve policy evaluation and decision-making. Off-policy learning involves using logged interaction data with bandit feedback, where only limited feedback is observed, such as scalar rewards for actions. This approach lacks information on alternative actions, potential rewards, and the relationship between policy changes and rewards. For example, in online recommendation systems, after suggesting an item, only subsequent interactions are observed, not potential rewards for other actions. BID34 introduced a new counterfactual risk minimization framework for off-policy learning in bandit feedback cases, addressing the challenge of distribution mismatch between logging and new policies. They added sample variance as a regularization term to the empirical risk minimization objective. However, their linear stochastic model for policy parametrization has limited representation power. Our contribution in this paper is three-fold: proposing a new learning principle for off-policy learning with bandit feedback by minimizing distribution divergence between new and logging policies, automatically trading off between empirical risk. The paper introduces a new learning principle for off-policy learning with bandit feedback, focusing on minimizing distribution divergence between the new and logging policies. The proposed method balances empirical risk and sample variance, utilizing neural networks for policy parametrization and variational divergence minimization techniques. Experimental results demonstrate improved performance over conventional baselines. The curr_chunk discusses off-policy learning with logged bandit feedback, where a policy maps inputs to structured outputs using stochastic policies parametrized by \u03b8. Actions are taken by sampling from the output distribution h(Y|x). In off-policy learning with logged bandit feedback, actions are taken by sampling from a distribution h(Y|x). The goal is to find a policy with minimum expected risk on test data, using data collected from a logging policy h0(Y|x). The goal of off-policy learning is to find a policy with minimum expected risk on test data. In the off-line logged learning setting, data is collected from a logging policy h0(Y|x), and the aim is to improve this policy to have lower expected risks. Challenges include skewed distribution of the logging policy and the need for empirical estimation using finite samples. In off-policy learning, data is collected from a logging policy h0(Y|x) to improve policy performance. Challenges include skewed distribution and the need for empirical estimation using finite samples. Propensity scoring approach using importance sampling addresses distribution mismatch, while counterfactual risk minimization aims to regulate variance and address flaws in the vanilla approach. Counterfactual risk minimization aims to regulate variance and address flaws in the vanilla approach by proposing a regularization term for sample variance derived from empirical Bernstein bounds. The modified objective function includes the average and sample variance terms, with an approximation method used for stochastic optimization due to the difficulty of handling the variance term dependent on the whole dataset. The text discusses a stochastic optimization algorithm that uses a first-order Taylor expansion but neglects non-linear terms. It explores deriving a variance bound directly from a parametrized distribution instead of estimating variance empirically. The importance sampling function and learning bounds are also mentioned. The importance sampling weight w(z) is defined by two probability density functions p(z) and p0(z). An identity is established for the weight w(z) in relation to the R\u00e9nyi divergence. This leads to an upper bound for the second moment of the weighted loss, involving random variables X and Y with respective distributions P and conditional divergences. The curr_chunk discusses the derivation of a generalization bound between expected risk and empirical risk using distribution divergence functions. The proof involves a theorem that establishes a bound with probability at least 1 - \u03b7. The proof utilizes Bernstein inequality and second moment bounds. Detailed proofs can be found in the appendix. The curr_chunk discusses the bias-variance trade-offs in empirical risk minimization problems and proposes minimizing variance regularized objectives in bandit learning settings. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance, but setting \u03bb empirically remains a challenge. In bandit learning settings, minimizing variance regularized objectives is crucial. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance, but setting \u03bb empirically remains a challenge. To address this, an alternative formulation of regularized ERM is explored, involving a constrained optimization approach with a pre-determined constant \u03c1 as the regularization hyper-parameter. This new formulation aims to provide a good surrogate for the robust objective. The new formulation for constrained optimization involves a regularization hyper-parameter \u03c1. The objective function serves as a good surrogate for the true risk, with the difference bounded by \u03c1. Estimating the divergence function becomes challenging with a parametrized distribution of h(y|x) and finite samples. Recent f-gan networks and Gumbel soft-max sampling can aid in solving this task. Recent f-gan networks and Gumbel soft-max sampling can assist in solving the task of constrained optimization by minimizing variational divergence. The need for stochasticity in the logging policy is emphasized, as a deterministic policy with peaked masses and zeros in unexplored regions can lead to difficulties in learning. This results in an unbounded generalization bound, highlighting the importance of counterfactual learning. The policy has a non-zero measure region of h 0 (Y|x) with a probability density of h 0 (y|x) = 0, while h(y|x) can have finite values in the region. This results in an unbounded generalization bound, making counterfactual learning not possible. The derived variance regularized objective requires minimizing the square root of the conditional dy. By examining the term inside the expectation operation, we get a minimization objective of D f (h||h 0 ; P(X)). This calculation connects our divergence to the f-divergence measure. Following the f-GAN for variational divergence minimization method, we can reach a lower bound. The f-divergence measure is connected to the minimization objective of D f (h||h 0 ; P(X)). By following the f-GAN method for variational divergence minimization, a lower bound can be reached through convex duality and neural network approximation theorems. The f-divergence measure is connected to the minimization objective of D f (h||h 0 ; P(X)). By utilizing the universal approximation theorem of neural networks, neural networks can approximate continuous functions on a compact set with any desired precision. The final objective is a saddle point of a function that maps input pairs to a scalar value, with the policy we want to learn acting as a sampling distribution. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence, denoted as D f = sup T T dhdx \u2212 f * (T )dh 0 dx. The estimation error is decomposed into terms related to neural networks and empirical mean estimation. The universal approximation theorem ensures accuracy in the approximation process. The universal approximation theorem guarantees accuracy in approximating the true distribution. By applying the strong law of large numbers and optimality conditions, the estimation error can be minimized. Using a generative-adversarial approach, the T function can be represented as a discriminator network, while the policy distribution h(y|x) can be parametrized as a generator. By applying Theorem 5 and a generative-adversarial approach, the T function is represented as a discriminator network parametrized as T w (x, y). The policy distribution h(y|x) is parametrized as a generator neural network h \u03b8 (y|x). Gumbel soft-max sampling methods are used for differential sampling from the distribution h(y|x). The complete training procedure is listed in Alg. 1. The algorithm optimizes the generator distribution to minimize divergence from the initialization, using real and fake samples from the data distribution. It includes a training algorithm for minimizing variance regularization and presents an end-to-end learning approach for counterfactual risk minimization. The algorithm presented optimizes the generator distribution to minimize divergence from the initialization, using real and fake samples from the data distribution. It includes a training algorithm for minimizing variance regularization and presents an end-to-end learning approach for counterfactual risk minimization. The algorithm solves the robust regularized formulation and includes training for the original ERM formulation. The algorithm works in two separate training steps: 1) update the parameters of the policy to minimize the reweighted loss 2) update the parameters of the generator and discriminator to regularize the variance for improved generalization performance. Exploiting historic data is crucial in multi-armed bandit problems and its variants, with approaches like doubly robust estimators proposed. Bandit and its variants, such as contextual bandit, have wide applications. Doubly robust estimators have been proposed, and recent theoretical studies explored the finite-time minimax risk lower bound of the problem. Bandit problems can be interpreted as single-state reinforcement learning problems, with techniques like Q function learning and temporal difference learning being alternatives for off-policy learning in RL. Recent works in deep RL have addressed off-policy updates through methods like multi-step bootstrapping and off-policy training of Q functions. Recent works in deep RL have addressed off-policy updates through methods like multi-step bootstrapping and off-policy training of Q functions. Learning from logs traces involves applying propensity scores to evaluate candidate policies, described as treatment effect estimation in statistics. Techniques like unbiased counterfactual estimators and reducing bandit learning to weighted supervised learning problems have been explored. Variance regularization aims at off-policy learning with bandit feedback. The curr_chunk discusses variance regularization in off-policy learning with bandit feedback and its connection to generalization bounds in importance sampling problems. It also mentions the application of divergence minimization technique to supervised learning and domain adaptation problems. The curr_chunk discusses the application of divergence minimization technique to supervised learning and domain adaptation problems, with connections to distributionally robust optimization techniques. It also mentions the empirical evaluation of proposed algorithms using bandit feedback method. The curr_chunk describes the process of converting supervised learning data to bandit feedback for algorithm evaluation. It involves constructing a logging policy, sampling predictions, collecting feedback, and creating bandit feedback datasets. Evaluation metrics are used to assess the probabilistic models. The curr_chunk discusses the evaluation metrics for the probabilistic policy h(Y|x), including expected loss (EXP) and average hamming loss of maximum a posteriori probability (MAP). The focus is on generalization performance and the limitations of MAP predictions. The curr_chunk compares different sampling algorithms for probabilistic policies, highlighting the importance of considering both MAP and EXP performance for generalization. Baselines like IPS and POEM are evaluated using L-BFGS and stochastic optimization solvers, with hyperparameters selected based on validation set performance. The study also includes a comparison with neural network policies without divergence regularization. The study compares different sampling algorithms for probabilistic policies, including neural network policies without divergence regularization. Four multi-label classification datasets are used, with hyperparameters selected based on validation set performance. The policy distribution is implemented using a three-layer feed-forward neural network, while a two or three layer feed-forward neural network is used as the discriminator for divergence minimization. Benchmark comparisons are made using a separate training version for faster convergence and better performance. The study compares different sampling algorithms for probabilistic policies using neural network policies without divergence regularization. The networks are trained with Adam optimizer and Nvidia K80 GPU cards. Results are reported with two evaluation metrics, and two Gumbel-softmax sampling schemes are used. The introduction of neural network parametrization improves the policies. The study compares different sampling algorithms for probabilistic policies using neural network policies without divergence regularization. Two Gumbel-softmax sampling schemes, NN-Soft and NN-Hard, are reported to improve test performance significantly compared to baseline CRF policies. The introduction of additional variance regularization further enhances testing and MAP prediction loss. No significant difference is observed between the two sampling schemes. Variance regularization effectiveness is studied quantitatively by varying the maximum number of iterations in each divergence minimization sub loop. The study compares different sampling algorithms for probabilistic policies using neural network policies without divergence regularization. Variance regularization is quantitatively studied by varying the maximum number of iterations in each divergence minimization sub loop. Results show that adding the regularization term improves generalization to test sets, leading to lower test loss and faster convergence rates. Adding a regularization term improves generalization to test sets, leading to lower test loss and faster convergence rates. The number of training samples in the bandit dataset affects test performance in expected loss. Varying the number of passes of training data to sample an action shows an increase in test performance for both models with and without regularization. Regularization improves generalization in test sets, leading to lower test loss and faster convergence rates. Increasing the number of training samples in the bandit dataset enhances test performance in expected loss. Regularized policies show better generalization performance compared to models without regularization. However, excessive training sample replay can lead to overfitting, as indicated by a decrease in MAP prediction performance. Experiments highlight the differences between cotraining in Alg. 3 and other training schemes. In this section, experiments compare cotraining in Alg. 3 with an easier version Alg. 2, along with different Gumbel-softmax sampling schemes. Blending weighted loss and distribution divergence slightly improves performance but makes training more challenging. No significant difference is observed between the two Gumbel-softmax sampling schemes. The impact of logging policies is also discussed. The effect of logging policies on learning performance is discussed, with a focus on the stochasticity of the logging policy. By modifying the parameter h0 with a temperature multiplier \u03b1, the distribution becomes more peaked, leading to a deterministic policy as \u03b1 increases. The prediction for CRF logging policies involves normalizing values of w T \u03c6(x, y), where w is the model parameter modified by \u03b1. Increasing \u03b1 results in a more peaked distribution for h0, leading to a deterministic policy. NN policies outperform logging policies when h0's stochasticity is sufficient, but learning improved NN policies becomes harder when the temperature parameter exceeds 2/3. Stochasticity does not affect expected loss values, with the drop in ratios mainly due to decreased loss of the logging policy h0. The stochasticity of the logging policy h0 affects the ability to learn improved neural network policies. Stronger regularization in policies shows better performance against weaker ones, indicating robustness in learning. As h0 improves, models consistently outperform baselines, but the difficulty increases. The regularization helps the model be more robust and achieve better generalization performance. The impact of logging policies on learned improved policies is discussed. Varying the proportion of training data points used to train the logging policy shows a trade-off between policy accuracy and sampling biases. As the logging policy improves, both NN and NN-Reg policies outperform it, addressing the sampling biases. The study compares the performance of improved policies obtained in Fig. 4b. Both NN and NN-Reg policies outperform the logging policy, indicating they address sampling biases. The increasing ratios of test expected loss to h0 performance reflect relative policy improvement. The paper proposes a new training principle inspired by learning bounds for importance sampling problems, focusing on regularizing variance to enhance generalization performance in off-policy learning for logged bandit datasets. Theoretical discussion led to a training objective combining importance reweighted loss and distribution divergence regularization. Variational divergence minimization and Gumbel soft-max sampling techniques were applied to train neural network policies effectively. Evaluations on benchmark datasets validated the learning principle and algorithm. Limitations include the need for propensity scores, which can be addressed by learning to estimate them. Learning to estimate propensity scores and incorporating them into the training framework will enhance the applicability of algorithms. Directly learning importance weights can provide comparable theoretical guarantees and may be a valuable extension. The techniques and theorems discussed can be extended to general supervised learning and reinforcement learning. Applying Lemma 1 to importance sampling weight function and loss function yields interesting results. The study applies Lemma 1 to importance sampling weight function and loss function to obtain bounds for bandit learning. By utilizing Reni divergence and Bernstein's concentration bounds, the variance can be bounded. The goal is to optimize the generator to minimize R(w) with an approximate minimizer h * \u03b8 (y|x). The study aims to optimize the generator by minimizing R(w) using an approximate minimizer h * \u03b8 (y|x). The algorithm involves updating the discriminator and generator iteratively while sampling fake and real samples. The variance regularized risk-co-training version is implemented, and statistics of the datasets are reported. The effect of stochasticity of h0 versus the ratio of test loss with MAP predictions is analyzed. The study analyzes the effect of stochasticity of h0 on the ratio of test loss with MAP predictions. NN policies show improvement over h0 in expected loss, but struggle to beat baselines in MAP predictions due to the good performance of h0. Further investigation is warranted. When the logging policy quality improves, NN policies can still outperform h0 in expected loss. However, it is challenging for NN policies to surpass h0 in MAP predictions if the logging policy has been exposed to full training data and trained in a supervised manner."
}