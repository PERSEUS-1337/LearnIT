{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness near the ground truth, allowing gradient descent to converge linearly to a point close to the true weights. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks. Neural networks have gained research interest for their success in practical domains like computer vision and artificial intelligence. Efforts are being made to understand the theoretical underpinnings behind their success, including which functions can be represented by deep neural networks and why they generalize well. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks. One important line of research focuses on model recovery in neural networks, aiming to recover the underlying model parameter W for effective generalization. Studies in this area involve two types of data generations, such as regression problems with Gaussian inputs and weight vectors for neurons. The research focuses on model recovery in neural networks for generalization, involving regression and classification problems with Gaussian inputs and weight vectors for neurons. Studies have examined single-neuron models, multi-neuron networks, and two-layer feedforward networks with ReLU activations. The research focuses on model recovery in neural networks for generalization, involving regression and classification problems with Gaussian inputs and weight vectors for neurons. Previous studies considered gradient descent over the squared loss for recovering neural network parameters, providing statistical guarantees for model recovery using the squared loss. Studies showed that the Hessian of the empirical loss function is positive definite in the local neighborhood of the ground truth, requiring a fresh set of samples at every iteration for gradient descent to converge to the ground truth. In this paper, the aim is to develop a strong statistical guarantee for the loss function in eq. (2) for model recovery in neural networks. This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross-entropy loss function, which is more practical than the squared loss for classification problems. This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function. For multi-neuron classification with sigmoid activations, the empirical risk function based on the cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth. The study guarantees recovery of one-hidden-layer neural networks using cross entropy loss for multi-neuron classification with sigmoid activations. Gradient descent converges linearly to a critical point with sample complexity O(dK 5 log 2 d) and computational complexity O(ndK 2 log(1/ )). The tensor method provides an initialization near the ground truth. The study guarantees recovery of one-hidden-layer neural networks using cross entropy loss for multi-neuron classification with sigmoid activations. It requires computational complexity of O(ndK 2 log(1/ )). The tensor method from BID38 provides an initialization near the ground truth by considering the curvature of activation functions. New techniques are developed to analyze the cross-entropy loss function, including statistical information of geometric curvatures and uniform concentration guarantees. Performance guarantees are also extended to the classification problem using squared loss. The focus is on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization, with a parameter recovery viewpoint relevant to signal processing problems. The statistical model for data generation removes worst-case instances, allowing for a focus on average-case performance and global convergence of simple local search. The statistical model for data generation removes worst-case instances, allowing a focus on average-case performance and global convergence of simple local search algorithms. In the landscape analysis of one-hidden-layer network models, it is found that there are no spurious local minima in the optimization landscape when the network size is large enough compared to the data input. However, in the case with multiple neurons in the under-parameterized setting, spurious bad local minima exist even at the population level. In the under-parameterized setting, Tian BID33 studied the landscape of the population squared loss surface with ReLU activations, revealing spurious bad local minima. Zhong et. al. BID38 provided characterizations for the local Hessian in regression with various activation functions. BID28 showed linear convergence of gradient descent with ReLU activation and Gaussian input when K = 1 and sample complexity is O(d). BID21 demonstrated that with bounded derivatives, there is only one global minimum in the optimization landscape. The study analyzes the cross entropy loss function in the model recovery classification problem under the multi-neuron case, a topic not previously explored. It differs from previous work by focusing on a different loss function form and studying a new problem domain. The paper discusses the model recovery classification problem under the multi-neuron case, which is a new area of study. Previous research focused on different neural network structures and loss functions, making direct comparisons challenging. The paper is organized into sections detailing problem formulation, main results, initialization method, numerical examples, and conclusions. The paper discusses the model recovery classification problem under the multi-neuron case. Section 2 describes the problem formulation, Section 3 presents main results on local geometry and convergence of gradient descent, Section 4 discusses the initialization method, Section 5 demonstrates numerical examples, and Section 6 draws conclusions.notations and definitions are provided throughout the paper. The generative model for training data and the gradient descent algorithm for learning network weights are described. Training samples are drawn i.i.d. from a normal distribution, with a sigmoid activation function used in a one-hidden layer neural network for classification. The one-hidden layer neural network model uses a sigmoid activation function for classification. The goal is to estimate the weights W by minimizing the empirical risk function, which is the cross entropy loss. To avoid getting stuck at local minima, a well-designed initialization scheme is implemented in the gradient descent algorithm. The gradient descent algorithm is implemented with a well-designed initialization scheme to prevent getting stuck at local minima. The update rule with step size \u03b7 is outlined, and the algorithm is summarized in Algorithm 1. The use of the same set of training samples throughout the execution is highlighted, contrasting with other methods that employ resampling. An important quantity regarding \u03c6(z) is introduced to capture the geometric properties of the loss function. The text introduces an important quantity \u03c6(z) to capture the geometric properties of the loss function, specifically for the sigmoid activation function. It characterizes the local strong convexity of f n (W) in a neighborhood of the ground truth W, with details on the Euclidean ball and singular values of W. The condition number \u03ba and \u03bb are defined in relation to the singular values. Theorem 1 guarantees the positive definiteness of the Hessian of the empirical risk function f n (W) in a local neighborhood of W for the classification model with sigmoid activation function, under certain conditions. Column permutations of W are equivalent global minima of the loss function, and the theorem applies to all such permutations. Theorem 1 ensures the positive definiteness of the Hessian of the empirical cross-entropy loss function for a classification model with sigmoid activation, under certain conditions. It guarantees this property in a neighborhood of the ground truth W, as long as W is full-column rank. The bounds in Theorem 1 depend on network dimension parameters, activation function, and ground truth values. In a special case with orthonormal columns, Theorem 1 guarantees a specific result when the sample complexity is sufficiently large. Theorem 2 guarantees the existence of a critical point Wn close to the ground truth W for a classification model with sigmoid activation function. Gradient descent converges linearly to Wn when the sample size is sufficiently large. Theorem 2 states that for a classification model with sigmoid activation function, there exists a critical point Wn close to the ground truth W. Gradient descent converges linearly to Wn with a rate of O(K 9/4 d log n/n) as the sample size increases. Theorem 2 guarantees the existence of a critical point Wn in the vicinity of W, converging at a rate of O(K 9/4 d log n/n). Gradient descent linearly converges to Wn if initialized in the basin of attraction. Achieving -accuracy requires O(ndK 2 log(1/ )) computational complexity. Initialization follows the tensor method from BID38, ensuring consistent recovery of W as n grows. The tensor method proposed in BID38 is briefly introduced, defining products and algorithms for estimating directions and decomposing tensors to approximate subspaces. The initialization algorithm in Algorithm 2 consists of two major steps for estimating the direction of each column of W and applying non-orthogonal tensor decomposition. The tensor method proposed in BID38 introduces products and algorithms for estimating directions and decomposing tensors to approximate subspaces. It involves reducing a third-order tensor to a lower-dimension tensor and applying non-orthogonal tensor decomposition to output estimates. Assumptions are made regarding the activation function and the curvature around the ground truth. The homogeneous assumption in BID38 is restrictive, so a new condition on the curvature of the activation function is assumed. This condition applies to a wider range of activation functions like sigmoid and tanh. The performance guarantee for the initialization algorithm is presented in Theorem 3, stating that under certain assumptions, the output of the algorithm satisfies DISPLAYFORM1 with high probability. The proof of Theorem 3 shows the accuracy of estimating the direction and norm of W in the classification model. Gradient descent is used to verify the strong convexity of the empirical risk function around W. The algorithm converges to the same critical point Wn with the same training samples. More details can be found in the supplementary materials. Gradient descent is used to verify the strong convexity of the empirical risk function around W. Multiple random initializations are done to ensure convergence to the same critical point Wn with the same training samples. The success rate of gradient descent is measured by the standard deviation of the estimator Wn, with a successful experiment defined as SDn \u2264 10\u22122. The success rate of gradient descent is high when the sample complexity is sufficient, converging to the same local minima with high probability. Statistical accuracy of the local minimizer is shown when initialized close to the ground truth, with average estimation error decreasing as sample size increases. Monte Carlo simulations show that estimation error decreases as sample size increases, matching theoretical predictions. Gradient descent with cross entropy loss outperforms squared loss in a classification problem. The study focuses on model recovery of a neural network using cross entropy loss in a multi-neuron classification problem, characterizing sample complexity for local strong convexity. In this paper, the model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem is studied. The sample complexity for local strong convexity is characterized, ensuring gradient descent converges linearly to the ground truth with high probability. Future work aims to extend the analysis to ReLU-like activations and convolutional neural networks. The population loss function is denoted as DISPLAYFORM0, and the proof of Theorem 1 follows specific steps. The population loss function is denoted as DISPLAYFORM0. The proof of Theorem 1 involves showing the smoothness and convexity properties of the Hessian \u2207 2 f (W) and \u2207 2 f n (W) in a neighborhood B(W, r). The Hessian of the population risk function is smooth around W, as shown in Lemma 1 for sigmoid activations. Lemma 2 establishes local strong convexity and smoothness of the population loss function in a neighborhood B(W, r). The Hessian of the population risk function is smooth around W, as shown in Lemma 1 for sigmoid activations. Lemma 2 establishes local strong convexity and smoothness of the population loss function in a neighborhood B(W, r). For sigmoid activations, there exists a constant C such that the Hessian of the empirical loss function is close to the Hessian of the population loss function in a uniform sense. This is proven in Appendix D.3 and D.4. Combining Lemma 3 and Lemma 1 leads to Theorem 1, showing that fn(W) is strongly. The proof of Theorem 1 establishes the strong convexity of fn(W) in a neighborhood B(W, r), ensuring at most one critical point in that region. The proof of Theorem 2 involves showing the concentration of the gradient \u2207fn(W) around \u2207f(W) in B(W, r), leading to the existence of a critical point Wn. Additionally, it is shown that Wn is close to W and gradient descent converges linearly to Wn with a properly chosen step size. Lemma 4 further confirms the uniform concentration of \u2207fn(W) around \u2207f(W) for sigmoid activation functions. Lemma 4 establishes that the gradient \u2207fn(W) concentrates around \u2207f(W) for sigmoid activation functions. It guarantees the existence of a critical point Wn in a neighborhood B(W*, r) with high probability. Wn is shown to be close to W, and gradient descent converges linearly to Wn with a properly chosen step size. The proof shows the existence of a critical point Wn near W, with gradient descent converging linearly to Wn using the proper step size \u03b7. The proof demonstrates that gradient descent converges linearly to the local minimizer Wn by ensuring accurate estimation of the direction of W and satisfying a mild condition in Assumption 2. The tensor operation is defined for matrices A, B, and C, facilitating the estimation process for each wi in regression. The tensor operation is used to estimate the direction of each wi in regression. Bernstein inequality is applied to bound the estimation error for the classification problem, with differences in proof compared to the regression problem. The classification problem applies Bernstein inequality to all neurons together, with bounded label y i. A different proof for estimating w i is provided, not requiring homogeneous activation function conditions. Quantity Q 1 is defined based on non-zero index l 1. The text discusses the definition of quantity Q1 based on the non-zero index l1 and how it can be estimated through an optimization problem. It also explains the process of estimating \u03b2 and w i in the initialization step. The text discusses estimating w i using an equation and the correct estimation of s i from the sign of \u03b2 i. It introduces useful definitions and results for the proofs. The text introduces definitions of sub-gaussian and sub-exponential norms for random variables, along with calculations of gradient and Hessian for a specific equation. The text provides calculations of the gradient and Hessian of E, focusing on the hessian block and the calculation of \u2206 j,l. It also discusses the upper bound of E T 2 j,l,k. The text discusses upper bounding E T 2 j,l,k using Cauchy-Schwarz inequality and presents a formula for sigmoid activation function. It also mentions the Hessian of the population risk and applying Lemma 1 for a uniform bound. The text presents upper and lower bounds of the Hessian of the population risk at ground truth, and applies Lemma 1 to obtain a uniform bound in the neighborhood of W. It further discusses bounding \u22072f(W) and concludes that if W - WF \u2264 0.7, then certain conditions hold. The text discusses bounding the terms P(A t), P(B t), and P(C t) separately, providing an upper bound for P(B t) before continuing. The text provides an upper bound for P(B t) before continuing, introducing a technical lemma for the proof."
}