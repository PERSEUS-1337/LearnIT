{
    "title": "HyerxgHYvH",
    "content": "The proposed solution for evaluating mathematical expressions involves a Lego bricks style architecture where small neural networks are trained independently to perform specific operations. Eight fundamental operations are identified and learned using simple feed forward neural networks, which can then be combined to solve more complex tasks like n-digit multiplication and division. The text discusses the use of simple feed forward neural networks to develop larger and more complex networks for solving mathematical operations like n-digit multiplication, n-digit division, and cross product. The approach focuses on reusability and generalization for computations involving n-digits, showing results for up to 7 digit numbers and generalizing for both positive and negative numbers. The success of feed-forward Artificial Neural Networks lies in their ability to learn and develop internal structures appropriate for specific tasks, dependent on the data provided during training. The learning process in artificial neural networks is heavily reliant on the data provided during training, leading to a lack of generalization and performance degradation on unseen data. While techniques like Domain Adaptation can help address these issues, ANNs primarily rely on memorization rather than understanding inherent rules, resulting in a lack of quantitative reasoning and systematic abstraction in decision-making. In contrast, other living species exhibit fundamental capabilities in numerical extrapolation and quantitative reasoning. The decision-making process in artificial neural networks lacks quantitative reasoning and systematic abstraction, unlike other living species that exhibit fundamental capabilities in numerical extrapolation and quantitative reasoning. Generalization in ANNs can be improved by identifying and learning fundamental operations that can be reused to develop complex functions. In this work, fundamental operations commonly used in arithmetic operations are identified and learned using simple neural networks. These operations are then reused to develop larger and more complex networks to solve problems like n-digit multiplication and division. This approach is inspired by human learning methodology and aims to improve quantitative reasoning in artificial neural networks. In this work, smaller neural networks are used to develop larger and more complex networks for solving arithmetic operations like n-digit multiplication and division. This approach is the first to propose a generalized solution for these operations, working for both positive and negative numbers. Neural networks are known for approximating mathematical functions, and previous work has focused on simple arithmetic operations. The proposed networks' architecture is complex due to the challenge of generalizing over unseen data. The architecture of proposed networks for arithmetic operations is complex due to the challenge of generalizing over unseen data. Recent works have attempted to train networks to generalize over minimal training data, such as Resnet, highway networks, and dense networks. However, existing models like EqnMaster and Neural Arithmetic Logic Unit (NALU) still struggle to generalize well beyond 3-digit numbers. Recent works in the field of arithmetic operations have introduced models like Neural Arithmetic Logic Unit (NALU) which use linear activations to predict arithmetic functions. However, these models still struggle with generalization beyond 3-digit numbers. Other approaches, such as Feed Forward Networks and Optimal Depth Networks, have been explored but may not result in the most efficient network architecture for solving arithmetic expressions. Using digital circuitry as a reference, neural networks can be designed to efficiently solve simple arithmetic problems. Our work builds on Binary Multiplier Neural Networks' premise to predict basic arithmetic functions over both positive and negative decimal numbers. We propose a network that can predict the output of arithmetic functions for both positive and negative decimal integers, using smaller networks for different sub tasks like signed multiplication and division. This approach allows for complex tasks to be performed efficiently. Our proposed network utilizes smaller networks for tasks like signed multiplication and division to perform complex arithmetic operations efficiently. Multiplication is akin to repeated addition, while division is akin to repeated subtraction. These operations are implemented on digital circuits known for accurate arithmetic operations and scalability. Previous work has shown neural networks can simulate digital circuits, inspiring our analysis of n-digit arithmetic operations. Neural networks can simulate digital circuits for efficient arithmetic operations like multiplication and division. By designing fundamental blocks for operations such as addition, subtraction, and multiplication, complex functions can be efficiently performed. Neural networks can simulate digital circuits for efficient arithmetic operations like multiplication and division. Fundamental blocks for addition, subtraction, and multiplication are designed to create complex functions. The basic function of a neuron network involves a sum transform using weighted inputs passed through an activation function to produce output. Addition and subtraction modules are implemented using single neurons with specific weights. This facilitates shift-and-add multiplication for arithmetic calculations. The subtraction module consists of a single neuron with two inputs and weights {+1, \u22121}. It facilitates shift-and-add multiplication by multiplying digits of the multiplier and multiplicand. The output is then placed at the appropriate position using a place value shifter and added to obtain the final output. The module can be unrolled for n inputs, each taking a 1-digit number. Fixed weights in powers of 10 are used for each preceding digit in the network. The proposed feed forward network for single digit multiplication uses fixed weights in powers of 10 for each preceding digit. It has two input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model computes the absolute value of a single number using a neural network with 2 hidden layers. The network computes the absolute value of a single number using 2 hidden layers. The first layer performs x + x and x \u2212 x operations, the second layer is a maxpool layer, and the final output layer subtracts the input from the maxpool output. The input sign calculator extracts the sign of an input number x using a single neuron with activation function x/(1 + mod(x)). The output sign calculator computes the sign of two numbers using a neural network with 2 hidden layers. The neural network model uses 2 hidden layers to compute the sign of 2 numbers. The first layer adds the numbers, the second layer applies modulus as activation, and the final layer subtracts 1. The network predicts the sign multiplication output using a soft-sign activation. Signed multiplication involves converting numbers to positive integers before performing operations. The neural network model uses 2 hidden layers to compute the sign of 2 numbers by adding, applying modulus, and subtracting. Signed multiplication involves converting numbers to positive integers before performing operations like multiplication and division. The process includes using fundamental operations, absolute operation, input/output sign calculators, and a multiply sub module for digit multiplication. The multiplication operation involves multiplying each token of the multiplicand with the 1st token of the multiplier, adding the results with carry forward, and combining them to form a single number. This process repeats for each token of the multiplier, with the final output assigned a sign using 1-digit sign multiply. The division model separates sign and magnitude during pre-processing, inspired by the long division model. The division model separates sign and magnitude during pre-processing, inspired by the long division model. The architecture involves multiplying the n-digit divisor with single digit multipliers and subtracting from the dividend. The selected node represents the remainder and quotient result for each chunk. The quotient is combined over iterations and the remainder is carried forward to the next digit in the divisor. The architecture of the multiplication network is shown in Figure 2(b,d) for a division model based on digital circuitry. A comparison is made with the results of signed arithmetic operations using Neural Arithmetic and Logic Unit (NALU) implementation. Test results are calculated on a prediction dataset ranging from 0 to 30 uniform numbers. Our model is trained to match the results claimed by Trask et al. (2018) using Neural Arithmetic and Logic Unit (NALU). We test our model on a dataset ranging from 0 to 30 uniform numbers, rounding them to integers. The experiment shows that our model outperforms recurrent and discriminative networks, achieving 100% accuracy within the testing range. Additionally, our model excels in signed multiplication, which is exclusive to our implementation. We also compare our results to the state-of-the-art model NALU for arithmetic operations on positive integers. In this experiment, we compare our model's results to the state-of-the-art Neural Arithmetic and Logic Unit (NALU) for arithmetic operations on positive integers. The NALU network is trained between the range of 10 to 20, and we use their generated test set for a fair comparison. The results are shown in Table 2. In this paper, it is demonstrated that complex tasks can be broken down into smaller sub-tasks, which can be solved by training independent small neural networks for each specific operation. Fundamental arithmetic operations are identified and learned using feed forward neural networks, which are then combined to solve more complex tasks like n-digit multiplication and division. The proposed work demonstrates breaking down complex tasks into smaller sub-tasks using independent neural networks for specific operations like arithmetic. Limitations include float operation in the tokenizer, but this doesn't hinder current work. Future plans include resolving this issue and testing a cross product network for accuracy. Additionally, a point cloud segmentation algorithm using multiple smaller networks is being developed."
}