{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks are crucial for image and object classification advancements. Wavelet Pooling is introduced as an alternative to traditional pooling methods, reducing feature dimensions and addressing overfitting. Experimental results show Wavelet Pooling outperforms or performs comparably with other pooling methods like max, mean, mixed, and stochastic. Our proposed method for pooling features outperforms traditional methods like max, mean, mixed, and stochastic pooling in classification datasets. CNNs are essential for image and object classification, consistently achieving higher accuracy rates than vector-based techniques. Researchers constantly innovate CNN components like convolutional and pooling layers to enhance accuracy and efficiency beyond previous benchmarks. Pooling, rooted in predecessors like Neocognitron, undergoes modifications to improve CNN performance. The convolutional layer and pooling layer in CNNs undergo constant modifications to improve accuracy and efficiency. Pooling, with roots in predecessors like Neocognitron and Cresceptron, reduces spatial dimensions of data, leading to benefits such as parameter reduction, computational efficiency, and regulation of overfitting. Max pooling and average pooling are popular methods, but other pooling operations exist with weaknesses that hinder optimal network learning. The text discusses different forms of pooling in CNNs, including max pooling and average pooling, which are deterministic and efficient but have weaknesses. Other pooling methods like mixed pooling and stochastic pooling use probabilistic approaches to address these issues. However, all pooling operations employ a neighborhood approach to subsampling, similar to nearest neighbor interpolation in image processing. A proposed wavelet pooling algorithm utilizes second-level wavelet decomposition to subsample features, aiming to minimize discontinuities in data for network regularization and improved classification accuracy. The text introduces a wavelet pooling algorithm that utilizes second-level wavelet decomposition to subsample features, aiming to minimize discontinuities in data for network regularization and improved classification accuracy. The proposed method is compared to other pooling techniques like max, mean, mixed, and stochastic pooling on various benchmark image classification datasets. The paper discusses various benchmark image classification datasets like MNIST, CIFAR-10, SHVN, and KDEF. The simulations are done in MATLAB R2016b. The paper is organized into sections covering background, proposed methods, experimental results, and summary. Pooling in convolutional layers involves dimensionality reduction through summarizing regions into one neuron value using max pooling or average pooling. Pooling in convolutional layers involves summarizing regions into one neuron value, with max pooling selecting the maximum value and average pooling calculating the average value. While effective, both methods can erase details from an image depending on the data. Researchers have developed probabilistic pooling methods to address the shortcomings of max and average pooling. Mixed pooling combines both methods by randomly selecting one over the other during training, applied in three different ways. Mixed pooling combines max and average pooling by randomly selecting one method over the other during training. This method is applied in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. Another probabilistic pooling method, called stochastic pooling, improves upon max pooling by randomly sampling from neighborhood regions based on probability values. Stochastic pooling improves upon max pooling by randomly sampling from neighborhood regions based on probability values, avoiding the shortcomings of max and average pooling while enjoying some of the advantages of max pooling. Our proposed pooling method utilizes wavelets to reduce feature map dimensions, minimizing artifacts from neighborhood reduction. By discarding first-order subbands, our approach captures data compression more organically, reducing jagged edges and other artifacts that could affect image classification. The wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT). The proposed wavelet pooling scheme reduces artifacts in image classification by performing a 2nd order decomposition using the fast wavelet transform (FWT). This involves applying the FWT twice on images to obtain detail and approximation subbands at each decomposition level. The proposed wavelet pooling algorithm performs backpropagation by reversing the process of its forward propagation. It involves 1st order wavelet decomposition and upsampling the detail coefficient subbands by a factor of 2 to create a new 1st level. The wavelet pooling algorithm performs backpropagation by reversing the forward propagation process. It involves 1st order wavelet decomposition, upsampling detail coefficient subbands by a factor of 2, and reconstructing the image feature using the Haar wavelet for further backpropagation. The experiments are conducted using MatConvNet, stochastic gradient descent, and specific hardware configurations. The Haar wavelet basis is used for its even, square subbands in the wavelet pooling algorithm. Experiments are conducted on a 64-bit operating system with specific hardware configurations. Different CNN structures are utilized for various datasets, with modifications like Dropout and Batch Normalization to examine their effects on pooling results. All pooling methods use a 2x2 window for consistency. Our proposed pooling method outperforms all others in the experiment conducted on the MNIST dataset. Different pooling methods show varying performance, with max pooling starting to overfit the data, while average and wavelet pooling exhibit smoother learning curves. The energy per epoch for each method is visualized in Figure 6. In experiments with different pooling methods, mixed and stochastic pooling show a rocky trajectory but do not overfit, while average and wavelet pooling exhibit smoother learning curves. Two sets of experiments are run, one without dropout layers and the other with dropout and batch normalization. The proposed method ranks second in accuracy in both cases, with wavelet pooling resisting overfitting. Our proposed method ranks second in accuracy in experiments with different pooling methods, showing resistance to overfitting. The network structure for the SHVN experiments includes 55,000 images for the case with no dropout and the full training set for the case with dropout. The network structure for the SHVN experiments includes 55,000 images for the case with no dropout and the full training set for the case with dropout. Our proposed method ranks second in accuracy among different pooling methods, showing resistance to overfitting. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. The dataset had errors that were fixed, including missing or corrupted images, which were mirrored in MATLAB. The KDEF dataset contains errors that were fixed by mirroring missing or corrupted images in MATLAB. The dataset does not designate a training or test set, so we shuffled the data and separated 3,900 images for training and 1,000 for testing. Images were resized to 128x128 due to memory and time constraints. Our proposed method in TAB5 showed the second highest accuracy, with dropout layers regulating the network to prevent overfitting. Max pooling tends to overfit, while wavelet pooling resists it. Average and mixed pooling are stable but not ideal for learning. Our proposed method in TAB5 demonstrated the second highest accuracy, with wavelet pooling resisting overfitting. Stochastic pooling showed consistent learning progression, while wavelet pooling also exhibited smooth learning progression. Computational complexity remains a challenge for wavelet pooling, requiring improvements in efficiency. The code implementation is a proof-of-concept, open to enhancements for optimization. The code implementation for the proposed method in TAB5 is not optimized for computational efficiency. The accuracy results and novelty serve as a starting point for future improvements by both our research and other researchers. Efficiency is measured in terms of mathematical operations for different pooling methods. In terms of computational efficiency, different pooling methods have varying numbers of mathematical operations. Average pooling requires the least computations, followed by mixed pooling and max pooling. Stochastic pooling is the least efficient, using about 3x more operations than average pooling. Wavelet pooling is the most computationally intensive method. Stochastic pooling is the least computationally efficient method, using about 3x more operations than average pooling. Wavelet pooling is the most computationally intensive, using 54 to 213x more operations than average pooling. However, with improvements in coding practices, GPUs, and the FTW algorithm, wavelet pooling can be a viable option. Various improvements to the FTW algorithm exist, such as utilizing multidimensional wavelets, lifting, and parallelization, to enhance efficiency in speed and memory. Wavelet pooling, with improvements in the FTW algorithm, can be a viable option. It outperforms other methods in the MNIST dataset and performs well in the CIFAR-10 and KDEF datasets. The addition of dropout and batch normalization shows improved network regularization. Wavelet pooling method outperforms others in MNIST dataset and performs well in CIFAR-10 and KDEF datasets. Results confirm no superior pooling method, with some performing better depending on dataset and network structure. Future work includes varying wavelet basis and altering upsampling/downsampling factors for better image feature reductions. Improving FTW method could greatly increase accuracy. Improving downsampling factors in decomposition and reconstruction can enhance image feature reductions beyond 2x2 scale. Retaining discarded subbands for backpropagation may increase accuracies. Enhancing FTW method can boost computational efficiency. Analyzing SSIM of wavelet pooling versus other methods can validate our approach's effectiveness."
}