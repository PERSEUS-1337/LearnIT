{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, and algorithms and convergence proofs are provided for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization allows adaptivity across manifolds in the cartesian product, with experimentally faster convergence shown. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when dealing with a large number of parameters. Riemannian adaptive methods show faster convergence and lower train loss values compared to standard algorithms, as demonstrated in experiments embedding the WordNet taxonomy in the Poincare ball. This advancement addresses the challenge of generalizing popular optimization tools like Adam, Adagrad, and Amsgrad to Riemannian manifolds, allowing adaptivity across different manifolds in the cartesian product. The development of powerful optimization algorithms is essential for handling a large number of parameters in various fields. Recent advancements have led to the creation of successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD for optimizing parameters in a Euclidean space. However, there is a growing interest in optimizing parameters on a Riemannian manifold, which offers applications in solving various problems such as Lyapunov equations, matrix factorization, and dictionary learning. On a Riemannian manifold, algorithms have been developed for various applications like solving Lyapunov equations, matrix factorization, geometric programming, dictionary learning, and hyperbolic taxonomy embedding. While first-order stochastic methods have been adapted to this setting, there is a need for Riemannian counterparts for adaptive algorithms with convergence analysis. The adaptivity of these algorithms involves assigning a learning rate per coordinate of the parameter vector, which poses challenges on a Riemannian manifold due to the lack of intrinsic coordinates. In this work, the authors explain the challenges of generalizing adaptive algorithms to Riemannian manifolds and propose generalizations for a product of manifolds. They also provide convergence analysis and empirical support using hyperbolic taxonomy embedding as a motivating application. The authors propose Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces, specifically benefiting hyperbolic taxonomy embedding. They highlight the importance of Riemannian adaptive algorithms for competitive optimization-based Riemannian embedding methods in hyperbolic spaces. The recent rise of embedding methods in hyperbolic spaces could benefit from developments in competitive optimization-based Riemannian embedding methods. A manifold is a space that can be approximated locally by a Euclidean space and is a generalization of the notion of surface to higher dimensions. A Riemannian manifold is an (n \u2212 1)-dimensional manifold embedded in R n with a Riemannian metric \u03c1 defining the geometry locally on M. It consists of a pair (M, \u03c1) and induces a global distance function on M. The Riemannian manifold (M, \u03c1) defines a global distance function on M using smooth paths and the length of speed vectors. Riemannian SGD updates involve the gradient of the objective function and the step-size. The exponential map allows updates along the shortest path in the relevant direction while staying in the manifold. The exponential map enables updates along the shortest path in the relevant direction on the Riemannian manifold. When the exact exponential map is unknown, it is common to use a retraction map as a first-order approximation. Algorithms like ADAGRAD and ADAM offer rescaled updates based on past gradients, with ADAM incorporating a momentum term for optimization. ADAM update rule proposed by BID9 includes momentum and adaptivity terms. RMSPROP is similar to ADAM but uses an exponential moving average. ADAM's momentum term for \u03b21=0 has shown significant improvements. AMSGRAD corrected a convergence issue in ADAM. AMSGRAD corrected a convergence issue in ADAM by proposing modifications to the algorithm. Intrinsic updates on a Riemannian manifold require a coordinate system choice. \u03b5 = 10^-8 is often added for numerical stability in coordinate-wise updates. The RSGD update of Eq. FORMULA1 is intrinsic to the Riemannian manifold (M, \u03c1) as it only involves exp and grad, which are intrinsic objects. The formalism on the manifold allows working with local coordinate systems called charts, where quantities can be defined intrinsically to M. The RSGD update in Eq. FORMULA1 is intrinsic to the Riemannian manifold (M, \u03c1) as it involves exp and grad. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free manner. One possible approach is to fix a canonical coordinate system in the tangent space at the initialization point and parallel-transport it along the optimization trajectory. In a general Riemannian manifold, parallel transport between two points depends on the chosen path and curvature introduces a rotational component, breaking the sparsity of gradients and adaptivity benefits. The interpretation of adaptivity as optimizing different features at different speeds is lost, as the coordinate system for gradients depends on the optimization path. The techniques used to prove the theorems may not apply to updates defined differently. Additional structure is assumed on the manifold for further analysis. The coordinate system for gradients in a general Riemannian manifold depends on the optimization path. Techniques used to prove the theorems may not apply to updates defined differently. Additional structure is assumed on the manifold for further analysis. In a general Riemannian manifold, adapting learning rates per coordinate is challenging due to the lack of intrinsic coordinates. Each component of x can be seen as a \"coordinate\", allowing for a simple adaptation of Eq. (3) on the adaptivity term. Different optimization techniques like ADAGRAD, ADAM, and AMSGRAD are briefly discussed. ADAM combines ADAGRAD with momentum and exponential moving averages, while AMSGRAD corrects convergence issues. ADAMNC is ADAM with a non-constant schedule for a parameter. ADAMNC is a modification of ADAM with a non-constant schedule for parameters. It incorporates a schedule proposed by BID18 for \u03b2 2, allowing it to recover the sum of squared-gradients of ADAGRAD. The assumptions and notations involve geodesically complete Riemannian manifolds with lower bounded sectional curvature. The set of feasible parameters is defined as X := X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xn. The text discusses Riemannian AMSGRAD on a product manifold with geodesically convex sets. It introduces projection operators, parallel transport, exponential, and log maps. The convergence guarantees bound the regret after T rounds. The method is compared with ADAMNC, which incorporates a non-constant schedule for parameters. The text presents Riemannian AMSGRAD on a product manifold with geodesically convex sets, introducing projection operators, parallel transport, exponential, and log maps. Convergence guarantees bound the regret after T rounds, compared with ADAMNC incorporating a non-constant schedule for parameters. The convergence guarantee for RAMSGRAD is presented in Theorem 1, with comparisons to AMSGRAD in appendix C. The convergence guarantee of RAMSGRAD is discussed, with comparisons to AMSGRAD in appendix C. The regret bound worsens at a speed determined by the curvature. RADAMNC's convergence guarantee is shown in Theorem 2, and a convergence proof for RADAGRAD is provided when \u03b2 1 = 0. The convergence of RAMSGRAD and RADAMNC is proven in Theorems 1 and 2 respectively. The role of convexity is highlighted in the proofs, comparing convex and geodesically convex functions. Regret bounds for convex objectives are derived using specific equations. Convex and geodesically convex functions play a role in proving the convergence of RAMSGRAD and RADAMNC. Regret bounds for convex objectives are obtained by bounding specific terms, with a focus on the cosine law for SGD updates. The use of well-chosen decreasing schedules for \u03b1 is crucial in Riemannian manifolds. The telescopic summation simplifies the second term with a decreasing schedule for \u03b1. In Riemannian manifolds, a generalized step using lemma 6 in Alexandrov spaces is applied to geodesically convex subsets with lower bounded sectional curvature. The curvature dependent quantity \u03b6 from Eq. (10) bounds \u03c1. Adaptivity benefits from sparse gradients, improving bounds for per-manifold gradients. The convergence theorems do not require specifying \u03d5 i, suggesting potential improvement in regret bounds by exploiting momentum/acceleration in proofs. The convergence theorems do not require specifying \u03d5 i, suggesting potential improvement in regret bounds by exploiting momentum/acceleration in proofs. Empirical assessment of RADAM, RAMSGRAD, and RADAGRAD compared to non-adaptive RSGD method is done by embedding the WordNet noun hierarchy in the n-dimensional Poincar\u00e9 model of hyperbolic geometry. The Poincar\u00e9 model of hyperbolic geometry is used to embed tree-like graphs, with each word in the same space of constant curvature. Optimization tools are explored for algorithms, justified by closed form expressions. Key features include rescaled gradients, distance functions, geodesics, exponential and logarithmic maps, and parallel transport along geodesics. The dataset involves the transitive closure of the WordNet taxonomy graph. The transitive closure of the WordNet taxonomy graph consists of 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in D n to minimize distances between connected words. The loss function used is similar to log-likelihood, with sampling of negative word pairs. Metrics include reporting loss value and mean average precision. In our case, the loss function does not consider the direction of edges in the graph. We focus on 5-dimensional hyperbolic spaces and use the same training settings as in previous work. Negative words are sampled based on their graph degree raised at power 0.75 during the \"burn-in phase\" for 20 epochs. This strategy improves all metrics. During the \"burn-in phase\" described in BID15, negative words are sampled based on their graph degree raised at power 0.75 for 20 epochs. This strategy improves metrics. RADAM is preferred over RAMS-GRAD for optimization. Replacing the true exponential map with its first-order approximation leads to lower loss values, possibly due to needing fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn. Results from the study show that \"retraction\"-based methods are not directly comparable to fully Riemannian methods. Different learning rates were tested, with the best settings shown for RADAM and RAMSGRAD. The use of \u03b21 = 0.9 and \u03b22 = 0.999 achieved the lowest training loss for these methods. RADAM consistently achieves the lowest training loss and outperforms other methods on the MAP metric for both reconstruction and link prediction in the full Riemannian setting. In the \"retraction\" setting, RADAM shows the lowest training loss value and performs similarly to RSGD on the MAP evaluation. RAMSGRAD converges faster for the link prediction task, indicating better generalization capability. RAMSGRAD converges faster for link prediction, indicating better generalization capability compared to other Riemannian optimization methods. Various first-order Riemannian methods have been introduced, along with new convergence analysis techniques in the geodesically convex case. Stochastic gradient Langevin dynamics has also been generalized to improve optimization on the probability simplex. Riemannian counterparts of SGD with momentum and RMSprop have been proposed, although no convergence guarantee is provided for their algorithm. The BID20 proposed Riemannian counterparts of SGD with momentum and RMSprop, suggesting to transport the momentum term using parallel translation. However, their algorithm compromises convergence guarantees by performing coordinate-wise adaptive operations in the tangent space. Another version of Riemannian ADAM for the Grassmann manifold G(1, n) by BID3 removes the adaptive component, lacking adaptivity across manifolds and convergence analysis. The proposed method generalizes adaptive optimization tools to Cartesian products of Riemannian manifolds, with convergence rates similar to Euclidean models. Experimental results show superiority over non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The curr_chunk discusses the application of lemma 6 in proving convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. The lemma involves a user-friendly inequality developed for this purpose. The lemma involves a user-friendly inequality developed for proving convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. It is used in a convergence proof for ADAMNC by BID18."
}