{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, the adversarial perturbation is decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. The proposed approach utilizes noise reduced gradient (NRG) to craft adversarial examples, enhancing transferability significantly across various ImageNet models. Low-capacity models show stronger attack capability compared to high-capacity models with similar test performance. These findings offer a principled method for constructing successful adversarial examples and insights for developing defense strategies against black-box attacks in the era of neural networks. The resurgence of neural networks has led to the use of large models in real-world applications like speech recognition and computer vision. Adversarial examples can manipulate these models into producing incorrect outputs, raising questions on understanding and defending against them. Adversarial examples can transfer across different models, posing a challenge for defense strategies. The transferability of adversarial examples poses a challenge for defending against them. Different views exist on the cause of adversarial instability, with some attributing it to the strong nonlinearity of deep neural networks and others to the linear nature and high dimensionality. The primary cause of adversarial instability in deep neural networks is debated, with some attributing it to strong nonlinearity and others to linear nature and high dimensionality. Various methods like FGSM, DeepFool, and iterative gradient sign method have been proposed for crafting adversarial attacks. Ensemble-based approaches and defensive distillation have also been suggested for effective defense against adversarial examples. The transferability of adversarial examples is explained in this work, leading to enhanced black-box attacks. Adversarial perturbation can be decomposed into two components, with stronger misclassified examples having higher transferability. Various defense methods have been proposed, including defensive distillation, adversarial training, and image transformation to mitigate the impact of adversarial perturbations. Some works focus on detecting adversarial examples, but they can be easily overcome by designing stronger attacks. The transferability of adversarial examples is explained in this work, attributing it to the data-dependent component of adversarial perturbations. By utilizing the data-dependent gradient component instead of the gradient itself, adversarial examples can be constructed with enhanced effectiveness. The proposed noise-reduced gradient (NRG) method enhances black-box attacks on ImageNet validation set by utilizing the data-dependent gradient component. Model-specific factors like capacity and accuracy influence the success rate of attacks, with higher accuracy and lower capacity models showing stronger attack capability. This phenomenon is attributed to transferability and provides guidance for attacking unseen models. In this work, models with higher accuracy and lower capacity demonstrate stronger capability to attack unseen models. The vulnerability of high-dimensional models to adversarial perturbations is highlighted, where small imperceptible perturbations can lead to adversarial examples. In the context of deep neural networks, adversarial perturbations can create imperceptible changes to input data, leading to adversarial examples. These attacks can be non-targeted or targeted, with the latter aiming to produce a specific misclassification. In a black-box attack, the adversary has no knowledge of the target model and cannot query it directly. In a black-box attack, the adversary has no knowledge of the target model and cannot query it directly. Adversarial examples are crafted on a local model and deployed to fool the target model. Crafting adversarial perturbations involves optimizing a loss function to measure prediction discrepancies. The optimization problem for crafting adversarial examples involves a loss function measuring prediction discrepancies and a metric for perturbation magnitude. There is an implicit constraint for image data, and commonly used loss functions and distortion metrics. Ensemble-based approaches suggest using multiple source models to improve adversarial example strength. Ensemble-based approaches suggest using a large ensemble of source models to improve adversarial example strength. The objective involves averaging predicted probabilities of each model, with various optimizers available. The Fast Gradient Based Method attempts to solve the optimization problem with normalized gradients for non-targeted and targeted attacks. The Fast Gradient Based Method (FGBM) and Iterative Gradient Method are two approaches for solving optimization problems in adversarial attacks. FGBM performs one-step iteration with normalized gradient vectors, while the Iterative Gradient Method uses projected normalized-gradient ascent for multiple steps. Both methods are empirically fast and effective, with good transferability. The normalized gradient is used in fast gradient based methods for adversarial attacks. Transferability of adversarial examples between models is crucial for black-box attacks and defenses. Previous works suggest that transferability is due to similarity in decision boundaries and spans a contiguous subspace. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Previous works suggest that similarity between models A and B enables transferability, with components on and off the data manifold playing a key role. The perturbation can be decomposed into data-dependent and model-specific components, with the former contributing to transferability between models A and B. The model-specific component has little impact on transferability due to different behaviors off the data manifold. The perturbation is illustrated in FIG0, showing how it can mislead both models A and B. The perturbation can be decomposed into data-dependent and model-specific components, with the former contributing to transferability between models A and B. The model-specific component has little impact on transferability due to different behaviors off the data manifold. In the left panel, decision boundaries of two models are similar in the inter-class area, with \u2207f A misleading both models. The right panel shows decision boundaries of resnet34 (model A) and densenet121 (model B) for ImageNet. To increase success rates of black-box adversarial attacks, enhancing the data-dependent component is crucial. The NRG method is proposed to reduce model-specific noise and achieve this goal. The NRG method aims to enhance the data-dependent component of black-box adversarial attacks by reducing model-specific noise. By applying local averaging to remove noisy information inherited from random initialization, the noise-reduced gradient captures more data-dependent information than the ordinary gradient. The Noise-reduced Gradient (NRG) method enhances black-box adversarial attacks by reducing model-specific noise and capturing more data-dependent information. Using NRG instead of the ordinary gradient can lead to smoother gradients and better generalization to other models. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) is proposed as a way to drive the optimizer towards more data-dependent solutions. The Noise-reduced Gradient (NRG) method enhances black-box adversarial attacks by reducing model-specific noise and capturing more data-dependent information. Using NRG instead of the ordinary gradient can lead to smoother gradients and better generalization to other models. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) is proposed as a way to drive the optimizer towards more data-dependent solutions, improving transferability. We utilize state-of-the-art classification models trained on the ImageNet dataset, specifically the ImageNet ILSVRC2012 validation set with 50,000 samples. For each attack experiment, 5,000 images are randomly selected that can be correctly recognized by all models. Pre-trained models provided by PyTorch such as resnet, vgg, densenet, alexnet, and squeezenet are used. Top-1 and Top-5 accuracies can be found on a website for reference. To ensure experiment reliability, all models are used, but only select models are chosen for specific experiments to save computational resources. The models used for experiments include densenet161, densenet169, densenet201, alexnet, and squeezenet1. The reliability of experiments is ensured by using all models, but only select ones are chosen for specific experiments to save computational time. Adversarial examples are evaluated for white-box attack performance using Top-1 and Top-5 success rates. The loss function chosen is cross entropy, and distortion is measured using \u221e norm and scaled 2 norm. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with commonly-used methods like FGSM and IGSM. Results show that nr-FGSM performs better than original FGSM consistently and dramatically for both blackbox and white-box attacks. The noise-reduced gradient technique, when combined with FGSM and IGSM, shows significant improvements in performance for both blackbox and white-box attacks. Nr-FGSM outperforms original FGSM consistently, while nr-IGSM generates adversarial examples that transfer more easily than IGSM. This indicates that the noise-reduced gradient guides the optimizer towards more data-dependent solutions. The noise-reduced gradient (NRG) in nr-IGSM leads to easier transfer of adversarial examples compared to IGSM. Large models like resnet152 are more robust to adversarial transfer. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet. This contradicts previous claims but aligns with the conclusions of other studies. The inappropriate choice of hyperparameters in generating adversarial examples can lead to underfitting, affecting transferability between source and target models. Alexnet's architecture and test accuracy make it significantly different from other models, causing IGSM to overfit and produce a lower fooling rate compared to FGSM. The noise reduced gradient technique helps prevent overfitting to source model-specific information, improving cross-model generalization. NRG is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images due to computational costs. Both FGSM and IGSM, along with their noise reduced versions, are used for non-targeted attacks. In this section, the NRG method is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images due to computational costs. Non-targeted attacks using FGSM, IGSM, and their noise reduced versions are tested. The Top-1 success rates of IGSM attacks are nearly saturated, so the corresponding Top-5 rates are reported to demonstrate method improvements. Targeted attacks are not considered due to the difficulty in generating adversarial examples predicted by unseen target models. The NRG method outperforms normal methods in generating targeted adversarial examples by using a larger step size. The Top-5 success rates show significant improvement with NRG methods compared to traditional methods for both targeted and non-targeted attacks. The NRG method outperforms traditional methods in generating adversarial examples, showing significant improvement in success rates for both targeted and non-targeted attacks. Sensitivity of hyper parameters is explored for black-box attacks using the NRG method, with results shown in FIG6. The sensitivity of hyperparameters m and \u03c3 in NRG methods for black-box attacks is explored using the nr-FGSM approach. Larger m leads to higher fooling rates, while an optimal value of \u03c3 is crucial for best performance. The optimal \u03c3 varies for different source models, being around 15 for resnet18 and 20 for densenet161 in this experiment. Additionally, the robustness of adversarial perturbations to image transformations is preliminarily explored. In this experiment, the optimal \u03c3 varies for different source models, with a value of about 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is explored to determine the fraction of adversarial images that are no longer misclassified after a transformation. Densenet121 and resnet34 are chosen as the source models. The study explores the robustness of adversarial images to various transformations using Densenet121 and Resnet34 as source models. Four image transformations are considered: rotation, Gaussian noise, Gaussian blur, and JPEG compression. Results show that adversarial examples generated by NRG methods are more robust than vanilla methods. Decision boundaries of different models are analyzed to understand the superior performance of NRG-based methods. Resnet34 is used as the source model with nine target models considered. The study analyzes the decision boundaries of different target models using Resnet34 as the source model. Various transformations are applied to images to generate adversarial examples, showing that NRG-based methods outperform vanilla methods in robustness. The sensitivity of target models to different directions of perturbations is examined, revealing that most models are more sensitive to sign \u2207f compared to sign (\u2207f \u22a5 ), except for alexnet. The study examines the sensitivity of target models to different perturbation directions, finding that most models are more sensitive to sign \u2207f than sign (\u2207f \u22a5 ). Removing \u2207f \u22a5 penalizes the optimizer along model-specific directions, preventing overfitting to the source model. The minimal distance u for adversarial transfer varies among models, with complex models requiring larger distances than smaller models. This geometric understanding explains why big models are more robust than small models. Big models like resnet152 are more robust than small models like resnet50. Adversarial attacks from densenet121 perform well across different target models, while attacks from alexnet generalize poorly. Different models show varying performances when attacking the same target model, guiding the selection of a better local model for generating adversarial examples. Different models exhibit different performances in attacking the same target model. By selecting vgg19 bn and resnet152 as target models, various models were used to conduct FGSM and IGSM attacks. Results show that models with lower test error and capacity have stronger attack capabilities. Smaller test error and lower capacity indicate stronger attack capability. The smaller the test error and lower the capacity of a model, the stronger its attack capability. This is due to the model's ability to provide strong adversarial examples that transfer more easily. In this study, it was found that models with lower capacity and higher test accuracy are more effective in generating adversarial examples that transfer easily. Adversarial perturbations were decomposed into model-specific and data-dependent components, with the latter contributing significantly to transferability. The proposed noise-reduced gradient (NRG) based methods for crafting adversarial examples were shown to be more effective than previous approaches. Additionally, models with lower capacity and higher test accuracy demonstrated stronger capabilities for black-box attacks. Future work will explore combining NRG-based methods with other techniques. In future research, the focus will be on combining NRG-based methods with adversarial training to defend against black-box attacks. The transferability component is data-dependent and low-dimensional, making black-box attacks defensible. White-box attacks, originating from high-dimensional space, are more challenging to defend against. Incorporating NRG strategy can help learn stable features for transfer learning and reduce model-specific noise for more accurate data representation. The research focuses on learning stable features for transfer learning by incorporating the NRG strategy to reduce model-specific noise. The study explores the impact of hyperparameters in IGSM for targeted black-box attacks, evaluating success rates on 1,000 randomly selected images using resnet152 and vgg16 bn as target models. Results show that an optimal step size is crucial for attack performance, with both too large and too small sizes yielding harm. In the experiment, a large step size of \u03b1=15 is optimal for attack performance, compared to the allowed distortion \u03b5=20. Small step sizes can lead to worse performance due to overfitting, while larger step sizes encourage exploration of model-independent areas. Additional experiments on the MNIST dataset confirm the impact of model redundancy on attack capability. The experiment on the MNIST dataset explores the impact of model redundancy on attack capability. Models with different depths are tested, showing that low-capacity models have stronger attack capabilities. The Top-1 success rates of cross-model attacks are reported, demonstrating the effectiveness of attacks on models with smaller capacities."
}