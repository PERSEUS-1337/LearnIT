{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks in computer vision, where perturbations to images can cause models to make mistakes. A new attack method reprograms the target model to perform a task chosen by the attacker without specifying the desired output for each input. This single perturbation can be added to all inputs to make the model perform the desired task, even if it was not trained for it. This method was demonstrated on six ImageNet classification models. Adversarial reprogramming can repurpose ImageNet models to perform new tasks, such as counting and classification. Adversarial examples pose a threat by causing model errors with small input changes, like tricking a self-driving car with a sticker or manipulating insurance claim values with doctored photos. Various methods have been proposed to address this vulnerability. Various methods have been proposed to construct and defend against adversarial attacks, which can deceive models with small input changes like stickers on self-driving cars or doctored photos for insurance claims. Adversarial attacks can be untargeted, degrading model performance, or targeted, aiming for specific outputs. In this work, a novel adversarial goal is considered: reprogramming a model to perform a task chosen by the attacker without needing to compute the specific desired output. This involves an adversary wanting the model to compute a function different from its original task, showcasing the ability to achieve this. An adversary can reprogram a model to perform a different task by learning adversarial reprogramming functions that map between the original and adversarial tasks. The parameters of the adversarial program are adjusted to achieve the desired outcome. Adversarial reprogramming involves repurposing a model to perform a new task by transforming input and output formats. The adversarial program, \u03b8, is adjusted to achieve the desired outcome, which may not necessarily be imperceptible to humans. Adversarial reprogramming involves repurposing a model by transforming input and output formats. The attack does not need to be imperceptible to humans to succeed. Potential consequences include theft of resources, repurposing AI assistants into spies or spam bots, and abusing machine learning services. Risks of this attack are discussed in more detail in Section 5.2. Flexibility in repurposing a neural network through changes to its inputs is consistent with the expressive power of deep neural networks. The flexibility of repurposing a neural network through input changes is demonstrated by the ability to achieve unique output patterns with network depth. Networks can be trained accurately even with parameter updates in a low-dimensional subspace. An additive offset to the input is equivalent to modifying the first layer biases, allowing for adversarial programs to update in a low-dimensional space. In this paper, the authors introduce adversarial reprogramming, which involves crafting adversarial programs to make a neural network perform a new task. They experimentally demonstrate how these programs can alter the function of convolutional neural networks designed for ImageNet classification to tasks like counting squares, classifying MNIST digits, and classifying CIFAR-10 images. The susceptibility of trained and untrained networks to adversarial programs is also examined. The authors introduce adversarial reprogramming, crafting programs to change a neural network's task from ImageNet classification to tasks like counting squares, MNIST digit classification, and CIFAR-10 image classification. They explore the susceptibility of trained and untrained networks to these programs, demonstrating the ability to reprogram tasks with data dissimilar to the original. Adversarial examples are intentionally designed inputs to cause model errors. Adversarial examples are inputs intentionally designed to cause machine learning models to make mistakes. These attacks can be untargeted or targeted, and have been proposed in various domains such as malware detection, generative models, and network policies for reinforcement learning tasks. In the domain of adversarial attacks, reprogramming methods aim to produce specific functionality rather than a hardcoded output. Adversarial examples can be applied to many inputs to manipulate model predictions, such as using an \"adversarial patch\" to switch predictions to a specific class. Adversarial reprogramming involves using a single program to manipulate a model to process images in a specific way, similar to parasitic computing and weird machines in exploiting network protocols and running arbitrary code on targeted computers. Transfer learning and adversarial reprogramming repurpose neural networks to perform new tasks by leveraging knowledge from one task to learn another. Neural networks possess versatile properties useful for various tasks, such as developing features resembling Gabor filters when trained on images with different datasets or training objectives. Transfer learning allows neural networks to adapt to new tasks by leveraging knowledge from previous tasks. This is different from adversarial reprogramming, where model parameters can be changed for the new task, unlike in typical adversarial settings where the model remains unchanged. In adversarial reprogramming, an attacker can change model parameters to achieve goals through input manipulation. The adversary aims to reprogram a neural network for a new task by crafting an adversarial program within the network input. This approach is more challenging than transfer learning and can be applied beyond ImageNet classification. The adversarial program is an additive contribution to network input, not specific to a single image. The adversarial program in adversarial reprogramming is an additive contribution to network input, not specific to a single image. It is formulated with parameters to be learned and a masking matrix for new task data. The mask is optional and used for visualization purposes. The adversarial perturbation is bounded within a range to match ImageNet images. The adversarial program in adversarial reprogramming is designed to enhance visualization and is bounded within a specific range to match ImageNet images. It involves a mapping function to assign labels from an adversarial task to ImageNet labels. The adversarial program in adversarial reprogramming involves a mapping function that assigns labels from an adversarial task to ImageNet labels. The optimization problem includes a weight norm penalty to reduce overfitting, optimized with Adam and exponentially decaying learning rate. Minimal computation cost is required for the adversary after optimization. Adversarial reprogramming involves minimal computation cost for the adversary, exploiting nonlinear behavior of the target model. Experiments on six architectures trained on ImageNet demonstrated reprogramming the network for different adversarial tasks like counting squares. Adversarial reprogramming experiments were conducted on six ImageNet-trained architectures, reprogramming them for tasks like counting squares, MNIST, and CIFAR-10 classification. The study also looked into resistance to reprogramming, comparing trained networks to random ones, and explored reprogramming with dissimilar data. Additionally, concealing the adversarial program and data was demonstrated. The study conducted adversarial reprogramming experiments on ImageNet-trained architectures for tasks like counting squares. Images were generated with white squares on a grid, embedded in an adversarial program. Each ImageNet model was trained with an adversarial program representing the number of squares in the image. The study conducted adversarial reprogramming experiments on ImageNet-trained architectures for tasks like counting squares. Adversarial programs were trained per ImageNet model to represent the number of squares in each image. Despite the dissimilarity of ImageNet labels and adversarial labels, the adversarial program mastered the counting task for all networks. The study demonstrated the vulnerability of neural networks to reprogramming by training adversarial programs on the task of classifying MNIST digits. Despite the different labels, the adversarial program achieved high accuracy on both test and train data, showing success in reprogramming ImageNet networks. The study showed that ImageNet networks can be reprogrammed to function as an MNIST classifier using an adversarial program. The program generalized well from training to test set and was not brittle to small changes in input. A more challenging task was then attempted, repurposing ImageNet models to classify CIFAR-10 images, resulting in increased accuracy from chance to moderate levels. The study demonstrated the successful repurposing of ImageNet models to classify CIFAR-10 images, achieving moderate accuracy levels. Adversarial programs showed visual similarities and low spatial frequency texture in ResNet architecture. The degree of susceptibility to adversarial reprogramming depends on the details of the model being attacked, as seen in the examination of attack success on an Inception V3 model. The Inception V3 model trained with adversarial training on ImageNet data is still vulnerable to adversarial reprogramming, with only a slight reduction in attack success. Standard approaches to adversarial defense have little efficacy against adversarial reprogramming due to differences between the two types of attacks. The Inception V3 model trained with adversarial training on ImageNet data is vulnerable to adversarial reprogramming, showing little defense efficacy. Adversarial reprogramming aims to repurpose the network, unlike traditional attacks, with large adversarial programs compared to small perturbations. Adversarial defense methods may not generalize to data from the adversarial task. Further exploration on models with random weights was conducted using the same MNIST reprogramming task. The study conducted adversarial reprogramming attacks on models with random weights, using the same MNIST reprogramming task as in the previous section. Results showed that networks pretrained on ImageNet performed better than random networks, with only ResNet V2 50 achieving similar accuracy. Adversarial programs generated for random networks were qualitatively different from those for pretrained networks, highlighting the importance of the original task the neural networks perform in adversarial reprogramming. The study found that networks pretrained on ImageNet outperformed random networks in adversarial reprogramming attacks. Randomly initialized networks may perform poorly due to weight scaling issues, while trained weights are better conditioned. Adversarial reprogramming may rely on similarities between original and adversarial data. The study showed that pretrained ImageNet networks can be reprogrammed to classify shuffled MNIST and CIFAR-10 images, even when they do not share spatial structure. This suggests that the network may rely on similarities between original and adversarial data. Our results demonstrate the possibility of reprogramming neural networks to classify shuffled CIFAR-10 images, indicating the potential for reprogramming across tasks with unrelated datasets and domains. The accuracy for shuffled CIFAR-10 decreased due to the convolutional structure not being useful for classification, but was comparable to fully connected networks. This suggests that transferring knowledge between original and adversarial data does not fully explain susceptibility to reprogramming. Additionally, it is possible to limit the visibility of adversarial perturbations. In a series of experiments, the researchers demonstrated the possibility of limiting the visibility of adversarial perturbations by constraining the size, scale, or concealing the adversarial task. Using an Inception V3 model pretrained on ImageNet, they successfully reprogrammed the network to classify MNIST digits with reduced accuracy when using a small adversarial program. They also made the adversarial program nearly imperceptible by limiting the L inf norm of the perturbation, showing that adversarial reprogramming remained successful. The researchers successfully reprogrammed a network to classify MNIST digits with reduced accuracy using a small adversarial program. They also made the program nearly imperceptible by limiting the perturbation size, showing that adversarial reprogramming remained successful even with nearly imperceptible programs. Additionally, they tested concealing the adversarial task by hiding both the data and program within a normal ImageNet image. The researchers extended their reprogramming method by adding the resulting image to a random ImageNet image. They optimized the adversarial program for the network to classify MNIST digits, resulting in adversarial images similar to normal ImageNet images. This demonstrates the possibility of hiding the adversarial task using a simple shuffling technique and an ImageNet image. The study showed that neural networks can be reprogrammed to classify MNIST digits with lower accuracy, even when the data structure is different. Trained networks are more susceptible to reprogramming than random networks, indicating the flexibility of repurposing trained weights for new tasks. This suggests that reusing neural circuits dynamically in artificial neural networks is practical. Our results demonstrate the flexibility of repurposing trained weights for new tasks in artificial neural networks. This could lead to more efficient and flexible machine learning systems. Further research is needed to understand the limitations in expressivity and trainability when targeting random networks or reprogramming for specific tasks like CIFAR-10 classification. Adversarial reprogramming shows potential for repurposing trained networks for new tasks in various domains like image classification. Future research could explore extending this technique to audio, video, text, and even recurrent neural networks, which could have implications for creating more flexible machine learning systems. Reprogramming RNNs with adversarial programs can lead to theft of computational resources, such as using a computer vision classifier to solve captchas for spam account creation. This technique could have implications for creating more flexible machine learning systems. Reprogramming RNNs with adversarial programs can lead to theft of computational resources, potentially violating ethical guidelines of ML service providers. Adversarial attacks aim to repurpose neural networks for novel tasks, showcasing their flexibility and vulnerability. The study introduces a new class of adversarial attacks that reprogram neural networks for novel tasks, demonstrating their flexibility and vulnerability. The results show the possibility of such attacks, highlighting the need for further research on mitigating or defending against them. The reprogramming process involves shuffling pixels in MNIST digits to create images that successfully reprogram neural networks like Inception V3. The study demonstrates a new adversarial attack that reprograms neural networks by shuffling pixels in MNIST digits to create images that successfully deceive models like Inception V3."
}