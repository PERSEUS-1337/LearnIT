{
    "title": "BygfghAcYX",
    "content": "Despite existing work on generalization of neural networks using complexity measures like norms, margin, and sharpness, they do not explain why networks generalize better with over-parametrization. This work introduces a novel complexity measure based on unit-wise capacities for two layer ReLU networks, resulting in a tighter generalization bound. The capacity bound correlates with test error behavior as network sizes increase, partially explaining the generalization improvement with over-parametrization. Additionally, a lower bound for Rademacher complexity is presented, surpassing previous capacity lower bounds for neural networks. Deep neural networks have been successful in various tasks, with a significant impact since the work of Krizhevsky et al. (2012). The Rademacher complexity improves capacity lower bounds for neural networks, which are over-parametrized but achieve smaller generalization error when trained with real labels. Traditional wisdom suggests that increasing model capacity leads to overfitting, controlled by limiting model size or adding regularization. Increasing model capacity can lead to overfitting, but in the case of neural networks, larger models can improve generalization error even without explicit regularization. Empirical observations show that training on models with more hidden units can decrease test error in image classification tasks. Increasing model capacity can lead to overfitting, but larger neural networks can improve generalization error without explicit regularization. Empirical observations show that more hidden units lead to decreased test error in image classification tasks on MNIST and CIFAR-10. Various complexity measures have been proposed to explain this generalization behavior, as traditional measures like VC bounds do not capture it. Different norm, margin, and sharpness based measures have been suggested to capture the capacity of neural networks and their generalization phenomenon. The text discusses using norm, margin, and sharpness based measures to analyze neural network capacity and generalization behavior. It highlights the observation that test error decreases even when the network is large enough to fit the training data. Unit capacity and unit impact are important factors in the network's output, with both shrinking faster than 1/ \u221a h as the number of hidden units increases. The impact of hidden units on network output and capacity bound is discussed. Empirical observations show that unit capacity and unit impact decrease faster than 1/ \u221a h as the number of hidden units increases. Existing complexity measures fail to explain why over-parametrization helps and increase with network size. In this study, Dziugaite & Roy (2017) evaluated generalization bounds based on PAC-Bayes for neural networks. The complexity measures increase with network size, even for two-layer networks, as shown by Neyshabur et al. (2017). To further investigate this phenomenon, two-layer ReLU networks were chosen for analysis, demonstrating similar behavior to more complex architectures. The study presents a tighter generalization bound for two-layer ReLU networks. In this paper, a tighter generalization bound is proven for two-layer ReLU networks, showing a correlation with test error and decreasing complexity with increasing hidden units. The key insight is characterizing complexity at a unit level, with measures shrinking faster than 1/ \u221a h for each hidden unit as network size increases. The generalization bound depends on layer norms, specifically the Frobenius norm of the top layer and the difference of hidden layer weights with initialization, decreasing with network size. The generalization bound for two-layer ReLU networks correlates with test error and decreases in complexity with more hidden units. The closeness of learned weights to initialization in over-parametrized settings is explained by the large number of hidden units representing all possible features, simplifying the optimization problem to minimize training error. The large number of hidden units in over-parametrized settings represent all possible features, simplifying the optimization problem to minimize training error. Studies have shown the significance of initialization in neural networks, with some suggesting a Fisher-Rao metric based approach. Our contributions in this paper include an empirical investigation on the role of over-parametrization in generalization of neural networks on different datasets. We also prove tighter generalization bounds for two layer ReLU networks, improving upon previous results. Our proposed complexity measure for neural networks decreases with the number of hidden units. Our proposed complexity measure for neural networks decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. Additionally, we provide a matching lower bound for the Rademacher complexity of two layer ReLU networks with a scalar output, significantly improving upon previous bounds. The text discusses the complexity measure for neural networks, focusing on two-layer fully connected ReLU networks for classification tasks. It introduces the margin operator and defines the ramp loss function. The text introduces the ramp loss function, which is defined based on the margin operator for neural networks. It provides a generalization bound for any function in a given function class. The text discusses the generalization bound for neural networks by bounding the Rademacher complexity of the function class, which captures the ability of functions to fit random labels. This complexity measure depends on the chosen function class and helps in understanding the generalization error. The text discusses the importance of choosing the right function class to bound the generalization error of neural networks. Experiments on CIFAR-10 dataset show that the spectral and Frobenius norms of network layers initially decrease but eventually increase with the number of hidden units. The spectral and Frobenius norms of network layers decrease initially but eventually increase with the number of hidden units, suggesting that the increase in norms is due to random initialization. The distance to initialization per unit decreases with increasing hidden units, leading to a shift in the distribution of angles between learned and initial weights. In the second layer of trained networks, the Frobenius norm and distance to initialization decrease with increasing hidden units, suggesting a limited role of initialization. The unit capacity, defined as the per unit distance to initialization, decreases with increasing h, leading to a shift in the distribution of angles between learned and initial weights. In the second layer of trained networks, the Frobenius norm and distance to initialization decrease with increasing hidden units, suggesting a limited role of initialization. The norm of outgoing weights from a hidden unit decreases faster than 1/ \u221a h, impacting the final decision. The unit impact, defined as the magnitude of outgoing weights, plays an important role in two-layer neural networks. The unit impact, defined as the magnitude of outgoing weights, is crucial in two-layer neural networks. A hypothesis class of neural networks is considered based on unit capacity and impact. Generalization properties of this function class are studied, with a focus on two-layer ReLU networks. The Rademacher complexity of the class is bounded. In this section, a generalization bound for two-layer ReLU networks is proven by bounding the Rademacher complexity of the class based on the unit capacity and impact. The proof technique decomposes the network complexity into that of hidden units, a novel approach compared to previous works. The proof technique in the supplementary Section C decomposes the network complexity into that of hidden units, providing a tighter bound on the Rademacher complexity of two-layer neural networks. The generalization bound in Theorem 1 applies to any function in the defined class with fixed \u03b1 and \u03b2 values. To extend the bound to all networks, a union bound over possible \u03b1 and \u03b2 values is sufficient. The generalization bound for two-layer ReLU networks is derived by fixing \u03b1 and \u03b2 values before training. The bound holds for all networks by covering the space of possible \u03b1 and \u03b2 values and taking a union bound. The generalization error is bounded with probability 1 - \u03b4 over the choice of the training set, and the bound improves over existing ones. The Rademacher complexity is also explicitly lower bounded, matching the first term in the generalization bound. The additive factor in the bound results from the union bound over the cover of \u03b1 and \u03b2. The Rademacher complexity is lower bounded, matching the first term in the generalization bound. The additive factor in the bound comes from the union bound over the cover of \u03b1 and \u03b2. The capacity decreases with over-parametrization, and a finer tradeoff between terms is presented for p norms. Comparisons with Golowich et al. (2018) show similarities in the first term of the bounds. The Rademacher complexity is lower bounded, matching the first term in the generalization bound. The key complexity term in their bound is U \u2212 U 0 1,2 V 2, and in our bound is U \u2212 U 0 F V F. Experimental comparison shows training and test errors for two layer ReLU networks on CIFAR-10 and SVHN datasets. The training and test errors for CIFAR-10 and SVHN datasets are compared for networks of varying sizes. Larger networks show better generalization even without regularization. Unit capacity and unit impact decrease with increasing network size. The number of epochs needed to reach 0.01 cross-entropy loss decreases for larger networks. Generalization bounds scale with effective capacity. The effective capacity of function classes is compared using different measures in the context of generalization bounds for networks of increasing sizes. The proposed bound decreases with complexity and outperforms other norm-based bounds, even surpassing VC-dimension for networks larger than 1024. While the numerical values are approximate, they provide insight into relative generalization behavior and can be improved with data-dependent techniques. Our capacity bound decreases with network size, unlike other norm-based bounds which increase significantly for larger networks. This suggests that our measure may capture properties that enable over-parametrized networks to generalize effectively. Additionally, we compare our complexity measure between networks trained on real and random labels to further understand its behavior. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. The lower bound is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer, allowing for comparison with existing results and extending the lower bound to the bigger class F W. The distribution of margin normalized by the complexity measure is plotted, showing correlation with generalization behavior. The lower bound on the Rademacher complexity of neural networks is proven on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer. This extends the lower bound to the bigger class F W and allows for comparison with existing results. The proof is detailed in the supplementary Section C.3. The lower bound on the capacity of neural networks shows a gap between the Lipschitz constant of the network and its capacity, even with bounded spectral norm. This lower bound improves upon previous capacity lower bounds for neural networks with scalar output and element-wise activation functions. The lower bound on the capacity of neural networks reveals a gap between the Lipschitz constant and network capacity, even with bounded spectral norm. This non-trivial lower bound excludes neural networks with all rank-1 weight matrices, showing a capacity gap between ReLU activated networks and linear networks. The construction can be extended to more layers by setting intermediate layer weight matrices to the Identity matrix. The lower bound on neural network capacity improves existing results by a factor of \u221ah. It also surpasses a previous \u2126(s1s2\u221ac) lower bound for networks with bounded spectral norm. The new capacity bound decreases with more hidden units, potentially explaining improved performance. The paper presents a new capacity bound for neural networks that decreases with more hidden units, potentially explaining better generalization performance. The focus is on understanding the role of width in two-layer networks, with future directions including exploring depth and the interplay between depth and width. Matching lower bounds are provided, but there is interest in obtaining bounds with smaller values compared to the number of training samples. In this paper, new capacity bounds for neural networks are discussed, focusing on the role of width in two-layer networks. Lower bounds are provided, but efforts are made to obtain bounds with smaller values compared to the number of training samples. An experiment with a pre-activation ResNet18 architecture on CIFAR-10 dataset is described. In an experiment, a pre-activation ResNet18 architecture was trained on the CIFAR-10 dataset using various settings. The architecture includes a convolution layer, 8 residual blocks, and a linear layer. Different numbers of channels and strides were used in the residual blocks, with kernel sizes of 3 in all convolutional layers. SGD was used for training with specific parameters, and 11 architectures were trained with varying values of k. In experiments, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets using SGD with specific parameters. Data augmentation techniques were applied, and 13 architectures were trained with increasing hidden units. Weight decay, dropout, and batch normalization were not used in the training process. For each experiment, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets using SGD with specific parameters. The number of hidden units was increased by a factor of 2 from 2 3 to 2 15. Training stopped when cross-entropy reached 0.01 or after 1000 epochs. Evaluation included calculating exact generalization bounds with specific margins set. BID2 and Neyshabur et al. (2015c) bounds were adjusted for multi-class classification. In experiments on CIFAR-10, SVHN, and MNIST datasets, fully connected feedforward networks were trained with specific parameters using SGD. The number of hidden units increased by a factor of 2 from 2^3 to 2^15. Training stopped when cross-entropy reached 0.01 or after 1000 epochs. Evaluation included adjusting BID2 and Neyshabur et al. (2015c) bounds for multi-class classification. The behavior of measures on networks of different sizes trained on SVHN and MNIST datasets is shown in Figures 6 and 7. The over-parametrization phenomenon in the MNIST dataset is illustrated in the left panel of FIG10, while the middle and right panels compare generalization bounds. Theorem 2 is generalized to p norm, with Lemma 11 providing a new ingredient in the proof. In this section, Theorem 2 is generalized to p norm with Lemma 11 providing a new ingredient in the proof. The generalization error is bounded for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1) and U 0 \u2208 R h\u00d7d. The bound improves on the additive term in Theorem 2 for p of order ln h, with a tighter bound that decreases with h for larger values. Corollary 6 provides a bound under the settings of Theorem 5. The generalization error is bounded for any function f(x) = V[Ux] +, with a tighter bound that decreases with h for larger values. Corollary 6 provides a bound under the settings of Theorem 5, with probability 1 \u2212 \u03b4 over the choice of the training set S = {xi}mi=1. A vector-contraction inequality for Rademacher complexities is used in the proof, along with a technical result from Maurer (2016). The Rademacher complexity of the class of networks can be decomposed to that of hidden units, as shown in Lemma 9. The complexity is bounded for the class F W with a proof provided through induction. Lemma 10 (Ledoux-Talagrand contraction, Ledoux & Talagrand (1991)). Let f: R+ \u2192 R+ be convex and increasing. Let \u03c6i: R \u2192 R satisfy \u03c6i(0) = 0 and be L-Lipschitz. Let \u03bei be independent Rademacher random variables. For any T \u2286 Rn, the lemma will be used in the following proof. The proof of Theorem 1 utilizes Lemma 10, which involves a convex, increasing function f, Lipschitz functions \u03c6i, and Rademacher random variables \u03bei. The proof concludes by summing up an inequality over a range. Additionally, a covering lemma (Lemma 11) is introduced to cover a ball with dominating elements entry-wise. The lemma presented covers a p ball with dominating elements entry-wise and bounds the size of such a cover. It involves constructing a set of vectors and bounding the generalization error with certain conditions. Lemma 14 provides specific results for the case p = 2, bounding the generalization error for a function f(x) = V[Ux] + with certain conditions on V, U, and the training set S. Lemma 14 provides bounds on the generalization error for a function f(x) = V[Ux] + with specific conditions. The proof involves upper bounding the generalization bound for p = 2 and a certain choice of \u00b5. Theorem 2 directly follows from Lemma 14, with Lemma 15 providing a looser generalization bound for any p \u2265 2. Lemma 15 provides a generalization bound for any p \u2265 2, which is looser than Lemma 14 for p = 2 due to extra constants and logarithmic factors. It states that with certain conditions, the generalization error for a function f(x) is bounded. The proof of Theorem 5 follows from Lemma 15, using notation to hide constants and logarithmic factors. The proof of Theorem 3 starts with a specific case and involves dividing the dataset into groups with different elements in a standard orthonormal basis. In the context of dividing the dataset into groups with different elements in a standard orthonormal basis, we define a matrix U(\u03be) as Diag(\u03b2) \u00d7 F(\u03be), where F(\u03be) is orthonormal. This leads to U(\u03be) 2 \u2264 max i \u03b2 i."
}