{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complexity of deep networks makes it impossible to explain hidden features in a way that humans can understand. However, a new method using \\textit{M-of-N} rules has been proposed to map the landscape of rules describing hidden features in Convolutional Neural Networks (CNNs), showing a balance between comprehensibility and accuracy. The shape of the complexity/accuracy landscape in a Convolutional Neural Network (CNN) is revealed by using \\textit{M-of-N} rules. Each latent variable has an optimal rule for behavior description. Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on rule extraction from deep networks and the value of decompositional knowledge extraction for explainability in Artificial Intelligence (AI). The value of decompositional knowledge extraction for explainability in Artificial Intelligence (AI) has gained interest due to the lack of explainability in neural network models. Distributed representations used in deep networks may not correlate with easily identifiable features of the data, relying on weak statistical correlations that are not easily detectable by humans. Knowledge extraction aims to enhance the explainability of neural networks by revealing implicit knowledge learned in their weights. Techniques involve translating networks into symbolic rules or decision trees, similar to those in symbolic AI and ML. Rule extraction methods, developed over decades, take either decompositional or pedagogical approaches to generate rules from network parameters. Rule extraction algorithms have been developed over decades, taking either decompositional or pedagogical approaches to generate rules from neural network parameters. The complexity of extracted rules poses a challenge, as large rule sets derived from CNNs may not be more comprehensible than the original network due to distributed representations in neural networks. The difficulty in extracting knowledge from neural networks lies in their distributed representations, where important concepts are not represented by single neurons but by patterns of activity across many neurons. This distributed nature is a fundamental property of connectionism, leading to the conclusion that symbolic knowledge extraction is ineffective, and methods like distillation should be used instead. However, the efficacy of distillation for improving robustness has been questioned. In this paper, a method is developed to examine the explainability of latent variables in neural networks by using rule extraction to analyze the relationship between error and complexity of rules. The method developed examines the explainability of latent variables in neural networks through rule extraction, mapping out a landscape of rule complexity and accuracy. Different layers in a CNN show varying accuracy in rule extraction, with an ideal M-of-N rule for each latent variable. The accuracy of rules depends on the variable being described, with explainability trends differing between layers and architectures. The study explores the explainability of latent variables in neural networks through rule extraction, revealing varying accuracy in different layers of a CNN. Rules extracted from convolutional layers are more complex compared to fully connected layers. Rules in the first and final layers have near 0% error, while those in the second and third layers have around 15% error. The paper also provides an overview of previous algorithms for knowledge extraction and defines accuracy and complexity for M-of-N rules. Experimental results of the rule extraction process are presented. In previous algorithms for knowledge extraction, a decompositional approach was used with Knowledge-based Artificial Neural Networks (KBANN) to extract M-of-N rules. More sophisticated algorithms followed, generating binary trees representing M-of-N rules in propositional logic. The more recent algorithms for knowledge extraction select M-of-N rules based on maximum information gain with respect to the output. These methods treat the model as a black box and can be queried to generate data for rule extraction. Other extraction methods combine pedagogical and decompositional approaches, while some opt for alternative techniques. Rule extraction methods can be eclectic, combining pedagogical and decompositional approaches, or opt for alternative visually oriented techniques. Most techniques focus on shallow networks and input/output relationships, with some attempting to explain the latent features of deep networks through hierarchical rule extraction. The use of decompositional techniques to explain the features of a deep network end-to-end seems impractical, as argued in BID7. However, experiments show that some layers of a deep network may have highly explainable rules, providing insight into the similarity and disentanglement of latent features. Rules can explain a network's behavior well in terms of certain features, offering a tool for modular explanation of network models. Rule extraction can provide insight into the similarity and disentanglement of latent features by comparing optimal extracted rules. In logic programming, a rule is an implication A \u2190 B, where A is the head and B is the body. Neurons' states are represented by literals in rules when explaining a neural network. When explaining a neural network, rules can represent neurons' states using literals. These literals define the states of neurons based on binary or continuous values. In neural networks, a single conjunctive rule may not fully describe a latent variable, leading to the use of M-of-N rules for rule extraction. M-of-N rules offer a compact representation for rule extraction in neural networks, allowing for a more general and reflective input/output dependency of neurons. They soften the conjunctive constraint by requiring only M variables to be true out of N, providing a subset of propositional formulas that are useful for representing complex dependencies. M-of-N rules provide a compact representation for rule extraction in neural networks, reflecting input/output dependencies. They are a subset of propositional formulas and share structural similarity with neural networks, acting as 'weightless perceptrons'. M-of-N rules in neural networks involve setting bias and weights to encode rules. This approach has resurfaced in the debate on explainability. Continuous activation values require choosing splitting values for rule extraction using information gain. Neurons are explained using information gain BID14 to generate literals for target neurons by selecting splits that maximize decrease in entropy of network outputs. Input literals are then generated by choosing splits for each input that maximize information gain with respect to the target literal. Each target literal in a layer will have its own set of input literals, corresponding to the same set of input neurons but with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. In rule extraction, the focus is on the comprehensibility and accuracy of the extracted rules. Accuracy is defined in terms of the expected difference between the predictions made by the rules and the network. By computing the state of a neuron from the input neurons, we can determine the truth of a literal, allowing for the extraction of a single rule for each feature map in a convolution layer. The focus in rule extraction is on the accuracy of the extracted rules, which is defined as the expected difference between predictions made by the rules and the network. By computing the state of a neuron from input neurons, the truth of a literal can be determined, allowing for the extraction of rules for each feature map in a convolution layer. Comprehensibility, on the other hand, is more subjective and challenging to define. The focus in rule extraction is on the accuracy of the extracted rules, defined as the average error of the rules when predicting the network output over a test set. Comprehensibility is subjective and based on the complexity of a rule, determined by the length of its body in disjunctive normal form. Complexity is measured relative to a maximum complexity, normalized with respect to the number of input variables. The complexity of a rule is normalized with respect to the number of input variables. For example, a perceptron with specific weights and bias is used to illustrate the concept. The most complex rule for 2 variables has a complexity of 1. A loss function for a rule is defined as a weighted sum balancing soundness and complexity. The 2 rule is the most complex rule for 2 variables with a complexity of 1. A loss function for a rule is defined as a weighted sum balancing soundness and complexity, determined by a parameter \u03b2. By using a brute force search procedure with various values of \u03b2, the relationship between rule complexity and maximum accuracy is explicitly determined. The 1-of-1 rule is a trivial rule that always predicts true or false. Neurons are split based on input weights, generating sets of literals. M-of-N rules are searched to minimize L(R) by reordering variables based on weight magnitude. The search procedure depends on variable ordering to find the best split for a neuron. The search procedure for generating splits in neurons relies on variable ordering to maximize information gain. Neurons with multiple inputs have a large number of possible M-of-N rules, but the assumption is made that the most accurate rules use literals corresponding to neurons with the strongest weights. The assumption is that the most accurate M-of-N rules use literals corresponding to neurons with the strongest weights, reducing the search space from exponential to polynomial. However, computational difficulty arises with a large number of test examples and input neurons. The algorithm for rule extraction was implemented in Spark and run on IBM cloud services to reduce computational difficulty. The accuracy of the extracted rules was measured using examples from the training set, and running the search in parallel allowed mapping of the accuracy/complexity graph for about 50 hidden neurons in several hours. By running the search in parallel, we can map the accuracy/complexity graph for about 50 hidden neurons in the second and third layer in several hours. We select 1000 random input examples from the training set to compute the activations of each neuron in the CNN. Each neuron corresponds to a different 5x5 patch of the input, with 784 neurons per feature in the first layer. The CNN pads input images with zeros and has 784 neurons per feature in the first layer, each corresponding to a 5x5 patch of the input. Neuron 96, with an information gain of 0.015 at split value 0.0004, is selected. The input splits are defined based on maximum information gain with respect to variable H. Test input consists of 1000 5x5 image patches centered at (3, 12). The input splits are defined based on maximum information gain with respect to variable H. Test input consists of 1000 5x5 image patches centered at (3, 12). Searching through M-of-N rules to determine optimal rules explaining H for error/complexity tradeoffs, three different rules are extracted. Visualized in Figure 1, the rules filter out weights based on complexity penalty. The neuron rules explain the neuron with decreasing complexity. Different rules filter out weights based on complexity penalty, with the most complex rule having a 97.5% agreement with the network. Applying the technique to the DNA promoter dataset shows error rates for different rules. In the DNA promoter dataset, a feedforward network with a single hidden layer of 100 nodes shows an exponential relationship between complexity and error. A rule in the output layer gives 100% fidelity to the network, with rules extracted from the input layer without complexity penalty. In the DNA promoter dataset, a feedforward network with a single hidden layer of 100 nodes shows an exponential relationship between complexity and error. Rules extracted from the input layer without complexity penalty include 64-of-119 for variable H 3 9 with one incorrect classification, and 32-of-61 for variable H 8 0 with no incorrect classifications. Errors propagate through layers when stacking rules that don't perfectly approximate each layer, exacerbated by choosing different splits for the same layer when treated as output. To replace a network with hierarchical rules, a single set of splits for each layer must be decided upon. When choosing splits for input neurons of a layer and moving down a layer, different splits are chosen for the same layer when treated as output. To replace a network with hierarchical rules, a single set of splits for each layer must be decided upon. Conducting experiments layer by layer independently provides an idealized complexity/error curve for rule extraction with M-of-N rules. This approach helps evaluate the rule extraction landscape of a neural network trained on a practical dataset. Rule extraction with M-of-N rules was tested on a basic CNN trained on fashion MNIST in tensorflow. The CNN had a standard architecture with convolutional and max pooling layers, followed by a fully connected layer. 1000 random inputs from the fashion MNIST training data were used to test the extracted rules against the network. In testing rule extraction on a CNN trained on fashion MNIST, 1000 random inputs were used to test extracted rules against the network. Different values of \u03b2 were used to produce sets of rules with varying error/complexity trade-offs. For each layer, rule extraction was repeated with 5 different values of \u03b2 (0, 0.1, 0.2, 1, 5, 1000), resulting in 5 sets of rules with varying error/complexity trade-offs. Averaging complexities and errors for each target neuron produced a graph showing different trade-offs for rules extracted from each layer. The first and final layers had accurate rules with near 0 error, while the second and third layers showed similar accuracy/complexity trade-offs. The third layer's minimum error could not be improved with more complex rules. The second layer shows a slight improvement in accuracy with more complex rules, while the third layer's minimum error cannot be improved with increasing complexity. The optimal accuracy/complexity tradeoff is not solely based on the number of input nodes, as the third layer performs similarly to the second layer despite having more input nodes. The final layer provides more accurate rules with less complexity compared to the first layer. The rule extraction landscape shows a critical point where error increases rapidly as complexity penalty rises. The rule extraction landscape reveals a critical point where error increases rapidly as complexity penalty rises, indicating a natural set of rules for explaining latent features. Current algorithms do not consider complexity optimization, but this study integrates rule complexity into the extraction algorithm. This paper integrates rule complexity into the extraction algorithm, highlighting the importance of empirical evaluation for validation. The limitations and potential of rule extraction algorithms are discussed, showing that while some cases have no simple explanations, others can be accurately captured by relatively simple rules. The black box problem of neural networks has been a longstanding issue, but the need for explainability has grown as they are more widely used in society. Rule extraction algorithms have shown mixed success in providing simple explanations, with some cases achieving near 0% error rates for complex rules. Selective use of decompositional algorithms depending on the layer being explained may be necessary. The need for explainability in neural networks has grown as they are more integrated into society. Critics argue that traditional knowledge extraction methods are not feasible due to the distributed nature of neural networks. A novel search method for M-of-N rules was applied to explain the latent features of a CNN, revealing that latent features can be described by an 'optimal' rule. The study applied a novel search method for M-of-N rules to explain latent features of a CNN, finding that an 'optimal' rule represents an ideal error/complexity trade-off. Rule complexity was included in the search, showing a large trade-off discrepancy between neurons in different layers. While rule extraction may not adequately describe all latent variables, simplifying explanations without reducing accuracy can be useful for easily understandable features in networks. Rule extraction can simplify explanations without reducing accuracy, especially for easily understandable features in networks. Further research is needed to explore the effects of different transfer functions, data sets, architectures, and regularization techniques on the accuracy and interpretability of rule extraction."
}