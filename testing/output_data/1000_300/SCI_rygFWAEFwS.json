{
    "title": "rygFWAEFwS",
    "content": "We propose SWAP, an algorithm to accelerate DNN training using large mini-batches and weight averaging of multiple models in parallel. The resulting models generalize well and are produced in a shorter time. Demonstrated on CIFAR10, CIFAR100, and ImageNet datasets, showing reduced training time and good generalization performance. Training with larger mini-batches accelerates DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss. In a distributed setting, multiple nodes can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates. This approach requires fewer updates and synchronization events, resulting in good overall scaling behavior. However, there is a maximum batch size that can be reached before the model's performance plateaus. Training with larger mini-batches accelerates DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss. However, there is a maximum batch size after which the resulting model tends to have worse generalization performance. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of a set of models sampled from the final stages of a training run. Stochastic Weight Averaging in Parallel (SWAP) is a strategy to accelerate DNN training by averaging models from multiple independent SGD sequences. It achieves good generalization performance by utilizing available compute resources efficiently. SWAP is a strategy to accelerate DNN training by better utilizing compute resources. It achieves good generalization performance for image classification tasks on popular datasets like CIFAR10, CIFAR100, and ImageNet. SWAP can reduce training times significantly and has beaten the state of the art for CIFAR10, training in 68% of the time of the winning entry in the DAWNBench competition. The impact of training batch size on generalization performance is still unknown. The impact of training batch size on generalization performance is still unknown. Larger mini-batches may lead to models getting stuck in sharper global minima, affecting generalization. Studies show that flatness, measured by curvature, can vary despite the batch size. In (Dinh et al., 2017), authors transform a flat minimizer into a sharp one without changing model behavior. In (Li et al., 2018), reverse behavior is shown when weight-decay is not used. In (McCandlish et al., 2018), batch size can be increased without accuracy drop up to a critical size. For image classification on CIFAR10, accuracy drops when batch sizes exceed 1k samples. Larger batch sizes imply fewer model updates, impacting generalization performance. In (Hoffer et al., 2017), using a larger batch size implies fewer model updates, affecting generalization performance. Training with large batches for longer times improves generalization but takes more time than small batches. Batch size also influences the optimization process. In the optimization process, batch size affects convergence rates. Some methods use adaptive batch sizes, but they may require specific datasets or extensive tuning. Local SGD is a distributed algorithm that balances gradient precision and communication costs. Post-local SGD is a distributed optimization algorithm that refines large-batch training with local-SGD, resulting in better generalization and significant speedups. SWAP, on the other hand, averages models after multiple epochs, different from Post-local SGD which lets models diverge for a few steps before synchronizing. Stochastic weight averaging (SWA) is a method that averages models after multiple epochs, resulting in better generalization properties. This approach differs from Post-local SGD, which lets models diverge for a few steps before synchronizing. SWA has been effective in various domains such as deep reinforcement learning, semisupervised learning, and Bayesian inference. Stochastic Weight Averaging (SWA) is a strategy adopted in various domains like deep reinforcement learning, semisupervised learning, and Bayesian inference to improve generalization properties. SWA involves three phases: initial training with large mini-batch updates and high learning rate, independent refinement of model copies with smaller batch size and lower learning rate, and averaging of resulting models' weights. During the last phase of Stochastic Weight Averaging (SWA), weights of models are averaged, and new batch-normalization statistics are computed to produce the final output. Phase 1 stops before reaching zero training loss or 100% training accuracy to prevent optimization from getting stuck. In Phase 2, small-batch training is performed independently and simultaneously with reduced batch size. The optimal stopping accuracy is a hyper-parameter that needs tuning. In phase 2 of Stochastic Weight Averaging (SWA), small-batch training is done independently by each worker, resulting in different models. The learning rates are the same during this phase, but models diverge due to stochasticity. The averaged model performs consistently better than individual models. In Stochastic Weight Averaging (SWA), workers have different testing accuracies due to stochasticity causing model divergence. The averaged model outperforms individual models. Small-batch training results in independent models with the same learning rates. In Stochastic Weight Averaging (SWA), workers have different testing accuracies due to stochasticity causing model divergence. The mechanism behind SWAP involves plotting the error achieved by the test network on a plane containing the outputs of different algorithm phases. By picking orthogonal vectors u, v that span the plane with \u03b8 1 , \u03b8 2 , \u03b8 3 , the loss value generated by model \u03b8 = \u03b8 1 +\u03b1u+\u03b2v is plotted at the location (\u03b1, \u03b2). The training and testing error for the CIFAR10 dataset is visualized in Figure 2, showing the outputs of different phases ('LB', 'SGD', 'SWAP'). After phase two, the model traversed to a different side of the basin, with the final 'SWAP' model closer to the center. The 'LB' and 'SGD' points fell in regions of higher error due to variations in the basin's topology, while 'SWAP' was less affected. In Figure 3, the plane spanned by 'SGD1', 'SGD2', 'SGD3' shows them at different sides of the training landscape. In Figure 3, the 'SWAP' point is closer to the center of the basin, less affected by topology changes. Worker points 'SGD1', 'SGD2', 'SGD3' are at different sides of the basin, while 'SWAP' remains central. The change in topology leads to higher testing errors for workers compared to 'SWAP'. The weight iterates in SGD behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution centered at the local minimum. The weight iterates in SGD behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution centered at the local minimum. The distribution has a covariance that grows proportionally with the learning rate and inversely proportional to the batch size, with a shape depending on the Hessian of the mean loss and covariance of the gradient. Sampling weights from an SGD run will choose weights spread out on the surface of an ellipsoid, with their average closer to the center. The weight iterates in SGD behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution centered at the local minimum. Sampling weights from an SGD run will choose weights spread out on the surface of an ellipsoid, with their average closer to the center. By sampling from different SGD runs, all runs will converge to the same stationary distribution, generating independent samples. SWA and SWAP show advantages over SGD, as seen in the cosine similarity between gradient descent direction and the direction towards the output of SWAP. In the later stages of training, the angle between the gradient direction and the basin center increases, slowing progress. SWAP shows advantages over SGD in image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. Hyper-parameters were optimized using grid searches, training with mini-batch SGD, Nesterov momentum, weight decay, and data augmentation with cutout. The best hyper-parameters were found using grid searches. Training was done with mini-batch SGD using Nesterov momentum and weight decay. Data augmentation was performed with cutout. A custom ResNet 9 model was used for training. Experiments were conducted on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for distributed computation. Statistics were collected over 10 runs with different settings for each phase of the experiment. The experiment involved training with different batch sizes and configurations. Small-batch training had 512 samples per batch on 2 GPUs for 100 epochs, while large-batch training had 4096 samples per batch on 8 GPUs for 150 epochs. The SWAP phase used 8 workers with one GPU each and 512 samples per batch for 30 epochs. Test accuracies and training times were compared for models trained with small-batch only, large-batch only, and SWAP methods. The experiment compared training times and test accuracies of models trained with small-batches, large-batches, and SWAP methods. Small-batch training achieved higher testing accuracy than large-batch training but took longer. SWAP terminated in a comparable time to large-batch training and achieved accuracies on par with small-batch training. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples, state-of-the-art training speeds for CIFAR10 were achieved. State-of-the-art training speeds for CIFAR10 were achieved using SWAP with 8 Tesla V100 GPUs, reaching 94% test accuracy in 27 seconds. The experiment also included small-batch and large-batch training on ImageNet models with modified schedules and batch sizes. The experiment achieved state-of-the-art training speeds for CIFAR10 using SWAP with 8 Tesla V100 GPUs, reaching 94% test accuracy in 27 seconds. Large-batch experiments on ImageNet models with modified schedules and batch sizes were run on 16 Tesla V100 GPUs. Doubling the batch size reduced test accuracies, but SWAP recovered generalization performance with reduced training times. Top1 and Top5 test accuracies were compiled in Table 3, showing accelerations achieved with no tuning other than adjusting learning rates. The experiment achieved state-of-the-art training speeds for CIFAR10 using SWAP with 8 Tesla V100 GPUs, reaching 94% test accuracy in 27 seconds. For larger batch experiments, doubling the batch size, number of GPUs, and learning rate resulted in accelerations without additional tuning. Comparing SWAP with SWA on the CIFAR100 dataset, both methods sampled models with the same number of epochs, with SWA averaging models every 10 epochs and SWAP running 8 epochs. In this section, experiments are conducted using the CIFAR100 dataset. Models are sampled for SWA and SWAP with the same number of epochs. SWA averages models every 10 epochs, while SWAP runs 8 independent workers for 10 epochs each. The study aims to see if SWA can improve test accuracy in large-batch training. Initial training cycles with cyclic learning rates are followed to sample 8 models. Results show that large-batch training achieves lower accuracy, and SWA does not improve it. The large-batch training run achieves lower accuracy, and SWA does not improve it. Small-batch SWA requires more time to compute the model compared to SWAP. Starting the SWA cyclic learning rate schedule from the best model found by solely small-batch training. The SWA cyclic learning rate schedule is started from the best model found by small-batch training. The peak learning rate is selected using grid-search. Small-batch SWA achieves better accuracy than SWAP with a 6.8x longer training time. SWAP achieves a speed-up over SWA when the precision of SWA is set as a target. SWAP achieves a speed-up over SWA by relaxing constraints and increasing the phase two schedule, resulting in a test accuracy of 79.11% in 241 seconds, 3.5x faster. SWAP is a novel algorithm that uses large mini-batches for quick approximate solutions and refines them by averaging weights from multiple models trained with small-batches, leading to good generalization performance in less time. The study introduces a novel variant of SWA that refines models by averaging weights from multiple models trained with small-batches. This approach results in good generalization performance in a shorter training time. Large-batches in the initial stages of training do not hinder the models from achieving good performance, as refining the output with SWA or SWAP leads to comparable results. Visualizations show that averaged weights are closer to the center of a training loss basin than models produced by stochastic gradient descent. The study introduces a novel variant of SWA that refines models by averaging weights from multiple models trained with small-batches. Visualizations show that averaged weights are closer to the center of a training loss basin than models produced by stochastic gradient descent. The basin where the large mini-batch run converges seems to be the same basin where refined models are found. The method requires choosing a transition point between large-batch and small-batch, which was done through grid search in experiments. Future work will focus on a principled method for choosing this transition point and exploring SWAP with other optimization schemes like LARS. In future work, the focus will be on exploring SWAP with other optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad. The design of SWAP allows for substituting these schemes in the large-batch stage. Parameters used in experiments were obtained through independent grid searches. Momentum and weight decay constants were kept at 0.9 and 5 \u00d7 10 \u22124 for all CIFAR experiments. Tables 5 and 6 list the remaining hyperparameters. The hyperparameters for the experiments were obtained through independent grid searches. Momentum and weight decay constants were set at 0.9 and 5 \u00d7 10 \u22124 for all CIFAR experiments. Tables 5 and 6 list the remaining hyperparameters, with a stopping accuracy of 100% indicating the maximum number of epochs used."
}