{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, despite individual incentives conflicting with the common good. The study focuses on intertemporal social dilemmas (ISDs) in multi-agent reinforcement learning (MARL) and evolutionary theory. By combining MARL with natural selection, a model-free way to learn cooperation biases is demonstrated using a modular architecture for deep reinforcement learning agents. In a study on multi-agent cooperation, individual biases for cooperation can be learned in a model-free way through a modular architecture for deep reinforcement learning agents. Results in challenging environments are interpreted in the context of cultural and ecological evolution, showcasing cooperation at various scales in nature despite natural selection favoring selfish interests. Altruism can be favored by selection when cooperating individuals interact with other cooperators, realizing the benefits of cooperation without being exploited by defectors. In multi-agent deep reinforcement learning, cooperation among self-interested agents is a key topic. The problem is formalized as an intertemporal social dilemma, where agents struggle to achieve collectively optimal outcomes due to evolutionary tendencies towards defecting strategies. In multi-agent deep reinforcement learning, the challenge lies in balancing collective welfare and individual utility. Evolutionary theory predicts that self-interested agents tend to converge on defecting strategies instead of achieving optimal outcomes. Various solutions have been proposed, including opponent modeling, long-term planning, and intrinsic motivation functions. Recent end-to-end model-free learning algorithms show promise in generalization, suggesting that evolution could help eliminate the need for hand-crafted intrinsic motivation. Evolution can be applied to remove hand-crafted intrinsic motivation in deep learning, as shown in various optimization tasks. However, success is not guaranteed in the ISD setting. Evolutionary simulations of predator-prey dynamics have used subpopulations to evolve populations of neurons for neural networks. Evolution can be applied to remove hand-crafted intrinsic motivation in deep learning, as shown in various optimization tasks. In the ISD setting, evolutionary simulations of predator-prey dynamics have used subpopulations to evolve populations of neurons for neural networks. The proposed system distinguishes between optimization processes unfolding over two time-scales: fast learning and slow evolution. Individual agents participate in an intertemporal social dilemma with fixed intrinsic motivation, while that motivation is subject to natural selection in a population. The intrinsic motivation is modeled as an additional term in the reward of each agent, implemented as a two-layer fully-connected feed-forward neural network. Evolutionary theory predicts that evolving individual intrinsic reward weights across a population does not lead to altruistic behavior. To achieve this goal, evolutionary dynamics must be structured. A \"Greenbeard\" strategy is implemented where agents choose interaction partners based on cooperativeness signals, termed assortative matchmaking. In response to the limitations of assortative matchmaking, a modular training scheme called shared reward network evolution is introduced. Agents consist of policy and reward network modules, allowing for more effective multi-agent reinforcement learning. The shared reward network evolution approach involves agents with policy and reward network modules, evolving separately on fast and slow timescales. Policy networks are trained using modified rewards from the reward network, with fitness based on individual rewards for policy networks and collective return for reward networks. This prevents overfitting in evolved reward networks. In a MARL setting, the study focuses on intertemporal social dilemmas within Markov games. The evolutionary paradigm involves separate evolution of policy and reward networks to prevent overfitting. Various parameters were explored such as environments, reward network features, matchmaking, and reward network evolution approaches. In this paper, intertemporal social dilemmas in Markov games are studied. Two dilemmas are considered, implemented as partially observable Markov games on a 2D grid. In the Cleanup game, agents collect apples with a respawn rate affected by the cleanliness of a separate aquifer. In a 2D grid Markov game, agents collect apples with a respawn rate linked to aquifer cleanliness. The aquifer fills with waste, reducing apple spawn until none can appear. Agents face a dilemma between cleaning for no reward or defecting to receive no rewards. In the Harvest game, apple spawn rate depends on nearby apples, creating a temptation to harvest all. In the Harvest game, agents collect apples with a spawn rate linked to nearby apples. There is a dilemma between harvesting quickly for short-term gain or depleting apples for lower total yield. The reward for players includes total, extrinsic, and intrinsic components used for different loss functions. The reward for players in the Harvest game consists of total, extrinsic, and intrinsic components. The extrinsic reward is obtained from the environment, while the intrinsic reward is calculated using a neural network with evolved parameters. The feature vector f i is player-specific and can be transformed into intrinsic reward via a reward network. Social preferences in Markov games should not be influenced by the precise temporal alignment of rewards, but rather on comparing temporally averaged reward estimates between players. In Markov games, social preferences should focus on comparing temporally averaged reward estimates between players. Two ways of aggregating rewards are considered, using off-policy importance weighted actor-critic and a reward network architecture with intrinsic and extrinsic value heads. The retrospective and prospective methods derive intrinsic reward from past and future extrinsic rewards of other agents, respectively. The retrospective variant updates temporally decayed rewards for agents at each timestep, while the prospective variant uses value estimates and stops gradients from flowing back into other agents. Training was done using a population of 50 agents with policies, sampled to populate 500 arenas running in a multi-agent environment. The study utilized a framework for distributed asynchronous training in multi-agent environments, training a population of 50 agents with policies. Sampling 5 players for each of 500 arenas running in parallel, agents were resampled within each arena after playing an episode. Weight updates were done using V-Trace, evolving parameters such as learning rate, entropy cost weight, and reward network weights. Policy network parameters were inherited in a Lamarckian fashion, with agents observing their last actions and intrinsic rewards. The weights evolved included learning rate, entropy cost weight, and reward network weights. Policy network parameters were inherited in a Lamarckian fashion. Agents observed their last actions, intrinsic rewards, and extrinsic rewards as input to the LSTM in the agent's neural network. The objective function comprised the value function gradient, policy gradient, and entropy regularization, weighted according to hyperparameters baseline cost and entropy cost. Evolution was based on a fitness measure calculated as a moving average of total episode return. The study involved two matchmaking methods: random and assortative. Random matchmaking selected agents uniformly, while assortative matchmaking grouped agents based on recent cooperativeness. This ensured cooperative agents played with similar ones, and defectors with other defectors. Cooperativeness was determined by the number of steps taken in the last episode. In the study, cooperative agents were matched with similar ones, and defectors with other defectors, based on recent cooperativeness. Cooperativeness was calculated differently for Cleanup and Harvest tasks. Cooperative metric-based matchmaking was used with individual reward networks or no reward networks, but not with the multi-level selection model. The reward network was separately evolved within its own population. In this study, the reward network was separately evolved within its own population, allowing for independent exploration of hyperparameters and competition among like components. This approach enabled a wider exploration of the hyperparameter landscape compared to using a single pool, with reward networks randomly assigned to policy networks to generalize to various policies. Five policy networks were paired with the same reward network in each episode, termed a shared reward network. In a single episode, 5 policy networks were paired with a shared reward network. The reward network parameters were evolved based on total episode return across the group of co-players, distinct from previous work on intrinsic rewards. This evolution aims to address the tension in Intrinsic Social Dilemmas (ISDs) by evolving social features for cooperation rather than remapping environmental events. The text discusses the use of shared reward networks in addressing the tension in Intrinsic Social Dilemmas (ISDs) for social cooperation. It contrasts with hand-crafted aggregation methods and shows the importance of using intrinsic reward networks for better performance in games. The text discusses the performance of reward networks in two games, Cleanup and Harvest. It compares random and assortative matchmaking with PBT and reward networks using retrospective social features. Individual reward network agents perform no better than PBT on Cleanup and only moderately better on Harvest with random matchmaking. Assortative matchmaking without a reward network performs the same as the PBT baseline, but with individual reward networks, performance is significantly higher. Conditioning internal rewards on social features and a preference for cooperative agents playing together are key to resolving social dilemmas. The performance of reward networks in Cleanup and Harvest games was compared. Individual reward networks showed high performance, indicating the importance of conditioning internal rewards on social features. Shared reward network agents performed as well as assortative matchmaking, suggesting that agents did not need immediate access to honest signals of cooperativeness to resolve dilemmas. Prospective reward network evolution generally resulted in worse performance compared to PBT. The prospective variant of reward network evolution generally results in worse performance and more instability compared to the retrospective variant. Sustainability measures the average time step on which agents received positive reward, showing that having no reward network results in players collecting apples extremely quickly. Sustainability measures the average time step on which agents received positive reward, showing that having no reward network results in players collecting apples extremely quickly. Equality is calculated as E(1 \u2212 G(R)), with the prospective version of reward networks leading to lower equality. Tagging measures the average number of times a player fined another player throughout the episode, with a higher propensity for tagging when using a prospective or individual reward network. In FIG4 (c), there is a higher propensity for tagging with prospective or individual reward networks compared to a retrospective shared reward network. The final weights of the shared reward network evolved differently for different games, suggesting varying social preferences were needed. In Cleanup, a simpler reward network sufficed, while Harvest required a more complex one. In Harvest, a more complex reward function was needed to prevent over-exploitation of resources by other agents. The first layer weights tended to have arbitrary positive values due to random matchmaking. Organisms develop internal drives based on primary or secondary goals, and intrinsic rewards were examined based on features from other agents in the environment. Natural selection via genetic algorithms did not lead to the desired outcomes. In Harvest, internal drives are developed based on primary or secondary goals. Intrinsic rewards from other agents were examined, but natural selection via genetic algorithms did not lead to cooperation. Assortative matchmaking was effective in generating cooperative behavior when honest signals were present. A new evolutionary paradigm based on shared reward networks promotes cooperation in various situations by improving credit assignment between selfish acts and negative group outcomes. The shared reward network evolution model improves credit assignment between selfish acts and negative group outcomes, promoting cooperation through mechanisms like competitive altruism and other-regarding preferences. Humans cooperate more readily when they can communicate. The shared reward network evolution model, inspired by multi-level selection, involves policy networks constantly swapping with a reward network. This modularity is seen in nature with microorganisms forming structures for adaptive problems and prokaryotes incorporating modules for cooperation. In humans, the reward network may represent a shared cultural norm based on accumulated cultural information. The curr_chunk discusses alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity. It suggests exploring how these mechanisms impact the weights in a reward network, shedding light on the evolutionary origins of social biases. Additionally, it proposes studying an emergent assortative matchmaking model to enhance the setup's generality and power. The curr_chunk explores the evolutionary origins of social biases and suggests combining an evolutionary approach with multi-agent communication to study cooperative behaviors. The games mentioned have specific rules and actions, such as moving, rotating, and tagging, with consequences for players. The Cleanup game involves moving, rotating, tagging, and cleaning waste actions with consequences for players. Training was done through joint optimization of network parameters and hyperparameters/reward network parameters using SGD and evolution in the PBT setup. Gradient updates were applied for every trajectory up to 100 steps with RMSProp optimization and an RL discount factor of 0.99. The baseline cost weight was fixed at 0.25, and the entropy cost evolved using PBT. Learning rates were initially set to 4 \u00d7 10 \u22124 and allowed to evolve. The baseline cost weight was fixed at 0.25, and the entropy cost evolved using PBT with genetic algorithms. Learning rates were initially set to 4 \u00d7 10 \u22124 and allowed to evolve through multiplicative and additive perturbations. A burn-in period of 4 \u00d7 10 6 agent steps was implemented for an accurate assessment of fitness before evolution."
}