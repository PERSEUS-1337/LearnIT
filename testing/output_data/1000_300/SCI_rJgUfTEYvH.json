{
    "title": "rJgUfTEYvH",
    "content": "Generative models for video prediction face the challenge of uncertain futures, with past observations leading to multiple possible outcomes. Existing probabilistic models are either computationally expensive or do not optimize data likelihood directly. This work introduces multi-frame video prediction using normalizing flows, enabling direct data likelihood optimization and high-quality stochastic predictions. The approach models latent space dynamics and demonstrates the effectiveness of flow-based generative models in video prediction. Progress in machine learning has advanced significantly, with flow-based generative models proving to be competitive for video modeling. The field has seen exponential growth in computational capabilities, leading to improvements in various applications like image classification, machine translation, and game-playing agents. However, the widespread use of machine learning is still limited to scenarios with ample supervision, such as image classification. The application of machine learning technology has been largely constrained to situations with ample supervision or accurate simulations. An alternative approach is to use large unlabeled datasets with predictive generative models to build an internal representation of the world for effective prediction of future events, such as physical interactions in videos. Predictive generative models can learn about real-world phenomena from unlabeled video datasets, enabling effective prediction of future events like physical interactions. This approach offers a rich understanding of the physical world without the need for labeled examples. Such models can be beneficial for downstream tasks or directly applied in decision-making and control scenarios, such as robotics. The main challenge in video prediction lies in the high uncertainty of future outcomes. In this paper, the focus is on stochastic prediction, specifically conditional video prediction, which involves synthesizing RGB video frames based on past observations. The challenge lies in the uncertainty of future outcomes, and the proposed models aim to address this issue efficiently. The paper focuses on conditional video prediction, aiming to synthesize realistic video frames by extending flow-based generative models. This approach addresses the challenges of high-dimensional video sequences and aims to provide exact likelihoods and diverse stochastic futures. VideoFlow is a model for conditional video prediction that utilizes a latent dynamical system to predict future values, achieving competitive results with state-of-the-art models on the BAIR dataset. VideoFlow achieves competitive results in stochastic video prediction on the BAIR dataset, rivaling VAE-based models. It produces high-quality results without common artifacts like blurry predictions, and offers faster test-time image synthesis for real-time applications like robotic control. VideoFlow optimizes likelihood directly, allowing for evaluation based on likelihood values. VideoFlow optimizes likelihood directly for training videos, focusing on deterministic predictive models and architectural changes in video frame prediction. The research explores different generation objectives and representations to model deterministic environments effectively. The next challenge after effectively modeling deterministic environments is addressing stochastic environments by building models that can reason over uncertain futures in real-world videos. Stochasticity in videos can be caused by random events or unobserved factors, leading deterministic models to either disregard potential futures or produce blurry predictions. Various methods have been developed to incorporate stochasticity in order to overcome this challenge. Various methods have been developed to incorporate stochasticity in video prediction models, including variational auto-encoders, generative adversarial networks, and autoregressive models. Among these, variational autoencoders have been explored most widely. The only prior class of video prediction models that directly maximize the log-likelihood of the data are auto-regressive models. However, synthesis with such models is typically inherently sequential, making synthesis substantially inefficient on modern parallel hardware. Prior work has aimed to speed up training and synthesis with auto-regressive models, but the proposed VAE model produces substantially better predictions, especially for longer horizons. The proposed VAE model outperforms auto-regressive models in producing high-quality long-term predictions with faster sampling. Flow-based generative models offer advantages such as exact latent-variable inference, log-likelihood evaluation, and parallel sampling. Flow-based generative models have advantages like exact latent-variable inference, log-likelihood evaluation, and parallel sampling. In these models, latent variables are inferred by transforming data through invertible functions. By learning these transformations, we can compute the log-likelihood of data exactly. This approach allows for generating samples from the data distribution by sampling latent variables and applying the inverse transformation. Generative flow for video is proposed using a multi-scale flow architecture for latent variables per timestep, allowing for invertible transformations of video frames. The architecture encodes information about frames at different scales, enabling exact latent-variable inference and log-likelihood evaluation. The architecture for latent variables in video frames involves invertible transformations using multiple levels to encode information at different scales. Invertible transformations with simple-to-compute Jacobian determinants, such as triangular, diagonal, or permutation matrices, are used. Techniques like Actnorm and Coupling are applied for data-dependent initialization and input splitting. The architecture for latent variables in video frames involves invertible transformations using multiple levels to encode information at different scales. Invertible transformations with simple-to-compute Jacobian determinants, such as triangular, diagonal, or permutation matrices, are used. Techniques like Actnorm, Coupling, SoftPermute, and Squeeze are applied for data-dependent initialization, input splitting, and reshaping the input for a larger receptive field. The architecture for latent variables in video frames involves invertible transformations using multiple levels to encode information at different scales. In Equation (3), the output of Flow is split equally across channels into h, the input to the next level, and the latent variable at the current level. This enables flows at higher levels to operate on lower dimensions and larger scales. The multi-scale architecture is a composition of flows at multiple levels, obtaining latent variables for each frame of the video. A latent prior is chosen using an autoregressive factorization. The architecture for latent variables in video frames involves invertible transformations using multiple levels to encode information at different scales. The latent prior is chosen using an autoregressive factorization with a factorized Gaussian density. The architecture includes a deep 3-D residual network augmented with dilations and gated activation units to predict the mean and log-scale. The architecture for latent variables in video frames involves invertible transformations using multiple levels to encode information at different scales. The architecture includes a deep 3-D residual network augmented with dilations and gated activation units to predict the mean and log-scale. The log-likelihood objective has two parts: the invertible multi-scale architecture contributes via the sum of log Jacobian determinants, and the latent dynamics model contributes log p \u03b8 (z). The parameters are jointly learned by maximizing this objective, with the prior modeling temporal dependencies and the flow acting on separate frames of video. In our architecture, we use a prior to model temporal dependencies in the data and constrain the flow to act on separate video frames. Comparing realism, SAVP-VAE had a fooling rate of 16.4%, VideoFlow 31.8%, and SV2P 17.5%. We experimented with 3-D convolutional flows but found them computationally expensive compared to an autoregressive prior. Due to memory limits, we could only perform SGD with a small number of sequential frames per gradient step. In our architecture, we use autoregressive priors with 2-D convolutions to synthesize long sequences without temporal artifacts. The generated videos show conditioning frames in blue and generated frames in red. VideoFlow models the Stochastic Movement Dataset. Visit the website to view all generated videos and qualitative results. VideoFlow models the Stochastic Movement Dataset by using autoregressive priors with 2-D convolutions. The first frame of each video shows a shape moving in one of eight directions with constant speed. By looking back at just one frame, the model can accurately predict the position of the shape at the next step. This approach reduces the bits-per-pixel on the holdout set. VideoFlow uses autoregressive priors with 2-D convolutions to model the Stochastic Movement Dataset. It extracts random temporal patches of 2 frames from each video and maximizes the loglikelihood of the second frame given the first. The model predicts the future trajectory of the shape accurately in one of eight random directions. Comparing with SV2P and SAVP-VAE models, VideoFlow outperforms in video generation quality based on a real vs fake test. VideoFlow outperforms baseline models in generating realistic trajectories in an unsupervised manner using the BAIR robot pushing dataset. The model is trained to generate 10 target frames based on 3 input frames, maximizing log-likelihood of Bits-per-pixel. VideoFlow is trained to generate 10 target frames based on 3 input frames, maximizing log-likelihood of Bits-per-pixel. Random temporal patches of 4 frames are extracted for training, ensuring all models have seen a total of 13 frames. VideoFlow outperforms SAVP-VAE and SV2P models in bits-per-pixel. VideoFlow outperforms SAVP-VAE and SV2P models in bits-per-pixel on the test set using importance sampling. The high values of bits-per-pixel in the baselines are attributed to their optimization objective, which does not directly optimize the variational bound on log-likelihood. The models were tested to generate 27 frames, with the best-performing video chosen based on PSNR, SSIM, and VGG perceptual metrics. VideoFlow outperforms SAVP-VAE and SV2P models in bits-per-pixel on the test set using importance sampling. The models were trained with ten target frames but tested to generate 27 frames. The best-performing video was selected based on PSNR, SSIM, and VGG perceptual metrics. The BAIR robot-pushing dataset is stochastic, leading to multiple plausible futures. To evaluate the model, 100 videos were generated from each stochastic model and compared to the ground truth using different metrics. In evaluating the VideoFlow model, 100 videos were generated from each stochastic model in the BAIR action-free test-set. The closest generated videos to the ground truth were compared using PSNR, SSIM, and cosine similarity metrics. Previous work focused on tuning pixel-level variance as a hyperparameter to improve sample quality and training stability. This approach was also applied to the VideoFlow model to reduce pixel-level noise. In 2018, pixel-level variance was tuned as a hyperparameter to improve sample quality and training stability. This approach was also used in the VideoFlow model to remove pixel-level noise. Sampling videos at a lower temperature can result in higher quality videos but lower diversity. Sampling frames with a temperature T involves scaling the standard deviation of the latent gaussian distribution. Results were reported with a temperature of 1.0 and the optimal temperature tuned using VGG similarity metrics. Low-temperature sampling applied to latent gaussian priors of other models was found to decrease performance. Our model with optimal temperature performs better or as well as the SAVP-VAE and SVG-LP models on VGG-based similarity metrics, which correlate well with human perception and SSIM. The hyperparameters that perform the best on these metrics are the ones that have disappearing arms. Our model with temperature T = 1.0 is also competitive with state-of-the-art video generation models on these metrics, including PSNR as a pixel-level metric. Our model with temperature T = 1.0 competes well with state-of-the-art video generation models on VGG-based similarity metrics and PSNR. VideoFlow, however, underperforms on PSNR as it models the conditional probability of the joint distribution of frames. In terms of diversity and quality, VideoFlow outperforms prior work in generating diverse samples and passes a real vs fake Amazon Mechanical Turk test. VideoFlow outperforms prior work in diversity and realism, achieving high fooling rates at T = 0.6. Lower temperatures result in less random arm behavior and clearer background objects, leading to higher realism scores. Higher temperatures produce more stochastic arm motion and noisier background objects, decreasing realism. The motion of the arm is more stochastic, leading to high diversity scores but a drop in realism with noisier background objects. Interpolations between different shapes and frames in the BAIR robot pushing dataset show cohesive motion. The multi-level latent representation allows for interpolating background object motion while keeping other levels fixed. The multi-level latent representation allows for interpolating background object motion while keeping other levels fixed. Shapes with different sizes and colors are encoded into the latent space, with smooth interpolation of size observed. Colors are sampled from a uniform distribution, and all interpolated colors are within the training set. VideoFlow is used to detect occlusions in generated videos. The VideoFlow model generates frames into the future, maintaining temporal consistency even in the presence of occlusions. The latent state z t in the model cannot store information other than that present in the frame x t. The VideoFlow model can forget objects if they are occluded for a few frames due to the bijection between the latent state z t and frame x t. Future work aims to address this by incorporating longer memory in the model, such as using recurrent neural networks or more memory-efficient backpropagation algorithms. The model is conditioned on 3 frames to detect the plausibility of a temporally inconsistent frame occurring in the immediate future. The VideoFlow model, conditioned on 3 frames, detects the plausibility of a temporally inconsistent frame in the immediate future. It computes the likelihood of a frame occurring as the 4th time-step and assigns a decreasing log-likelihood to frames further in the future. The architecture is inspired by the Glow model for image generation and introduces a latent dynamical system model for video prediction. The VideoFlow model, inspired by the Glow model for image generation, introduces a latent dynamical system for video prediction. It achieves competitive results with VAE models in stochastic video prediction by optimizing log-likelihood directly. Future work includes incorporating memory for long-range dependencies and applying the model to challenging tasks. Incorporating memory in VideoFlow for modeling long-range dependencies and applying the model to challenging tasks. Data consists of 8-bit videos with added uniform noise to prevent infinite densities at datapoints. Evaluations repeated with low noise levels. Incorporating memory in VideoFlow for modeling long-range dependencies and applying the model to challenging tasks. Data consists of 8-bit videos with added uniform noise to prevent infinite densities at datapoints. Evaluations repeated with low noise levels show that decreasing temperature from 1.0 to 0.0 monotonically decreases the performance of VAE models due to a tradeoff between noise removal and reduced stochasticity of the robot arm. The VAE models show a slightly blurry background from T = 1.0 to T = 0.0, reducing stochasticity hurts performance. Training progression correlates with video quality. Lower bits-per-pixel improves VideoFlow model's ability to capture arm structure and motion in high-quality videos. Training details include a learning rate schedule and optimizer choice. The training details for the models include using specific hyperparameters, learning rate schedules, optimizer choices, and tuning parameters for different models like SAVP-VAE and SAVP-GAN. The comparison of P(X4 = Xt|X<4) and VGG cosine similarity is also discussed, along with the correlation between cosine similarity and bits-per-pixel using the VideoFlow model. In Figure 12, the correlation between VGG perceptual metrics and bits-per-pixel using a pretrained VGG network and VideoFlow model is analyzed. A weak correlation of -0.51 is observed. Additionally, evaluations are repeated with a smaller version of the VideoFlow model in Figure 13, showing competitive results with SVG-LP on VGG perceptual metrics."
}