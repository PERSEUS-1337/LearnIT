{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity and mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators can lead to overfitting, while weak discriminators may not detect mode collapse. In this paper, it is demonstrated that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by using discriminators with strong distinguishing power against specific generator classes. Various generator classes like mixture of Gaussians and neural networks can be paired with corresponding discriminators to approximate Wasserstein distance and KL-divergence. Successful training results in distributions close to the true distribution, avoiding mode collapse. Our preliminary experiments suggest that the lack of diversity in GANs may be due to sub-optimal optimization rather than statistical inefficiency. Various techniques have been proposed to enhance the quality of learned distributions and training stability in Generative Adversarial Networks (GANs). Recent work has highlighted concerns that distributions learned by GANs may suffer from mode collapse or lack of diversity, missing significant modes of the target distribution. The paper suggests that designing proper discriminators with strong distinguishing power against specific families of generators can alleviate mode collapse. The paper addresses mode collapse in GANs and proposes designing discriminators with strong distinguishing power against specific generator subclasses. It focuses on the Wasserstein GAN formulation and introduces the F-Integral Probability Metric for distribution comparison. The paper discusses mode collapse in GANs and suggests creating discriminators with strong distinguishing power against certain generator subclasses. It introduces the F-Integral Probability Metric for comparing distributions, addressing concerns about the weakness of IPM compared to W 1 in generating diverse examples. The paper addresses mode collapse in GANs by proposing discriminators with stronger distinguishing power against specific generator subclasses. It highlights the issue of the learned distribution generating high-quality but low-diversity examples due to the weakness of IPM compared to W 1. Increasing the strength of the discriminator to larger families like all 1-Lipschitz functions is suggested as a solution, despite the generalization issues of Wasserstein-1 distance. Arora et al. (2017a) points out that increasing the discriminator to larger families like all 1-Lipschitz functions may not solve the generalization issues of Wasserstein-1 distance. The empirical Wasserstein distance used in optimization may not accurately reflect the population distance, leading to overfitting with powerful discriminators and diversity issues with weak discriminators in GANs. The paper addresses the dilemma in GAN theories where powerful discriminators lead to overfitting, while weak discriminators result in diversity issues. It proposes a solution by designing a strong discriminator class F against a specific generator class G, with restricted approximability. The paper focuses on designing discriminators F to approximate the Wasserstein distance W1 for data distribution p and any q \u2208 G. It explores restricted approximability with respect to G, emphasizing the realizable case where p \u2208 G. The paper discusses designing discriminators F to approximate the Wasserstein distance W1 for data distribution p and any q \u2208 G, focusing on the realizable case where p \u2208 G. A discriminator class F with restricted approximability resolves mode collapse and allows for passing from population-level guarantees to empirical-level guarantees. The paper introduces a theoretical framework for analyzing the statistical properties of Wasserstein GANs by bounding the classical capacity and addressing the diversity and generalization properties of the distance W F. It is the first framework to tackle the statistical theory of GANs with polynomial samples, focusing on designing discriminator class F with restricted approximability to prevent mode collapse. This theoretical framework is the first to address the statistical theory of GANs with polynomial samples. It focuses on designing discriminator class F with restricted approximability for various generator classes, providing diversity guarantees. The paper starts with simple distribution families like Gaussians and exponential families, using specific techniques for designing F to distinguish between distributions in G. In Section 4, the paper explores distributions generated by invertible neural networks. A special type of neural network discriminator with one extra layer compared to the generator ensures restricted approximability. This leads to the emergence of exponentially large modes in the learned distribution q, with implications for the invertibility assumption. The invertibility assumption in distributions generated by invertible neural networks can lead to an exponentially large number of modes. However, this assumption only produces distributions supported on the entire space, which may not align with the low-dimensional manifold where natural images reside. In cases where both distributions have low-dimensional supports, the KL-divergence is not an appropriate measurement of statistical distance. The paper aims to establish the approximation of Wasserstein distance by IPMs. The paper focuses on approximating Wasserstein distance by IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning distributions. It develops tools for approximating the log-density of a neural network generator and demonstrates the correlation between IPM and Wasserstein distance for low-dimensional distributions. The paper discusses using the IPM as an alternative to measure diversity and quality of learned distributions in more complex settings where KL-divergence or Wasserstein distance may not be measurable. It also highlights the importance of optimizing the discriminator in GANs to achieve the test IPM, which can distinguish learned distributions from real data. The lack of diversity in GANs may be due to sub-optimal optimization rather than statistical inefficiency. Various tests have been developed to assess diversity, memorization, and generalization in GANs, indicating that lack of diversity is a common issue. Various tests have shown that lack of diversity is a common issue in GANs, with proposed solutions including architectures and algorithms to address mode collapse. Some studies have provided guarantees for training GANs with specific discriminators, but a general solution to the problem remains elusive. The text discusses the challenges in training GANs and the lack of diversity in generated samples. Previous studies have shown guarantees for training GANs with specific discriminators, but a general solution to the problem is still lacking. The work by Zhang et al. is highlighted for providing insights on using IPM as a proper metric. The current study extends this by developing statistical guarantees in Wasserstein distance for distributions with low-dimensional manifolds. The study focuses on developing statistical guarantees in Wasserstein distance for distributions with low-dimensional manifolds. It discusses the challenges in training GANs and the importance of invertible neural networks in successful GAN training. The research shows that successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. Our theoretical result and experiments suggest that successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. This implies that real data cannot be generated by an invertible neural network. Additionally, if the data can be generated by an injective neural network, we can bound the closeness between the learned distribution and the true distribution in Wasserstein distance. The IPM includes statistical distances like TV and Wasserstein-1 distance. F-IPM refers to neural net IPM when F is a class of neural networks. KL divergence and Wasserstein-2 distance are distances of interest between distributions. Rademacher complexity is defined for function classes. The Rademacher complexity of a function class with finite second moments is defined, and the training IPM loss for the Wasserstein GAN is discussed. Generalization of the IPM is governed by the Rademacher complexity over a distribution set G. Theorem 2.1 on generalization is presented, along with notation for Gaussian distributions and universal constants. One-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees. The discriminators considered have restricted approximability with respect to the set of Gaussian distributions with bounded mean and well-conditioned covariance. The discriminators considered have restricted approximability with respect to Gaussian distributions in the sense that the bounds differ by a factor of 1/ \u221a d. The upper bound is based on the maximum Wasserstein distance between one-dimensional projections of p, q, which is on the order of W 1 (p, q)/ \u221a d when p, q have spherical covariances. The proof of the maximum Wasserstein distance between one-dimensional projections of p, q is deferred. A discriminator family with restricted approximability can be designed for mixture of Gaussians. Linear combinations of sufficient statistics in exponential families are shown to be discriminators with restricted approximability. The log partition function condition is satisfied for the exponential family and discriminators defined. Theorem 3.2 states that for an exponential family G and a discriminator family F of linear functionals over features T(x), if certain conditions on the log partition function are met, then certain bounds hold for the Rademacher complexity. Additional assumptions on the diameter of X and the Lipschitz continuity of T(x) are also considered. The proof involves geometric assumptions on the sufficient statistics due to the intrinsic dependence of the Wasserstein distance on the underlying geometry. In this section, discriminators with restricted approximability for neural net generators are designed, focusing on a family of distributions commonly used in GANs to model real data. The discussion extends to invertible neural networks generators with proper densities in Section 4.1 and injective neural networks generators in Section 4.2, where latent variables can have lower dimensions than observable dimensions. In Section 4.2, the discussion extends to injective neural networks generators, allowing latent variables with lower dimensions than observable dimensions. The generators are parameterized by invertible neural networks, enabling the modeling of data around a \"k-dimensional manifold\" with noise. The discussion extends to invertible neural networks that model data around a \"k-dimensional manifold\" with noise, parameterized by parameters \u03b8 = (W i , b i ) i\u2208[ ] belonging to a set where the activation function \u03c3 is twice-differentiable and the standard deviation of hidden factors satisfy \u03b3 i \u2208 [\u03b4, 1]. The neural networks G \u03b8 are parameterized by \u03b8 = (W i , b i ) i\u2208[ ] from set DISPLAYFORM2. The activation function \u03c3 is twice-differentiable. The standard deviation of hidden factors is \u03b3 i \u2208 [\u03b4, 1]. The neural net is invertible, with its inverse also a feedforward neural net with activation \u03c3 \u22121. Assumptions are needed on generator networks to prevent pseudo-random functions. Lemma 4.1 states log p \u03b8 can be computed by a neural network with at most + 1 layers, O( d 2 ) parameters, and activation function among {\u03c3 DISPLAYFORM4 where DISPLAYFORM5. Family F of neural networks is discussed. The function log p \u03b8 can be computed by a neural network with at most + 1 layers, O( d 2 ) parameters, and activation function among {\u03c3. The family F of neural networks contains all the functions {log p \u2212 log q : p, q \u2208 G}. The proof involves the change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The function log p \u03b8 can be computed by a neural network with at most + 1 layers, O( d 2 ) parameters, and activation function among {\u03c3. The family F of neural networks contains all the functions {log p \u2212 log q : p, q \u2208 G}. The proof involves the change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The formula log p \u03b8 (x) = log \u03c6 \u03b3 (G \u22121 DISPLAYFORM7 | (where \u03c6 \u03b3 is the density of Z \u223c N(0, diag(\u03b3 2 ))) and the log-det of the Jacobian involves computing the determinant of the (inverse) weight matrices. Theorem 4.2 states that the discriminator class F has restricted approximability w.r.t. G. The discriminator class F, defined in Lemma 4.1, has restricted approximability with respect to the set of invertible-generator distributions G. The proof of Theorem 4.2 relates the KL divergence to the IPM when log densities exist in the family of discriminators. Lemma 4.3 states that for every q in G, there exists a function in F that satisfies a certain condition. The proof sketch of Theorem 4.2 involves choosing the discriminator class implementing log p - log q for p, q in G, and bounding the Wasserstein distance. The proof of Theorem 4.2 relates the KL divergence to the IPM when log densities exist in the family of discriminators. It involves choosing the discriminator class implementing log p - log q for p, q in G, and bounding the Wasserstein distance. The upper bound is immediate if functions in F are Lipschitz globally in the whole space. The upper bound (2) would have been immediate if functions in F are Lipschitz globally in the whole space. Two workarounds are provided in Theorem D.2 to address this issue, either through a truncation argument for a W 1 bound or a W 2 bound with a linear growth in the Lipschitz constant. The combination of restricted approximability and generalization bound implies that a small expected IPM during training results in the estimated distribution q being close to the true distribution p in Wasserstein distance. If training succeeds with small expected IPM, the estimated distribution q is close to the true distribution p in Wasserstein distance. The training error is measured by Eqm [W F (p n ,q m )], and efficient algorithms to achieve a small training error are an open question for future work. Injective neural network generators are considered in this section, generating distributions on a low dimensional manifold. In this section, injective neural network generators are discussed for generating distributions on a low dimensional manifold. A novel divergence between distributions is designed, sandwiched by Wasserstein distance and optimized as IPM. The key idea is to approximate the Wasserstein distance with a variant of the IPM, using a smoothed F-IPM between distributions p and q. The text discusses a family of distributions generated by neural nets in G and introduces a variant of the IPM to approximate the Wasserstein distance. The smoothed F-IPM between distributions p and q is optimized with an additional variable \u03b2. The theorem states that for certain discriminator classes, dF approximates the Wasserstein distance for distributions in G. The text discusses theoretical results on neural network generators and mode collapse prevention in GAN training. The IPM W F (p, q) is bounded by the Wasserstein distance W 1 (p, q) when the discriminator family F has restricted approximability with respect to the generator family G. Specific discriminator classes are designed to ensure this, with synthetic experiments confirming the theory in practice. In practice, the Wasserstein distance W 1 (p, q) is correlated with IPM, suggesting that restricted approximability holds true in GAN training. Experiments show that optimization difficulty, rather than statistical inefficiency, may be the main challenge in GAN training. The difficulty of GAN training may stem from optimization challenges rather than statistical inefficiency. Experiments demonstrate a correlation between IPM and Wasserstein distance, indicating restricted approximability in GAN training. In synthetic experiments with WGANs, the KL divergence is used to measure the quality of learned generators for various curves in two dimensions, such as the unit circle and a \"swiss roll\" curve. The IPM W_F is strongly correlated with the Wasserstein distance W_1, indicating restricted approximability in GAN training. The IPM W F is strongly correlated with the Wasserstein distance W 1 in GAN training. Ground truth distributions like the unit circle or Swiss roll curve are used, with standard two-hidden-layer ReLU nets as generators and discriminators. The RMSProp optimizer is used with learning rates of 10^-4 for both generator and discriminator. Comparisons are made between the ground truth distribution and the learned distribution using the neural net IPM W F metric. The neural net IPM W F is compared with the Wasserstein distance W 1 in GAN training using ground truth distributions like the unit circle or Swiss roll curve. The learned generator closely matches the ground truth distribution at iteration 10000, with strong correlation between the neural net IPM and Wasserstein distance. The learned generator closely matches the ground truth distribution at iteration 10000, with a strong correlation between the neural net IPM and Wasserstein distance. Sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented. The analysis technique involves designing discriminators tailored to the generator class for mode collapse avoidance. The analysis technique involves designing discriminators tailored to the generator class for mode collapse avoidance and better restricted approximability bounds. This can be extended to other distribution families with tighter sample complexity bounds, exploring approximation theory results in the context of GANs. The proof of Theorem 3.1 involves establishing upper and lower bounds for the discriminator family in the context of mode collapse avoidance and restricted approximability. Linear discriminators are shown to be the sum of two ReLU discriminators, leading to the derivation of mean and covariance distance bounds. The linear discriminator is the sum of two ReLU discriminators, expressed as t = \u03c3(t) \u2212 \u03c3(\u2212t). The neuron distance between two Gaussians is computed as a function of R(a) = E[max {W + a, 0}] for W \u223c N(0, 1). By manipulating v, the covariance distance can be further bounded using perturbation bounds. The linear discriminator is expressed as the sum of two ReLU discriminators. The covariance distance can be bounded using perturbation bounds, leading to a lower bound with c = 1/(2 \u221a 2\u03c0). The W 2 distance is used to bridge the KL and F-distance for two Gaussian distributions. The text discusses bounding the growth of gradients for two Gaussian distributions with parameters. The Rademacher contraction inequality is used to bound the term involving the logarithm of the distributions. The exponential family properties and assumptions on the second derivative are also considered. The proof of Theorem 3.2 establishes KL bounds and Wasserstein bounds for exponential families. The Rademacher complexity is computed for a mixture of identity-covariance Gaussians on R^d using a neural network. The family F is suitable for learning a mixture of k distributions. The Rademacher complexity is used to analyze a mixture of k identity-covariance Gaussians on R^d with a neural network. The family F is suitable for learning this mixture, with bounds established for approximability and generalization. The Gaussian concentration result is utilized in the proofs. The Rademacher complexity is utilized to analyze a mixture of Gaussians with a neural network. The discriminator is shown to be D-Lipschitz, and the KL divergence is implemented. The distributions are considered in the Bobkov-Gotze sense, and the Rademacher complexity of the neural network is bounded. The Rademacher complexity is used to analyze a mixture of Gaussians with a neural network, where the discriminator is shown to be D-Lipschitz. The Rademacher complexity of the neural network is bounded by reparametrizing it and using a one-step discretization bound. The covering number of the process is upper bounded by the product of each separate covering, leading to the expected supremum of the max over a covering set. The Rademacher complexity of the neural network is bounded by reparametrizing it and using a one-step discretization bound. The covering number of the process can be upper bounded by the product of each separate covering, leading to the expected supremum of the max over a covering set. The term \u03b5 i log is shown to be 1)-subGaussian, and by sub-Gaussian maxima bounds, we can derive an upper bound for the f-contrast by Wasserstein. The f-contrast is bounded by Wasserstein distance for two distributions on R d with positive densities. The proof involves a truncation argument and bounding terms using Cauchy-Schwarz inequality. The Lipschitzness of f in the D-ball is utilized, leading to a straightforward extension of a previous proposition. The proof involves utilizing Lipschitzness of f in the D-ball and Cauchy-Schwarz inequality to bound the f-contrast by Wasserstein distance for two distributions on R d. This extends a previous proposition and involves computing the inverse of x = G \u03b8 (z) using a feedforward net with activation \u03c3 \u22121. The problem of representing log p \u03b8 (x) by a neural network is also considered. The problem of representing log p \u03b8 (x) by a neural network is considered. The inverse network implementing G \u22121 \u03b8 has layers with parameters in each layer and \u03c3 \u22121 as the activation function. By adding branches, the log determinant of the Jacobian can also be computed. The log determinant of the Jacobian can be computed by adding branches to the inverse network. This allows for a neural network to compute log p \u03b8 (x) with a limited number of layers and parameters. Additionally, a restricted approximability bound is stated in terms of the W 2 distance. The theorem is proven by combining three lemmas, showing that p \u03b8 satisfies the Gozlan condition for any \u03b8, and applying Theorem D.1. The network G \u03b8 is also L-Lipschitz, and the random variable is L2-sub-Gaussian, satisfying the Gozlan condition with \u03c32 = L2. Additionally, applying Theorem D.1(b) results in a bound related to W2 and W1 distances. The network G \u03b8 is L-Lipschitz and the random variable is L2-sub-Gaussian. Applying Theorem D.1(b) results in a bound related to W2 and W1 distances. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 is upper bounded through Theorem D.2. The network G \u03b8 is L-Lipschitz and the random variable is L2-sub-Gaussian. Applying Theorem D.1(b) results in a bound related to W2 and W1 distances. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 is upper bounded through Theorem D.2. Furthermore, p \u03b8 (x) is differentiable in x with norm bounds for each layer h k being C(R W , R b , k)-Lipschitz in x. The terms in the equations are manipulated to derive bounds and apply Theorem D.2(c) for further analysis. The log-density neural network can be reparametrized to represent weights and biases. The Rademacher complexity is at most two times a certain quantity. An additional re-parametrization is done to bound the constant C(\u03b8). The Rademacher complexity of the log-density neural network can be reparametrized to represent weights and biases. The constant C(\u03b8) is bounded by a certain quantity, and the discretization error and expected max over a finite set are analyzed in separate lemmas. The text discusses the reparametrization of the Rademacher complexity of the log-density neural network to represent weights and biases. It analyzes the constant C(\u03b8) bounded by a certain quantity and examines the discretization error and expected max over a finite set in separate lemmas. The text also delves into the Lipschitzness of hidden layers in the inverse network G\u22121. The text discusses the Lipschitzness of hidden layers in the inverse network G\u22121, showing bounds for each layer and analyzing the Lipschitz property for the log \u03c3 \u22121 term and quadratic term. The text discusses the Lipschitzness of hidden layers in the inverse network G\u22121, showing bounds for each layer and analyzing the Lipschitz property for the log \u03c3 \u22121 term and quadratic term. The log \u03c3 \u22121 term is shown to be Lipschitz with a sub-Gaussian random variable, while the quadratic term is proven to be subexponential with a bounded mean. The text discusses bounding the mean of terms in the inverse network G\u22121, showing that the sum is Cd-sub-Gaussian with a O(Cd) mean. The term A \u03b3 h is proven to be subexponential with a bounded mean of Cd/\u03b4 2. The parameter K is also upper bounded. Overall, Y \u03b8 is shown to be mean-zero sub-exponential. The expected maximum is bounded using a standard covering argument. The text discusses bounding the mean of terms in the inverse network G\u22121, showing that the sum is Cd-sub-Gaussian with a O(Cd) mean. Y \u03b8 is proven to be mean-zero sub-exponential. The expected maximum is bounded using a standard covering argument. The covering number of \u0398 is bounded by the product of independent covering numbers. Jensen's inequality is used to derive a bound for any \u03bb \u2264 \u03bb 0 \u03b4 2 n. The text introduces regularity conditions for the family of generators G, including bounds on the partial derivatives of the function f. It also discusses truncating the distribution to a high-probability region in both the latent and observable domains. The text introduces regularity conditions for the family of generators G, including bounds on the partial derivatives of the function f. It also discusses truncating the distribution to a high-probability region in both the latent and observable domains. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.1 states that the generator class G satisfies the assumption E.1 and F is the family of functions as defined in Theorem. The main theorem states that for certain F, d, F approximates the Wasserstein distance. Theorem E.1 states that the generator class G satisfies the assumption E.1 and F is the family of functions as defined in Theorem E.2. The proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. Theorem E.2 shows that for \u03b2 = O(poly(1/d)), there exists a family of neural networks F that can approximate log p for typical x. The main theorem states that for certain F and d, F approximates the Wasserstein distance. Theorem E.1 shows that for \u03b2 = O(poly(1/d), there exists a family of neural networks F that can approximate log p for typical x. The approach involves approximating p \u03b2(x) using Laplace's method of integration and calculating a normalization constant. Theorem E.1 proves the existence of neural networks that approximate log p for typical x when \u03b2 = O(poly(1/d). The proof involves using neural networks N1 and N2 from Theorem E.2 to approximate log p \u03b2 and log q \u03b2, leading to a lower bound. The upper bound is obtained by considering the difference between N1(x) and N2(x), ultimately proving the lower bound using the Bobkov-G\u00f6tze theorem. The proof involves combining equations to obtain a lower bound using neural networks N1 and N2. The upper bound is achieved by setting \u03b2 = W 1/6. The claim is proven by considering the optimal coupling C of p, q, and the induced coupling Cz on the latent variable z. The generalization claim follows analogously to Lemma D.5. The rest of the section is dedicated to proving Theorem E.2. The proof involves combining equations to obtain a lower bound using neural networks N1 and N2. The claim is proven by considering the optimal coupling C of p, q, and the induced coupling Cz on the latent variable z. The generalization claim follows analogously to Lemma D.5. The rest of the section is dedicated to proving Theorem E.2, which will be discussed in Section E.3. The proof involves showing that DISPLAYFORM5 by induction. The size/Lipschitz constant of the neural network is determined by the cdf of a Gaussian with covariance DISPLAYFORM11. The claim is completed by bounding the smallest eigenvalue of the covariance matrix. The algorithm presented approximates the integral and can be implemented by a small, Lipschitz network. Parameters are set, and calculations are performed to find the nearest matrix in a net with bounded spectral norm. The algorithm approximates the integral using an \"invertor\" circuit to calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91), and find the nearest matrix in a net with bounded spectral norm. The output of the circuit provides approximate eigenvector/eigenvalue pairs for further calculations. The \"invertor\" circuit output, denoted as (x), is used to find matrices E i, chosen from a \u03b2 2 -net, such that at least one of M + E i has eigenvalues \u2126(\u03b2)-separated. This approximation method is further supported by synthetic WGAN experiments. The integral in DISPLAYFORM17 can be evaluated similarly to Theorem E.9, with a bound on T \u03b2 that holds universally on a neighborhood of radius D x. Part (3) is easily derived from FORMULA0 and FORMULA1. Synthetic WGAN experiments are conducted with invertible neural net generators and discriminators designed with restricted approximability. The goal is to show the correlation between empirical IPM W F (p, q) and the KL-divergence between p and q on synthetic data. Data is generated from a ground-truth invertible neural net generator. The data is generated from a ground-truth invertible neural net generator using Leaky ReLU activation function. The discriminator architecture is chosen for restricted approximability guarantee. Training involves constraints on parameters and modeling non-differentiable functions with neural networks. The training process involves using a neural network to model non-differentiable functions and constraints on parameters. Stochastic batches are generated for training the generator and discriminator networks in a Wasserstein GAN formulation. The RMSProp optimizer is used for updates, and evaluation metrics include computing the KL divergence between the true and learned generator. The evaluation metrics for the true and learned generator include the KL divergence, training loss (IPM W F train), and neural net IPM (W F eval). The KL divergence is considered a strong criterion for distributional closeness, while the training IPM may be far from the true W F due to balancing steps for discriminator and generator training in GANs. The training IPM in GANs may be far from the true W F due to balancing steps for discriminator and generator training. The neural net IPM evaluates a separately optimized WGAN loss to find an approximate maximizer for contrast. The theory suggests that WGAN can learn the true generator in KL divergence, with the F-IPM indicative of the KL divergence. In experiments testing WGAN, a two-layer net in 10 dimensions is used as the generator. The discriminator is trained with either Vanilla WGAN or WGAN-GP, with results showing that WGAN training can learn the true distribution in KL divergence. Results are plotted in FIG5. The main findings show that WGAN training with a discriminator design of restricted approximability can learn the true distribution in KL divergence. The KL divergence starts around 10-30 and the best run achieves a KL lower than 1, indicating that GANs are finding the true distribution without mode collapse. The W F (eval) and KL divergence are highly correlated, with adding gradient penalty improving optimization reflected in both curves. The W F metric can be used to monitor convergence effectively. The quantity W F can serve as a good metric for monitoring convergence, correlating well with the KL-divergence. Results with vanilla discriminators also show good correlation with the KL-divergence. The estimated IPM in evaluation correlates well with the KL-divergence, indicating the effectiveness of the discriminator design. The inferior performance of the WGAN-Vanilla algorithm in terms of KL divergence does not stem from the statistical properties of GANs but rather from the convergence of the IPM during training. This phenomenon is likely to occur in training GANs with real-life data as well. The correlation between perturbations and p is directly tested by comparing KL divergence and neural net IPM on pairs of perturbed generators. In this section, the correlation between perturbations and p is tested by comparing KL divergence and neural net IPM on pairs of perturbed generators. The results show a clear positive correlation between the KL divergence and neural net IPM, with most points falling around the line W F = 100D kl. The experiments show a positive correlation between KL divergence and neural net IPM, with most points falling around the line W F = 100D kl. Outliers with large KL are due to perturbations causing poorly conditioned weight matrices. Experiments with vanilla fully-connected discriminator nets also show convergence in KL divergence, but with slightly weaker correlation compared to restricted approximability. The experiments demonstrate a positive correlation between KL divergence and neural net IPM, with most points aligning around the line W F = 100D kl. Outliers with large KL are attributed to perturbations causing poorly conditioned weight matrices. Vanilla fully-connected discriminator nets also exhibit convergence in KL divergence, albeit with a slightly weaker correlation compared to restricted approximability. The estimated IPM in evaluation correlates well with the KL-divergence. Moving average is applied to all curves. Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators and plotted in FIG8. The correlation (0.7489) is roughly the same as for discriminators with restricted approximability (0.7315)."
}