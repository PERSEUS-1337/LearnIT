{
    "title": "Byl5NREFDr",
    "content": "We study model extraction in natural language processing, where an adversary can reconstruct a victim model using only query access. The attacker can successfully mount the attack without real training data or meaningful queries, using random word sequences and task-specific heuristics. This exploit is enabled by the shift towards transfer learning methods in NLP. With a query budget of a few hundred dollars, the attacker can extract a model that performs slightly worse than the victim model. Machine learning models are valuable intellectual property, often only accessible through web APIs. Attackers can extract a model using transfer learning methods with a query budget of a few hundred dollars. Defense strategies like membership classification and API watermarking can be circumvented by clever adversaries. Model stealing or model extraction is a threat where adversaries train a local copy of a model by issuing queries to a web API. This can lead to theft of intellectual property, leakage of sensitive information, or the generation of adversarial examples. NLP APIs based on ELMo and BERT have become popular targets for such attacks. In this paper, it is demonstrated that NLP models obtained by fine-tuning a pretrained BERT model can be extracted without access to the API provider's training data. Extraction attacks are possible even with randomly sampled queries. BERT model extraction attacks are possible without access to training data or well-formed queries. Performance improves with randomly-sampled Wikipedia sentences. Attacks are cost-effective, with the most expensive one estimated at $500. BERT model extraction attacks are enhanced by using randomly-sampled Wikipedia sentences as queries, making them cost-effective. The attacker samples words to form queries, fine-tunes their own BERT model using the victim's outputs, and extracts models successfully despite the nonsensical nature of the queries. The study analyzes randomly-generated queries for model extraction, finding that despite being nonsensical, they are effective. Pretraining on the attacker's side facilitates extraction. Simple defenses like membership classification and API watermarking are ineffective against clever adversaries. This research aims to inspire stronger defenses against model extraction vulnerabilities. Model extraction attacks are effective against naive adversaries but fail against clever ones. The research aims to inspire stronger defenses against these vulnerabilities and relates to prior efforts on computer vision applications and zero-shot distillation. Previous studies have focused on synthesizing queries close to decision boundaries of victim classifiers. Prior work on model extraction attacks focused on synthesizing queries close to decision boundaries of victim classifiers in computer vision applications. However, this method does not transfer to text-based systems due to the discrete nature of the input space. Pal et al. (2019) attempted extraction on NLP systems using pool-based active learning, while this study explores a more realistic extraction setting with nonsensical inputs on modern BERT-large models for tasks expecting pairwise inputs like question answering. Our work explores a realistic extraction setting with nonsensical inputs on modern BERT-large models for tasks like question answering. Prior work on model extraction focused on synthesizing queries close to decision boundaries in computer vision, but this method does not transfer well to text-based systems. BERT-large, a 24-layer transformer, is studied for model extraction in NLP tasks. Unnatural text inputs are shown to effectively train models without real examples. BERT-large is a 24-layer transformer that converts word sequences into vector representations. These representations are contextualized and BERT's parameters are learned through masked language modeling on unlabeled text. The release of BERT revolutionized NLP by achieving state-of-the-art performance on various tasks with minimal supervision. NLP systems typically leverage fine-tuning with task-specific networks to construct composite functions. The text discusses the use of fine-tuning methodology with task-specific networks like BERT to construct composite functions. It also explains extraction attacks where a malicious user tries to reconstruct a local copy of a black-box API model without access to training data. The text discusses extraction attacks where a task-specific query generator is used to construct nonsensical word sequences as queries to the victim model. The resulting dataset is used to train a new model. The attacks are demonstrated on four NLP tasks with different input and output spaces. The text discusses extraction attacks using task-specific query generators to construct nonsensical word sequences as queries. Two types of query generators, RANDOM and WIKI, are studied. The text discusses using task-specific query generators to construct word sequences for tasks with complex interactions. In the WIKI setting, input queries are formed from WikiText-103 corpus. Additional heuristics are applied for tasks like MNLI and SQuAD/BoolQ to enhance query generation. The text discusses using task-specific query generators to construct word sequences for tasks with complex interactions, such as SQuAD and BoolQ. Queries are formed by sampling words from the passage and adding a question starter word and a question mark. The evaluation includes using different query budgets and commercial cost estimates. Cost estimates for query budgets are calculated using Google Cloud Platform's Natural Language API calculator. Evaluation metrics include accuracy of extracted models on the original development set and agreement with victim models. Extracted models show high accuracy even with nonsensical inputs, with further improvement on WIKI dataset. The extracted models show high accuracy on original development sets, even with nonsensical inputs. However, their agreement with victim models is only slightly better than accuracy. Agreement is lower on held-out sets, indicating poor functional equivalence between the victim and extracted models. An ablation study with alternative query generation heuristics is conducted for SQuAD and MNLI datasets. An ablation study was conducted with alternative query generation heuristics for SQuAD and MNLI datasets. The classification experiments assumed the API returns a probability distribution over output classes, but results with argmax labels showed minimal accuracy drop. This suggests that access to the full probability distribution is not crucial for model extraction. Additionally, the effectiveness of extraction algorithms was measured with varying query budgets. In analyzing the effectiveness of extraction algorithms with varying query budgets, it was found that access to the full probability distribution is not essential for model extraction. Even with small query budgets, extraction can still be successful. The results raise questions about the properties of nonsensical input queries that make them effective for model extraction, especially without relying on large pretrained language models. In this section, an analysis is conducted to understand the effectiveness of nonsensical input queries for extracting NLP models based on BERT. The study examines if different victim models produce the same answer when given nonsensical queries and if some queries are more representative of the original data distribution. The focus is on the RANDOM and WIKI extraction configurations for SQuAD to determine the agreement among victim models on answers to nonsensical queries. The study analyzes the effectiveness of nonsensical input queries for extracting NLP models based on BERT, focusing on RANDOM and WIKI extraction configurations for SQuAD. Different victim models show varying agreement on answers to nonsensical queries, with higher agreement on SQuAD training and development set queries compared to WIKI and RANDOM queries. High-agreement queries may be more useful for model extraction despite models being brittle on nonsensical inputs. The study explores the effectiveness of nonsensical input queries in extracting NLP models based on BERT. High-agreement queries are found to be more useful for model extraction, showing significant F1 improvements compared to random and low-agreement subsets. This suggests that agreement between victim models serves as a good indicator of input-output pair quality for extraction. The study investigates the effectiveness of nonsensical input queries in extracting NLP models based on BERT. High-agreement queries are more beneficial for model extraction, indicating that agreement between victim models is a good indicator of input-output pair quality for extraction. The researchers leave the integration of this observation into an active learning objective for future work and question whether high-agreement nonsensical queries are interpretable to humans. The study found that human annotators matched victim models' answers 23% of the time on the WIKI subset and 22% on the RANDOM subset, using a word overlap heuristic. Annotators scored significantly higher on original SQuAD questions. Most nonsensical question-answer pairs remain mysterious to humans. In practical scenarios, the attacker's lack of information about the victim's architecture can impact extraction accuracy. The study examines how different pretraining setups affect accuracy when using different configurations of BERT models. BERT comes in two sizes: BERT-large and BERT-base. Accuracy is higher when the attacker uses BERT-large, even if the victim starts with BERT-base. Results are better when both attacker and victim use the same model. Fine-tuning BERT gives attackers an advantage as parameters start from a good representation of language. Fine-tuning BERT models gives attackers an advantage as parameters start from a good representation of language. QANet achieves high accuracy with original SQuAD inputs but F1 significantly degrades with nonsensical queries, highlighting the importance of better pretraining for model robustness. BERT-based models benefit from better pretraining, simplifying extraction. Defense strategies against model extraction focus on preserving API utility and remaining undetectable to attackers. Two defenses using membership inference are explored, effective against weak adversaries. The defense strategy against model extraction involves using membership inference to detect nonsensical inputs or adversarial examples. By treating membership inference as a binary classification problem and using model features as input, the API can issue random outputs to prevent signal extraction. The defense strategy against model extraction involves using membership inference and watermarking as defenses. Watermarking involves modifying a fraction of queries to return incorrect outputs, stored on the API side. The defense strategy against model extraction involves using watermarking as a defense. Watermarked queries are modified to return wrong outputs, stored on the API side. This defense anticipates that extracted models will memorize some watermarked queries, making them vulnerable to post-hoc detection. The evaluation of watermarking on MNLI and SQuAD shows results on watermarked models. Results on watermarked models show that non-watermarked models perform poorly on watermarked queries, predicting the victim model's outputs, while watermarked models behave oppositely with high WM Label Acc and low Victim Label Acc. Watermarking works effectively on training data, with non-watermarked models performing poorly and predicting victim model outputs. Watermarked models show opposite behavior. However, limitations exist as watermarking can only be used post-attack and assumes public model deployment with black-box access. Attackers can evade detection through differential private training, fine-tuning, or issuing random outputs. Model extraction attacks against NLP APIs serving BERT-based models are effective, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Further research is necessary to develop robust defenses against adaptive adversaries in NLP APIs serving BERT-based models. Future directions include leveraging nonsensical inputs for model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity. In this paper, the distribution of agreement between victim SQuAD models on RANDOM and WIKI queries is shown in Figure 3. The cost estimate from Google Cloud Platform's Calculator was used, with Natural Language APIs allowing inputs of up to 1000 characters per query. Costs for different datasets were calculated by counting input instances with more than 1000 characters multiple times. The costs of entity analysis and sentiment analysis APIs were extrapolated for tasks like natural language inference and reading comprehension. It is challenging to provide a widely applicable estimate for the price of issuing a certain number of queries. The cost of issuing queries through APIs can vary significantly depending on factors like computing infrastructure and revenue models. Attackers could exploit free query budgets or use web scraping techniques to extract data at scale. It is important to consider these factors when estimating API costs. The cost of using APIs can vary based on factors like computing infrastructure and revenue models. Web scraping allows for extracting data at scale for free. Extracting datasets is relatively low cost, even for complex tasks like machine translation and speech recognition. For example, it costs -$430.56 to extract a large speech recognition dataset and $2000.00 for 1 million translation queries. Input generation algorithms for datasets like SST2 and RANDOM involve building a vocabulary using wikitext103. Input generation algorithms for datasets like SST2, RANDOM, and MNLI involve building a vocabulary using wikitext103 and sampling tokens or sentences in specific ways. The process involves sampling sentences from wikitext103 and replacing words not in the top-10000 vocabulary. The final hypothesis is constructed by replacing words from the premise multiple times. Different sampling methods are used for generating premises and hypotheses in datasets like MNLI, SST2, and SQuAD. A vocabulary is built using wikitext103, and paragraphs are constructed by sampling tokens based on the vocabulary's unigram distribution. The process involves building a vocabulary from wikitext103 and sampling tokens to construct paragraphs. Questions are generated by sampling paragraph tokens and appending them with a question starter word and a question mark. Random paragraphs from wikitext103 are used to create questions in a similar manner. In this section, additional query generation heuristics are studied. Table 11 compares extraction datasets for SQuAD 1.1, showing that RANDOM works better when paragraphs are sampled based on unigram frequency in wikitext103. Starting questions with common question starter words is also found to be effective. Our findings show that RANDOM extraction works better when paragraphs are sampled based on unigram frequency in wikitext103. Starting questions with common question starter words like \"what\" improves performance. A similar ablation study on MNLI reveals that lexical overlap between premise and hypothesis impacts model predictions, with low overlap leading to neutral or contradiction predictions, high overlap to entailment predictions, and a few different words resulting in balanced datasets. When lexical overlap is high, the model tends to predict entailment, creating an unbalanced dataset. However, when there are a few different words between the premise and hypothesis, datasets are more balanced with a strong extraction signal. Using frequent words aids extraction. Human studies involved fifteen annotators annotating question sets, including original SQuAD questions, WIKI questions with high agreement among victim models, and RANDOM questions with high agreement. In an ablation study on input features for the membership classifier, five question sets were analyzed with varying levels of agreement among victim models. The inter-annotator agreement was shown in Table 10, with the average pairwise F1 following a specific order. The study hypothesized that this order reflects the closeness to the actual input distribution. In an ablation study on input features for the membership classifier, it was found that the last layer representations are more effective than the logits in distinguishing between real and fake inputs. However, the best results are obtained by using both feature sets. The ablation study found that using both feature sets yields the best results for the membership classifier. The last layer representations are more effective in classifying points as real or fake."
}