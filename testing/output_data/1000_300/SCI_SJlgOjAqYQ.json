{
    "title": "SJlgOjAqYQ",
    "content": "We conducted experiments on convolutional and capsules neural networks to test global translation-invariance in deep learning models trained on the MNIST dataset. Both models initially showed poor performance in this aspect, but performance improved with data augmentation. While the capsule network performed better on the MNIST testing dataset, the convolutional neural network generally had better performance in translation-invariance. The success of convolutional neural networks is attributed to reduced computation cost with weight sharing in convolutional layers and generalization with local invariance in subsampling layers. The success of CNN is mainly due to reduced computation cost with weight sharing in convolutional layers and generalization with local invariance in subsampling layers. Capsule network, on the other hand, is robust in dealing with different viewpoints by using capsules to include pose, color, lighting, and deformation of visual entities. Capsule network aims for 'rate-coded' equivariance, where weights code viewpoint-invariant knowledge, leading to linear effects on pose matrices with viewpoint changes. Capsule networks aim for 'rate-coded' equivariance by encoding viewpoint-invariant knowledge in weights, leading to linear effects on pose matrices with viewpoint changes. However, it is unclear if capsule networks can generalize for global translation invariance. Visualizing and quantifying translation invariance in deep learning models is crucial for understanding architectural choices and developing generalization models. An analysis using translation-sensitivity maps for the MNIST digit dataset has been conducted to investigate translation invariance in CNN BID5. This paper introduces a simple method to test the performance of global translation invariance in convolutional and capsule neural network models trained on the MNIST dataset. In this study, a method was introduced to test global translation invariance in convolutional and capsule neural network models trained on the MNIST dataset. A testing dataset was created by shifting the centre of mass of digit images one pixel at a time. This testing dataset consisted of 2520 images covering all possible cases of translational translations. The deep learning models were trained on the MNIST dataset and tested on both training and testing datasets. In total, there are 2520 testing images for global translation invariance testing. Deep learning models are trained on the MNIST dataset and tested on both MNIST and GTI datasets. The GTI dataset is advantageous for capturing tiny differences in models and overcoming random noise from miss labeling in the MNIST dataset. The GTI dataset is beneficial for testing global translational invariance in models. A CNN model with nine layers is used, including convolutional and fully connected layers with dropout. The total number of parameters is 361578, significantly smaller than the Capsule model. The CNN model with nine layers includes fully connected layers with sizes 256, 128, and 10. Dropout with a rate of 0.5 is applied to certain layers. The total number of parameters is 361578, much smaller than Capsule networks. The model uses ReLU activation for all layers except the last one, which uses softmax. The optimizer is Adam with default parameters, and the objective function is cross entropy loss. The CNN achieves high accuracy on the MNIST dataset but performs poorly on the GTI dataset due to global translational invariance issues. The CNN model, without data augmentation, has low accuracy on the GTI dataset due to global translational invariance issues. Images with the digit's center predicted correctly, while those at the corner were assigned to incorrect classes. The model's performance suggests it is 'place-code' equivariant. To improve performance, data augmentation by shifting images is used during training. To improve CNN performance on the GTI dataset, data augmentation is used by shifting images during training. This increases accuracy to 98.05% by moving the image center in x and y-direction. Data augmentation implies 'place-code' equivariance in CNN, activating neurons at feature map corners. CapsNet is tested on GTI dataset with 8.2M parameters, trained with Adam optimizer and exponential decay of learning rate. The CapsNet with 8.2M parameters is tested on the GTI dataset, trained with Adam optimizer and exponential decay of learning rate. Data augmentation in MNIST training helps improve CapsNet accuracy on the GTI dataset, especially in recognizing objects at the edge. The CapsNet fails to predict classes correctly and generates incorrect images for digits close to the edge. Data augmentation in the MNIST training dataset improves CapsNet accuracy on the GTI dataset. CNN outperforms CapsNet on the GTI dataset, even with wider receptive fields in CapsNet's convolutional layers. The removal of max-pooling layers in CapsNet may contribute to its lower performance on the GTI dataset. There is potential for improvement in CapsNet's performance. The CapsNet struggles with global translational invariance, leading to lower performance on the GTI dataset compared to CNN. Despite the potential of CapsNet's architecture, it currently requires data augmentation to handle this issue. Capsules in CapsNet can learn viewpoints regardless of information received from the center or edge, suggesting a capability for improved performance in the future. CapsNet may outperform CNN in handling global translational invariance by learning viewpoints from all angles. Testing on the GTI dataset shows accuracy differences between models trained on CNN and CapsNet with varying levels of random shifting in the MNIST dataset. This method is easily implementable for other computer vision tasks by using clear, labeled images and applying translational shifting to cover all scenarios."
}