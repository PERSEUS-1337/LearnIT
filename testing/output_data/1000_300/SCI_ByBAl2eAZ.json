{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods can use noise injection in the action space for exploratory behavior. Alternatively, adding noise directly to the agent's parameters can lead to more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods. Exploration is a key challenge in deep RL to prevent premature convergence to local optima. Efficient exploration in deep reinforcement learning is crucial to prevent premature convergence to local optima. Various methods have been proposed to address this challenge, including noise injection in the action space and adding noise to the agent's parameters. Additionally, the addition of temporally-correlated noise and parameter noise has been shown to enhance exploration in high-dimensional and continuous-action environments. The addition of parameter space noise can enhance exploration in deep reinforcement learning algorithms like DQN, DDPG, and TRPO. This method effectively combines with off-the-shelf algorithms to improve exploratory behavior, addressing the limitations of other noise-based approaches. This paper explores the effectiveness of combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to improve exploratory behavior. Results show that parameter noise outperforms traditional action space noise in tasks with sparse reward signals, applicable to both high-dimensional discrete environments and continuous control tasks. The environment is modeled as a Markov decision process (MDP) with states, actions, rewards, transition probabilities, time horizon, and discount factor. The agent aims to maximize expected return through a policy \u03c0 \u03b8, which can be deterministic or stochastic. Off-policy RL methods allow learning from data collected by different policies. This paper focuses on two off-policy algorithms, Deep Q-Networks. The paper discusses off-policy reinforcement learning methods using Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG). DQN utilizes a deep neural network to estimate the optimal Q-value function, while DDPG is an actor-critic algorithm. Both algorithms aim to maximize return through policy optimization. The Boltzmann policy in DDPG encourages exploration by sampling noise in the action space. The Q-network predicts Q-values for actions and is updated using off-policy data. DDPG is an actor-critic algorithm for continuous action spaces, where the actor maximizes the critic's Q-value estimates through back-propagation. Exploration is achieved through a stochastic policy with action space noise. On-policy methods update function approximators based on the current policy. Trust Region Policy Optimization (TRPO) is an on-policy method that improves upon traditional policy gradient methods by computing an ascent direction that ensures a small change in the policy distribution. It solves a constrained optimization problem using discounted state-visitation frequencies and an advantage function. The work considers policies represented as parameterized functions, denoted as \u03c0 \u03b8, with \u03b8 as the parameter vector. Policies are sampled by adding Gaussian noise to the parameter vector of the current policy. The perturbed policy is sampled at the beginning of each episode and kept fixed for the entire rollout. State-dependent exploration involves a crucial difference between action space noise and parameter space noise. Gaussian action noise results in different actions being sampled for the same state in a rollout, while parameter space noise perturbs the policy parameters at the beginning of each episode. Perturbing deep neural networks with spherical Gaussian noise can introduce consistency in actions and create a dependence between state and exploratory action. This reparameterization technique, as demonstrated by Salimans et al. (2017), involves using layer normalization between perturbed layers. The reparameterization technique by Salimans et al. (2017) involves using layer normalization between perturbed layers to achieve consistency in actions. Adaptive noise scaling in parameter space requires selecting a suitable scale \u03c3, which can vary over time as parameters become more sensitive to noise during learning. The proposed solution involves adapting the scale of parameter space noise over time to relate it to the variance in action space, using a distance measure to adjust noise levels based on a threshold value. This approach addresses limitations in understanding noise scales in parameter space and is applicable to various algorithms like DQN, DDPG, and TRPO. The proposed solution involves adapting parameter space noise based on a threshold value, with a scaling factor \u03b1 and threshold \u03b4. Parameter space noise can be applied in off-policy methods by perturbing the policy for exploration and training the non-perturbed network. In on-policy methods, parameter noise can be incorporated using an adapted policy gradient. The text discusses the optimization of policy gradient methods for stochastic policies, incorporating parameter space noise to explore sparse reward environments effectively and comparing it with evolution strategies for deep policies. Reference implementations of DQN and DDPG with adaptive parameter space noise are available. The text explores the use of parameter space noise to optimize policy gradient methods for sparse reward environments and compares it with evolution strategies for deep policies. Reference implementations of DQN and DDPG with adaptive parameter space noise are provided online. The effectiveness of parameter space noise over action space noise is measured on high-dimensional discrete-action environments and continuous control tasks using DQN, DDPG, and TRPO. In comparing a baseline DQN agent with -greedy action noise to a version with parameter noise, the scale is adjusted based on KL divergence. Reparameterizing the network to represent the greedy policy \u03c0 implied by Q-values is found to be useful for parameter perturbation. Reparameterizing the network to represent the greedy policy \u03c0 implied by Q-values is found to be useful for parameter perturbation. This involves adding a fully connected layer and softmax output layer to predict a probability distribution over actions. Training the policy \u03c0 to output the greedy action according to the current Q-network results in more meaningful changes compared to perturbing the Q-function directly. Trained by maximizing the probability of outputting the greedy action according to the current Q-network, the policy is designed to exhibit the same behavior as running greedy DQN. The parameter space noise approach is compared against regular DQN and two-headed DQN with -greedy exploration. Random actions are sampled for the first 50 thousand timesteps to fill the replay buffer before training. Combining parameter space noise with a bit of action space noise yields better results. Experimental details are provided in Section A.1, with 21 games of varying complexity chosen for the study. The study compared parameter space noise with regular DQN and two-headed DQN with -greedy exploration on 21 games of varying complexity. Results showed that parameter space noise often outperforms action space noise, especially on games requiring consistency. Learning progress starts sooner with parameter space noise. Parameter space noise is effective in outperforming action space noise, particularly in games that require consistency like Enduro and Freeway. It also leads to quicker learning progress compared to traditional methods. However, in extremely challenging games like Montezuma's Revenge, more sophisticated exploration methods such as BID4 may be necessary. Combining parameter space noise with other exploration methods could be an interesting area for further study. Parameter space noise is suggested as a more effective method than action space noise for learning games, especially those requiring consistency. Further research is needed to evaluate its combination with other exploration methods. Proposed improvements to DQN are considered orthogonal to these findings and may enhance results. Experimental validation of this theory is left for future work. Comparisons between parameter noise and action noise are conducted on continuous control environments in OpenAI Gym using DDPG as the RL algorithm with specific hyperparameters. In OpenAI Gym, DDPG is used as the RL algorithm with various noise configurations tested for performance on continuous control tasks. The addition of layer normalization after each layer is found to be beneficial, especially for parameter space noise. The study compares no noise, uncorrelated additive Gaussian noise, correlated additive Gaussian noise, and adaptive parameter space noise. Performance results are shown in FIG3. The study evaluates different noise configurations in DDPG for continuous control tasks in OpenAI Gym. Parameter space noise outperforms other configurations on HalfCheetah, breaking out of sub-optimal behavior quickly. Parameter space noise outperforms other exploration schemes by quickly breaking out of sub-optimal behavior. It performs on par with other strategies on two environments, indicating well-shaped reward functions that require less exploration. TRPO results are depicted in FIG4 for the Walker2D environment. The results for TRPO in the Walker2D environment show that adding parameter noise reduces performance variance between seeds, indicating it helps escape local optima. Parameter noise is evaluated on environments with sparse rewards to see if it improves learning for existing RL algorithms. In a toy problem with a chain of states, parameter noise is evaluated on adaptive DQN, bootstrapped DQN, and \u03b5-greedy DQN. The environment offers small and large rewards in different states, with increasing difficulty as the chain length varies. Performance is evaluated after each episode with noise disabled. The study evaluates parameter noise on adaptive DQN, bootstrapped DQN, and \u03b5-greedy DQN in a toy problem with a chain of states. The performance is assessed by evaluating the current policy after each episode with noise disabled. The problem is considered solved if one hundred subsequent rollouts achieve the optimal return. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN in a simple environment where the optimal strategy is always to go right. However, in more complex environments where the optimal action depends on the state, parameter space noise may not work as well. This highlights the difference in exploration behavior compared to action space noise in this specific case. In challenging continuous control environments with sparse rewards, rewards are only given after significant progress towards a goal. Tasks include SparseCartpoleSwingup, SparseDoublePendulum, SparseHalfCheetah, SparseMountainCar, and SwimmerGather, each with specific reward conditions. In challenging continuous control environments with sparse rewards, tasks like SparseDoublePendulum are relatively easy to solve with DDPG, while SparseCartpoleSwingup and SparseMountainCar present more interesting results. The performance of DDPG in various environments is evaluated using different random seeds to plot the median return and interquartile range. In challenging continuous control environments with sparse rewards, DDPG performs well in solving tasks like SparseDoublePendulum quickly. However, for tasks like SparseCartpoleSwingup and SparseMountainCar, only parameter space noise leads to successful policy learning. SparseHalfCheetah shows some progress in finding non-zero rewards but struggles to learn a successful policy. On the SwimmerGather task, all DDPG configurations fail. Parameter space noise can enhance exploration behavior in off-the-shelf algorithms, but its benefits vary case by case. Parameter space noise can enhance exploration behavior in off-the-shelf algorithms, but improvements are not guaranteed for all cases. By combining parameter space noise with traditional RL algorithms, temporal information can be included while still benefiting from improved exploratory behavior. The comparison between Evolution Strategies (ES) and traditional RL with parameter space noise is done directly on performance in 21 ALE games. Comparing ES and traditional RL with parameter space noise directly on performance in 21 ALE games shows that DQN with parameter space noise outperforms ES on 15 out of 21 Atari games, despite being exposed to significantly less data. Parameter space noise, despite being exposed to significantly less data, outperforms ES on 15 out of 21 Atari games. This demonstrates that parameter space noise combines the exploration properties of ES with the sample efficiency of traditional RL. In real-world reinforcement learning problems with continuous and high-dimensional state and action spaces, existing algorithms become impractical even with discretization. Various techniques in deep reinforcement learning have been proposed to enhance exploration, but they are non-trivial. In the context of deep reinforcement learning, various techniques have been proposed to improve exploration, but they are often computationally expensive. R\u00fcckstie\u00df et al. (2008) suggested perturbing policy parameters, which outperforms random exploration. However, their method is limited to low-dimensional policies and state spaces. In contrast, our method is evaluated for both on and off-policy settings. Our method is applied and evaluated for both on and off-policy settings, using high-dimensional policies and environments with large state spaces. It is closely related to evolution strategies and policy optimization techniques. ES has shown success in high-dimensional environments but lacks efficiency in handling temporal structures in trajectories. Bootstrapped DQN has been proposed for more directed exploration using a network with multiple heads. Parameter space noise is proposed as a simpler and sometimes superior method for exploration in deep reinforcement learning compared to traditional approaches like -greedy and additive Gaussian noise. This approach perturbs the network parameters directly, showing success when combined with various deep RL algorithms such as DQN, DDPG, and TRPO. Parameter space noise is suggested as a more effective alternative to traditional action space noise in deep reinforcement learning. It has shown improved performance when combined with deep RL algorithms like DQN, DDPG, and TRPO, especially in environments with sparse rewards. This method perturbs network parameters directly, offering a promising exploration strategy. The network architecture for ALE BID3 includes 3 convolutional layers followed by a hidden layer and a linear output layer. ReLUs are used in each layer, with layer normalization in the fully connected part. A policy network with softmax output is included for parameter space noise. Target networks are updated every 10 K timesteps, and the Q-value network is trained using the Adam optimizer with a learning rate of 10 \u22124. The replay buffer can hold 1 M state transitions, and a -greedy baseline is implemented. The Q-value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32. The replay buffer can hold 1 M state transitions. For the -greedy baseline, the policy is perturbed at the beginning of each episode and the standard deviation is adapted every 50 timesteps. The policy head is perturbed after the convolutional part of the network to avoid getting stuck. To avoid getting stuck, the policy head is perturbed after the convolutional part of the network. -greedy action selection with = 0.01 is used to prevent potential perturbed policy issues. Initial data collection involves 50 K random actions, with \u03b3 = 0.99 and rewards clipped to [-1, 1]. Observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale, with a concatenation of 4 subsequent frames provided to the network. Up to 30 noop actions are used at the start of each episode, following a setup similar to BID19. The DDPG network architecture is akin to BID18, with both actor and critic utilizing 2 hidden layers. The setup for the DDPG network involves concatenating 4 subsequent frames and using up to 30 noop actions at the beginning of each episode. The network architecture for both the actor and critic includes 2 hidden layers with 64 ReLU units each. Layer normalization is applied to all layers, and the target networks are soft-updated with \u03c4 = 0.001. The critic is trained with a learning rate of 10 \u22123 while the actor uses a learning rate of 10 \u22124. Both actor and critic are updated using the Adam optimizer with batch sizes of 128. The critic is regularized with an L2 penalty of 10 \u22122, and the replay buffer holds 100 K state transitions with \u03b3 = 0.99. Each observation dimension is normalized by an online estimate of the mean and variance. Additionally, parameter space noise with DDPG is adaptively scaled to be comparable. The TRPO algorithm uses a step size of \u03b4 KL = 0.01, a policy network with 2 hidden layers of 32 tanh units for nonlocomotion tasks, and 2 hidden layers of 64 tanh units for locomotion tasks. The Hessian calculation is subsampled with a factor of 0.1, \u03b3 = 0.99, and the batch size per epoch is set to 5 K timesteps. The baseline is a learned linear transformation of observations. Various environments from OpenAI Gym are utilized. The TRPO algorithm uses a step size of \u03b4 KL = 0.01, a policy network with 2 hidden layers of 32 tanh units for nonlocomotion tasks, and 2 hidden layers of 64 tanh units for locomotion tasks. The Hessian calculation is subsampled with a factor of 0.1, \u03b3 = 0.99, and the batch size per epoch is set to 5 K timesteps. Various environments from OpenAI Gym and rllab are used for different tasks. The SparseDoublePendulum and DISPLAYFORM5 environments are used for training agents with DQN. The agents are trained with a simple network structure and layer normalization. Performance is evaluated after each episode, and the problem is considered solved if optimal performance is achieved in one hundred subsequent trajectories. After training agents in SparseDoublePendulum and DISPLAYFORM5 environments with DQN, performance is evaluated by sampling trajectories. The problem is considered solved if one hundred subsequent trajectories achieve optimal return. Different DQN variations are compared, including adaptive parameter space noise DQN, bootstrapped DQN, and -greedy DQN. Parameter space noise is adaptively scaled, and learning starts after 5 initial episodes. Target network is updated regularly. The training process involves using a single head and perturbing Q directly, with adaptively scaled parameter space noise. The network is trained with specific settings such as \u03b3 = 0.999, a replay buffer of 100 K state transitions, and updates to the target network every 100 timesteps. The Adam optimizer with a learning rate of 10 \u22123 and a batch size of 32 is used. The expected return is expanded using likelihood ratios and the reparametrization trick for N samples, allowing for the subtraction of a variance-reducing baseline. The proposed adaption method is used to re-scale \u03a3 := \u03c3 2 I appropriately. The challenge lies in selecting a suitable scale \u03c3 for the parameter space noise. The proposed adaption method involves re-scaling parameter space noise by adapting the scale over time. This resolves limitations related to selecting a suitable scale \u03c3, which can vary with network architecture and learning progress. The time-varying scale \u03c3 k is updated every K timesteps based on a simple heuristic, related to action space variance. The time-varying scale \u03c3 k is updated every K timesteps based on a simple heuristic related to action space variance. The distance measure and threshold value depend on the policy representation, with \u03b1 = 1.01 used in experiments. For DQN, the policy is implicitly defined by the Q-value function, leading to challenges in measuring distance between Q-values. In experiments, \u03b1 = 1.01 is always used. For DQN, the policy is implicitly defined by the Q-value function, leading to challenges in measuring distance between Q-values. A probabilistic formulation is used for both non-perturbed and perturbed policies, applying the softmax function over predicted Q values. The policy for both non-perturbed and perturbed policies is defined using the softmax function over predicted Q values. The distance in action space is measured using the Kullback-Leibler divergence, which normalizes the Q-values. This approach avoids the need for an additional hyperparameter and allows for a fair comparison between greedy and \u03b5-greedy policies. The KL divergence between a greedy policy and an \u03b5-greedy policy is used to relate action space noise and parameter space noise in DDPG. The distance measure is adapted to match the KL divergence, setting \u03b4 accordingly. This allows for a comparison between noise induced by parameter space perturbations and additive Gaussian noise. The distance measure between non-perturbed and perturbed policies is adjusted to match the KL divergence in DDPG. By setting \u03b4 as the adaptive parameter space threshold, effective action space noise with the same standard deviation as regular Gaussian noise is achieved. For TRPO, noise scaling is done by computing a trust region around the noise direction to keep the perturbed policy close to the original one. This is achieved through the conjugate gradient algorithm. The trust region policy optimization (TRPO) algorithm computes a natural step around the noise direction to keep the perturbed policy close to the original one. This is achieved through the conjugate gradient algorithm and line search for constraint conformation. Performance comparisons are made between TRPO and other exploration methods in various Atari games. The performance of TRPO with noise scaled according to the parameter curvature is compared to other exploration approaches in Atari games. The results show that adaptive parameter space noise achieves stable performance on InvertedDoublePendulum, indicating that these environments are not well-suited for testing exploration. Adding parameter space noise improves the performance of TRPO in challenging sparse environments, as shown in FIG10. The baseline TRPO uses action noise, while the variant with parameter space noise learns more consistently."
}