{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) has been introduced. GODE extends the idea of continuous-depth models to graph data by parameterizing the derivative of hidden node states with a graph neural network. Two efficient training methods for GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. Direct backprop has been shown to outperform the adjoint method in experiments. Additionally, a family of bijective blocks has been introduced to enable efficient memory consumption. The GODE model, utilizing direct backpropagation through the ODE solver, outperforms the adjoint method. Bijective blocks enable $\\mathcal{O}(1)$ memory consumption. GODE can be easily adapted to different graph neural networks, improving accuracy in various tasks. CNNs excel in tasks like image classification and segmentation but are limited to Euclidean domain data like images. Graph data structures represent objects as nodes and relations as edges, allowing for modeling of irregularly structured data like social networks, protein interaction networks, and citation graphs. Traditional methods like random walk and graph embedding have limitations in expressive capacity. Graph neural networks (GNN) have been proposed as a new class of models to model graphs, inspired by the success of CNNs. These models generalize convolution operations to graphs to capture local information, with two main types of methods: spectral and non-spectral. Spectral methods compute the graph Laplacian and perform filtering in the spectral domain, while non-spectral methods directly perform convolution in the graph domain for faster speed. The recently proposed GraphSAGE learns a convolution kernel in an inductive manner, while existing GNN models have discrete layers, making it difficult to model continuous diffusion processes. The neural ordinary differential equation (NODE) views a neural network as an ordinary differential equation, extending from the Euclidean domain. In this work, the authors propose graph ordinary differential equations (GODE) to model message propagation on graphs as an ODE. While NODEs have advantages like adaptive evaluation and accuracy-speed control, they are found to be inferior to state-of-the-art discrete-layer models in benchmark image classification tasks. This inferiority is attributed to errors in gradient estimation during training. In this work, the authors propose a memory-efficient framework for accurate gradient estimation in free-form NODEs, addressing the error in gradient estimation during training. The framework significantly improves performance on benchmark classification tasks, reducing test error from 19% to 5% on CIFAR10. Additionally, the framework is shown to be memory-efficient for free-form ODEs, achieving constant memory usage when applied to restricted-form invertible blocks. The authors propose a memory-efficient framework for accurate gradient estimation in free-form NODEs, improving benchmark classification performance on CIFAR10 from 19% to 5%. They also generalize ODE to graph data with GODE models, demonstrating enhanced performance on various datasets. Previous studies have explored neural networks as differential equations, with NODE treating the network as a continuous ODE and being used in continuous normalizing flow for generative models. The curr_chunk discusses the development of neural ordinary differential equations (NODE) for generative models and the training methods for NODE. It also mentions the use of GNNs in graph data analysis. The focus is on addressing the inaccurate gradient estimation issue and improving the performance of NODE in classification tasks. Spectral and non-spectral methods in GNNs for graph data analysis are discussed. Spectral GNNs operate in the Fourier domain, requiring information of the entire graph, while non-spectral GNNs focus on message aggregation from neighboring nodes, leading to localized computations. Various spectral methods are briefly introduced, highlighting efforts to optimize filters for improved performance. Spectral methods in GNNs involve graph estimation procedures and parameterized filters for localized computations. Chebyshev expansion and fast localized spectral filtering techniques have been proposed to accelerate running speed. Non-spectral methods focus on convolution operations considering only neighboring nodes, with approaches like MoNet and GraphSAGE for graph data analysis. In GNNs, methods like MoNet, GraphSAGE, graph attention networks, and GIN generalize convolution operations on graphs. Invertible blocks are used in normalizing flow models for accurate data reconstruction. Invertible blocks are utilized in normalizing flow models for accurate data reconstruction. Bijective blocks were introduced by Jacobsen et al. (2018) to create invertible networks. Gomez et al. (2017) proposed using invertible blocks for memory-efficient network structures by discarding activation of middle layers. This is achieved as each layer's activation can be reconstructed from the next layer with invertible blocks. Discrete-layer models with residual connections can be represented as neural ordinary differential equations (NODE). In the discrete case, different layers have their own function, while in the continuous case, the function is shared across all time. The forward pass involves applying an output layer after the total number of layers. In discrete-layer models with residual connections, each layer has its own function, while in the continuous case, the function is shared across all time. The forward pass involves applying an output layer after the total number of layers. Integration in the forward pass can be done with various ODE solvers, and the adjoint method is commonly used in optimal process control and functional analysis. Model parameters are denoted as \u03b8, which is independent of time. The adjoint method, widely used in optimal process control and functional analysis, involves defining model parameters as \u03b8, independent of time. It compares two methods for back-propagation on NODE, showing the forward pass with discretized ODE solver points and the reverse-time hidden state in the adjoint method. Direct back-propagation saves evaluation time points during both forward and backward passes. The reverse-time solution in back-propagation through ODE solver can be numerically unstable, causing errors in gradient calculation. Direct back-propagation involves saving evaluation time points during the forward pass and reconstructing the computation graph during the backward pass to accurately evaluate the gradient. This method allows for accurate reconstruction of the hidden state and precise gradient evaluation for optimization. Eq. 6 represents a reverse-time integration that can be solved using any ODE solver, requiring determination of z(t) by solving Eq. 2 in reverse-time. Storing z(t) during the forward pass would lead to high memory consumption due to the equivalent infinite-layer model. In reverse-time integration, Eq. 6 can be solved with any ODE solver to determine z(t). The backward pass involves solving Eq. 2 and 6 in reverse time, with initial conditions from Eq. 5 at time T. The reverse-time ODE solver can lead to inaccurate gradients in adjoint methods due to instability, causing a mismatch between hidden states solved forward-time and reverse-time. In reverse-time integration, the hidden state solved forward-time and reverse-time may not be equal due to instability in the reverse-time ODE. If the Jacobian of the ODE has eigenvalues with non-zero real parts, either the forward-time or reverse-time ODE is unstable. Large real parts of eigenvalues indicate sensitivity to numerical errors. This instability affects gradient calculations. When the system's eigenvalues have a non-zero real part, either the forward-time or reverse-time ODE is unstable, leading to sensitivity to numerical errors. This instability affects the accuracy of solutions and gradient computations. To address this, direct back-propagation through the ODE solver is proposed, ensuring accurate hidden states. This can be achieved by saving activation values or reconstructing them when needed. Direct back-propagation through the ODE solver ensures accurate hidden states by saving activation values or reconstructing them when needed. The adjoint for discrete forward-time ODE solution can be defined, leading to accurate gradient estimation. Detailed derivations are provided in appendix E and F. The text discusses the derivation and optimization perspective of equations 6 and 7, with detailed derivations in appendices E and F. It also presents Algorithm 1 for accurate gradient estimation in ODE solvers for free-form functions, involving forward and backward steps with adaptive step sizes and error tolerance. The text presents Algorithm 1 for accurate gradient estimation in ODE solvers for free-form functions, involving adaptive step sizes and error tolerance during forward and backward passes. The solver performs numerical integration with varying step sizes based on error estimation, saves evaluation time points, and rebuilds the computation graph during the backward pass without adaptive searching. The algorithm for accurate gradient estimation in ODE solvers involves rebuilding the computation graph without adaptive searching and performing reverse-time integration during the backward pass. Memory consumption analysis shows significant reduction compared to naive solvers, especially with step-wise checkpoint methods. The algorithm for accurate gradient estimation in ODE solvers involves rebuilding the computation graph without adaptive searching and performing reverse-time integration during the backward pass. Memory consumption can be reduced by using step-wise checkpoint methods and restricting the form of f to invertible blocks, leading to more memory-efficient solvers. The algorithm for accurate gradient estimation in ODE solvers involves rebuilding the computation graph without adaptive searching and performing reverse-time integration during the backward pass. Memory consumption can be reduced by using step-wise checkpoint methods and restricting the form of f to invertible blocks, leading to more memory-efficient solvers. Bijective blocks can split input x into x1 and x2, with the output denoted as (y1, y2). Differentiable neural networks F and G are used, along with bijective function \u03c8(\u03b1, \u03b2) for different tasks, making it memory-efficient by not storing activations. Graph neural networks (GNNs) can be memory-efficient by applying different \u03c8 functions for tasks and not storing activations. GNNs are represented with nodes and edges, and can generally be represented in a message passing scheme. Differentiable functions parameterized by neural networks are used in GNNs. Graph neural networks (GNNs) utilize differentiable functions parameterized by neural networks for message passing, aggregation, and updating of node states. In the message passing stage, neighbor nodes send information to a specific node u through a function \u03c6(\u00b7). The node then aggregates messages from its neighbors using a permutation invariant operation like mean or sum. Finally, the node updates its state based on the original state and the aggregated messages using a function \u03b3(\u00b7). Graph neural networks (GNNs) use permutation invariant operations like mean or sum for message aggregation and updating node states. The graph ordinary differential equation (GODE) replaces discrete-time GNNs with a continuous-time approach, capturing non-linear functions and potentially outperforming discrete-layer models. GODE's asymptotic stability is linked to over-smoothing phenomena, and graph convolution is a form of Laplacian smoothing. The asymptotic stability of GODE is related to over-smoothing phenomena. Graph convolution is a special case of Laplacian smoothing, represented by Y = (I \u2212 \u03b3D \u22121/2LD\u22121/2 )X. The continuous smoothing process ensures all trajectories are close enough as time grows large, indicating asymptotic stability. The ODE is asymptotically stable if all eigenvalues of the normalized Laplacian are non-zero. This implies that as time grows large, all trajectories become close enough. Experiments show that with a large enough integration time T, nodes from different classes will have very similar features, leading to a drop in classification accuracy. The method was evaluated on various image and graph classification tasks, including CIFAR10, CIFAR100, bioinformatic and social network graph datasets, and citation networks. For graph classification tasks, datasets like MUTAG, PROTEINS, IMDB-BINARY, REDDIT-BINARY, Cora, CiteSeer, and PubMed were used without pre-processing. Transductive inference was performed for node classification tasks following a specific train-validation-test split. ResNet18 was modified for image classification tasks by converting it into a NODE model with a sequence of conv-bn-relu layers. GODE can be applied to any graph neural network for tasks on graph datasets. GODE can be easily applied to various graph neural network architectures like GCN, GAT, ChebNet, and GIN for tasks on graph datasets. The function f in GODE can be replaced with different structures, making it easily generalized to existing models. The study compared different depths of layers in GNN models like GCN, ChebNet, and GAT for graph classification and node classification tasks. Various hyperparameters were set for each model, and the mean and variance of accuracy were calculated for 10 runs. For GNN models like GCN, ChebNet, and GAT, different depths of layers were tested along with various hyperparameters. Direct back-propagation showed higher accuracy compared to the adjoint method in both tasks. The instability of the adjoint method was attributed to the reverse-time ODE. Our training method, back-propagation, consistently outperforms the adjoint method on image classification tasks and benchmark graph datasets. NODE18, with the same parameters as ResNet18, outperforms deeper networks like ResNet101. Additionally, our method shows robustness to ODE solvers of different orders, providing equivalent model depth adjustments during inference without re-training. Our method supports NODE and GODE models with free-form functions, allowing for easy generalization of bijective blocks. Different \u03c8 mappings can be used, with results for various \u03c8 reported in Table 3. The study demonstrates the effectiveness of GODE models over discrete-layer models in various tasks, with different \u03c8 functions yielding similar results. Additionally, the continuous-time model is highlighted as more crucial than the coupling function \u03c8, with lower memory cost validated. Results for different models on graph classification tasks are summarized in Table 4, including GCN, ChebNet, and GIN structures. The study compares GODE models with discrete-layer models on graph classification tasks, showing GODE models perform significantly better. Integration time in NODE and GODE models affects information gathering and over-smoothing issues. The study compares GODE models with discrete-layer models on graph classification tasks, showing GODE models perform significantly better. Integration time affects information gathering and over-smoothing issues. GODE enables continuous diffusion process modeling on graphs with a memory-efficient back-propagation method for gradient determination. The paper addresses the fundamental problem of gradient estimation for NODE and improves accuracy on benchmark tasks. Our paper improves accuracy on benchmark tasks for NODE models, making them comparable to state-of-the-art discrete layer models. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with important modifications for bijective blocks. The paper introduces modifications to bijective blocks, allowing for a family of blocks with different functions. A parameter state checkpoint method is proposed to enable multiple calls to bijective blocks while maintaining accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch. Memory consumption is reduced by keeping only necessary outputs in the forward function. The algorithm is detailed in Algo. 2, demonstrating the effectiveness of the bijective blocks. The paper introduces modifications to bijective blocks for memory efficiency. Outputs y1, y2 are kept, and unnecessary variables are deleted. The bijective block is shown to be memory efficient in training a GODE model. Memory consumption comparison between memory-efficient and conventional methods is demonstrated in Table 2. The paper presents a memory-efficient approach for bijective blocks in training a GODE model. Memory consumption comparison between conventional and memory-efficient methods shows significant reduction in memory usage. The paper introduces a memory-efficient method for bijective blocks in GODE model training, reducing memory usage significantly compared to conventional methods. The approach involves computing bijective blocks delete computation graphs generated by F and G, and proving stability conditions for ODEs in both forward and reverse time. The paper presents a memory-efficient method for bijective blocks in GODE model training, reducing memory usage significantly. The stability conditions for ODEs in both forward and reverse time are proven by ensuring the eigenvalues of J have a non-positive real part. Theorem 1 states that a bijective block with defined forward and reverse mappings is a bijective mapping. The proof involves showing the mapping is both injective and surjective. The paper introduces a memory-efficient method for bijective blocks in GODE model training, reducing memory usage significantly. The stability conditions for ODEs in both forward and reverse time are proven by ensuring the eigenvalues of J have a non-positive real part. Theorem 1 establishes that a bijective block with defined forward and reverse mappings is a bijective mapping, shown through injective and surjective properties. The computation graph is used to derive gradients in a neural-ODE model optimization perspective, extending from continuous to discrete cases. In this section, the gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. Notations include z(t) for hidden states, \u03b8 for parameters, x for input, y for target, and \u0177 for predicted output. The loss function J(\u0177, y) is defined, and the training process is formulated as an optimization problem for ODE blocks. The forward pass is defined as \u0177, with the loss function J(\u0177, y) formulated as an optimization problem for ODE blocks. The Lagrangian Multiplier Method is used to solve the problem, with Karush-Kuhn-Tucker (KKT) conditions being necessary for optimality. The derivative w.r.t. \u03bb is derived at the optimal point using calculus of variation. The KKT condition is used to derive results for the optimal point, where the derivative w.r.t. \u03bb is set to zero. By considering continuous and differentiable perturbations, the conditions for Leibniz integral rule are checked. Moving from continuous to discrete cases, the ODE condition transforms into a finite sum representation."
}