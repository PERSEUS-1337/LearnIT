{
    "title": "rygDeZqap7",
    "content": "Natural language understanding research has shifted towards complex Machine Learning and Deep Learning algorithms, which often outperform simpler models. To address the challenge of limited labeled data availability, a methodology for extending training datasets and training data-hungry models using weak supervision is proposed. This approach is applied to biomedical relation extraction, a task crucial for drug discovery. Small-scale experiments show consistent performance enhancements of an LSTM network, comparable to hand-labeled data. Optimal settings for applying weak supervision are discussed. The amount of scientific papers in the biomedical field is increasing, containing important information in unstructured text. Extracting this information and storing it in a knowledge base can impact tasks like drug design. Efforts have been made to automate Information Extraction due to the labor-intensive nature of manual annotation. Efforts have been made to automate Information Extraction in the biomedical field to address the labor-intensive nature of manual annotation. This work focuses on automating semantic triple extraction from biomedical abstracts, specifically targeting Regulations (CPR) and Chemically Induced Diseases (CID) for drug design and safety purposes. The goal is to enable researchers to filter or select chemical substances with specific properties more efficiently. Automating semantic triple extraction from biomedical abstracts aims to enable researchers to filter or select chemical substances with specific properties more efficiently. The focus is on relation extraction, which involves identifying entities of interest and building a classifier to recognize target relationships. An increase in the learning algorithm's capacity should be balanced with an appropriate increase in the training dataset size to address the complexity of the task. Annotating training datasets for this task is labor-intensive, so a new methodology is proposed to tackle this issue. The methodology proposed for relation extraction in biomedical abstracts involves training base learners on a small labeled dataset to predict labels for a larger unlabeled dataset. Weak supervision is used to derive labels for the unlabeled set, and a meta-learner is trained using this weak supervision. This approach aims to address the labor-intensive process of annotating training datasets for relation extraction. The methodology for relation extraction in biomedical text involves training base learners on a small labeled dataset and using weak supervision to label a larger unlabeled set. A strong meta-learner is then trained using this weak supervision. The approach aims to reduce the labor-intensive process of annotating training datasets for relation extraction. The literature covers information extraction, relation extraction from biomedical text, and semi-supervised learning methods. Information extraction can be fully-, semi-, or unsupervised. Fully-supervised methods require labeled examples, while unsupervised methods like Open Information Extraction BID4 do not use training data. Semi-supervised methods, such as bootstrapping algorithms like DIRPE BID6, leverage both labeled and unlabeled data. Other approaches include Snowball BID0, KnowItAll BID11, and TextRunner BID4. Distant supervision is a method for generating weak labels for unlabeled data in relation extraction tasks using Knowledge Bases (KB) instead of pre-trained classifiers. This approach has been beneficial for large-scale datasets as it eliminates the need for human annotation. Our work focuses on creating weak labels using Knowledge Bases (KB) instead of pre-trained classifiers for large-scale datasets in biomedical relation extraction. This approach eliminates the need for human annotation and can benefit from algorithms like distant supervision. The BioCreative competitions have motivated research in this area, with recent studies showing that extending the training set using distant supervision can improve performance. The best performing team in a competition focused on identifying relations between Chemicals and Proteins (or Genes) on a sentence level implemented an ensemble of LSTM, CNN, and SVMs. Other approaches using solely Deep Neural Networks demonstrated overfitting problems, highlighting the importance of the lack of training data in this domain. The curr_chunk discusses the importance of using ensemble methods for improving generalization in Machine Learning models, particularly when Deep Neural Networks are involved. The focus is on combining ensemble techniques with semi-supervised learning to enhance performance, which has not been thoroughly explored before. This approach aims to address overfitting issues due to limited training data and leverage unlabeled data for improved generalization. Ensembles can enhance semi-supervised learning by providing multiple views and improving performance with less data. Co-training was the first system proposed for this purpose, using two independent learning algorithms with unlabeled data. Complete independence was found to be unnecessary for success in real-world settings. Recent research has shown that complete independence between learning algorithms is not necessary for success in real-world settings. Expert-defined lexicons and natural language processing systems are used to create noisy annotations and reduce noise in data samples. Tri-training extends co-training to three learners, improving performance in semi-supervised learning. The methodology described involves supervised methods trained on annotated examples, with extensions like Tri-training and Co-forest using multiple learners to generate weak labels without re-training the base learners. This allows for the use of all unlabeled data, unlike previous paradigms. Weak supervision and data programming have heavily influenced the development of the methodology described. Weak supervision involves training models using labels of questionable quality, while data programming focuses on creating training sets programmatically when no ground-truth labels are available. This approach allows for the use of all unlabeled data, unlike previous paradigms that only added a few examples annotated with high confidence for re-training. Weak supervision involves training models using labels of questionable quality, and data programming focuses on creating training sets programmatically when no ground-truth labels are available. This process includes defining weak supervision sources, encoding them into Labeling Functions (LF), applying LFs over unlabeled data points to derive a vote matrix \u039b, and denoising to derive weak labels close to the true labels. Data programming aims to derive weak labels close to true labels by applying Labeling Functions (LF) over unlabeled data points to create a vote matrix \u039b. A probabilistic graphical Generative Model (GM) is used as a denoiser, with trainable parameters for labeling accuracy. The GM structure, a hyperparameter, captures LF correlations and is trained to maximize the likelihood of observed votes occurring. Data programming maximizes the marginal log-likelihood of observed votes under the Generative Model, using predicted label distributions as weak labels. A noise-aware discriminative model is trained with generated labels, minimizing a noise-aware loss function. This methodology combines weak supervision and data programming for semi-supervised learning, leveraging a gold-labeled training set when ground-truth labels are insufficient. In contrast to scenarios with limited ground-truth labels, the approach advocates augmenting lower quality training data using machine learning models of lower complexity as weak supervision sources. This allows for scaling the dataset size without relying on heuristics or crowd-sourced labels. The method assumes a gold-labeled training set of limited size, and an unlabeled dataset drawn from the same distribution, enabling adaptation of an existing pipeline with minimal additional effort. In the context of data, a labeled training set D B of size m is assumed for task T, along with an unlabeled dataset D U of size M m from the same distribution. Other requirements include a validation set D V for hyperparameter tuning and a held-out test set D T for evaluation. K base learners are trained on D B to solve T, aiming to maximize individual performance and capture different data views. Various design choices in the relation extraction pipeline lead to the creation of 162 base learners, with important decisions like sentence pruning to remove irrelevant words. The relation extraction pipeline involves creating 162 base learners. Key design choices include sentence pruning to remove irrelevant words, using sequential features up to tri-grams, and considering different text representations. The relation extraction pipeline involves creating 162 base learners by employing different machine learning algorithms such as Logistic Regression, Support Vector Machines, Random Forest Classifiers, Long-Short Term Memory Networks, and Convolutional Neural Networks. Feature engineering steps were not applicable when using LSTMs and CNNs. Selecting a subset of base learners is necessary due to computational cost. After creating 162 base learners using various machine learning algorithms, including LSTMs and CNNs, feature engineering steps were not needed. To reduce computational cost, a subset of base learners is selected based on performance and diversity. A performance threshold is set above random guess baseline to include diverse classifiers with acceptable accuracy levels. To select diverse classifiers, a performance threshold is set above random guess baseline. A similarity-based clustering method is used to pick the most representative base learners. The number of clusters is determined using the silhouette score coefficient. The selected base learners are used to predict labels for a new dataset. The silhouette score coefficient is used to determine the number of clusters and select base learners closest to cluster centroids. A denoiser is then used to reduce the vote matrix into weak labels, with hyperparameters selected using a validation dataset. Different denoisers, including Majority Vote and Average Vote, are considered to unify the label matrix. Finally, a discriminative model is used in the last step. In practice, when training the meta-learner with weak supervision, label quality is traded for quantity. High-capacity models like Deep Neural Networks are used as meta-learners to learn their own features and build a more accurate representation with a larger, albeit noisy, training dataset. The experiments are conducted using part of the functionality of Snorkel, a framework designed for relation extraction with data programming and weak supervision. In experiments using Snorkel, a framework for relation extraction with weak supervision, high-capacity models like Deep Neural Networks learn features from a larger, noisy training dataset. The BioCreative CHEMPROT and CDR datasets are used, with three gold-labeled datasets and a held-out test set. The original training and development sets are merged and shuffled to create training, validation, and unlabelled datasets. The original training and development sets are used to create three datasets: one for training base learners, one for validation and hyperparameter selection, and the remaining documents as unlabelled data. This ensures no bias in document selection and all documents undergo the same pre-processing steps. This controlled approach helps in better controlling the algorithm's results. The datasets contain manually annotated entities for better control of the algorithm's results. SpaCy is used for text pre-processing tasks like sentence splitting and tokenization. Named Entity Tags are required for Candidate Extraction. Relationship candidates are identified within the text. Cross-sentence relationships are not considered. In the context of text pre-processing tasks using SpaCy, the curr_chunk discusses candidate extraction and relationship classification using Snorkel. Entities of interest are replaced with tokens for prediction, and a bi-directional LSTM network is used for analysis. In the context of text pre-processing tasks using SpaCy, candidate extraction and relationship classification are discussed using Snorkel. Entities of interest are replaced with tokens for prediction, and a bi-directional LSTM network is used for analysis. The research questions aim to enhance biomedical relation extraction with Machine Learning classifiers. Different hyperparameter settings are explored, including dropout values and training epochs based on the validation dataset. More details on RQ1 and RQ2 are discussed in Subsections 6.1 and 6.2. The research aims to enhance biomedical relation extraction using Machine Learning classifiers as weak supervision sources. The optimal setting for using weak supervision is explored, with theoretical evidence suggesting that adding weakly labeled data can improve the performance of the meta-learner. As the amount of weakly labeled data increases, the performance of the meta-learner is expected to improve quasi-linearly compared to using ground-truth labels. The research explores using Machine Learning classifiers as weak supervision sources to enhance biomedical relation extraction. It is crucial that the weak supervision sources have accuracy better than random guess, overlap, and capture diverse 'views' of the problem space. However, it is uncertain if there is a diverse and sufficiently large set of base learners that meet these conditions. This uncertainty impacts the usability of the methodology. The study investigates using Machine Learning classifiers for weak supervision in biomedical relation extraction. The key question is whether a diverse set of base learners trained on the same dataset meets the necessary conditions. To assess weak supervision's effectiveness, experiments are conducted with different setups, comparing the performance of the meta-learner trained on various modes. The optimal number of base learners is a challenging decision, as adding more may compromise performance. The study explores the use of Machine Learning classifiers for weak supervision in biomedical relation extraction. It discusses the challenge of selecting the optimal number of base learners and the importance of the denoising component in dictating the quality of weak labels used for training. Different denoising methods are evaluated, producing binary or marginal weak labels with varying distributions. An error analysis is conducted to assess the results. The denoiser can generate binary or marginal weak labels with varying distributions. An error analysis is conducted to assess their impact on training and final performance. The study answers RQ1 on using supervised machine learning classifiers as weak classifiers and RQ2 on the optimal weak supervision setting. Base learners are selected using a specific strategy, experimenting with different numbers and reporting performance results. The study compares performance using weak supervision with base learners and denoisers generating weak labels. Training the meta-learner with weak labels shows improved performance compared to using fewer gold labels. Weak supervision can achieve comparable performance to full supervision. Using weak supervision to augment training data can achieve comparable performance to full supervision, with some cases even showing slightly better results. However, differences are minor and not statistically significant due to high variance in meta-learners' performance. Under-sampled training sets in weak supervision were larger, as undersampling was based on weak labels. Simple Majority Vote sometimes outperforms the meta-learner, but this does not diminish the significance of the results. The study found that undersampling based on weak labels led to comparable performance to full supervision. Simple Majority Vote occasionally outperformed the meta-learner, but this did not diminish the significance of the results. Visualizing the learning curves showed an upward trend, with statistically significant results. The F1 score on the training set was consistently higher than the test score, indicating high variance in the meta-learner. The study indicates statistically significant results. The F1 score on the training set is consistently higher than the test score, suggesting overfitting. Additional training data may improve the meta-learner's performance. The number of base learners affects the F1 score, with weak Majority Vote labels performing the lowest. The Generative model weak marginals show inconsistent results. The F1 score of weak Majority Vote labels for 5 learners is the lowest. The Generative model weak marginals show inconsistent results. The meta-learner's performance improves with more than 10 base learners. Using Average Marginals yields the best performance. Generative Model marginals also improve performance compared to Majority Vote weak labels. GM marginals depend on hyperparameters chosen based on F1 score validation. The Generative Model marginals improve performance compared to Majority Vote weak labels, except for one exception. GM marginals depend on hyperparameters chosen based on F1 score validation. Marginal weak labels enhance the meta-learner's performance over binary labels. Majority Vote weak labels always perform worse than Average marginals without hyperparameter tuning. Generative Model tends to produce marginals following a U-shaped distribution. The Generative Model marginals, following a U-shaped distribution, outperform Majority Vote weak labels without hyperparameter tuning. Error analysis shows that Average Vote labels have higher quality, with misclassified labels closer to 0.5. F1 score is deemed unsuitable for evaluating weak labels. Training loss and validation scores are depicted in FIG1 as LSTM is trained for more epochs. The training loss and validation scores change as the LSTM is trained for more epochs, showing that using marginal labels results in higher training error. The model starts predicting binary labels accurately after a few epochs, despite a delay with noisy-labeled MV weak labels. Training with marginal labels is akin to a regression problem, where the model is penalized for not predicting the exact number. The distributions of predicted logits on unseen examples become more spread with training marginals. In practice, the model is trained to predict an exact number and penalized for any failures. The predicted logits become more spread as the training marginals become more uniform, resembling regression instead of classification. Efforts are made to apply this methodology on the CPR task, expanding datasets with weakly labeled data. The meta-learner's performance decreases with the addition of weak labels, indicating issues with data quality or weak label generation. Predicted class imbalances are observed in outgoing citations compared to the original dataset. The meta-learner's performance decreases with the addition of weak labels, indicating issues with data quality or weak label generation. Class imbalances are observed in outgoing citations compared to the original dataset, highlighting different dataset distributions. Visualization using t-SNE algorithm shows that the new dataset is unsuitable for the intended use case, emphasizing the importance of constructing appropriate unlabeled datasets. Weak supervision can enhance the performance of complex models like deep neural networks. The proposed methodology utilizes weak supervision to enhance the performance of complex models like deep neural networks by utilizing both unlabeled data and multiple base learners. It shifts human effort from hand-labeling examples to feature engineering and construction of diverse learners, making it practically feasible for the task at hand. The requirement is that unlabeled data should be from the same domain as labeled data for base learners to generalize and perform adequately. The methodology shifts human effort from hand-labeling examples to feature engineering and constructing diverse learners. It allows for scaling training datasets while consistently improving performance over supervised learning. The same pipeline can be reused for similar tasks with appropriate datasets, eliminating the need for repeated hand-labeling of large datasets. Further exploration is needed to construct a large enough unlabeled dataset for experimentation. Further exploration is crucial to construct a large enough unlabeled dataset for experimentation, which could improve metalearner performance and draw stronger conclusions on research questions. Preliminary experiments show that collecting an appropriate unlabeled dataset is challenging, along with defining what is considered \"appropriate\". Using weak supervision, it is challenging to collect an appropriate unlabeled dataset and define what is considered \"appropriate\". Semi-supervised algorithms should not assume the existence of such a dataset. It is important to find a more suitable metric than the F1 score for evaluating weak labels. Without a proper metric, conclusions cannot be drawn directly from weak labels without an additional step of training the meta-learner. Selecting optimal hyperparameters for the Generative Model could significantly impact performance. Further investigation areas include experimenting with the meta-learner and defining a better selection method. Further investigation areas for improving the Generative Model include experimenting with the meta-learner and defining a more appropriate selection method for the Base Learners. It would also be interesting to explore how the system would perform if Base Learners abstained from voting on uncertain examples, potentially providing a modeling advantage compared to unweighted methods."
}