{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features to improve CNN robustness. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. EdgeGANRob is compared to a simplified version called EdgeNetRob for further insights. To enhance CNN robustness, a robust edge detection approach called Robust Canny is proposed. EdgeGANRob is compared to a simplified version, EdgeNetRob, which boosts model robustness but reduces clean model accuracy. EdgeGANRob improves clean model accuracy without sacrificing robustness benefits. Extensive experiments demonstrate EdgeGANRob's resilience in various learning tasks. Convolutional neural networks (CNNs) have shown state-of-the-art performance but are vulnerable to adversarial examples and data poisoning attacks. Recent studies indicate that CNNs struggle to generalize due to learning surface statistical regularities instead of high-level abstractions. Recent studies show that CNNs struggle to generalize due to learning surface statistical regularities instead of high-level abstractions. This leads to a lack of robustness under distribution shifting. To improve CNN robustness, researchers are exploring the vulnerability causes. For instance, Ilyas et al. (2019) attribute adversarial examples to non-robust but highly-predictive features. They propose training classifiers only on \"robust features\" that are insensitive to perturbations. Additionally, human recognition relies on global object shapes rather than local patterns, while CNNs are biased towards local patterns. Recent studies show that CNNs struggle to generalize due to learning surface statistical regularities instead of high-level abstractions. To improve CNN robustness, researchers are exploring the causes of vulnerability. For instance, training classifiers only on \"robust features\" that are insensitive to perturbations is proposed. Human recognition relies on global object shapes, while CNNs are biased towards local patterns. This bias potentially contributes to CNN's vulnerability to adversarial examples, distribution shifting, and backdoor attacks. The proposed pipeline aims to improve CNN robustness by focusing on global shape structure, specifically edges. Using edges as a shape representation can enhance model performance and make it more resilient to adversarial attacks and distribution shifts. The paper proposes EdgeGANRob, a new approach to enhance CNN robustness by leveraging structural information in images, specifically edges. This approach, illustrated in Figure 1, involves a two-stage procedure called EdgeNetRob, which extracts structural information by detecting edges and trains the classifier on them. This forces CNNs to base predictions solely on shape information, improving resilience to adversarial attacks and distribution shifts. The EdgeNetRob procedure extracts structural information by detecting edges and trains the classifier solely on shape information, improving CNNs' robustness. A robust edge detection algorithm, Robust Canny, significantly enhances EdgeNetRob's robustness against attacks, outperforming adversarial retraining methods. The EdgeNetRob procedure improves CNNs' robustness by extracting structural information through edge detection. Robust Canny enhances EdgeNetRob's robustness, outperforming adversarial retraining methods. EdgeGANRob refills texture/colors based on edge images before classification, improving overall robustness against multiple tasks simultaneously. To enhance CNNs' robustness against various tasks, an approach is proposed that extracts edge/structure information from input images and reconstructs them using GAN. A robust edge detection method, Robust Canny, is introduced to mitigate attacks on the defense algorithm. The effectiveness of the inpainting GAN in EdgeGANRob is demonstrated through evaluation on adversarial attacks, distribution shifting, and backdoor attacks, showing significant improvements in robustness. The text discusses the evaluation of EdgeNetRob and EdgeGANRob in various tasks such as adversarial attacks, distribution shifting, and backdoor attacks. It highlights the importance of robust defense methods against adaptive attacks and the need for evaluation against customized white-box and strong adaptive attacks. Defense methods should be evaluated against customized white-box attacks and strong adaptive attacks. Distribution shifting is common in real-world applications, with CNNs tending to learn superficial statistical cues. Wang et al. (2019a) proposes a method to robustify CNNs by penalizing the predictive power of local representations. Benchmark datasets are proposed by Hendrycks and Dietterich (2019) for evaluating model robustness under common perturbations. Backdoor attacks work by injecting malicious code. Recent work has highlighted the importance of robust visual features in model defense against backdoor attacks. Geirhos et al. (2019) and Baker et al. (2018) found that CNNs rely more on textures than global shape structure for image recognition, while humans rely more on shape structure. Itazuri et al. (2019) proposes an approach to protect models from backdoor attacks using neuron pruning. In this work, a new classification pipeline called EdgeGANRob is proposed, which focuses on using edge features as robust features for image recognition. The method extracts edge/structure features from images to enhance robustness. The proposed method, EdgeGANRob, utilizes edge features for image classification. It involves extracting edge/structure features from images, reconstructing them using a GAN, and then feeding them into a classifier. The simplified backbone, EdgeNetRob, consists of two stages: edge map extraction and training a classifier on these maps. EdgeNetRob consists of two stages: extracting edge maps from images and training a classifier on these maps. The decision of CNN is solely based on edges, making it less sensitive to local textures. Despite its simplicity, EdgeNetRob degrades CNN performance on clean test data due to missing texture/color information. This led to the development of EdgeGANRob, which uses a generative model to refill texture/colors of the edge. EdgeGANRob was developed to refill texture/colors of edge images, improving clean accuracy. The robustness of edge-based classification systems depends on the edge detector used, motivating the proposal of a robust edge detection algorithm called Robust Canny. Most neural network-based edge detectors are non-robust, leading to low accuracy in recognition tasks. The development of EdgeGANRob aimed to enhance the accuracy of edge image texture/color refilling. To address the non-robust nature of neural network-based edge detectors, a robust edge detection algorithm called Robust Canny is proposed. This algorithm improves on traditional methods like Canny by truncating noisy pixels in its intermediate stages, enhancing its robustness. The proposed Robust Canny edge detector enhances traditional methods by truncating noisy pixels in its intermediate stages. It includes 6 stages: noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The Robust Canny edge detector enhances traditional methods by truncating noisy pixels in its intermediate stages. It includes 6 stages: noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The algorithm suppresses gradient pixels along the direction, applies double thresholding, and tracks edges using hysteresis. Noise masking is added after computing image gradients to mitigate perturbation noise. The Robust Canny edge detector improves traditional methods by truncating noisy pixels in intermediate stages. Parameters like \u03c3 and thresholds \u03b8 l , \u03b8 h affect robustness, with larger values leading to better robustness but potentially sacrificing clean accuracy. Careful parameter selection is crucial for a robust edge detector. Training a Generative Adversarial Network (GAN) in EdgeGANRob is described in detail in the experiment section. To create a robust edge detector, careful parameter selection is essential. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves two stages: first, training a conditional GAN using adversarial and feature matching losses, and second, fine-tuning for high accuracy in generated RGB images. In the second stage of training EdgeGANRob, the classifier is fine-tuned along with the GAN to achieve high accuracy in generating RGB images. The objective function includes the classification loss of generated images to improve realism. The method enhances robustness against adversarial, distribution shifting, and backdoor attacks by leveraging the invariant nature of edges. EdgeGANRob is designed to improve robustness against adversarial, distribution shifting, and backdoor attacks by focusing on the invariant nature of edges. In adversarial attacks, edges are less susceptible to small perturbations, while in distribution shifting, leveraging edge features can enhance generalization. Additionally, in backdoor attacks, the model is less likely to be tricked into predicting a specific class when a pattern is injected during testing. The proposed method, EdgeNetRob, aims to enhance robustness against backdoor attacks by removing malicious patterns through edge extraction. It is evaluated for its effectiveness in resisting adversarial attacks and distribution shifting. EdgeNetRob is considered a robust recognition method with unique advantages in certain scenarios. The proposed method, EdgeNetRob, focuses on enhancing robustness against backdoor attacks by utilizing edge extraction. It is evaluated for its effectiveness against adversarial attacks, distribution shifting, and performance on gender classification tasks using the CelebA dataset. The method is compared with EdgeGANRob and is considered a robust recognition method with unique advantages in specific scenarios. The study evaluates the EdgeNetRob method's robustness against adversarial attacks using standard perturbation budgets on Fashion MNIST and CelebA datasets. The evaluation includes white-box attacks using the BPDA attack and measures the method's effectiveness against adaptive attacks. The study evaluates the robustness of EdgeNetRob against white-box attacks using the BPDA attack. Three edge detection methods are compared: RCF, Canny, and Robust Canny, with results reported on Fashion MNIST in Table 1. For each edge detection method, a classifier is trained on the extracted edge maps. Results on Fashion MNIST and CelebA datasets are compared with the state-of-the-art baseline Adversarial training. EdgeNetRob and EdgeGANRob show a slight drop in clean accuracy but outperform adversarial training with = 8 in terms of clean accuracy. EdgeGANRob performs better overall. In Table 2, EdgeNetRob and EdgeGANRob exhibit a slight decrease in clean accuracy compared to the baseline model. However, they outperform adversarial training with = 8 in clean accuracy. EdgeGANRob shows higher clean accuracy than EdgeNetRob on the CelebA dataset, highlighting the importance of using GANs on complex datasets. Both EdgeNetRob and EdgeGANRob demonstrate robustness against strong adaptive attacks, with EdgeNetRob being more time-efficient as it does not require adversarial training. Test accuracy under different attack iterations is illustrated in Figure 3, and the method's generalization ability under distribution shifting is also evaluated. EdgeNetRob does not use adversarial training, making it more time-efficient. The method is tested for generalization ability under distribution shifting using perturbed Fashion MNIST and CelebA datasets with various transformations. Comparison with the state-of-the-art method PAR shows promising results, as illustrated in Table 3. EdgeNetRob and EdgeGANRob improve accuracy on negative color, radial kernel, and random kernel patterns compared to PAR. When tested on greyscale images, they maintain high accuracy. Edge features aid CNN generalization under distribution shifting and can defend against backdoor attacks by embedding invisible watermarks. Visual results are shown in Figure 4 for CelebA and Figure D in the appendix for Fashion MNIST. In the study, invisible watermark patterns are embedded into images to defend against backdoor attacks. Results on Fashion MNIST and CelebA datasets are compared with a baseline method. The poisoning ratio and attack pairs are specified, with test accuracy presented in tables. The embedding pattern successfully attacks the vanilla Net with high poisoning. The study introduced invisible watermark patterns to defend against backdoor attacks on images. Results on Fashion MNIST and CelebA datasets showed high poisoning accuracy. The embedding pattern successfully attacked the vanilla Net, while EdgeNetRob and EdgeGANRob had consistently low poisoning accuracy. The effect of the invisible watermark pattern could be removed by the edge detector. EdgeGANRob achieved better clean accuracy compared to EdgeNetRob, validating the benefit of inserting an inpainting GAN. Our method combines a robust edge feature extractor with a generative adversarial network to improve model robustness against backdoor attacks. Results show competitive performance in terms of adversarial robustness and generalization under distribution shifting. Using shape information is crucial for enhancing model robustness, indicating a promising direction for future research. Data pre-processing involves resizing images in CelebA to 128 \u00d7 128 using bicubic interpolation and using 10% of total images as test data. Our method utilizes shape information to enhance model robustness. Images in CelebA are resized to 128 \u00d7 128 using bicubic interpolation, with 10% used as test data. Data is normalized to [-1, 1]. Different models are used for Fashion-MNIST and CelebA datasets. Training involves stochastic gradient descent with momentum, PGD attacks, and CW attacks. Step sizes and distances vary for different attacks. CW attack evaluation involves 1,000 randomly sampled images due to computational complexity. For PGD attacks, step sizes and distances vary based on the attack type. CW attack evaluation involves randomly sampling 1,000 images. Hyper-parameters for Robust Canny are chosen using the validation set to balance robustness and accuracy. Fashion MNIST and CelebA datasets have specific parameter settings for Robust Canny. Athalye et al. (2018) demonstrate the challenge of backpropagating gradients through non-differentiable transformations in white-box attack scenarios. In a white-box attack scenario, backpropagating gradients through non-differentiable transformations is a challenge. Athalye et al. (2018) introduced the Backward Pass Differentiable Approximation (BPDA) technique to construct adversarial examples by replacing non-differentiable transformations with differentiable approximations. To strengthen attacks, a differentiable approximation of the Robust Canny algorithm is found by breaking the transformation into two stages: C1 (steps 1-3) and C2 (steps 4-6). The Robust Canny algorithm is broken down into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 is a non-differentiable operation where the output is a masked version of the input. To make R-Canny differentiable for BPDA, the mask is assumed to be constant. Test accuracy changes are shown under radial and random mask transformations in Figure A. In Figure A, test accuracy changes are shown under radial and random mask transformations with varying parameters. Additional visualization results for CelebA under distribution shifting are shown in Figure B and ??, while qualitative results for backdoor attacks on Fashion MNIST are displayed in Figure D. EdgeNetRob can slightly remove poisoning patterns and generated images do not share similar patterns."
}