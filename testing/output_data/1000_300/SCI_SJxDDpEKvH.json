{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging without supervision. A non-statistical framework is proposed, based on identifying a modular organization of the network through counterfactual manipulations. Modularity between groups of channels is achieved to some extent on various generative models, enabling targeted interventions on image datasets for applications like style transfer and assessing robustness to contextual changes. Deep generative models have been successful in designing realistic images in various domains using Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE). Efforts have been made to produce disentangled latent representations for controlling interpretable properties of images. Efforts have been made to create generative models with disentangled latent representations for controlling interpretable properties of images. However, these models may not be mechanistic or causal in attributing interpretable properties to specific parts of the network architecture. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds. This ability aligns with human representational capabilities and the modular organization of the visual system. In this paper, a causal framework is proposed to explore modularity in generative models, aiming to leverage deep architectures for extrapolations. The framework relates to the principle of Independent Mechanisms, emphasizing the need for interpretable computations in successive layers. The paper proposes a causal framework to explore modularity in generative models, emphasizing the principle of Independent Mechanisms. It focuses on the effect of direct interventions in the network, allowing for modifications of individual causal mechanisms without influencing each other. Causality is used to assess counterfactuals and the role of specific internal variables in deep generative models. The paper explores the use of counterfactuals in assessing the role of internal variables in deep generative models. It focuses on disentanglement in a causal framework and demonstrates how VAEs and GANs exhibit modularity in their hidden units, allowing for counterfactual editing of generated images. This work is related to the interpretability of generative models, which requires a different approach compared to discriminative architectures. The interpretability of convolutional neural networks has been extensively studied in discriminative architectures. Generative models, on the other hand, require a different approach due to the high-dimensional downstream effects of changes in intermediate representations. Various works address disentanglement of latent variables, focusing on extrinsic and intrinsic disentanglement to uncover the internal organization of networks. This concept is related to a framework proposed by Bau et al. (2018) based on interventions on internal variables. Our approach, introduced independently, focuses on uncovering the internal organization of networks through arbitrary continuous transformations, contrasting with other proposals that require semantic information or strong requirements of representation theory. Suter et al. (2018) also take an interventional approach to disentanglement in a graphical model setting. The theory section introduces a framework for disentanglement in generative models, focusing on extrinsic disentanglement and causal concepts. It presents a general framework for mapping latent space to data points in Euclidean space. For mathematical details, readers can refer to the appendix. The curr_chunk discusses the mapping of data points in Euclidean space Y to a representation space R using a latent representation M. It introduces a Causal Generative Model (CGM) implemented by a non-recurrent neural network. The curr_chunk introduces the Causal Generative Model (CGM), which implements the mapping of data points to a latent representation through a series of operations. It involves choosing endogenous variables represented by nodes in a causal graph to compute the mapping. The variables can include output activation maps of hidden layers in a neural network. The Causal Generative Model (CGM) defines endogenous variables as output activation maps of hidden layers in a neural network. The model ensures independence of mechanisms and uses mild conditions to guarantee invertibility, defining the internal representation of the network. The dimensions of variables are constrained to subsets of smaller dimension, allowing for the definition of counterfactuals in the network. The CGM framework allows defining counterfactuals in the network, inducing a transformation of the generative model's output. Faithfulness of a counterfactual mapping is introduced to ensure that interventions on internal variables align with the original model. Non-faithful counterfactuals may not produce outputs consistent with the original model. In the context of generative models, non-faithful counterfactuals can lead to outputs that deviate from the original model's distribution, potentially causing artifactual results or enabling extrapolation to unseen data. The idea of disentangled representation suggests that individual latent variables encode real-world transformations sparsely, driving supervised approaches to manipulating relevant transformations explicitly. Real-world transformations are key in supervised approaches to disentangling representations. Unsupervised learning methods aim to learn these transformations from unlabeled data. State-of-the-art approaches encode transformations through changes in latent factors, enforcing conditional independence between them. However, this statistical approach faces challenges due to constraints on the prior distribution of latent variables. The statistical approach to disentangling latent factors faces challenges due to independence constraints on the prior distribution, making it difficult to specify a disentangled representation for real-world data. Current unsupervised approaches are limited to synthetic datasets and struggle with complex real-world data. Approaches to disentangling latent factors face challenges with real-world data. Disentangled generative models struggle with complex datasets, showing lower visual quality compared to non-disentangled models. A non-statistical definition of disentanglement involves transformations on the data manifold and latent space. The notion of extrinsic disentanglement involves transformations on the latent space that act on individual variables, allowing other variables to encode different properties. This concept follows the causal principle of independent mechanisms and is agnostic to subjective choices or statistical notions of independence. The functional definition of extrinsic disentanglement involves transformations in the latent space to uncover statistically related properties that are disentangled according to the definition. This extends the concept of independence of mechanisms beyond statistical independence. The graphical model variables may not be statistically independent due to common latent causes, but can still be intervened on independently. Disentanglement is extended to allow transformations of internal variables. Modularity is defined as a structural property of the internal representation space. Modularity is defined as a structural property of the internal representation, allowing for arbitrary disentangled transformations when endogenous variables do not share common latent ancestors. This framework links functional disentanglement with the intrinsic property of the trained network. The framework is based on a functional definition of disentanglement and involves partitioning the intermediate representation into modules to create a disentangled representation. This approach requires introducing a partition of the latent variables into modules, which was not considered in classical approaches to disentanglement. Our framework introduces the concept of grouping neurons into modules to achieve a disentangled representation, which was not previously considered in classical approaches. This modular structure allows for independent interventions on each group, leading to a broad class of disentangled transformations. Propositions 1 and 2 suggest that finding a modular structure in a network enables various disentangled transformations. Transformations within the input domain are considered good candidates for disentanglement. Counterfactual interventions define transformations implicitly. By assigning a constant value to a subset of endogenous variables, faithful counterfactuals can be achieved. Sampling from the joint marginal distribution of these variables is used to avoid characterizing them explicitly. This approach is illustrated using a standard feed-forward multilayer neural network with endogenous variables being the output activations of channels in a specific layer. The hybridization procedure involves sampling from the joint marginal distribution of endogenous variables in a neural network to generate original examples of output. By choosing a subset of channels as endogenous variables, different aspects of generated images can be encoded in tuples v(z1) and v(z2). This modular structure allows for disentangled transformations within the network. The counterfactual hybridization framework assesses how a module E influences the generator's output by generating pairs of vectors (z1, z2) independently from the latent space. Hybrid outputs are then collected to estimate an influence map through mean absolute effect calculation. The approach involves generating pairs from the latent space, calculating unit-level causal effects, and averaging the results over different interventions. The influence map is then obtained by averaging across color channels. The approach involves estimating individual influence maps for each output channel of convolutional layers in a network. These influence maps are then grouped based on similarity to select subsets to intervene on, addressing the challenge of networks with a large number of units or channels per layer. The approach involves estimating influence maps for each output channel of convolutional layers in a network. These maps are grouped by similarity to define modules at a coarser scale, showing functional segregation of channels in a VAE trained on the CelebA face dataset. This supports the idea of grouping channels into modules dedicated to specific aspects of the output in an unsupervised manner through clustering based on their influence maps. The text discusses grouping channels into modules based on their influence maps in a VAE trained on the CelebA face dataset. This is achieved through clustering channels using their EIMs as feature vectors, followed by NMF algorithm to obtain cluster template patterns and weights representing their contribution to individual maps. The text discusses using the NMF algorithm to obtain cluster template patterns and weights representing their contribution to individual maps, justified by its success in isolating meaningful parts of images. The approach will be compared to the classical k-means clustering algorithm, and a toy generative model will be introduced to further justify the NMF based approach. A toy generative model is introduced, involving a neural network with hidden layers and random choice for model parameters. The hidden layer in the neural network is partitioned into modules, each with specific coefficients sampled from a distribution. This setup ensures that different areas of the image are influenced by different modules, leading to a disentangled representation. The hidden layer in the neural network is partitioned into modules with specific coefficients, ensuring a disentangled representation. NMF is used on the influence map matrix for endogenous variables to generate a binary matrix summarizing their influences. Sliding window application enforces similarity between influence maps in the same module. Investigating modularity of generative models on the CelebA dataset using basic architectures like \u03b2-VAE. Model 1, favoring low-rank matrix factorization, was investigated on the CelebFaces Attributes Dataset (CelebA). The number of clusters was set to 3, leading to interpretable cluster templates for background, face, and hair. The number of clusters was set to 3, leading to highly interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed this choice, showing consistency above 90% for 3 clusters and dropping considerably for 4 clusters. The clustering analysis showed that 3 clusters were a reasonable choice, with high consistency (>90%) and better performance than k-means. The cosine similarity between templates also supported this, with an average similarity of .9 achieved with 3 clusters. Influence maps further confirmed the observation that some maps may spread over the image. The clustering analysis revealed that 3 clusters were optimal, with a high consistency (>90%) and superior performance compared to k-means. A similarity of .9 was achieved between templates, with influence maps showing that some maps may spread over different image locations. Additionally, applying the hybridization procedure to the resulting 3 modules obtained by clustering led to a replacement of features without introducing discontinuities in the overall image structure. The Original 2 samples' features are inserted into the Original 1 image while preserving the hair. Further research is needed to explore better extrinsic disentanglement for models like GANs, which typically outperform VAEs in sample quality. Results were reproduced in the tensorlayer DCGAN implementation, indicating the approach's applicability to different models. Our approach was successfully applied to different models, including the official tensorlayer DCGAN and a pretrained Boundary Equilibrium GAN (BEGAN). The BEGAN model, known for its high-quality face images, allowed us to test our hypothesis with minimal modifications to the computational graph. Increasing the number of layers in BEGAN required interventions on channels from the same cluster to obtain counterfactuals with noticeable effects. The architecture of BEGAN was used to test a hypothesis with minimal modifications to the computational graph. Interventions on channels from the same cluster in two successive layers were needed to obtain counterfactuals with noticeable effects. Selective transfer of features from Original 2 to Original 1 was observed by intervening on layers 5 and 6. The counterfactual images showed clear hair transfer and encoding of different face features in separate modules. The counterfactual images from the BEGAN architecture showed clear hair transfer and encoding of various face features in separate modules. The hybridization procedure had a mild impact on image quality, as evaluated using the Frechet Inception Distance. The approach was tested on the BigGAN-deep architecture, pretrained on the ImageNet dataset, to scale to high-resolution generative models and complex image datasets. The BigGAN-deep architecture, pretrained on the ImageNet dataset, consists of 12 Gblocks with 4 convolutional layers each. It can generate hybrids by mixing features of different classes and produce high-quality counterfactuals with modified backgrounds while keeping similar foreground objects. Intervening on two successive layers within a Gblock was found to be more effective for generating counterfactuals. The study demonstrates the ability to create high-quality counterfactual images with modified backgrounds while maintaining similar foreground objects. It also explores how these generated images can be used to enhance classifier robustness to contextual changes. The comparison of various state-of-the-art pretrained classifiers on Tensorflow-hub shows the recognition rates of original classes like teddy bear or koala. The study compares different state-of-the-art pretrained classifiers on Tensorflow-hub to recognize original classes like teddy bear or koala. The recognition rates tend to increase with layer depth, with Inception resnet performing better at intermediate blocks 5-6. Different classifiers rely on different aspects of image content for decision-making. The study introduces a mathematical definition of disentanglement and applies it to characterize the representation encoded by different groups of channels in deep generative architectures. Evidence for interpretable modules of internal variables in generative models is found, opening up possibilities for better understanding complex generative architectures and applications such as style transfer and assessing object recognition systems' robustness to contextual changes. The research explores generative architectures like style transfer and assessing object recognition systems' robustness. It aims to enhance interpretability and expand the use of deep neural networks for new tasks, potentially leading to more sustainable AI research in the future. The approach involves treating trained generator architectures as mechanistic models that can be manipulated independently, with a focus on using structural causal models for mathematical representation. Trained generator architectures can be used as mechanistic models, represented by structural causal models using structural equations. These equations assign values to variables based on other variables in the system, including exogenous influences. Structural equations remain valid even after interventions, allowing for modeling operations in computational graphs of neural networks. Trained generator architectures can be represented by structural causal models using structural equations. These equations assign values to variables based on other variables in the system, remaining valid even after interventions. They can model operations in computational graphs of neural networks, depicting SCMs made of interdependent modules represented by a directed acyclic graph. A Causal Generative Model (CGM) captures computational relations between input latent variables, generator's output, and endogenous variables forming an intermediate representation. A Causal Generative Model (CGM) comprises a directed acyclic graph and a set of deterministic continuous structural equations that assign values to variables based on other variables in the system. The CGM aligns with the definition of a deterministic structural causal model by Pearl (2009) and reflects the structure of models encountered in practice. The definition of a deterministic structural causal model by Pearl (2009) is aligned with a Causal Generative Model (CGM), which includes specificities reflecting practical model structures. Variable assignments in CGMs may involve latent/exogenous variables in their right-hand side, allowing for modeling feed-forward networks with deterministic operations. This guarantees unambiguous assignments of endogenous variables once certain variables are chosen, facilitating the introduction of useful properties in generative networks. Generative networks involve unambiguous assignments of variables once certain variables are chosen. Latent and endogenous mappings are defined, with variables constrained to subsets of their euclidean space. The latent and endogenous mappings in generative networks are constrained to subsets of their euclidean space. These mappings are defined as proper embeddings, ensuring they are invertible. The output can be computed unambiguously from the inputs, with defined image sets constrained by the parameters of the model. The image sets of generative models are defined by the model parameters and can be difficult to characterize. The goal is for the image set to approximate the data distribution. Transformations must respect the topology of the image set, using embeddings for structure. Generative models' image sets are defined by model parameters and aim to approximate the data distribution. Transformations respecting the topology of the image set use embeddings for structure. For embedded CGMs, injectivity of the transformation is crucial, especially when the latent space is compact. Generative models with uniformly distributed latent variables are embedded CGMs if injective. VAEs' latent space, typically non-compact, can be restricted to compact intervals for better performance. Latent variables in many GANs are embedded CGMs if injective. Restricting VAEs' non-compact latent space to compact intervals can approximate the original CGM for most samples. The CGM framework allows defining counterfactuals in the network following Pearl (2014). The interventional CGM is obtained by replacing structural assignments for variables with specific assignments. Counterfactuals transform the output of the generative model and relate to disentanglement of internal variables. Intrinsic disentanglement in a CGM involves a transformation of endogenous variables with respect to a subset, allowing for different outputs based on latent variables. In a CGM, intrinsic disentanglement involves a transformation of endogenous variables with respect to a subset, allowing for different outputs based on latent variables. This relates to a causal interpretation of the generative model's structure and expresses robustness to perturbations of subsystems. Counterfactuals represent examples of such perturbations and may be disentangled given their faithfulness. The generative model's structure shows robustness to perturbations of subsystems, with counterfactuals representing such perturbations. The compactness of Z and the Hausdorff property of g M imply that g M is an embedding. Additionally, the injectivity of g M implies injectivity on their domains V M, making the respective g M 's embeddings. The equivalence between faithful and disentangled transformations is proven. The equivalence between faithful and disentangled transformations is proven by showing that if a transformation is disentangled, it is an endomorphism of Y M, and if a transformation is faithful, it is disentangled with respect to E in M. This is achieved by ensuring values in subsets are unambiguously assigned by non-overlapping latent variables, guaranteeing that T is an endomorphism of V M for any choice of endomorphism T E. The text discusses how the subsets of variables ensure that the transformation is well-defined and an endomorphism. It also highlights the disentangled representation achieved through modular subsets of endogenous variables. The choice of dimensions and sampling method guarantee injectivity and counterfactual hybridization outcomes. Additionally, the conditions on subsets and thresholding approach ensure a rank K binary factorization. The \u03b2-VAE architecture, similar to DCGAN, guarantees a rank K binary factorization of matrix B with indicator vectors for each V k. The uniqueness of this factorization is ensured by classical NMF identifiability results. Hyperparameters and architecture details are specified for both structures. The architecture used for the CelebA dataset is based on Berthelot et al. (2017) and consists of three blocks of convolutional layers with skip connections for image sharpness. The pretrained model from Tensorflow-hub is based on the BigGan-deep architecture by Brock et al. (2018) for 256x256 ImageNet without retraining. ResBlocks are used as building blocks for the generator with BatchNorm-ReLU-Conv Layers and skip connections. The architecture for the CelebA dataset is based on Berthelot et al. (2017) with three blocks of convolutional layers and skip connections for image sharpness. The pretrained model is based on the BigGan-deep architecture by Brock et al. (2018) for 256x256 ImageNet without retraining. ResBlocks are used as building blocks for the generator with BatchNorm-ReLU-Conv Layers and skip connections. Influence maps are generated by a VAE on the CelebA dataset, showing the variance and influence of perturbations on pixels. FID analysis of BEGAN hybrids measures the distance between different pairs of classes, normalized by the FID between real and generated data. The FID analysis of BEGAN hybrids shows that the hybrids have a small distance to the generated data and to each other, indicating visually plausible images. Entropy values for different modules and Gblocks show that poorer quality leads to larger entropy values. The probabilistic output for the 10 classes receiving top ranking across all hybrids normalized to provide a total probability of 1. Gblock 6 shows larger entropy values for modules with poorer quality, while hybrids based on interventions on Gblock number 4 have smaller entropy values. Results suggest that object texture in hybrids from Gblock 4 is key for the classifier's decision. Figure 15 shows a larger collection of hybrids for the BIGAN between classes \"cock\" and \"ostrich\", with interventions on different Gblocks. The NMF algorithm extracts modules from hybrids created by intervening on different Gblocks. Entropy is computed for the top-ranking classes across all hybrids, showing large entropy for the first module of Gblock 5. Table 3 displays the classification outcome of discriminative models for koala+teddy hybrids, assessing the robustness of classifiers. The experiment investigates discriminative models for koala+teddy hybrids to assess classifier robustness. The hybrids resemble a teddy bear in a koala context, emphasizing the importance of object recognition over contextual information. The nasnet large classifier shows greater robustness to context changes compared to others."
}