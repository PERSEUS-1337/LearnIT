{
    "title": "SJeQi1HKDH",
    "content": "In this work, the social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, leading to improved performance and diverse behaviors. This approach is inspired by cognition and animal studies. Interior Policy Differentiation (IPD) in Reinforcement Learning (RL) promotes performance improvement and diverse behaviors by solving tasks with distinct policies. Previous works have focused on encouraging behavioral diversity in RL through designing interactive environments with richness and diversity. In RL, behavioral diversity can be increased by designing rich environments or motivating agents to explore beyond maximizing rewards. Previous approaches include designing complex environments and maximizing novelty between policies. This work focuses on policy differentiation in RL to enhance agent diversity while maintaining task-solving abilities, drawing inspiration from animal social influence. In this work, the focus is on policy differentiation in RL to enhance agent diversity while maintaining task-solving abilities. The concept of social influence is drawn from animal society, where agents not only maximize rewards but also differentiate their actions to be unique from other agents. Social uniqueness motivation is implemented as a constrained optimization problem in the reinforcement learning paradigm. In this work, social influence is implemented as social uniqueness motivation in a constrained optimization problem in reinforcement learning. A novel method called Interior Policy Differentiation (IPD) is introduced to encourage agents to perform well in tasks while taking different actions from other agents. In this study, a novel method called Interior Policy Differentiation (IPD) is introduced to encourage agents to perform well in tasks while taking different actions from other agents. The method involves learning with social influence and introduces an additional constraint to motivate the target agent to perform well in tasks and take different actions from existing agents. The proposed method is benchmarked on locomotion tasks and shows the ability to learn diverse and well-behaved policies based on the Proximal Policy Optimization (PPO) algorithm. Additionally, intrinsic motivation methods like Variational Information Maximizing Exploration (VIME) and curiosity-driven methods are discussed to tackle sparse reward problems and encourage exploration in reinforcement learning algorithms. In VIME, an intrinsic reward term based on information gains is added to RL algorithms to encourage exploration. Curiosity-driven methods define intrinsic rewards based on prediction errors of neural networks. Random Network Distillation (RND) quantifies intrinsic reward by prediction differences between networks. Competitive Experience Replay (CER) uses two actors and a centralized critic to define intrinsic reward by state coincidence. The Task-Novelty Bisector (TNB) learning method, introduced by Zhang et al. (2019), aims to optimize the trade-off between external rewards and intrinsic rewards by updating the policy in the direction of the angular bisector of the gradients of the extrinsic and intrinsic objective functions. The TNB method optimizes the trade-off between external and intrinsic rewards by updating the policy in the direction of the angular bisector of the gradients. However, the foundation of joint optimization is not solid, requiring additional neural networks for evaluating novelty, leading to extra computation expenses. DPPO method by Heess et al. (2017) enables agents to learn complex skills in diverse environments, showcasing impressive and effective results in traveling terrains. The DPPO method allows agents to learn complex locomotion skills in challenging environments, showcasing impressive results. Different RL algorithms may converge to different policies for the same task, with policy gradient algorithms tending to converge to the same local optimum while off-policy and value-based algorithms learn sophisticated strategies. This paper focuses on learning different policies through a single algorithm and avoiding local optima. In this paper, the focus is on learning different policies through a single algorithm and avoiding local optima. The approach involves maintaining model uncertainty using an ensemble of deep neural networks to encourage behavioral diversity in RL. The learned policies are denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}, with a metric defined to measure the difference between policies. The metric should satisfy important properties such as identity and symmetry. The focus is on learning different policies through a single algorithm and avoiding local optima. Policies are denoted as \u03b8 i, with a metric defined to measure the difference between them. The Total Variance Divergence is used as a distance measure between policies in a metric space. The metric should satisfy properties like identity and symmetry. The approach involves maintaining model uncertainty using an ensemble of deep neural networks to encourage behavioral diversity in RL. In reinforcement learning, policies are represented as \u03b8 i and the Total Variance Divergence (D \u03c1 T V) is used as a metric to measure the difference between them. The goal is to maximize the uniqueness of a new policy by sampling from \u03c1(s) for Monte Carlo estimation, especially in continuous state cases where obtaining enough samples is challenging. In reinforcement learning, policies are represented as \u03b8 i and the Total Variance Divergence (D \u03c1 T V) is used as a metric to measure the difference between them. When facing continuous state cases, efficiently obtaining enough samples for Monte Carlo estimation becomes challenging. To improve sample efficiency, it is proposed to approximate \u03c1(s) using \u03c1(s|s \u223c \u03b8) under the condition that the domain of possible states is similar between different policies. This approximation holds true in practice by adding noise on \u03b8, ensuring the properties of the estimation still hold. In practice, policies can be represented as \u03b8 i, and \u03c1(s|s \u223c \u03b8) can be used as an approximation for \u03c1(s) to ensure the properties hold. By adding noise on \u03b8, Condition 1 is always satisfied, allowing for efficient estimation of \u03c1(s). Additionally, unbiased estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4 is possible. The text discusses the unbiased estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4 in reinforcement learning. It emphasizes the need for efficient learning algorithms that consider both reward from the primal task and policy uniqueness to improve behavioral diversity among agents. Previous approaches have focused on maximizing the expectation of cumulative rewards as the objective. To enhance agent diversity, the learning objective should consider both primal task rewards and policy uniqueness. Previous methods directly combine these rewards, but the choice of weighting parameter \u03b1 and intrinsic reward formulation r int can significantly impact results. A balance is needed in selecting \u03b1 to avoid undermining intrinsic rewards or ignoring primal task importance. To address the trade-off in selecting \u03b1 for balancing intrinsic rewards and primal task importance, the approach changes the multi-objective optimization problem into a constrained optimization problem. This transformation includes a threshold r0 for minimal uniqueness and a penalty term rint with a penalty coefficient 1\u2212\u03b1/\u03b1. The selection of \u03b1 for balancing intrinsic rewards and primal task importance is crucial. Zhang et al. proposed using the Task Novel Bisector (TNB) approach to tackle this challenge. In contrast, this work suggests solving the constrained optimization problem using Interior Point Methods (IPMs) instead. This method reformulates the problem into an unconstrained form with an additional barrier term in the objective. In contrast to using the Task Novel Bisector (TNB) approach for balancing intrinsic rewards and primal task importance, this work proposes solving the constrained optimization problem with Interior Point Methods (IPMs). This method reformulates the problem into an unconstrained form with a barrier term in the objective, making it computationally challenging and numerically unstable, especially when \u03b1 is small. In the proposed RL paradigm, where an agent's behavior is influenced by peers, a more natural approach is used by bounding collected transitions in the feasible region based on previous trained policies. The proposed RL paradigm uses a natural approach by bounding collected transitions in the feasible region based on previous trained policies, ensuring uniqueness in the new policy without the need to balance intrinsic and extrinsic rewards. The proposed Interior Policy Differentiation (IPD) method ensures uniqueness in new policies without the trade-off between intrinsic and extrinsic rewards. Tested on MuJoCo environments, including Hopper-v3, Walker2d-v3, and HalfCheetah-v3, the method demonstrates robust learning with default environment parameters. In experiments on MuJoCo environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3, the proposed Interior Policy Differentiation (IPD) method showcases robust learning with default parameters. The study compares uniqueness generated from stochasticity in training processes of vanilla RL algorithms and random weight initialization. The method is based on PPO (Schulman et al., 2017) and is compared with TNB and weighted sum reward (WSR) approaches for combining task goals and uniqueness motivation. Further implementation details are available in Appendix D. The proposed Interior Policy Differentiation (IPD) method is compared with TNB and weighted sum reward (WSR) approaches for combining task goals and uniqueness motivation. The uniqueness metric is utilized directly in learning new policies without reshaping. Different policies are trained sequentially to be unique, with qualitative results shown in Fig.2. The proposed Interior Policy Differentiation (IPD) method trains policies sequentially to be unique, with the 1st policy trained without social influence. Qualitative results are visualized in Fig.2, showing the motion of agents and experimental results in terms of uniqueness and performance in Fig.3. The proposed Interior Policy Differentiation (IPD) method trains policies sequentially to be unique, with the 1st policy trained without social influence. Experimental results in terms of uniqueness and performance are shown in Fig. 3, where our method outperforms others in Hopper and HalfCheetah. Walker2d shows improvement in uniqueness but not in performance compared to PPO. Detailed comparisons are presented in Table 1, with performance and reward curves in Fig. 5 and Fig. 6. Additional results on uniqueness are in Fig. 7. Success rate is also used as a metric for performance comparison. The success rate of different approaches is compared using the averaged final performance of PPO as the baseline. If a new policy surpasses this baseline during training, it is considered successful. Results show that the proposed method consistently outperforms the baseline in terms of success rate. Our method consistently outperforms the PPO baseline in terms of success rate, ensuring improved performance in environments like Hopper and HalfCheetah. It prevents policies from getting stuck in local minima and encourages exploration of different action patterns. Our proposed method prevents policies from getting stuck in local minima, encouraging exploration of different action patterns to improve performance in environments like HalfCheetah. The environment of HalfCheetah lacks an explicit termination signal, requiring policies to explore more instead of remaining in the same local minimum. In the environment of HalfCheetah, there is no explicit termination signal, leading to random actions by the agent initially. Our learning scheme involves receiving termination signals from peers to avoid wasteful random actions. The agent learns to terminate itself early to reduce control costs, then explores different behaviors for higher rewards, resembling an implicit curriculum. The process involves imitating previous policies and adapting with social influence. The learning process involves imitating previous policies and adapting with social influence to pursue higher rewards. As the number of learned policies with social influence grows, finding unique policies becomes more difficult. An ablation study shows performance changes under different scales of social influence, with Hopper environment showing a more significant performance decrease due to limited possible diverse policies. An efficient approach is developed to motivate RL to learn diverse strategies inspired by social influence. In this work, an efficient approach called Interior Policy Differentiation (IPD) is developed to motivate RL to learn diverse strategies inspired by social influence. IPD draws insights from Interior Point Methods and helps agents avoid local minimums. Experimental results show that IPD can learn various well-behaved policies and can be seen as a form of implicit curriculum learning. The method addresses the limited number of possible diverse policies in the Hopper environment and demonstrates the maximal and minimal policy uniqueness between different environments. The text discusses the importance of maximizing policy uniqueness in reinforcement learning environments like Hopper, Walker2d, and HalfCheetah. It highlights the trade-off between uniqueness and task-related performance, emphasizing the need for careful hyper-parameter tuning and reward shaping. The method uses deterministic policies and MLP with 2 hidden layers for actor models in PPO. In the implementation details, deterministic policies are used for calculating DTV in PPO. MLP with 2 hidden layers is employed for actor models, with varying unit numbers based on task requirements. Training timesteps are fixed for each task, and threshold selection allows control over policy magnitude. In the implementation details, deterministic policies are used for calculating DTV in PPO. MLP with 2 hidden layers is employed for actor models, with varying unit numbers based on task requirements. Training timesteps are fixed for each task, and threshold selection allows control over policy magnitude. The timesteps are fixed to be 1M in Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The proposed method allows flexibility in controlling policy uniqueness by adjusting the constraint threshold. Different thresholds result in different policy behaviors, with larger thresholds driving more distinct behavior and smaller thresholds imposing a lighter constraint. Constraints are not used to force every single action to be different, but rather focus on long-term differences. In the implementation details, deterministic policies are used for calculating DTV in PPO. MLP with 2 hidden layers is employed for actor models, with varying unit numbers based on task requirements. Training timesteps are fixed for each task, and threshold selection allows control over policy magnitude. The proposed method allows flexibility in controlling policy uniqueness by adjusting the constraint threshold. Different thresholds result in different policy behaviors, with larger thresholds driving more distinct behavior and smaller thresholds imposing a lighter constraint. Constraints are not used to force every single action to be different, but rather focus on long-term differences. Additionally, constraints can be applied after the first t timesteps for considering similar starting sequences. The constraints in the optimization problem can be applied after the first t timesteps for considering similar starting sequences. Different methods like WSR, TNB, and IPD correspond to approaches in constrained optimization. The Penalty Method handles constraints by incorporating them into a penalty term and solving the unconstrained problem iteratively. The WSR method approximates with a fixed weight term for optimization. The Penalty Method incorporates constraints into a penalty term and solves the unconstrained problem iteratively. The WSR method approximates with a fixed weight term for optimization, while the Feasible Direction Method finds a direction that satisfies constraints. The TNB method selects a direction using the bisector of gradients. The TNB method selects a direction using the bisector of gradients \u2207 \u03b8 f and \u2207 \u03b8 g, with a fixed learning stride. The shape of g is crucial for the success of TNB, and a barrier term of \u2212\u03b1 log g(\u03b8) can be used to influence the optimization process. The objective with the barrier term will get closer to the primal objective as \u03b1 approaches zero. In practice, methods choose a sequence of decreasing \u03b1 values to approach the primal objective. Directly applying this method can be computationally challenging and unstable. A more natural approach is to bound collected transitions within a feasible region by terminating new agents that step outside it. This ensures that all collected samples during training are within the feasible region. During the training process of new agents, samples are collected within a feasible region to ensure uniqueness in the obtained policy. This eliminates the need to balance intrinsic and extrinsic rewards, leading to a more robust learning process without objective inconsistency. The pseudo code of IPD based on PPO includes additions to the primal PPO algorithm."
}