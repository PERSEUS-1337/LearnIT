{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models used for high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) within the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the transformation of a source measure to a target measure. The proposed algorithm utilizes stochastic alternating gradient updates, akin to GANs, and is validated through numerical experiments in population modeling. The paper introduces a scalable method for unbalanced optimal transport within the generative-adversarial framework. It involves learning a transport map and scaling factor simultaneously to optimize the transformation of measures. The algorithm utilizes stochastic alternating gradient updates, similar to GANs, and is validated through numerical experiments in population modeling. Optimal transport involves pushing a source to a target distribution optimally without mass variations. The Kantorovich formulation seeks the optimal coupling between measures and can be solved using linear programming. Regularizing the objective with an entropy term improves efficiency using the Sinkhorn algorithm. Stochastic methods based on the dual objective are proposed for the continuous setting. Optimal transport has applications in computer graphics and domain adaptation. Transport maps can be learned using generative models like GANs when a transport cost is unavailable. Optimal transport has various applications in computer graphics, domain adaptation, image translation, natural language translation, and biological data integration. Transport maps can be learned using generative models like GANs to push a source distribution to a target distribution. Optimal transport applications in various fields use transport maps to push source distributions to target distributions. Different formulations and algorithms have been developed to handle unbalanced masses in measures, extending the theory of optimal transport.GANs have been used with conditioning and cycle-consistency strategies to enforce correspondence between original and transported samples. Formulations and algorithms have been developed to extend optimal transport theory to handle unbalanced masses in measures. Scaling algorithms have been created to approximate solutions to optimal entropy-transport problems, allowing for mass variation in applications such as computer graphics, tumor growth modeling, and computational biology. However, current methods cannot explicitly model mass variation between continuous measures. The novel framework presented addresses the limitations of current methods in handling unbalanced optimal transport by directly modeling mass variation. It proposes a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport. Additionally, scalable methodology is developed for solving this problem. The methodology presented addresses limitations in handling unbalanced optimal transport by modeling mass variation. It proposes a novel formulation for cost-optimal transport and scalable methodology for solving the problem. The approach is demonstrated using various datasets and a new scalable method is proposed in the Appendix. The curr_chunk introduces a new scalable method (Algorithm 2) in the Appendix for solving the optimal-entropy transport problem in the continuous setting. It extends previous work to unbalanced OT and is a scalable alternative for large or continuous datasets. The notation and definitions for optimal transport (OT) are also provided. The curr_chunk discusses optimal transport (OT) problems, introducing functions \u03c0X, \u03c0Y for projections onto X and Y, and addressing the issue of transporting measures in a cost-optimal manner. It contrasts the non-convex Monge problem with the convex Kantorovich OT problem, which formulates OT as a search over probabilistic transport plans. The conditional probability distributions \u03b3y|x are highlighted as stochastic maps from X to Y, providing a \"one-to-many\" version of the deterministic map from the Monge problem. The curr_chunk discusses optimal transport (OT) problems, focusing on stochastic maps from X to Y and introducing entropic regularization for a simpler dual optimization problem. It also mentions stochastic algorithms for computing transport plans and formulations for handling mass variation in OT. The curr_chunk discusses extending classical optimal transport (OT) to handle mass variation using optimal-entropy transport formulations. Numerical methods are based on a Kantorovich-like formulation, allowing for mass variation in the measures. State-of-the-art algorithms for discrete settings include iterative scaling algorithms that generalize the Sinkhorn algorithm for computing regularized OT plans. The curr_chunk introduces a new algorithm for unbalanced optimal transport (OT) that directly models mass variation, especially in high-dimensional spaces. It presents a Monge-like formulation for unbalanced OT, aiming to learn a stochastic transport map and scaling factor for cost-optimal transport between continuous measures. The curr_chunk introduces a stochastic transport map and scaling factor for cost-optimal transport between measures, including mass variation. It considers the more general case of stochastic maps compared to the deterministic approach in unbalanced Monge OT. The unbalanced Monge OT problem is addressed with a stochastic transport map and scaling factor for cost-optimal transport between measures, considering one-to-many maps for practical problems like cell biology. The transport map models movement from source to target measures, while the scaling factor represents growth. The transport map T models the movement of points from a source measure to a target measure, with a scaling factor \u03be representing growth or shrinkage. Different transformation models are optimal based on mass transport and variation costs. An unbalanced transport map with a scaling factor can address class imbalances by adjusting sample weights in the source distribution to balance classes with the target. The transport map T moves points between source and target measures, with a scaling factor \u03be adjusting sample weights for class balance. A relaxation method using a divergence penalty replaces the equality constraint, leading to the optimal-entropy transport problem formulation. The optimal-entropy transport problem is formulated using a joint measure \u03b3 \u2208 M + (X \u00d7 Y) specified by (T, \u03be). The objective function is obtained by reformulating in terms of \u03b3. The search space differs between formulations, with restrictions on the joint measures \u03b3 \u2208 M + (X \u00d7 Y) based on the transport map T. In the asymmetric Monge formulation, mass transported to Y must come from within the support of \u00b5. Equivalence is generally established by restricting the support of \u03b3 to supp(\u00b5) \u00d7 Y. The mass transported to Y must come from within the support of \u00b5, with equivalence established by restricting the support of \u03b3 to supp(\u00b5) \u00d7 Y. Theoretical results for optimal entropy-transport follow from the analysis by BID27, showing convergence of solutions from the relaxed problem to the original problem under certain conditions. The relaxation of unbalanced Monge OT allows for learning the transport map and scaling factor using stochastic gradient methods, with convergence of solutions from the relaxed problem to the original problem under specific conditions. The divergence penalty can be minimized by defining it as a penalty witnessed by an adversary function. The transport map and scaling factor can be learned using stochastic gradient methods. The divergence term can be minimized by defining it as a penalty witnessed by an adversary function. The optimization procedure involves alternating stochastic gradient updates after parameterizing T, \u03be, and f with neural networks. This process is similar to GAN training and involves minimizing the divergence between transported samples and real samples from \u03bd measured by the adversary f. The optimization procedure involves minimizing the divergence between transported samples and real samples from \u03bd using stochastic gradient methods and neural networks parameterized with T, \u03be, and f. The objective is to update parameters \u03b8, \u03c6, \u03c9 to find a cost-efficient strategy while minimizing divergence. Further practical considerations and examples of divergences are provided in the Appendix. The probabilistic Monge-like formulation is similar to the Kantorovich-like entropy-transport problem in theory but results in different numerical methods in practice. Algorithm 1 solves the non-convex formulation and uses neural networks to learn a transport map and scaling factor, enabling scalable optimization. The neural architectures imbue their function classes with a particular structure, allowing effective learning in high-dimensional settings. The neural architectures of T, \u03be imbue their function classes with a particular structure, enabling effective learning in high-dimensional settings. Algorithm 1 may not find the global optimum due to non-convexity, while the scaling algorithm of BID8 solves a convex optimization problem but is limited in scalability. A new stochastic method in the Appendix can handle transport between continuous measures, overcoming BID8's scalability limitations. The new stochastic method in the Appendix generalizes the approach for handling transport between continuous measures, overcoming scalability limitations of BID8. However, the output is less interpretable compared to Algorithm 1, which directly learns a transport map and scaling factor. The problem of learning a scaling factor arises in causal inference. In causal inference, the problem of learning a scaling factor to balance distributions from control and treated populations arises. Algorithm 1 directly learns a transport map and scaling factor, unlike BID23's generative-adversarial method. This approach eliminates selection biases in treatment effect inference, with applications in unbalanced optimal transport for population modeling. In practice, Algorithm 1 performs unbalanced optimal transport for population modeling by addressing class imbalances between source and target datasets. The target dataset reflects changes in popularity and brightness to simulate population drift. The algorithm is evaluated for transporting the source distribution to the target distribution. Algorithm 1 is used for unbalanced optimal transport to model population drift by addressing class imbalances between source and target datasets. The scaling factor learned reflects the imbalance ratio between digit classes, allowing for modeling growth or decline in a population. Experiments validate the effectiveness of the algorithm in reweighting distributions, as shown in FIG4. The unbalanced optimal transport model uses Algorithm 1 to simulate population evolution between MNIST and USPS datasets. The transport cost is based on Euclidean distance between images, visualized in FIG1 with arrows showing image transformation. The size of the image indicates scaling factor changes between datasets. The unbalanced optimal transport model uses Algorithm 1 to analyze image scaling factors between MNIST and USPS datasets. MNIST digits with higher scaling factors appear brighter and cover more pixels, consistent with USPS digits. The unbalanced optimal transport model was applied to the CelebA dataset to model the transformation of young faces to aged faces using a variational autoencoder. The transport cost was based on Euclidean distance in the latent space, visualized in FIG2. The unbalanced optimal transport model was applied to the CelebA dataset using a variational autoencoder to transform young faces into aged faces. The transported faces retained key features, with exceptions like gender swaps. Young faces with higher scaling factors were enriched for males, predicting growth in male face prominence. The CelebA dataset showed that young female faces had a lower scaling factor compared to young male faces, indicating a predicted growth in male face prominence as the population ages. There was a gender imbalance between young and aged populations, with the latter being predominantly male. Lineage tracing in biology involves tracking cells between different developmental stages or during disease progression, with unbalanced distributions. Using Algorithm 1 on zebrafish embryogenesis data, cells from blastulation and gastrulation stages were compared. Cells with higher scaling factors were analyzed for differential gene expression, revealing upregulated genes. In zebrafish embryogenesis data, cells from blastulation and gastrulation stages were compared using Algorithm 1. Cells with higher scaling factors showed upregulated genes associated with differentiation and development of the mesoderm, indicating potential biological discovery through analysis of scaling factors. A stochastic method for unbalanced OT based on the regularized dual formulation was presented, offering a natural generalization for constrained optimization problems. The regularized dual formulation of BID7 is a natural generalization of FORMULA2, involving a constrained optimization problem that is challenging to solve. Adding a strongly convex regularization term to the primal objective, such as an entropic regularization term BID13, helps make the dual problem unconstrained. This term encourages transport plans with high entropy, leading to a smoothing effect. The dual of the regularized problem is expressed through the Fenchel-Rockafellar theorem, with a relationship between the primal optimizer \u03b3* and dual optimizer (u*, v*). The regularized dual formulation of BID7 is a natural generalization of FORMULA2, involving a constrained optimization problem that is challenging to solve. Adding a strongly convex regularization term to the primal objective, such as an entropic regularization term BID13, helps make the dual problem unconstrained. This term encourages transport plans with high entropy, leading to a smoothing effect. The relationship between the primal optimizer \u03b3* and dual optimizer (u*, v*) is given by DISPLAYFORM5. We rewrite the equation in terms of expectations and assume access to samples from \u00b5, \u03bd, and their normalized measures. By parameterizing u, v with neural networks and optimizing using stochastic gradient descent, Algorithm 2 provides a generalization of classical OT to unbalanced OT. The dual solution learned from Algorithm 2 for Unbalanced Optimal Transport can reconstruct the primal solution, indicating mass transport between points in X and Y. The transport map \u03b3* does not necessarily have marginals \u00b5 and \u03bd, allowing for implicit mass variation in the problem. Additionally, an \"averaged\" deterministic mapping from X to Y can be learned using \u03b3*. The transport map \u03b3* learned from Algorithm 2 for Unbalanced Optimal Transport allows for mass variation between points in X and Y. An \"averaged\" deterministic mapping from X to Y can also be obtained using \u03b3*. The objectives in the formulations are equivalent when reformulated in terms of \u03b3 instead of (T, \u03be). The formulations are equivalent when expressed in terms of \u03b3 instead of (T, \u03be). Lemma 3.3 formalizes the relation between the formulations, showing that L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) and L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) for any solution (T, \u03be) or \u03b3. The text discusses the relation between different formulations involving probability measures and measurable functions. It shows that certain inequalities hold for any solution, and utilizes the disintegration theorem to establish the existence of measurable functions. The Radon-Nikodym derivative and the Radon-Nikodym theorem play a crucial role in the analysis. The text discusses the relation between different formulations involving probability measures and measurable functions, showing that certain inequalities hold for any solution. The Radon-Nikodym derivative and theorem are crucial in the analysis, leading to the existence of measurable functions. The proof of Proposition B.1 establishes the uniqueness of the joint measure specified by any minimizer of L \u03c8 (\u00b5, \u03bd). The uniqueness of the joint measure specified by any minimizer of L \u03c8 (\u00b5, \u03bd) is proven by showing that the marginals are uniquely determined for any solution \u03b3 of W c1,c2,\u03c8 (\u00b5, \u03bd). This is established through the construction of the proof of Lemma 3.3 and Corollary 3.6 in BID27. The uniqueness of the joint measure specified by any minimizer of L \u03c8 (\u00b5, \u03bd) is proven by showing that the marginals are uniquely determined for any solution \u03b3 of W c1,c2,\u03c8 (\u00b5, \u03bd). This uniqueness follows from the proof of Corollary 3.6 in BID27. Additionally, for certain cost functions and divergences, L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd, corresponding to the Hellinger-Kantorovich BID27 or the Wasserstein-Fisher-Rao BID9 metric. Theoretical analysis shows that solutions of the relaxed problem converge to solutions of the original problem. The uniqueness of the joint measure is proven by showing that marginals are uniquely determined for any solution. The cost function defines a proper metric between positive measures. The text discusses the convergence of solutions from the relaxed problem to the original problem. It proves the uniqueness of the joint measure by showing uniquely determined marginals for any solution. The cost function serves as a proper metric between positive measures. The sequence of minimizers is shown to be bounded and equally tight, leading to the weak convergence of a subsequence to a minimizer. In this section, the text presents the convex conjugate form of \u03c8-divergence to transform the main objective into a min-max problem. It discusses a lemma regarding non-negative finite measures and measurable functions, providing a simple proof for the result. The text presents a lemma regarding non-negative finite measures and measurable functions, providing a simple proof for the result. It discusses the convex conjugate form of \u03c8-divergence to transform the main objective into a min-max problem. The proof involves optimal functions over the support of Q and P\u22a5, with conditions on cost functions for the problem to be well-posed. The text discusses the choice of cost functions for a well-posed problem, with c1 often representing the cost of transport and c2 being the cost of mass adjustment. It is important to choose appropriate convex functions for c2 to prevent undesirable outcomes. The text discusses the use of entropy functions and \u03c8-divergences for training generative models to match generated distributions to true data distributions. Jensen's inequality ensures unique minimization when P = Q for probability measures, but not necessarily for non-probability measures. The text explains how \u03c8-divergences are used to match generated distributions to true data distributions, with unique minimization when P = Q for probability measures. Additional constraints are needed for non-probability measures to ensure divergence minimization matches P to Q. The text discusses \u03c8-divergences and their role in matching distributions, emphasizing unique minimization when P = Q for probability measures. Additional constraints are required for non-probability measures to ensure divergence minimization aligns P with Q. Examples of common divergences for unbalanced optimal transport are provided in Table 1. The text discusses the role of \u03c8-divergences in matching distributions, emphasizing unique minimization when P = Q for probability measures. Constraints are needed for non-probability measures to align P with Q. Examples of common divergences for unbalanced optimal transport are provided in Table 1. Choice of functions, activation layers, and neural architectures are also discussed for practical implementation."
}