{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework using spelling error correction, tokenization, and neural models was proposed to standardize non-systematic names to systematic names, achieving a 54.04% accuracy on the test dataset. The framework utilizes byte pair encoding tokenization and a neural sequence to sequence model to standardize chemical names. It achieves a 54.04% accuracy on the test dataset, a significant improvement from previous results. Chemical substances are identified by systematic names based on their structures, defined by IUPAC. Additionally, common or trivial names are used for familiar chemicals like sucrose. Chemical substances can have multiple names for various reasons. Common names, like sucrose for sugar, are familiar and simpler, while systematic names are more complex. Proprietary names, like Aspirin, are created by producers to differentiate their products. This results in chemical substances having multiple names due to historical reasons and industry practices. Chemical information extraction research focuses on extracting useful chemical knowledge from text and converting it into databases using standard chemical names. Chemical databases like PubChem and SciFinder store information such as chemical names, structures, and formulas. Updating these databases involves extracting information from chemical papers. Using systematic names for chemical substances makes it easier to generate other representations. Updating chemical databases involves extracting information from chemical papers using systematic names. Systematic names can be converted to other representations like SMILES and InCHI with high precision. Non-systematic names are considered errors in natural language processing, categorized into four types including spelling errors. Non-systematic names in chemical databases are considered errors in natural language processing, categorized into four types: spelling error, ordering error, common name error, and synonym error. These errors occur when non-systematic names differ from systematic names in spelling, order, common names, or synonyms. The most common error type in chemical databases is the spelling error. Non-systematic names can have multiple error types simultaneously, such as ordering and synonym errors. A framework is proposed to automatically convert non-systematic names to systematic names, involving spelling error correction, BPE tokenization, and a sequence to sequence model for fixing remaining errors. The BID2 work developed an online system called ChemHits for chemical name standardization, relying on transformation rules and online chemical databases. However, it heavily depends on chemical knowledge, limiting its effectiveness. In contrast, a sequence to sequence model is used in this study for standardization, similar to machine translation tasks. The study applies a sequence to sequence model for chemical name standardization, similar to machine translation tasks. The model is trained end-to-end without external chemical knowledge, achieving 54.04% accuracy on the test dataset. The corpus used contains chemical names from high-impact Chemical Journals, verified through manual work. In a corpus of chemical names from high-impact Chemical Journals, a sequence to sequence model is used for standardization, achieving 54.04% accuracy on the test dataset. The corpus includes non-systematic and systematic names of chemical substances, with 384816 data pairs. The Levenshtein distance between non-systematic and systematic names is analyzed. Training, test, and development sets are divided as 80%, 19%, and 1% respectively. The goal is to correct spelling errors in chemical substance names. The experiment aims to correct spelling errors in chemical substance names by separating them into elemental words and building vocabularies of systematic and non-systematic elemental words. The systematic names are split to build the systematic elemental vocabulary, while the non-systematic names are used to identify frequently occurring elemental words. The experiment corrects spelling errors in chemical substance names by separating them into elemental words. Non-systematic names are used to identify common elemental words, which are combined with systematic names to create a final elemental vocabulary structured using BK-Tree for efficient correction. BK-Tree is a data structure used for efficiently correcting spelling errors in non-systematic chemical substance names by separating them into elemental words and utilizing the Levenshtein distance. It allows for easy insertion of new training data, making it scalable. The BK-Tree is used to correct spelling errors in chemical substance names by separating them into elemental words and utilizing Levenshtein distance. It helps in training the sequence to sequence model by reducing noise in elemental words. Each node in the BK-Tree represents an elemental word, with edges showing the Levenshtein distance between them. The Levenshtein distance between nodes in the BK-Tree represents the similarity of chemical names. Tokenization using Byte Pair Encoding (BPE) BID11 helps in preparing the names for the sequence-to-sequence model. The symbol set is iteratively expanded by merging frequent symbol pairs. The symbol set is expanded by merging frequent symbol pairs, creating a new symbol for each merge operation. The final symbol set size is determined by the initial character size plus the number of merge operations. Byte Pair Encoding (BPE) is chosen for tokenization as it can handle out-of-vocabulary words and separate names into meaningful subwords. Examples of BPE applied to chemical names are shown in TAB1. In this work, the symbol set is expanded using Byte Pair Encoding (BPE) to tokenize chemical names into small molecules. The sequence to sequence model, based on a bidirectional LSTM (BiLSTM), is used for machine translation. The encoder generates a context vector from non-systematic names, while the decoder generates systematic names. The model is adapted from OpenNMT with modifications. The model uses a bidirectional LSTM for machine translation, generating a context vector for non-systematic names in the encoder. The decoder produces systematic names with probabilities calculated for each token. Parameters for spelling error correction include the threshold of the BK-Tree, with experiments conducted using different values. The BPE stage involves merging a specified number of units. In the experiment, various parameters were tested: threshold values of 1, 2, and 3 for the BK-Tree, and merge operation values of 2500, 5000, 10000, and 20000 for BPE. The sequence to sequence model used 500 dimensions for word embeddings and hidden states, with a vocabulary size determined by basic characters and BPE merge operations. Both encoder and decoder had 2 layers. Spelling error correction was done for non-systematic names in the training data before training the model. Stochastic gradient descent was used to train all model parameters jointly with a cross-entropy loss function. During training, parameters of the sequence to sequence model are trained jointly using stochastic gradient descent (SGD) with a cross-entropy loss function. The loss is computed over a minibatch of size 64 and normalized. Weights are initialized with a random uniform distribution. The model is trained for 15 epochs with a dropout rate of 0.3. A beam size of 5 is set for decoding. An experiment is also conducted using a Statistical Machine Translation (SMT) model with the Moses system. In the experiment, the sequence to sequence model is trained for 15 epochs with a beam size of 5 for decoding. Data augmentation is used to handle noisy data, specifically spelling errors, by inserting errors with a probability of 0.025. Error insertion includes randomly adding, deleting, exchanging, or replacing characters in non-systematic names. In the experiment, errors are inserted into non-systematic names with a probability of 0.025 using four methods: random character insertion, deletion, exchange, and replacement. Accuracy and BLEU score are used to measure standardization quality, with accuracy being a strong performance metric. Results for different models on the test dataset are shown in TAB3. The experiment results for different models on the test dataset are shown in TAB3, indicating that the combination of spelling error correction, BPE tokenization, and sequence to sequence model performs the best. The framework shows significant improvement compared to the SMT model and the ChemHits system. Results for different numbers of BPE merge operations are shown in TAB4, with 5000 being the optimal value. Spelling error correction and data augmentation are beneficial for the framework, with spelling error correction proving to be particularly helpful. The experiment results show that spelling error correction and data augmentation are beneficial for the framework. Spelling error correction is particularly helpful, while data augmentation also provides some improvement. Overcorrection may reduce standardization quality when the threshold is too large. Examples in Table 6 demonstrate the capabilities of the sequence to sequence model in correcting non-systematic names and errors. The sequence to sequence model can correct non-alphabet spelling errors and synonym errors, as demonstrated in examples provided. The model successfully standardizes chemical names, even when there are ordering errors or proprietary names involved. Visualization of attentions in an example further illustrates the model's capabilities in correcting common name errors and ordering errors. The seq2seq model successfully standardizes chemical names, correcting common name errors and ordering errors. The model can find the relation between adenine and 9H-purin-6-amine, as shown in Figure 6. The system also analyzes failed standardization attempts, with synonym errors being a common issue. The system successfully standardizes chemical names, correcting common name errors and ordering errors. However, synonym errors are a common issue in failed standardization attempts. Among 100 samples, synonym errors are the most confusing, while spelling errors are handled well. The system struggles with common errors due to the difficulty in finding a rule between unseen common names and systematic names. Nearly half of non-systematic names are not successfully standardized. The system successfully standardizes chemical names, correcting common name errors and ordering errors. Nearly half of non-systematic names are not successfully standardized. The accuracy for systematic names of different lengths is shown in Figure 6, with the best performance for names between 20 and 40 characters. The model does not consider chemical rules, leading to some names that disobey the rules and subwords that are not explainable. Examples of failed attempts are shown in Table 8. The model successfully standardizes chemical names, but some generated names disobey chemical rules. Examples of failed attempts are provided in Table 8, showcasing partially correct and totally incorrect predictions. The proposed framework includes spelling error correction, byte pair encoding tokenization, and a sequence to sequence model to convert non-systematic names to systematic names with high accuracy. The framework proposed in this work successfully converts non-systematic chemical names to systematic names with an accuracy of 54.04%, outperforming previous rule-based systems. It is trained end-to-end, data-driven, and independent of external chemical knowledge, opening up new possibilities for chemical information extraction."
}