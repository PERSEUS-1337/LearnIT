{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. Extensive empirical investigations on the NSynth dataset show that GANs outperform WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis faces challenges in modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but have slow sampling speeds. Efforts to speed up generation introduce overhead, such as training secondary networks or custom kernels. Generative Adversarial Networks (GANs) have been successful in generating high resolution images efficiently. They use transposed convolutions and latent vectors for global latent control. There is potential for audio GANs to achieve similar domain transformations as seen in image GANs. The potential for audio GANs to achieve domain transformations like image GANs using convolutions on a latent vector. Frame-based techniques for audio waveforms struggle with alignment due to differences in periodicity and output stride. Transposed convolutional filters in audio GANs face challenges in covering necessary frequencies and phase alignments for phase coherence. STFT unwraps phase over 2\u03c0 boundary to derive instantaneous radial frequency, showing constant relationship between audio and frame frequency. GAN researchers have made progress in image modeling by starting with focused datasets and gradually moving to less constrained domains. NSynth dataset focuses on individual notes from musical instruments, aligning and cropping the data to emphasize fine-scale details like timbre. Similar to CelebA for faces, NSynth restricts variance to improve model performance in audio generation. NSynth dataset consists of individual notes from various musical instruments, aligned and cropped to emphasize timbre and fidelity. Different models have been explored for conditional generation, including autoregressive WaveNet autoencoders and bottleneck spectrogram autoencoders. Recent work has focused on noncausal convolutional generation with techniques like adversarial training and effective representations. This work explores adversarial training and effective representations for noncausal convolutional generation in audio waveforms, focusing on maintaining the regularity of periodic signals over short to intermediate timescales. Unlike images, audio waveforms are highly periodic, and convolutional filters commonly learn logarithmically-scaled frequency selective filter banks. Maintaining alignment of frames with waveform periodicity is crucial for human perception. The regularity of periodic signals over short to intermediate timescales is crucial for maintaining alignment with waveform periodicity. A synthesis network must learn appropriate frequency and phase combinations to produce a coherent waveform, similar to the phase precession observed in short-time Fourier transform. Phase precession also occurs in situations where filterbanks overlap. Another approach to generating coherent waveforms is loosely inspired by the phase vocoder. In Figure 1, a method for generating coherent waveforms inspired by the phase vocoder is depicted. A pure tone leads to phase precession, which can be linearly increased by unwrapping the phase. The derivative of the unwrapped phase over time represents the instantaneous frequency, a measure of the true signal oscillation. The spectra at the bottom of the figure show how pure harmonic frequencies cause the wrapped phase spectra to oscillate. The paper investigates the interplay of architecture and representation in synthesizing coherent audio with GANs. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherence, and the importance of preventing harmonics from overlapping. Increasing STFT frame size and switching to mel frequency scale improve performance. The study focuses on generating waveforms with strided convolutions using GANs on the NSynth dataset. It emphasizes the importance of estimating IF spectra for coherent audio and preventing harmonics from overlapping. Increasing STFT frame size and switching to mel frequency scale improve performance by creating more separation between harmonic frequencies. GANs on NSynth dataset outperform WaveNet baseline in automatic and human evaluations, generating examples much faster. Global conditioning on latent and pitch vectors allows GANs to produce smooth timbre interpolation and consistent timbral identity across pitch. The study focuses on generating waveforms with strided convolutions using GANs on the NSynth dataset, emphasizing the importance of estimating IF spectra for coherent audio and preventing harmonics from overlapping. The dataset contains 300,000 musical notes from 1,000 different instruments, with samples four seconds long and sampled at 16kHz. Training was restricted to acoustic instruments and fundamental pitches ranging from MIDI 24-84, resulting in 70,379 examples from strings, brass, woodwinds, and mallets. A new 80/20 test/train split was created from shuffled data for evaluation. The study focused on generating waveforms with GANs on the NSynth dataset, using strided convolutions to prevent harmonics from overlapping. The dataset includes 300,000 musical notes from 1,000 instruments, with samples at 16kHz. Training was limited to acoustic instruments with MIDI 24-84, resulting in 70,379 examples from strings, brass, woodwinds, and mallets. A new 80/20 test/train split was created for evaluation. The model samples a random vector and uses transposed convolutions to generate output data. The model generates output data by sampling a random vector and using transposed convolutions. It includes a discriminator network to estimate divergence between real and generated distributions, with a gradient penalty for Lipschitz continuity and pixel normalization. Progressive training shows slightly better convergence time and sample diversity. The method involves conditioning on a one-hot representation of musical pitch appended to the latent vector. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. An auxiliary classification BID24 loss is added to encourage the generator to use the pitch information. Spectral representations are computed using STFT magnitudes and phase angles with specific parameters. The resulting \"image\" size is (256, 512, 2) with magnitude and phase as the two channel dimensions. The STFT frame size is adjusted to have 75% overlap and 513 frequency bins. The resulting \"image\" size is (256, 512, 2) with magnitude and phase as the two channel dimensions. Magnitudes are scaled to be between -1 and 1, and phase angles are also scaled to match the tanh output nonlinearity. Additionally, there are \"instantaneous frequency\" (\"IF\") models created by unwrapping the phase angle and taking the finite difference. High frequency resolution variants are achieved by doubling the STFT frame size and stride. To improve frequency resolution, the STFT frame size and stride are doubled, resulting in spectral images with size (128, 1024, 2). Log magnitudes and instantaneous frequencies are transformed to a mel frequency scale without compression, known as \"IF-Mel\" variants. WaveGAN is adapted for pitch conditioning and retrained on a subset of the NSynth dataset. Additionally, new waveform generating GANs are trained independently. WaveGAN, the current state of the art in waveform generation with GANs, is adapted for pitch conditioning and retrained on a subset of the NSynth dataset. Strong WaveNet baselines are created by adapting the architecture to accept the same one-hot pitch conditioning signal as the GANs. WaveNet baselines are created by adapting the architecture to accept one-hot pitch conditioning signals. Evaluating generative models is challenging due to the subjective nature of audio quality, so human evaluation is used as a gold standard. The evaluation metrics for WaveNet baselines include human evaluation using Amazon Mechanical Turk to compare audio quality of different models. 3600 ratings were collected with each model involved in 800 comparisons. The participants evaluated audio quality of different models on a Likert scale. 3600 ratings were collected with each model involved in 800 comparisons. Metrics like Number of Statistically-Different Bins (NDB) and Inception Score (IS) were used to measure diversity and evaluate GANs. The Inception Score (IS) is a metric used to evaluate GANs by measuring the mean KL divergence between imageconditional output class probabilities and the marginal distribution. It penalizes models that don't produce easily classifiable examples or examples that belong to only a few classes. To enhance this metric, features from a pitch classifier trained on spectrograms of an acoustic NSynth dataset are used. Additionally, Pitch Accuracy (PA) and Pitch Entropy (PE) are measured separately to address issues with distinct pitches and pitch diversity. The Fr\u00e9chet Inception Distance (FID) is a metric proposed for evaluating GANs based on the distance between multivariate Gaussians fit to features extracted from a pretrained Inception classifier. It correlates with perceptual quality and diversity on synthetic distributions. The study uses pitch-classifier features instead of Inception features for evaluation. The study evaluates GANs using pitch-classifier features instead of Inception features. Results show a clear trend in audio quality, with IF-Mel model rated slightly inferior to real data. WaveNet baseline produces high-fidelity sounds but occasionally breaks down. Sample diversity correlates with audio quality, as seen in NDB scores. High frequency resolution improves NDB scores, with WaveNet receiving the worst score. The NDB score correlates with sample diversity and audio quality, with high frequency resolution improving scores across model types. The WaveNet baseline receives the worst NDB score due to a lack of diversity in autoregressive sampling. FID scores also reflect the impact of high frequency resolution on sample quality, with phase models performing poorly. High frequency resolution significantly impacts metrics for IF models, with Mel scaling having less effect on FID compared to listener study. Phase models show high FID even at high frequency resolution, indicating poor sample quality. Classifier metrics like IS, Pitch Accuracy, and Pitch Entropy perform well for models with explicit conditioning. High-resolution models generate examples classified similarly to real data, but differences in accuracy and entropy may not reflect sample quality due to distribution issues. Discriminative information about sample quality from high scores is limited. The metrics provide a rough measure of model reliability in generating classifiable pitches, with low frequency models and baselines showing less reliability. Visualizing qualitative audio concepts is recommended, with accompanying audio examples available for further understanding. Waveform irregularities are observed in WaveGAN and PhaseGAN models compared to real data. The WaveGAN and PhaseGAN models exhibit waveform irregularities compared to real data, with PhaseGAN showing phase discontinuities and WaveGAN being quite irregular. In contrast, the IFGAN model displays more coherence with only small variations from cycle-to-cycle. Visualizations show that real data and IF models produce consistent waveforms, while PhaseGAN and WaveGAN have more irregularities. The WaveGAN and PhaseGAN models exhibit waveform irregularities compared to real data, with PhaseGAN showing phase discontinuities and WaveGAN being quite irregular. The PhaseGAN has some phase discontinuities, while the WaveGAN is quite irregular. Rainbowgrams BID9 depict the log magnitude of frequencies as brightness and the IF as color, showing clear phase coherence in real data and IFGAN. GANs allow conditioning on the same latent vector for the entire sequence, unlike WaveNet which learns local latent codes for generation on a millisecond scale. WaveNet autoencoders in BID9 learn local latent codes for generation on a millisecond scale, but have limited scope and do not fit a compact prior. Interpolating between examples in the raw waveform, distributed latent code, and global code of an IF-Mel GAN results in perceptual mixing of sounds. However, the linear interpolation does not correspond to the complex prior on latents, leading to intermediate sounds that may fall apart, oscillate, or whistle. WaveNet improves upon local latent codes for sound generation but struggles with linear interpolation, resulting in unrealistic intermediate sounds. In contrast, IF-Mel GAN's global conditioning allows for perceptual mixing of sounds during interpolation. Linear interpolation in WaveNet produces less realistic intermediate sounds due to venturing off the true data manifold, while IF-Mel GAN's global conditioning allows for smooth perceptual mixing during interpolation. The timbre of sounds morph smoothly between instruments in the audio examples. In the audio examples, timbre is interpolated between random points in latent space while using pitches from Bach's Suite No. 1 in G major. The timbre of the sounds smoothly morphs between instruments while pitches consistently follow the composed piece. The GAN's timbral identity remains intact across varying pitches, creating a unique instrument identity. The GAN's timbral identity remains intact across varying pitches, creating a unique instrument identity for the given point in latent space. GANs with upsampling convolutions offer advantages over autoregressive models, allowing for parallel processing of training and generation on modern GPU hardware. This results in significantly faster synthesis times compared to WaveNet autoencoders, making it around 53,880 times faster. The IF-Mel GAN is significantly faster than WaveNet autoencoders, allowing for real-time neural network audio synthesis on devices. This opens up possibilities for exploring a wider range of expressive sounds compared to previous methods focused on speech synthesis. Future work could compare adversarial audio synthesis to methods using variable-length conditioning or recurrent models. In comparison to speech, audio generation for music is relatively under-explored. Previous work has demonstrated the ability to synthesize musical instrument sounds using autoregressive models, but these suffer from slow generation. Our work proposes a modification to GANs' loss function for improved training stability and architectural robustness, building on recent advances in GAN literature. The text chunk discusses advancements in GAN literature, proposing modifications to the loss function for improved training stability and architectural robustness. It introduces progressive training for better generation quality within limited training time and suggests architectural tricks for further improvement. The NSynth dataset is mentioned as a benchmark for audio generation, with previous work using WaveNet autoencoders for timbre interpolation. Other studies have achieved significant speedups in sampling by training regression models for pitch mapping. BID23 incorporated an adversarial domain confusion loss for timbre transformations in audio sources. BID5 achieved fast sampling speeds by training a regression model for pitch and instrument labels mapping to raw waveforms. Their approach improved frequency estimation but lacked phase coherency learning and handled multimodal distributions. GANs on the NSynth dataset surpassed WaveNet in audio generation quality and speed. In a study on audio generation with GANs using the NSynth dataset, high-quality audio generation was demonstrated, surpassing WaveNet in fidelity and speed. Further research is needed to validate and expand this to different types of natural sounds and address issues like mode collapse and diversity. The potential for domain transfer and other applications of adversarial losses in audio generation was also highlighted. In audio generation with GANs using the NSynth dataset, high-quality audio was produced, surpassing WaveNet in fidelity and speed. Further research is needed to validate and expand to different natural sounds, address mode collapse, and enhance diversity. The potential for domain transfer and other applications of adversarial losses in audio generation was emphasized. Additionally, combining adversarial losses with encoders or regression losses could better capture the full data distribution. Different models were trained with the ADAM optimizer, sweeping over learning rates and weights of the auxiliary classifier loss to determine optimal performance. The use of pitch classifiers and specific learning rates and classifier loss values were found to be most effective across various model variants. The GAN models in the study use box upscaling/downscaling and pixel normalization. The discriminator includes the standard deviation of minibatch activations. Real data is normalized before passing to the discriminator. The GAN variants were trained for 4.5 days on a single V100 GPU with different batch sizes. For nonprogressive models, training equates to 5M examples, while progressive models train on 1.6M examples per stage. The GAN variants were trained for 4.5 days on a single V100 GPU, with a batch size of 8. Progressive models train on 1.6M examples per stage, totaling around 11M examples. The WaveNet baseline uses a Tensorflow implementation with a decoder composed of 30 layers of dilated convolution. The WaveNet model consists of 3 stacks of 10 layers each, with increasing dilation from 2^0 to 2^9. A conditioning stack operates on a pitch conditioning signal, with 5 layers of dilated convolution and 3 layers of regular convolution. The model uses mulaw encoding for the 8-bit version and a quantized mixture for the 16-bit version. Training involved 150k iterations over 2 days with 32 GPUs."
}