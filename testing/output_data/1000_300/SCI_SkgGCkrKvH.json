{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models enables data privacy and on-device learning over networks. Communication compression in decentralized training is proposed to overcome network bandwidth limitations. Choco-SGD achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training deep learning models on decentralized user devices and in datacenters. Distributed machine learning has enabled successful applications in research and industry, offering computational scalability. Distributed machine learning has enabled successful applications in research and industry by leveraging computational scalability and data-locality. Recent theoretical results suggest that decentralized schemes can be as efficient as centralized approaches in terms of convergence of training loss. Gradient compression techniques have been proposed for standard distributed training to improve efficiency. Gradient compression techniques have been proposed for decentralized training of deep neural networks to reduce data sent over communication links. CHOCO-SGD, introduced for convex problems, overcomes restrictions on compression operators and supports high compression ratios. The focus is on generalization performance evaluation on the test-set. CHOCO-SGD is a gradient compression technique introduced for convex problems, supporting high compression ratios. The evaluation focuses on generalization performance on test sets, departing from previous works that mainly considered training performance. Two scenarios are studied: training in a peer-to-peer setting with distributed data and training in a datacenter setting with decentralized communication. Speed-ups for CHOCO-SGD over decentralized baselines are demonstrated with less communication overhead. In a datacenter setting, decentralized communication patterns allow better scalability than centralized approaches. CHOCO-SGD can improve time-to-accuracy on large tasks like ImageNet training. However, decentralized algorithms struggle to match the performance of centralized schemes when scaled to a larger number of nodes. These findings highlight deficiencies in current decentralized training schemes and call for further research. The study highlights deficiencies in current decentralized training schemes compared to centralized approaches, calling for further research. CHOCO-SGD shows comparable performance to centralized schemes and converges at a rate of O 1 / \u221a nT on non-convex smooth functions, with potential for linear speedup in the number of workers. A version with momentum is analyzed for practical performance on relevant scenarios. The study discusses the performance of CHOCO-SGD in decentralized training compared to centralized approaches, highlighting its potential for linear speedup with the number of workers. A version with momentum is analyzed for practical scenarios in on-device training and datacenter settings, emphasizing the importance of reducing bandwidth requirements and improving computational scalability. Additionally, the challenges faced by current decentralized learning approaches when scaling to larger nodes are systematically investigated. Decentralized learning approaches face difficulties in communication restricted settings. Various methods have been proposed, including decentralized schemes, gradient compression, asynchronous methods, and multiple local SGD steps before averaging. Combining decentralized SGD schemes with gradient compression is advocated for in this paper. The paper advocates for combining decentralized SGD schemes with gradient compression, focusing on gossip averaging approaches. Convergence rates depend on the spectral gap of the mixing matrix, with recent studies showing similar results for related schemes. Recent studies have shown similar results for related schemes in decentralized optimization with quantization, where communication compression with quantization has gained popularity in the deep learning community. Theoretical guarantees have been established for schemes with unbiased compression, and schemes with error correction tend to perform best in practice. Additionally, proximal updates and variance reduction have been studied in combination with quantized updates. Decentralized optimization with quantization has been studied extensively, with research showing that gossip averaging may not converge in the presence of quantization noise. Adaptive schemes with increasing compression accuracy have been proposed, but they come at a higher communication cost. For deep learning applications, the DCD and ECD algorithms have been introduced to converge at the same rate. Adaptive schemes for decentralized optimization with increasing compression accuracy have been proposed, converging at higher communication costs. The CHOCO-SGD algorithm can handle arbitrary high compression and has shown promising results for non-convex functions. In comparison, DeepSqueeze also converges with arbitrary compression ratio but CHOCO-SGD achieves higher test accuracy in experiments. In experiments, CHOCO-SGD achieves higher test accuracy compared to DeepSqueeze, an alternative method with arbitrary compression ratio. The decentralized optimization problem is formally introduced along with compression operators and the gossip-based stochastic optimization algorithm CHOCO-SGD. The decentralized optimization problem involves local distributions for sampling data on each node, communication limited to local neighbors in a weighted graph, and compressed message transmission using compression operators. The decentralized optimization problem involves local distributions for sampling data on each node, communication limited to local neighbors in a weighted graph, and compressed message transmission using compression operators. CHOCO-SGD algorithm is described in Algorithm 1, where each worker updates its private variable using stochastic gradient and gossip averaging steps. The CHOCO-SGD algorithm involves workers updating their private variables using stochastic gradient and gossip averaging steps, communicating with neighbors using compressed updates. The publicly available copies of the private variables are shared among neighbors to preserve averages of iterates despite quantization noise. The communication and gradient computation can be executed in parallel, with each node only needing to store 3 vectors at most. A momentum-version of CHOCO-SGD is proposed in Algorithm 2 for non-convex problems, extending the analysis. Technical assumptions are made regarding bounded variance of stochastic gradients on each worker. In extending the analysis of CHOCO-SGD to non-convex problems, technical assumptions are made regarding bounded variance of stochastic gradients on each worker. The convergence rate of the consensus averaging scheme is denoted by c, showing linear speed-up compared to SGD on a single node. Compression and graph topology only affect the higher order term. Experimental comparisons with relevant baselines are conducted, leveraging momentum in all algorithms. In this section, experimental comparisons are conducted between CHOCO-SGD and relevant baselines using commonly used compression operators. Momentum is leveraged in all algorithms, including the newly developed momentum version of CHOCO-SGD. The experiments are conducted using a ring topology with 8 nodes and training the ResNet20 architecture on the Cifar10 dataset. For the first set of experiments in (Tang et al., 2018), a ring topology with 8 nodes is used to train ResNet20 on the Cifar10 dataset. Various algorithms including DCD, ECD, DeepSqueeze, and CHOCO-SGD with momentum are implemented with a momentum factor of 0.9. The learning rate is fine-tuned and warmed up gradually for the first 5 epochs, with decay at 150 and 225 epochs, and training stops at 300 epochs. Compression schemes are applied to every layer of ResNet20 separately, with two unbiased schemes: quantization and sparsification, and two biased schemes: top-k and random. The learning rate is fine-tuned and warmed up gradually for the first 5 epochs, with decay at 150 and 225 epochs, and training stops at 300 epochs. The compression schemes applied to each layer of ResNet20 include unbiased quantization and sparsification, as well as biased top-k and random sparsification methods. The combination of DCD and ECD with biased schemes is not supported by theory, while CHOCO-SGD and DeepSqueeze have only been studied with biased schemes. Unbiased compression schemes can be scaled down to meet specifications. Results show that unbiased compression schemes ECD and DCD perform well at low compression ratios but struggle at high ratios, consistent with previous theoretical and experimental findings. DCD performs better with biased sparsification compared to unbiased random sparsification, despite lacking theoretical support. CHOCO-SGD demonstrates good generalization across scenarios with minimal accuracy drop. In challenging real-world scenarios, decentralized training is essential due to limited communication bandwidth and locally stored data on devices like sensor networks, mobile devices, and hospitals. Centralized methods are inefficient in these cases. Sign compression achieves high accuracy with significantly fewer bits per weight compared to full precision. Decentralized training scenarios involve sensor networks, mobile devices, and hospitals jointly training a machine learning model with limited communication bandwidth and locally stored data. Privacy is a key motivation, with data split between nodes and not shuffled during training. This setting has not been extensively studied before. Decentralized training involves data split between nodes without shuffling during training. Prior works have not studied this scenario extensively. CHOCO-SGD with sign compression scaled to a large number of devices on the Cifar10 dataset, achieving a compromise between accuracy and compression level. Comparison was made with decentralized SGD without compression and centralized SGD without compression. All nodes route updates to a central coordinator for aggregation in the centralized baseline. CHOCO-SGD with sign compression was compared to decentralized SGD without compression and centralized SGD without compression for aggregation. The study included training on 4, 16, 36, and 64 nodes using different topologies. Results showed that CentralizedSGD performed well across all node numbers, while CHOCO-SGD was affected by graph topology. The study compared CHOCO-SGD with sign compression to decentralized SGD without compression and centralized SGD without compression for aggregation on different node numbers and topologies. Results showed that CentralizedSGD performed well across all nodes, while CHOCO-SGD was affected by graph topology and communication compression, leading to slower convergence. Increasing the number of epochs improved decentralized scheme performance but did not fully close the performance gap between centralized and decentralized algorithms. In the real decentralized scenario, the goal is to reduce communication costs by fixing the number of transmitted bits to 1000 MB. CHOCO-SGD performs the best, with slight degradation as the number of nodes increases. Torus topology is beneficial for large networks due to good mixing properties, while for small networks, there is not much difference between torus and other topologies. Experiments on a Real Social Network Graph involve training models on user devices connected by a social network. The chosen network has 32 nodes, and models are trained for image classification and language modeling tasks. The experiments involved training models on a social network graph with 32 nodes. The models used were a CNN for image classification and a three-layer LSTM for language modeling on WikiText-2. Results showed that the decentralized algorithm performed best in training accuracy, while the centralized scheme had the highest test accuracy. CHOCO-SGD outperformed the exact decentralized scheme in test accuracy for the same transmitted data. CHOCO-SGD outperforms exact decentralized scheme in test accuracy for the same transmitted data, with a slight drop in accuracy compared to baselines. In language modeling, CHOCO-SGD surpasses centralized SGD in test perplexity. For a fixed data volume, CHOCO-SGD performs best among decentralized and centralized algorithms. CHOCO-SGD outperforms exact decentralized and centralized algorithms for a fixed data volume. The benefits of CHOCO-SGD are more pronounced when scaling to more nodes, offering a solution for scaling issues even in well-connected datacenter settings. Decentralized schemes have shown to outperform centralized ones in certain scenarios, with impressive speedups in training on multiple GPUs. Assran et al. (2019) achieved impressive speedups for training on 256 GPUs with asynchronous gossip updates and time-varying communication topology. Their setup differs from CHOCO-SGD with exact communication. Experimenting with ImageNet-1k using Resnet-50 on 8 machines with Tesla P100 GPUs, they utilized decentralized communication with compressed communication. In experiments on 8 machines with Tesla P100 GPUs, decentralized communication with compressed communication (sign-CHOCO-SGD) in a ring topology was used. Mini-batch size on each GPU is 128, following the general SGD training scheme. Results show CHOCO-SGD takes less time than all-reduce with a slight 1.5% accuracy loss. CHOCO-SGD, a decentralized and parallel training approach, outperforms all-reduce in terms of time efficiency with only a 1.5% accuracy loss. It shows a 20% time gain over the common all-reduce baseline on commodity hardware clusters. The algorithm enables decentralized deep learning training in bandwidth-constrained environments and offers theoretical convergence guarantees in the non-convex setting. The algorithm CHOCO-SGD enables decentralized deep learning training in bandwidth-constrained environments with theoretical convergence guarantees. It shows a linear speedup in the number of nodes and performs well on image classification and language modeling tasks. This approach allows training in strongly communication-restricted environments while respecting the locality of training data. The decentralized CHOCO-SGD algorithm enables training in communication-restricted environments with theoretical convergence guarantees. It demonstrates high communication compression and data-locality, expanding the reach of decentralized deep learning applications. The proof of Theorem 4.1 is presented, showing Algorithm 1 as a special case of a more general class of algorithms. The decentralized CHOCO-SGD algorithm allows training in communication-restricted environments with theoretical convergence guarantees. Algorithm 1 is a special case of a more general class of algorithms, Algorithm 3, which involves stochastic gradient updates and averaging among nodes. Linear convergence is crucial for the success of these algorithms, as shown in previous research. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, which involves blackbox averaging/gossip. An assumption is made for the averaging scheme to preserve iterates' average and converge linearly. Exact Averaging is discussed as a consensus averaging algorithm with a defined eigengap. CHOCO-SGD can be recovered by choosing CHOCO-GOSSIP as the consensus averaging scheme. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, involving blackbox averaging/gossip. CHOCO-SGD can be recovered by choosing CHOCO-GOSSIP as the consensus averaging scheme, with the order of communication and gradient computation parts exchanged for better illustration. The convergence rate is not affected by this change. The iterates of Algorithm 3 with constant stepsize \u03b7 satisfy the recursion verifying that rt \u2264 \u03b7^2 / 4Ac^2. The averaged iterates i of Algorithm 3 satisfy a linear speed up compared to SGD on one node, with the convergence rate affecting only the second-order term. CHOCO-SGD with CHOCO-GOSSIP averaging scheme converges at a rate dependent on the eigengap of the mixing. The convergence rate of CHOCO-SGD with CHOCO-GOSSIP averaging scheme is affected by the eigengap of the mixing matrix W. The theorem provides guarantees for the averaged vector of parameters x in a decentralized setting. The theorem provides guarantees for the averaged vector of parameters x in a decentralized setting, but it can be expensive and difficult to average all parameters across multiple machines. Similar guarantees can be obtained for individual iterates x_i. Corollary A.3 shows convergence of local weights under certain conditions, which can be relaxed. Theorem A.4 provides guarantees under specific assumptions and step sizes. Theorem A.4 guarantees convergence of Algorithm 3 with specific step sizes under certain assumptions. It converges at a speed determined by the convergence rate of the underlying averaging scheme. The rate holds for any T, but the first term is less favorable compared to Theorem A.2 due to the difference in \u03c3^2 and G^2. Corollary A.5 shows the convergence of local weights x_i under different conditions. Theorem A.4 guarantees convergence of Algorithm 3 with specific step sizes under certain assumptions, with the speed determined by the averaging scheme. Corollary A.5 demonstrates the convergence of local weights x_i under different conditions. Algorithm 4 CHOCO-SGD (Koloskova et al., 2019) introduces Error Feedback and mixing matrix W for stochastic gradient updates in a decentralized setting. Algorithm 2 combines CHOCO-SGD with weight decay and momentum, showing adaptability to Nesterov momentum. CHOCO-SGD with weight decay and momentum combines error feedback algorithm for decentralized stochastic gradient updates. It saves quantization errors in internal memory and corrects them before compression. The value transmitted is the difference in local variable evolution at each step. In this section, the procedure of model training and hyper-parameter tuning is explained. Different methods like CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression are compared using ResNet20 for image classification and a three-layer LSTM architecture for language modeling tasks. The study compared CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression using ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. The LSTM had 28.95 million parameters and used a three-layer setup with a hidden dimension size of 650. Training involved fine-tuning gradient clipping (0.4) and applying dropout (0.4) only on the output of LSTM. Both models were trained for 300 epochs with a per node mini-batch size of 32. The learning rate of CHOCO-SGD followed a linear scaling rule proportional to the node degree. During training, the learning rate of CHOCO-SGD is scaled linearly with the node degree. Dropout (0.4) is applied only on the output of LSTM, while momentum (0.9) is used for ResNet20 training. The initial learning rate is gradually warmed up from 0.1 to the fine-tuned value over 5 epochs, and decayed by a factor of 10 at 50% and 75% of training. The optimal learning rate per sample is determined by the node degree and mini-batch size. The learning rate in CHOCO-SGD is tuned based on the node degree and mini-batch size, with the optimal value searched in a predefined grid. The consensus stepsize is also fine-tuned using a similar approach. Tables 4, 5, and 6 show the hyperparameters for training ResNet-20 on Cifar10 and a social network topology with 32 nodes. The fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20/LSTM on a social network topology with 32 nodes are shown in Table 5. The training data is split between nodes with a fixed partition, no shuffling. The per node mini-batch size is 32, and the maximum node degree is 14. Learning curves for the social network topology are plotted in Figure 6. Additionally, plots for training accuracy and test accuracy in a datacenter experiment are provided in Figure 8. The topology has 32 nodes with each node accessing a subset of the dataset. Local mini-batch size is 32. Training and test accuracies are depicted in Figures 8 and 9. Local models reach consensus towards the end of optimization, with individual test performances matching the averaged model. Divergence from the averaged model occurs before decreasing stepsize at epoch 225, as reported in Assran et al. (2019)."
}