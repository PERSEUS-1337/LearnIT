{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. The challenge lies in the instability of derivatives, especially in networks with piecewise linear activation functions. A new learning problem is proposed to promote stable derivatives over larger regions. The algorithm involves identifying stable linear approximation regions and expanding them. This approach is demonstrated on residual and recurrent networks using image and sequence datasets. The text discusses a novel relaxation method to scale algorithms for realistic models, focusing on derivatives with respect to input coordinates for sensitivity analysis and model explanations. The key challenge is stabilizing derivatives in complex mappings, demonstrated on residual and recurrent networks with image and sequence datasets. The derivatives of functions parameterized by deep learning models are unstable, leading to instability in both function values and derivatives. This instability affects the robustness of first-order approximations used for explanations. Gradient stability is different from adversarial examples, as stable gradients can vary in size within a local region. The text discusses the difference between gradient stability and adversarial examples in deep learning models. It mentions that stable gradients can vary in size within a local region, while adversarial examples are small perturbations that change predicted outputs. Robust estimation techniques focus on stable function values rather than stable gradients but can indirectly impact gradient stability. The paper focuses on deep networks with piecewise linear activations. The paper focuses on deep networks with piecewise linear activations to ensure gradient stability. It investigates lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable, particularly for p = 2. The regularization problem is formulated to maximize this lower bound, resembling support vector machines. The paper explores deep networks with piecewise linear activations for gradient stability. It introduces a perturbation algorithm for collecting exact gradients without back-propagation, even when GPU memory is limited. The algorithms are tested on various network types and datasets through quantitative and qualitative experiments. The paper focuses on inference and learning algorithms for neural networks with piecewise linear activation functions. It introduces algorithms for identifying stable input regions and expanding regions of stable derivatives. The study includes experiments with fully-connected, residual, and recurrent networks on image and time-series datasets. The paper focuses on neural networks with piecewise linear activation functions, such as ReLU and its variants. These networks are inherently piecewise linear due to their activation functions. Various types of networks like FC, CNN, RNN, and ResNet fall under this category. The proposed approach involves a mixed integer linear representation of piecewise linear networks, encoding the active linear piece of the activation function for each neuron. The paper discusses neural networks with piecewise linear activation functions, focusing on a mixed integer linear representation of these networks. It explains how an activation pattern encodes the active linear piece of the activation function for each neuron, leading to a degeneration into a linear model. The feasible set corresponding to an activation pattern in the input space is a stable region where derivatives are consistent. The concept of a linear region induced by an activation pattern and a complete linear region in the input space is also introduced. Activation patterns have been studied in various contexts, such as visualizing neurons and reachability of specific output values. The text discusses the concept of activation patterns in neural networks with piecewise linear activation functions. It focuses on local linear regions and expanding them via learning, as opposed to quantifying the number of linear regions as a measure of complexity. The notion of stability considered differs from adversarial examples. The text focuses on expanding local linear regions in neural networks through learning, contrasting with quantifying complexity based on the number of linear regions. It discusses the notion of stability different from adversarial examples and the challenges in finding exact adversarial examples. The defense methods are computationally challenging on ImageNet scale images, while the proposed inference algorithm certifies a 2-margin around a point based on its activation pattern. The proposed inference algorithm certifies a 2-margin around a point based on its activation pattern by forwarding O(D) samples in parallel, scaling to high-dimensional images like ResNet on 299 \u00d7 299 \u00d7 3 dimensional images. The learning algorithm maximizes the 2 margin of linear regions around each data point in an unsupervised manner, akin to transductive/semi-supervised SVM. Our approach maximizes the 2 margin of linear regions around each data point in an unsupervised manner, similar to transductive/semi-supervised SVM. It involves a smooth relaxation of the margin and novel perturbation algorithms for gradient stability, addressing interpretability and transparency of complex models. The gradient is crucial for various explanation methods in deep models. The gradient is essential for explanation methods in deep models, including gradient saliency maps and its variants. The focus is on establishing robust derivatives in neural networks with ReLU activations. Inference and learning algorithms are presented for FC networks with M hidden layers and Ni neurons. The text introduces notation for FC networks with ReLU activations, presents inference and learning algorithms for a neural network with hidden layers and neurons, and explains the computation of neurons and activated neurons in each layer using transformation matrices and biases. The text introduces notation for FC networks with ReLU activations, presents inference and learning algorithms for a neural network with hidden layers and neurons, and explains the computation of neurons and activated neurons in each layer using transformation matrices and biases. Neurons are functions of specific instances denoted by x, and the output of the network is a linear transformation of the last hidden layer. The piecewise linear property of neural networks is represented by f \u03b8 (x), and a generic loss function L(f \u03b8 (x), y) is used. The activation pattern used in this paper is defined as a set of indicators for neurons. The activation pattern BID20 used in this paper is defined as a set of indicators for neurons that specify functional constraints. Each linear region of f \u03b8 is characterized as a convex polyhedron with linear constraints in the input space R D. Lemma 2 characterizes linear regions of f \u03b8 as convex polyhedrons with linear constraints in the input space R D. The activation pattern defines the feasible set S(x) as a convex polyhedron. The p margin of x, denoted as \u02c6 x,p, is a lower bound subject to a derivative specification. Directional feasibility is checked using the convexity of S(x) and a unit vector \u2206x. Directional feasibility is determined by checking if a point x + \u00af\u2206x is within the feasible set S(x) using activation patterns. This method can be applied to 1-ball and \u221e-ball feasibility problems, with the latter being intractable in high dimensions due to the exponential number of extreme points. The number of extreme points in a 1-ball is linear to the dimensionality. Proposition 5 can be generalized for an \u221e-ball, but in high dimensions, the number of extreme points becomes exponential, making it intractable. The number of extreme points in a 1-ball is linear to the dimensionality. Feasibility can be verified using binary searches for directional perturbations and 1-balls. Certification for 1-balls is tractable due to convexity, while further exploiting the polyhedron structure allows for analytical certification of 2-balls. Proposition 6 introduces the concept of a 2-ball Certificate, which calculates the minimum 2 distance between a point x and a union of hyperplanes. The computation involves evaluating neuron activations efficiently through forward passes. The number of linear regions in the function f \u03b8 is challenging to count due to activation patterns, leading to the proposal of certifying the number of complete linear regions (#CLR) among data points D x. The text discusses certifying the number of complete linear regions (#CLR) of a function f \u03b8 among data points D x. It focuses on maximizing the 2 margin\u02c6 x,2 through a regularization problem in the objective to maximize the margin. In this section, methods are discussed to maximize the 2 margin\u02c6 x,2 through a regularization problem in the objective. The rigid loss surface is addressed by a hinge-based relaxation similar to SVM, optimizing Eq. (4) and deriving a smoother problem with a hinge loss. The text discusses deriving a relaxation to solve a smoother problem by relaxing constraints and regularization in a neural network scenario. It also mentions maximizing margins in a linear model and visualizing the proposed methods on a binary classification dataset. In a linear model scenario, margins are computed between a linear hyperplane and training points. Visualization of proposed methods is done on a 2D binary classification dataset using different regularization techniques. Distance regularization enlarges linear regions around training points, while relaxed regularization generalizes properties to the whole space with a smoother prediction boundary. Relaxed regularization generalizes properties to the whole space with a smoother prediction boundary, allowing gradients to change directions smoothly. The generalized loss for learning RObust Local Linearity (ROLL) involves a set of neurons with top \u03b3 percent relaxed loss on x. The final objective is to scale the effect to large networks by considering neurons that incur high losses to the given point. The final objective for learning RObust Local Linearity (ROLL) is to scale the effect to large networks by considering neurons with top \u03b3 percent relaxed loss on x. Setting \u03b3 = 100 simplifies the structure, stabilizes training, and allows for an approximate learning algorithm without back-propagation. Each hidden neuron z i j is a linear function of x \u2208 S(x), leading to heavy computation on gradient norms. The parallel algorithm developed avoids back-propagation by exploiting the functional structure of f \u03b8. It constructs a linear network g \u03b8 based on the same parameters as f \u03b8, allowing for the computation of gradients without calling back-propagation. The complexity of the approach is analyzed assuming no overhead for parallel computation. The proposed parallel algorithm avoids back-propagation by constructing a linear network g \u03b8 based on the same parameters as f \u03b8. The complexity analysis assumes no overhead for parallel computation and presents an unbiased estimator for the loss in high-dimensional settings. The proposed unbiased estimator for the ROLL loss in high-dimensional settings can be efficiently computed using a parallel algorithm that constructs a linear network based on the same parameters as the original model. This approach avoids back-propagation and allows for an unbiased approximation of the loss function. The proposed algorithms for deep learning models with affine transformations and piecewise linear activation functions can be applied by enumerating neurons with ReLU-like activation. Comparisons with a baseline model ('vanilla') were conducted on a testing set, showing the effectiveness of the approach ('ROLL'). Experiments were performed on a single GPU with 12G memory, evaluating accuracy as a key measure. In this section, the approach 'ROLL' is compared with a baseline model 'vanilla' in various scenarios using evaluation measures such as accuracy, number of complete linear regions, and p margins of linear regions. Experiments are conducted on a single GPU with 12G memory, using a 4-layer FC model with ReLU activations on the MNIST dataset. The models are evaluated based on the median of linear regions among the testing data. The experiments on a 4-layer FC model with ReLU activations on the MNIST dataset resulted in tuned models with different parameters achieving larger margins with ROLL loss compared to the vanilla loss. The Spearman's rank correlation between certain variables among testing data was consistently high. The approach showed lower number of complete linear regions compared to the baseline model. The proposed method achieved larger margins with ROLL loss compared to vanilla loss by sacrificing 1% accuracy. Spearman's rank correlation was consistently high among testing data. Parameter analysis showed that increased C and \u03bb led to decreased accuracy with larger margins. Higher \u03b3 values resulted in less sensitivity to hyper-parameters C and \u03bb. The method was validated for efficiency by measuring running time. The proposed method achieved larger margins with ROLL loss compared to vanilla loss by sacrificing 1% accuracy. Higher \u03b3 values resulted in less sensitivity to hyper-parameters C and \u03bb. Efficiency was validated by measuring running time for gradient descent steps. The approximate ROLL loss showed comparable accuracy and margins to the full loss, being 9 times faster. The perturbation algorithm achieved about 12 times faster computation compared to back-propagation. Our approach achieves minimal computational overhead compared to the vanilla loss, utilizing a perturbation algorithm and approximate loss. Training RNNs for speaker identification on a Japanese Vowel dataset with variable sequence lengths and channels. Our approach utilizes the state-of-the-art scaled Cayley orthogonal RNN for speaker identification on a Japanese Vowel dataset with variable sequence lengths and channels. The results show significant improvements in model margins on testing data compared to the vanilla loss, with a high Spearman's rank correlation between certain variables. Sensitivity analysis on derivatives was also conducted. The study conducted sensitivity analysis on derivatives to identify stability bounds for stable derivatives. The ROLL regularization showed consistently larger stability bounds compared to the vanilla model. Experiments were performed on the Caltech-256 dataset using a 18-layer ResNet model. The approximate ROLL loss was used for training with 120 epochs. Experiments were conducted on the Caltech-256 dataset using a 18-layer ResNet model. The ROLL loss was utilized for training with 120 random samples per channel. Evaluation measures were challenging due to high input dimensionality, leading to a sample-based approach for gradient stability analysis. The stability of gradients f \u03b8 (x) y for the ground-truth label in a local region is evaluated using a sample-based approach. The gradient distortion is measured in terms of expected 1 distortion and maximum 1 distortion within an intersection B ,\u221e (x). Adversarial gradients are found by maximizing the distortion over an \u221e -norm ball. In Figure 4, examples in Caltech-256 are visualized to show P 50 and P 75 of maximum 1 gradient distortions on the ROLL model. Adversarial gradient is obtained by maximizing distortion over an \u221e -norm ball. Genetic algorithm BID33 is used for black-box optimization due to gradient-based optimization limitations. 8000 samples are used to approximate expected 1 distortion, with 1024 random images evaluated for both maximum and expected 1 gradient distortions. The \u221e -ball radius is set to 8/256. The results in Table 4 show that the ROLL loss provides more stable gradients and slightly better precision compared to the vanilla loss. Only 40 and 42 out of 1024 images had their prediction labels changed with ROLL and vanilla models, respectively. The ROLL loss maintains stable shapes and intensities of gradients, while the vanilla loss does not. Additional examples with integrated gradient attributions are available in Appendix K. This study introduces a new learning problem to enhance deep learning models with robust local linearity. This paper introduces a new learning problem to endow deep learning models with robust local linearity by constructing locally transparent neural networks with stable derivatives. The proposed ROLL loss expands regions with stable gradients and generalizes the stable gradient property across linear regions. The feasible set of the activation pattern is equivalent to satisfying linear constraints in the first layer. The feasible set of the activation pattern is equivalent to satisfying linear constraints in the first layer. If x satisfies the fixed activation pattern, it satisfies the linear constraint in the first layer. The proof follows by induction, showing directional feasibility and 1-ball feasibility. The proof demonstrates that the feasible set of the activation pattern satisfies linear constraints in the first layer, with induction showing directional feasibility and 1-ball feasibility. Additionally, the linear constraints in the convex set are proven to hold for 1-ball and 2-ball feasibility. The proof involves constructing a neural network feasible in Eq. (5) with linear activation functions, denoted as z i j and a, highlighting their relationship with a new input x. The network g \u03b8 is built with the same weights and biases as f \u03b8, using a well-crafted linear activation function. Each layer in g \u03b8 is represented accordingly. The linear activation function in g \u03b8 is fixed given x, applying the same linearity to\u1e91. Derivatives of neurons with respect to an input axis k can be computed using a zero vector and a unit vector. This allows for the computation of all neuron derivatives with 2 forward passes. The proposed approach allows for the computation of neuron derivatives with 2 forward passes, while back-propagation requires sequential computation. The proposed approach enables computing neuron derivatives with 2 forward passes, while back-propagation requires sequential computation. Gradients of neurons can be computed using dynamic programming and the chain-rule of Jacobian. The proposed approach allows for efficient computation of neuron derivatives using dynamic programming and the chain-rule of Jacobian. It involves iterating through layers with the Jacobian of previous layers and is particularly effective for fully connected networks. However, it is not recommended for convolutional layers due to the high cost of explicitly representing the convolutional operation. The focus is on deriving methods for piecewise linear networks with max-pooling nonlinearity, but caution is advised due to the introduction of new linear constraints by max-pooling neurons. The proposed approach allows for efficient computation of neuron derivatives using dynamic programming and the chain-rule of Jacobian. It involves iterating through layers with the Jacobian of previous layers and is particularly effective for fully connected networks. However, caution is advised when deriving methods for piecewise linear networks with max-pooling nonlinearity, as max-pooling neurons introduce new linear constraints. It is feasible to derive inference and learning methods upon a piecewise linear network with max-pooling nonlinearity, but using convolution with large strides or average-pooling is suggested instead to avoid constraints. The network degenerates to a linear model with max-pooling nonlinearity disappearing, inducing stable derivatives in a feasible input space. The feasible activation pattern forms a convex polyhedron with linear constraints, allowing for application of inference and learning algorithms. Each max-pooling neuron induces N-1 linear constraints in the fully connected model with 4 hidden layers and 100 neurons each. Input dimension is 2, output dimension is 1. The FC model consists of 4 hidden layers with 100 neurons each. Input dimension is 2, output dimension is 1. The model is trained for 5000 epochs with Adam optimizer. Regularization parameters are tuned to 1. Data is normalized with mean 0.1307 and standard deviation 0.3081. Margin values are computed and reported in the table. The FC model has 4 hidden layers with 300 neurons each, ReLU activation function, and trained for 20 epochs using stochastic gradient descent with Nesterov momentum. Data normalization is done with mean 0.1307 and standard deviation 0.3081. Margin values are computed and reported in the table, with tuning done on regularization parameters \u03bb, C, \u03b3. The validation loss is minimized using stochastic gradient descent with Nesterov momentum, with specific parameters. Grid search is conducted on \u03bb, C, \u03b3 values. The data is not normalized, and the ROLL loss is computed during training. A single-layer scoRNN is used for representation learning, with LeakyReLU activation and 512 hidden neurons. The loss function is cross-entropy. The scoRNN model uses LeakyReLU activation with 512 hidden neurons for representation learning. Cross-entropy loss with soft-max is used, and AMSGrad optimizer is applied. Grid search is done on \u03bb, C, \u03b3 values. Pre-trained ResNet-18 is utilized for image normalization and distance computation. The model is trained on normalized images using a bijective mapping to compute distances. The pre-trained ResNet-18 is modified by replacing max-pooling with average-pooling and enlarging the receptive field for higher dimensional data. The ResNet-18 model is adapted for higher dimensional data by adjusting the output to 512 dimensions. Training involves stochastic gradient descent with Nesterov momentum for 20 epochs, with a learning rate of 0.005 adjusted to 0.0005 after 10 epochs. The model is tuned with a fixed C value of 8 and \u03bb values ranging from 10^-6 to 0.001. Increasing C beyond 8 did not improve results. The study fine-tunes the model by adjusting \u03bb values and fixing it at 0.001, while exploring C values and determining 8 as the optimal value. A genetic algorithm with 4800 populations and 30 epochs is implemented to improve approximation quality through random sampling and selection processes. The study uses a genetic algorithm with 4800 populations and 30 epochs to improve approximation quality through random sampling and selection processes. The algorithm involves sorting samples based on distance, keeping the top 25% samples, replacing the rest with a random linear combination, projecting updated samples to ensure feasibility, and returning the sample with the maximum distance. Mutation is not implemented due to computational reasons, and the crossover operator is explained as analogous to a gradient step. Visualizations include original image, original gradient, adversarial gradient, and image of adversarial gradient. The study uses a genetic algorithm with 4800 populations and 30 epochs to improve approximation quality through random sampling and selection processes. The algorithm involves sorting samples based on distance, keeping the top 25% samples, replacing the rest with a random linear combination, projecting updated samples to ensure feasibility, and returning the sample with the maximum distance. Mutation is not implemented due to computational reasons, and the crossover operator is explained as analogous to a gradient step. Visualizations include original image, original gradient, adversarial gradient, and image of adversarial gradient. The direction is determined by other samples and the step size is determined randomly. Various gradients and integrated gradients are visualized using a common implementation procedure. The study visualizes gradients and integrated gradients by aggregating derivatives, taking absolute values, normalizing, and clipping values. The derivatives are then visualized as gray-scaled images. Examples from the Caltech-256 dataset are shown to highlight differences in settings. The study visualizes gradients and integrated gradients by aggregating derivatives, taking absolute values, normalizing, and clipping values. Examples from the Caltech-256 dataset are shown to highlight differences in settings, including the visualization of maximum 1 gradient distortions on the ROLL model. The ROLL model shows maximum 1 gradient distortions for 'Bear' and 'Rainbow' at 1367.9 and 3882.8 respectively, while the vanilla model has distortions at 1547.1 for 'Bear' and 5473.5 for 'Rainbow'."
}