{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires expensive per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing to new domains without annotations, a domain adaptation method is proposed. This method involves learning discriminative feature representations of patches based on label histograms in the source domain and using an adversarial learning scheme to align feature distributions between source and target patches. The framework also includes a global alignment process, leading to state-of-the-art performance in semantic segmentation. The framework proposed involves an adversarial learning scheme to align feature distributions between source and target patches, achieving state-of-the-art performance in semantic segmentation. Extensive experiments on benchmark datasets show the effectiveness of the method in various scenarios. Domain adaptation methods have been developed to bridge the gap between annotated training data and unlabelled test domains, especially for pixel-level prediction tasks like semantic segmentation. Existing methods focus on aligning feature distributions or output spaces to improve generalization to new domains, crucial for tasks where annotating ground truth is costly due to varying appearance distributions in different cities or changing conditions within the same city. Existing state-of-the-art methods use feature-level or output space adaptation to align distributions between source and target domains using adversarial learning. Global distribution alignment may differ significantly between domains due to camera pose or field of view differences. Instead of global statistics, patch-level information is considered for distribution alignment through adversarial learning. Instead of global alignment, patch-level alignment is considered for distribution alignment through adversarial learning to match shared patches across domains. This approach utilizes label histograms of patches and discriminative representations to align distributions at the patch level. The proposed approach focuses on patch-level alignment using discriminative representations to align patches between source and target domains. Two adversarial modules are used to align global and patch-level distributions, with guidance from pixel-level annotations in the source domain. The proposed approach involves patch-level alignment by learning discriminative representations and using K-means clustering to group patches into clusters. An adversarial loss is then used to align feature representations of target patches with the distribution of source patches in a clustered space. The approach involves using K-means clustering to group patches into clusters and aligning feature representations with an adversarial loss. Experiments include synthetic-to-real and cross-city scenarios, showing favorable performance against state-of-the-art methods. The proposed framework involves domain adaptation for structured output prediction using global and patch-level adversarial learning modules. It also includes a method for learning discriminative representations through clustering based on label histograms of patches. The framework outperforms various baselines in terms of accuracy and visual quality, and is applicable to other structured outputs such as depth. In this work, a method for learning discriminative representations through clustering based on label histograms of patches is developed. The proposed adaptation method outperforms various baselines and state-of-the-art methods on semantic segmentation tasks. Domain adaptation approaches for image classification and pixel-level prediction tasks are discussed, including the use of hand-crafted features and deep architectures to learn domain-invariant features. Domain adaptation methods aim to align feature distributions between different domains, using hand-crafted features or deep architectures to learn domain-invariant features. Adversarial learning and Maximum Mean Discrepancy are common techniques, with variations in classifiers and loss functions. Recent work focuses on enhancing feature representations through pixel-level transfer and domain separation. Unlike image classification, domain adaptation for structured pixel-level predictions, such as semantic segmentation, is less explored. Initial efforts have used adversarial networks to address domain adaptation in scenarios like synthetic-to-real images. Domain adaptation for structured pixel-level predictions, like semantic segmentation, is a less explored area. BID14 introduced domain adaptation for road-scene images using adversarial networks to align global feature representations. BID36 utilized SVM classifier to capture label distributions on superpixels for training the adapted model. BID4 performed class-wise domain adversarial alignment by assigning pseudo labels to target data. Our proposed method focuses on learning discriminative representations for patches to aid in patch-level alignment during domain adaptation for structured pixel-level predictions. Unlike existing methods that rely on global distribution alignment and class-specific priors, our approach preserves structured information at the patch level. Additionally, our framework does not require additional priors or annotations, allowing for end-to-end training of the entire network. Compared to other adaptation methods, our approach offers a more effective and efficient solution. Our framework focuses on learning patch-level representations for alignment during domain adaptation, without the need for additional priors or annotations. This approach aims to improve understanding and performance in tasks such as facial recognition, image generation, and view synthesis by learning disentangled representations. Our framework proposes learning discriminative representations for patches to aid in domain adaptation tasks, leveraging label distributions as a disentangled factor. This approach aims to enhance performance in tasks like facial recognition, image generation, and view synthesis. Our proposed domain adaptation framework leverages label distributions as a disentangled factor to learn discriminative representations for patches. The goal is to align predicted output distributions between source and target data using supervised learning and adversarial loss functions. Our goal is to align the predicted output distribution of the target data with the source distribution. A loss function is used for supervised learning on the source data, while an adversarial loss aligns the global distribution. Additionally, a classification loss in a clustered space is incorporated to learn patch-level discriminative representations from the source output distribution. Another adversarial loss is employed for the target data to align patch-level distributions between the source and target representations. The adaptation task involves various loss functions including supervised loss for structured prediction and discriminative representation on source data, clustering process for ground truth label distribution, global and patch-level adversarial loss functions for aligning target distribution. The baseline model includes a supervised cross-entropy loss and an output space adaptation module for global alignment. The method involves a baseline model with supervised cross-entropy loss and global alignment using an output space adaptation module. It includes optimizing a fully-convolutional network for structured output prediction and adversarial training with a discriminator for source-target image classification. The method involves optimizing GAN training for a generator G and discriminator D g to distinguish source and target images. Patch-level alignment is proposed for transferable structured output representations shared across images. The network architecture includes a generator G and a categorization module H for learning discriminative patch representations. The proposed network architecture includes a generator G and a categorization module H for learning discriminative patch representations. Clustering is used to disentangle source and target domain patches, guiding them to select the closest cluster for alignment. The proposed patch-level alignment method aims to disentangle source and target domain patches by selecting the closest cluster for alignment using adversarial objective. To achieve this, label histograms for patches are used as the disentangled factor, leveraging per-pixel annotations in the source domain. The proposed method uses label histograms for patches in the source domain to construct a disentangled space of patch representations. Patches are randomly sampled and spatial label histograms are extracted, followed by K-means clustering. A classification module is added after the predicted output to simulate the label histogram construction process and learn a discriminative representation. The learned representation is denoted as F s = H(G(I s )) through the softmax function, where K is the number of clusters. The proposed method constructs a disentangled space of patch representations using label histograms in the source domain. A classification module is added to simulate the label histogram construction process and learn a discriminative representation denoted as F s = H(G(I s )). The learning process involves formulating a cross-entropy loss for patch-level adversarial alignment to align target patches to the clustered space. An adversarial loss is utilized between F s and F t to align patches regardless of their spatial location in the image. The method aligns patches in the image without spatial constraints by reshaping the feature representation F and using an adversarial objective with a discriminator. The optimization process involves updating two discriminators and integrating the objectives into a min-max problem. In network optimization, the training process involves updating discriminators Dg and Dl to distinguish between source and target distributions, and classifying feature representations. The network G and H are then updated to align the target distribution with the source distribution. The training process involves updating discriminators Dg and Dl to classify feature representations from the source or target domain. The network G and H are then updated to align the target distribution with the source distribution, maintaining good performance on main tasks. The discriminator Dg uses a spatial map O as input with 5 convolution layers. The discriminator Dl utilizes 3 fully-connected layers. The generator consists of network G with a categorization module H, following the DeepLab-v2 framework with ResNet-101 architecture pre-trained on ImageNet. The generator network G includes a categorization module H and utilizes leaky ReLU activation with channel numbers {256, 512, 1}. The architecture is based on DeepLab-v2 with ResNet-101 pre-trained on ImageNet. Implementation details involve using PyTorch on a Titan X GPU for training with the Adam optimizer. The proposed architecture is implemented using PyTorch on a Titan X GPU. The discriminators are trained with the Adam optimizer, while the generator uses Stochastic Gradient Descent. Learning rates are adjusted using polynomial decay. Ablation study on GTA5-to-Cityscapes is conducted with ResNet-101 network. During training, the model uses specific hyperparameters and is trained with different loss functions. The proposed framework for domain adaptation is evaluated on semantic segmentation, showing favorable performance compared to state-of-the-art approaches on various benchmark datasets and settings. The method is tested under different scenarios, including synthetic-to-real and cross-city situations. Our method outperforms state-of-the-art approaches in domain adaptation for semantic segmentation on benchmark datasets. We adapt datasets like GTA5 BID27 and SYNTHIA BID28 to Cityscapes BID5, and Cityscapes to Oxford RobotCar BID23, considering different weather conditions and city settings. In the Oxford RobotCar BID23 dataset, sunny images are added to the rainy scenes. 10 sequences are manually selected, with 7 for training and 3 for testing. 895 images are used for training, and 271 images are annotated for per-pixel semantic segmentation ground truth. The evaluation metric used is intersection-over-union (IoU) ratio. An ablation study on the GTA5-to-Cityscapes scenario is conducted to analyze the impact of different loss functions and design choices in the proposed framework. In an ablation study on the GTA5-to-Cityscapes scenario, different loss functions and design choices were analyzed. The proposed method includes disentanglement, global alignment, and patch-level alignment. Adding disentanglement alone improved performance, while combining global and patch-level alignments achieved the highest IoU at 43.2%. Both losses, Ld and Ll adv, were found to be necessary for the patch-level alignment process. In experiments validating patch-level alignment effectiveness, both losses Ld and Ll adv are crucial. Removing either results in a performance drop of 1.9% and 1.5%. Reshaping features in the clustered space is essential for effective alignment. Without reshaping, performance drops by 2.4% in IoU, confirming the importance of aligning patches with similar representations. Without reshaping, performance drops by 2.4% in IoU, confirming the importance of aligning patches with similar representations. Visualization in FIG1 shows t-SNE visualization of patch-level features in the clustered space, highlighting the effectiveness of adaptation. Experimental results comparing the proposed method with state-of-the-art algorithms in various scenarios are presented, including synthetic-to-real and cross-city cases. The proposed method outperforms state-of-the-art algorithms in adapting GTA5 to Cityscapes and SYNTHIA to Cityscapes, showing improvements in IoU and achieving the best results in multiple categories. Visual comparisons are also provided in Figure 5. In addition, the proposed method achieves the best IoU on 14 out of 19 categories when adapting SYNTHIA to Cityscapes. Visual comparisons are shown in Figure 5, with more results in the appendix. Adapting Cityscapes to Oxford RobotCar in different weather conditions also demonstrates the effectiveness of the method. The paper presents a domain adaptation method for structured output, combining global and patch-level alignments to improve segmentation results. Results show higher segmentation detail and less noise compared to existing methods. The proposed method achieves a mean IoU of 63.6%, outperforming previous approaches by 1.4%. Extensive experiments validate the effectiveness of the approach. The proposed method aims to improve semantic segmentation by aligning target patch distributions with source ones using an adversarial learning scheme. Extensive experiments validate the effectiveness of the approach under various challenges, showing favorable performance compared to existing algorithms. Training the model involves randomly sampling one image from each domain in a training iteration and following a specific optimization strategy. As shown in TAB3, image and patch sizes are maintained during training and testing without cropping. The model introduces an entropy regularization (BID21) to push target feature representation closer to source clusters, achieving an IoU of 41.9%. This is lower than the patch-level adversarial alignment at 43.2% due to the entropy minimization approach not using source distribution guidance. The model utilizes patch-level adversarial alignment to achieve 43.2% IoU by guiding target patches closer to the source distribution in a clustered space. Visual comparisons and results for adapting Cityscapes to Oxford RobotCar are presented in figures and tables. The proposed method achieves better segmentation outputs with more details and less noisy regions when adapting Cityscapes to Oxford RobotCar in rainy scenes. Visual comparisons are provided for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes scenarios, showing the effectiveness of the approach. The proposed method demonstrates improved segmentation results when adapting Cityscapes to Oxford RobotCar in rainy scenes. Visual comparisons for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes scenarios highlight the effectiveness of the approach."
}