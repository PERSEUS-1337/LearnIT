{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for studying representation learning and generative modeling of geometric data represented as point clouds. The learned representations outperform state-of-the-art methods in 3D recognition tasks and enable shape editing applications. Different generative models, including GANs and Gaussian mixture models, are also studied, with GMMs trained in the latent space of the autoencoders producing samples of the best fidelity and diversity. The paper introduces a deep autoencoder network for studying representation learning and generative modeling of geometric data represented as point clouds. It proposes simple measures of fidelity and diversity based on matching point clouds for quantitative evaluation of generative models. Different encodings like view-based projections, volumetric grids, and graphs are discussed, highlighting their limitations in capturing semantics for generative models. Recent advances in deep learning offer a data-driven approach for designing new objects without the need for complex parametric models. Deep learning tools like autoencoders and Generative Adversarial Networks have proven successful in learning complex data representations and generating realistic samples. Deep learning architectures like autoencoders and Generative Adversarial Networks have been successful in learning complex data representations and generating realistic samples. Point clouds, a relatively unexplored 3D modality, provide a compact representation of surface geometry, making them attractive for learning purposes. Point clouds are a compact representation of surface geometry, commonly used in range-scanning devices like the Kinect and iPhone. Deep architectures for 3D point clouds are limited, with PointNet BID17 being successful in classification and segmentation tasks. Generative models for point clouds have gained attention in the deep learning community, particularly with the introduction of GANs BID10. Generative models for point clouds have gained attention in the deep learning community, especially with the introduction of GANs. Evaluating generative models involves assessing fidelity and coverage, with issues such as training difficulty and mode collapse. Simple methods are proposed to address these challenges in the target domain, including a new AE architecture inspired by recent classification architectures. The text discusses addressing challenges in training and evaluating generative models for point clouds, focusing on mode collapse and training difficulty. A new AE architecture is designed to learn compact representations with good reconstruction quality, suitable for classification and semantic operations. Additionally, the first set of generative models is created to generate point clouds similar to training and test data, without the need for joint learning of representation and GAN training. The text proposes a workflow where an AE learns a compact representation first, followed by training a GAN in that fixed latent space. This approach is supported by theory and empirically verified to be easier and more effective than training a GAN from raw data. Latent GANs are shown to be easier to train and achieve superior reconstruction quality. Additionally, GMMs trained in the latent space of fixed AEs perform the best overall. Multi-class GANs are found to work almost as well as dedicated GANs when trained in the latent space. The text discusses the performance of fixed latent space AEs and multi-class GANs, comparing them to dedicated GANs trained per object category. Various metrics are evaluated for learning good representations and evaluating generated samples, including proposing fidelity and coverage metrics for generative models. The paper is organized into sections outlining background, evaluation metrics, and models for latent space. The paper discusses the performance of fixed latent space AEs and multi-class GANs, comparing them to dedicated GANs trained per object category. It outlines necessary background, evaluation metrics, and models for latent representations and generation of point clouds. The models are evaluated quantitatively and qualitatively in Section 4, with further results available in the appendix. The code for all models is publicly available. Generative Adversarial Networks (GANs) are advanced models that involve a game between a generator (G) and a discriminator (D) to create realistic data samples. The Encoder (E) compresses data into a low-dimensional representation, while the Decoder (D) reconstructs the data from this representation. The discriminator and generator networks use specific loss functions during training. The discriminator distinguishes between real and synthesized samples generated by the generator function G using specific loss functions. Parameters \u03b8 (D) and \u03b8 (G) are used for the discriminator and generator networks. Improved Wasserstein GAN is utilized for enhanced stability during training. Point clouds pose unique challenges due to the absence of a grid-like structure, making them more difficult to encode compared to images or voxel grids. Recent work focuses on classification of point clouds. Point clouds present challenges due to the lack of a grid-like structure, making encoding difficult compared to images or voxel grids. Recent classification work bypasses this issue by avoiding 2D convolutions. Unordered nature of point clouds complicates comparisons between sets, requiring permutation-invariant features. Two metrics for comparing unordered point sets have been proposed in the literature. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature: Earth Mover's distance (EMD) and Chamfer distance (CD). EMD transforms one set to the other through a transportation problem, while CD measures the squared distance between each point in one set to its nearest neighbor in the other set. These metrics are used to evaluate representations and generative models of point clouds. In the paper, evaluation metrics for representations and generative models of point clouds are discussed. The comparison between a set of points and its ground truth counterpart is essential for assessing model quality. Metrics like Coverage, COV-CD, and COV-EMD are used to measure how well a point-cloud distribution matches a ground truth distribution. The evaluation metrics for representations and generative models of point clouds include Coverage, COV-CD, COV-EMD, MMD-CD, MMD-EMD, and Jensen-Shannon Divergence (JSD). Coverage measures the fraction of point-clouds in G matched to A, while MMD captures fidelity by matching every point cloud of G to the one in A with the minimum distance. JSD calculates the Jensen-Shannon divergence between marginal distributions in 3D space. The text discusses evaluation metrics for representations and generative models of point clouds, including MMD-CD, MMD-EMD, and Jensen-Shannon Divergence (JSD). MMD measures distances in pairwise matchings to assess realism, while JSD calculates the divergence between marginal distributions in 3D space. The text also describes architectures for representation and generative models, starting with an autoencoder design and later introducing a GAN tailored to point-cloud data. The text discusses architectures for representation and generative models of point clouds, starting with an autoencoder design. The encoder architecture uses 1-D convolutional layers with increasing features and a symmetric function to encode each point independently. The output is a k-dimensional vector. The implementation uses 5 1-D conv layers with ReLU and batch-norm, producing a k-dimensional vector for the latent space. The decoder consists of 3 fully connected layers to generate a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, are explored using EMD-distance and Chamfer-Distance as structural losses. Different bottleneck sizes are tested for the latent-space in 8 AEs trained with point-clouds of a single object class. The study explored different bottleneck sizes for the latent-space in 8 autoencoders trained with point-clouds of a single object class. The best generalization error was found with a bottleneck size of k = 128, while achieving minimal reconstruction error on the training data. The raw point cloud GAN model was introduced, operating directly on the raw 2048 \u00d7 3 point set input, making it the first GAN for point clouds. The discriminator's architecture is similar to the autoencoder, with leaky ReLUs used instead of ReLUs. The study introduced a GAN for point clouds, with the discriminator's architecture similar to an autoencoder but without batch-norm and using leaky ReLUs. The generator operates on a 128-dimensional noise vector and the output is decoded to a point cloud via the AE decoder. The l-GAN passes data through a pre-trained autoencoder before GAN operation on the bottleneck variable. The GAN operates on a 128-dimensional bottleneck variable of the AE. The l-GAN architecture is simpler than the r-GAN, with shallow designs for the generator and discriminator. Gaussian Mixture Models are also trained on the latent spaces learned by the AEs, which can be turned into point-cloud generators. The GMMs were experimented with diagonal and full covariance matrices for the Gaussians. They can be used as point-cloud generators by sampling the latent-space from the GMM distribution and using the AE's decoder. Shapes from the ShapeNet repository were reconstructed using class-specific AEs, trained with an 85%-5%-10% split for training/testing/validation. The quality of unsupervised representation learning algorithms can be evaluated by applying them as feature extractors on supervised datasets. The latent features computed by an AE were evaluated by using them as feature extractors on supervised datasets. The AE was trained on 57,000 models from 55 categories of man-made objects with a bigger bottleneck of 512. Features for an input 3D shape were obtained by feeding its point-cloud to the network and extracting a 512-dimensional bottleneck layer vector, which was then processed by a linear classification SVM. The 512-dimensional bottleneck layer vector extracted from the input 3D shape is processed by a linear classification SVM trained on ModelNet BID32. The decoupling of latent representation from generation allows flexibility in choosing the AE loss, affecting the learned feature. CD loss performs better on collections with increased variation, possibly due to its more local and less smooth nature. The CD loss performs better on collections with increased variation, possibly due to its more local and less smooth nature, allowing it to understand rough edges and high-frequency geometric details. The experiment also demonstrates the domain-robustness of the learned features, as shown in the qualitative evaluation with reconstruction results. Our learned representation can generalize to unseen shapes, enabling shape editing applications like interpolations, part editing, and analogies. The AEs can reconstruct unseen shapes with high fidelity and coverage, as shown in quantitative measurements. Five generative models were trained and compared on chair point-cloud data distribution. Five generative models were trained on chair point-cloud data distribution, including two AEs with a 128-dimensional bottleneck trained with CD or EMD loss, referred to as AE-CD and AE-EMD. Additionally, l-GANs were trained in each AE's latent space, along with a GMM and r-GAN directly on the point cloud data. Model selection involved training GANs for a maximum of 2000 epochs and selecting one epoch for the \"final\" model. The study trained five generative models on chair point-cloud data, including AEs with a 128-dimensional bottleneck, l-GANs, GMMs, and r-GAN. Model selection involved training GANs for a maximum of 2000 epochs and selecting the \"final\" model based on synthetic results matching the ground-truth distribution using JSD or MMD-CD metrics. The distance between synthetic and validation sets was measured to select models every 100 epochs (50 for r-GAN). The study trained five generative models on chair point-cloud data, including AEs with a 128-dimensional bottleneck, l-GANs, GMMs, and r-GAN. Model selection involved training GANs for a maximum of 2000 epochs and selecting the \"final\" model based on synthetic results matching the ground-truth distribution using JSD or MMD-CD metrics. The synthetic dataset was compared to the validation dataset every 100 epochs (50 for r-GAN). GMMs performed better with full covariance matrices, suggesting strong correlations between latent dimensions. Models were selected based on JSD criterion, with 32 components for GMMs and 40 Gaussians for MMD-CD criterion. The study evaluated five generative models on chair point-cloud data, comparing their ability to generate synthetic samples resembling the ground truth distribution. The optimal number of Gaussians for the models was determined to be 40. The average classification score for the ground-truth point clouds was 84.7%. The models were compared based on their capacity to generate synthetic samples that match the train and test splits of the ground truth distribution. The results are reported in Table 2. The study evaluated five generative models on chair point-cloud data, comparing their ability to generate synthetic samples resembling the ground truth distribution. The models were tested on train and test splits, with results reported in Table 2. A Gaussian mixture model with 32 components was used to match the test split dataset, with average measurements reported in TAB10. Training a simple Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of fidelity and coverage. The achieved fidelity and coverage are very close to the reconstruction baseline, with comparable MMD values to the ground truth training data. The generalization ability of the models is established by comparing performance on training vs. testing splits in TAB10. The generalization ability of the models is highlighted by comparing performance on training vs. testing splits in TAB10. Synthetic datasets are generated for testing and validation splits to reduce sampling bias when measuring MMD or Coverage statistics. The small size of the test and validation sets helps reduce sampling bias when measuring MMD or Coverage statistics. The MMD-CD distance to the test set appears small for r-GANs, but qualitative inspection shows otherwise. The inadequacy of the chamfer distance to distinguish pathological cases is highlighted with examples in Fig. 3. The study compares l-GAN and r-GAN on synthetic point clouds using chamfer distance and earth mover's distance. Results show r-GAN generates lower quality clouds due to points concentrated in likely occupied areas. Chamfer distance fails to capture this difference. The study compares l-GAN and r-GAN on synthetic point clouds using chamfer distance and earth mover's distance. Results show r-GAN generates lower quality clouds with points concentrated in likely occupied areas. Chamfer distance is blind to partial matches, leading to a larger coverage metric compared to EMD. EMD promotes one-to-one mapping, correlating more strongly to visual quality and heavily penalizing r-GAN in terms of MMD and coverage. Training trends were monitored during model training to understand their behavior. The study compared l-GAN and r-GAN on synthetic point clouds using chamfer distance and earth mover's distance. Results show that r-GAN struggles to provide good coverage of the test set, indicating the difficulty in training end-to-end GANs. The l-GAN (AE-CD) performs better in terms of fidelity with much fewer epochs. The study compared l-GAN and r-GAN on synthetic point clouds using chamfer distance and earth mover's distance. The l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs, but its coverage remains low. Switching to an EMD-based AE (l-GAN, AE-EMD) dramatically improves coverage and fidelity. However, both l-GANs suffer from mode collapse issues. The study compared l-GAN and r-GAN on synthetic point clouds using chamfer distance and earth mover's distance. The l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs, but its coverage remains low. Switching to an EMD-based AE (l-GAN, AE-EMD) dramatically improves coverage and fidelity. However, both l-GANs suffer from mode collapse issues. Training starts with good fidelity levels but coverage drops, indicating overfitting. A catastrophic collapse occurs with coverage as low as 0.5%. Switching to a latent WGAN largely eliminates this collapse. Comparisons to voxel-based methods are made, with GANs on point-cloud data being proposed for the first time. Comparisons are made with a recent voxel-grid based approach in terms of JSD on the training set of the chair category. The study compared l-GAN and r-GAN on synthetic point clouds using chamfer distance and earth mover's distance. The r-GAN outperforms BID31 in diversity and realism, while l-GANs perform even better with less training time. The l-GAN outperforms the r-GAN in generating synthetic point clouds with higher quality and less noise, showcasing the advantage of its smaller architecture and dimensionality. The study also highlights the strength of the learned representation, as evidenced by the high-quality results produced by both models. The l-GAN produces crisper and less noisy results compared to the r-GAN when generating synthetic point clouds. Experiments were conducted with an AE-EMD trained on a mixed set of point clouds from 5 categories, with a bottleneck size of 128 and trained for 1000 epochs. The training and testing datasets for the AE were constructed by randomly selecting models from each class, with 2K models per class for training, 200 for testing, and 100 for validation. The multi-class AE with a bottleneck size of 128 was trained for 1000 epochs and compared against class-specific AEs trained for 500 epochs. Additionally, six l-WGANs were trained for 2K epochs and evaluated for fidelity using MMD-CD. The l-WGANs based on the multi-class AE performed similarly to the class-specific ones. The study compared l-WGANs based on multi-class AE with dedicated class-specific ones, finding similar performance. Qualitative comparison showed no significant sacrifice in visual quality. Limitations included failure cases with rare geometries and missed high-frequency details. Some failure cases of models include chairs with rare geometries not being faithfully decoded and missing high-frequency details like a hole in the back of a chair, altering the input shape's style. The r-GAN struggles to create realistic shapes for certain classes, particularly cars. Designing more robust raw-GANs for point clouds is an interesting avenue for future work. Over-regularization in VAEs can lead to a strong regularization term affecting reconstruction quality. In the literature, over-regularization in VAEs can impact reconstruction quality. Methods exist to gradually increase the regularizer weight. Fixing the AE before training generative models has shown good results in 3D point-cloud representation learning. The novel architectures demonstrate strong generalization to unseen data and meaningful semantic encoding. The best-performing generative model is a GMM trained in the fixed latent space of an AE, producing faithful samples without memorizing specific examples. The best-performing generative model is a GMM trained in the fixed latent space of an AE, showing that simple classic tools should not be dismissed. The AE used for experiments had specific encoder and decoder configurations, with batch normalization and online data augmentation applied during training for 1000 epochs. The AE used for experiments had specific encoder and decoder configurations with batch normalization and online data augmentation. The AE was trained for 1000 epochs with CD loss and 1100 epochs with EMD. Different AE setups did not provide noticeable advantages over the \"vanilla\" architecture. Adding drop-out layers resulted in worse reconstructions, while using batch-norm on the encoder only sped up training and slightly improved generalization error. The discriminator in the r-GAN model consists of 1D-convolutions with specific filter sizes and leaky-ReLU activation, followed by max-pooling. The generator has multiple fully connected layers with varying numbers of neurons. The model was trained using Adam optimizer with specific hyperparameters and a noise vector drawn from a Gaussian distribution. The r-GAN model includes neurons of different sizes and is trained with specific hyperparameters using Adam optimizer. The discriminator and generator have specific architectures with fully connected layers. For the l-Wasserstein-GAN, a gradient penalty regularizer is used, and the critic is trained for 5 iterations per generator iteration. Classification experiments use a linear SVM classifier with specific parameters. The classification experiments in Section 4.1 utilized a one-versus-rest linear SVM classifier with specific parameters. Table 5 displays the training parameters of SVMs for each dataset with structural loss of the AE, including C-penalty, intercept, and loss functions. The reconstruction quality of CD and EMD-based AEs is compared in terms of JSD with their ground truth counterparts. The reconstruction quality of CD and EMD-based AEs is comparable between training and test datasets, showcasing their ability to generalize across different shapes. The AE-EMD embedding trained across all 55 object classes enables shape editing applications, such as tuning car appearance, adding armrests to chairs, and removing handles from mugs using vector arithmetic in the AE latent space. The AE latent space allows for editing shapes by using vector arithmetic, such as tuning car appearance, adding armrests to chairs, and removing handles from mugs. Structural differences between object sub-categories can be modeled using the average latent representations. Interpolation between different point clouds in the latent space showcases variations in shapes. The latent space representation allows for editing shapes by transforming their properties through vector arithmetic. Interpolating between different shapes in the latent space produces intermediate variants, enabling morphing between shapes with significantly different appearances. The latent space representation enables morphing between shapes of different classes by interpolating between shapes and finding shape analogies through linear manipulations and nearest-neighbor searching. In this section, the authors demonstrate finding shape analogies by adding shape A to shape B and searching for the nearest-neighbor in the latent space, yielding shape B. They present preliminary results of point-cloud generators working with voxel-based autoencoders, using a full-GMM model with 32 centers on ShapeNet's chair class. The generated voxel-grids were converted into 2048. The authors compared their full-GMM model with 32 centers to two different grid resolutions on ShapeNet's chair class. They converted the generated voxel-grids into 2048 points and compared their results with Wu et al.'s voxel-based GANs. The latent AE-based GMM models outperformed Wu et al.'s GAN architecture significantly. The latent representation provided a vast improvement in coverage. For more details and quantitative results, refer to Table 7. The latent representation in Wu et al.'s GAN architecture provides a significant improvement in coverage compared to the raw voxel GAN architecture. The performance of the 64^3 voxel-based GMM is comparable to the one operating at 32^3 resolution, indicating that fidelity is not solely dependent on high-frequency details. Point-cloud-based models outperform voxel-based models in fidelity, as measured by the MMD. The coverage boost of voxel-based latent-space models is likely due to the computation of the coverage metric. Our point-cloud-based models outperform voxel-based models in fidelity, as measured by the MMD. Voxel-based models often produce shapes with missing components, leading to poor quality matchings with the ground truth. The histogram analysis shows a heavier \"tail\" for voxel-based methods, indicating the presence of low-quality matchings. The voxel-based method shows poor quality matchings with the ground truth, especially for very poor quality partial shapes. The GMMs with full covariances and different dimensional latent codes were used for the mesh conversion. The voxel-based method shows poor quality matchings with the ground truth, especially for very poor quality partial shapes. GMMs with full covariances and different dimensional latent codes were used for mesh conversion. The mesh conversion utilized the marching cubes algorithm with an iso-surface value of 0.5. The point-cloud based GMM results are shown in the rightmost column. The voxel-based AEs are fully-convolutional with specific layer parameters listed for the encoders and decoders. Each AE was trained for 100 epochs with Adam under the binary. The dense voxel-based AE architectures were trained for 100 epochs using Adam and binary cross-entropy loss. Reconstruction quality was compared to the state-of-the-art method of BID28 for the ShapeNetCars dataset, measuring intersection-over-union between input and synthesized voxel grids. The GMM-generator is compared against a model that memorizes the training data of the chair class. Different sizes of training sets are evaluated for coverage/fidelity with respect to the test set, showing the validity of the metrics used. The advantage of using a learned representation is highlighted. The memorization of the training set in the GMM-generator case shows good coverage and fidelity with the test set, validating the metrics used. Using a learned representation allows for compactly representing data and generating novel shapes. Despite some mode collapse, the generative models achieve excellent fidelity. Additional comparisons with BID32 for ShapeNet classes are provided in Tables 10, 11, and 12. JSD-based comparisons for two models are also included. In Tables 10, 11, 12, comparisons with BID32 for ShapeNet classes are provided. JSD-based comparisons for two models are included. Generalization error of various GAN models is shown in FIG0, measuring closeness to training and test distributions. GMM model selection is illustrated in Figure 17. The JSD and MMD-CD metrics are used to estimate generalization of various GANs at different training epochs. GMM model selection is shown in Figure 17, where models with full covariance achieve smaller JSD than those with diagonal covariance. The number of clusters needed for minimal JSD is discussed, along with the covariance matrix components. In Figure 18, a matrix of a Gaussian component is displayed in pseudocolor, highlighting strong off-diagonal components. The 32 centers of the GMM fitted to latent codes are shown, along with evaluation results of five generators on a test-split of chair data. The models were selected based on minimal MMD-CD on the validation-split, with reported scores being averages of three repetitions. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. Synthetic point-clouds were sampled for each model, and their similarity to the ground truth was measured in terms of MMD-CD. This evaluation complements a previous one using a different measure, showing similar behavior."
}