{
    "title": "SyerXXt8IS",
    "content": "Auto-generate enhanced input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, particularly in the insect olfactory network. The network utilizes competitive inhibition, sparse connectivity, and Hebbian updates for rapid odor learning. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming traditional methods like PCA and NNs. \"Insect cyborgs\" combining BNN and ML methods show significantly improved performance on MNIST and Omniglot datasets, reducing test set errors by 20% to 55%. The MothNet feature generator outperforms traditional methods like PCA and NNs, showing significant improvements in performance on vectorized MNIST and Omniglot datasets. It reduces test set errors by 20% to 55%, highlighting the potential value of BNN-inspired feature generators in the ML context. The MothNet feature generator, inspired by biological neural nets, improves ML methods' ability to learn from limited data by automatically generating new class-separating features. It is based on the insect olfactory network, containing the Antennal Lobe (AL) and Mushroom Body (MB), and incorporates competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. This computational model shows significant performance improvements on vectorized MNIST and Omniglot datasets. The MothNet model, inspired by biological neural nets, incorporates competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. It demonstrates rapid learning of vectorized MNIST digits with superior performance compared to standard ML methods. The model includes competitive inhibition in the Antennal Lobe (AL) and sparsity in the Mushroom Body (MB). The MothNet model incorporates competitive inhibition and sparsity in the Mushroom Body (MB), with weight updates affecting only MB\u2192Readout connections through Hebbian updates. It was tested as a front-end feature generator for an ML classifier. The MothNet model was tested as a front-end feature generator for an ML classifier, combining MothNet with a downstream ML module to improve accuracies of ML methods on a non-spatial dataset. The MothNet model significantly improved ML method accuracies by generating stronger features compared to other methods like PCA, PLS, NNs, and transfer learning. vMNIST dataset was created by downsampling and vectorizing MNIST data to 85 pixels-as-features, providing an advantage for baseline ML methods. The insect-derived network (MothNet) outperformed other methods in generating strong features for the vMNIST dataset. Baseline ML methods did not achieve full accuracy at low N due to training data restrictions. Full network architecture details and Matlab code for the experiments can be found in references [11] and [12]. MothNet instances were randomly generated from connectivity templates for experiments comparing Cyborg vs baseline ML methods on vMNIST. MothNet outperformed other methods in generating features for vMNIST dataset. Experiments compared Cyborg vs baseline ML methods. MothNet trained using stochastic differential equation simulations and Hebbian updates. Trained ML accuracies of baselines and cyborgs were compared to assess gains. The effectiveness of MothNet features was compared to features generated by conventional ML methods using vMNIST experiments. Different feature generators were tested, including PCA, PLS, and NN pre-trained on vMNIST training samples. NN with weights initialized by training on an 85-feature vectorized Omniglot data was also used. The MothNet architecture was used as a feature generator for SVM and Nearest Neighbors in vMNIST experiments. Transfer learning from an 85-feature Omniglot dataset was applied to improve NN baseline performance. Adding extra hidden layers did not enhance performance, indicating MothNet features were not equivalent to simply adding layers. MothNet features significantly improved ML accuracy, showcasing their effectiveness in capturing new class-relevant features. MothNet features significantly improved ML accuracy across all models, with gains ranging from 10% to 88%. The relative reduction in test set error was 20% to 55%, demonstrating the effectiveness of MothNet in capturing new class-relevant features. MothNet features increased raw accuracy across all ML models, with a relative reduction in test set error of 20% to 55%. NN models saw the greatest benefits, with a 40% to 55% relative reduction in test error. Gains were significant in almost all cases with N > 3, and the MothNet readouts contained clustering information that ML methods leveraged effectively. The MothNet architecture, specifically the competitive inhibition layer (AL) and high-dimensional sparse layer (MB), was tested for effectiveness. Results showed that MothNet features were more effective than other methods, with significant gains in accuracy. When using a pass-through AL layer for MothNet, the effectiveness of the MB layer alone was tested in vMNIST experiments. The MothNet architecture was tested for effectiveness, with the high-dimensional, trainable layer (MB) being the most important. Cyborgs with a pass-through AL layer still showed significant improvements in accuracy, with gains between 60% and 100% of those with normal ALs. The competitive inhibition of the AL layer added value in generating strong features, contributing up to 40% of the total gain. NNs benefitted the most from the AL layer. The MothNet architecture deployed an automated feature generator based on a bio-mimetic BNN, significantly improving learning abilities of standard ML methods on vMNIST and vOmniglot. MothNet features were more useful than PCA, PLS, NNs, and pre-training, with the competitive inhibition layer enhancing classification by creating attractor basins for inputs. The MothNet architecture utilizes features that outperform standard methods like PCA, PLS, NNs, and pre-training. The competitive inhibition layer enhances classification by creating attractor basins for inputs, increasing the effective distance between samples of different classes. The sparse connectivity from AL to MB provides computational and anti-noise benefits, resembling sparse autoencoders but with key differences. The MothNet architecture differs from Reservoir Networks as MB neurons lack recurrent connections. The Hebbian update mechanism in MothNet is distinct from backprop, with weight updates occurring on a \"use it or lose it\" basis. The dissimilarity of optimizers (MothNet vs ML) may increase total encoded information."
}