{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. This method relies on reparametrizations and deals with breakdowns in inference-network gradients as the number of Monte Carlo samples increases. Different approaches, such as 'sticking-the-landing' IWAE (IWAE-STL) and 'doubly-reparametrised' IWAE (IWAE-DREG) gradients, have been proposed to address these issues. In this work, the authors argue that directly optimizing the proposal distribution in importance sampling is preferable to optimizing IWAE-type multi-sample objectives. They introduce an adaptive importance sampling framework called AISLE, which generalizes the RWS algorithm. AISLE admits IWAE-STL and IWAE-DREG as special cases, avoiding breakdowns in the IWAE gradients. The curr_chunk discusses variational inference algorithms for learning generative models and constructing tractable variational approximations. It focuses on optimizing the proposal distribution and avoiding breakdowns in IWAE gradients. The curr_chunk discusses optimizing generative models using variational inference algorithms, focusing on avoiding breakdowns in IWAE gradients and employing stochastic gradient-ascent algorithms with Monte Carlo samples. The curr_chunk discusses different algorithms for optimizing generative models using stochastic gradient-ascent with Monte Carlo samples. Roeder et al. (2017) propose a method that introduces bias in the IWAE objective, while Tucker et al. (2019) present the unbiased IWAE-DREG algorithm. Bornschein & Bengio (2015) introduce the RWS algorithm, which optimizes two separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling with K particles. The curr_chunk discusses the RWS algorithm, an adaptive importance-sampling approach that optimizes \u03b8 and \u03c6 simultaneously. It is compared to IWAE, with IWAE-STL gradient performing well but suffering from \u03c6-gradient breakdown. The study aims to determine the preferable approach between multi-sample objective of IWAE and adaptive importance-sampling of RWS. In this work, it is shown that directly optimizing the proposal distribution, as done by RWS, is preferable to optimizing the IWAE multi-sample objective. The IWAE approach relies on reparametrizations, leading to \u03c6-gradient breakdown, while modifications like IWAE-STL and IWAE-DREG can be justified through an adaptive importance-sampling view. This conclusion aligns with previous findings that IWAE can be inferior to RWS, especially for discrete latent variables. Our work introduces the adaptive importance sampling for learning (AISLE) framework, which generalizes the RWS algorithm and includes IWAE-DREG and IWAE-STL gradients. The AISLE framework allows for the derivation of various gradient estimators in a principled manner, ensuring they do not degenerate as K \u2192 \u221e. Specifically, we establish the recovery of the IWAE-STL gradient. Most previously proposed gradient estimators can be derived in a principled manner, ensuring they do not degenerate as K \u2192 \u221e. Connections are established, such as recovering the IWAE-STL gradient as a special case of AISLE. The breakdown of RWS observed in previous work may not be due to its lack of a joint objective. The AISLE framework also admits the IWAE-DREG gradient as a special case, with the learning rate scaled as O(K) for the IWAE \u03c6-gradient. Theoretical foundation for IWAE-STL and AISLE, proving AISLE admits IWAE-DREG gradient as a special case. Learning rate scaling for IWAE \u03c6-gradient and AISLE. AISLE leads to new gradient estimators for \u03b1-divergences. Insights on self-normalisation bias and comparison of algorithms in supplementary materials. Focus not on degeneration as K \u2192 \u221e. In this work, the focus is on the impact of self-normalization bias on importance-sampling based gradient approximations and empirical comparisons of various algorithms. The notation is simplified for concise presentation. The notation is simplified for concise presentation, focusing on the impact of self-normalization bias on importance-sampling based gradient approximations and empirical comparisons of algorithms. The estimator's bias vanishes at rate O(K \u22121) under mild assumptions. The importance weighted autoencoder (IWAE) seeks to maximize a lower bound on the log-marginal likelihood by optimizing the inference-network parameters. As the number of samples K increases, the evidence bound tightens, and the bound approaches log Z \u03b8 as K approaches infinity. For K = 1, the IWAE reduces to the variational autoencoder (VAE), but for K > 1, the algorithm differs according to Cremer et al. (2017) and Domke & Sheldon (2018). The IWAE tightens the evidence bound as K increases. For K = 1, it is equivalent to the VAE, but for K > 1, it becomes another VAE on an extended space. The gradient of the IWAE objective can be approximated using a Monte Carlo approach, but the high variance often makes it impractical. The reparametrisation trick is commonly used to address this issue. The IWAE tightens the evidence bound as K increases, with the gradient often being impractically noisy. The reparametrisation trick is used to address this issue, involving a distribution q and differentiable mappings. The IWAE gradient has drawbacks, with three specifically mentioned. The IWAE gradient has drawbacks related to reparametrisations and vanishing signal-to-noise ratio, leading to high variance and implementation challenges. The IWAE gradient has drawbacks related to reparametrisations and vanishing signal-to-noise ratio, leading to high variance and implementation challenges. Two modifications have been proposed to address these issues, including IWAE-STL and IWAE-DREG, which aim to stabilize the signal-to-noise ratio and achieve zero variance under certain conditions. The IWAE gradient has limitations due to reparametrisations and vanishing signal-to-noise ratio, resulting in high variance and implementation difficulties. Two approaches, IWAE-STL and IWAE-DREG, have been introduced to address these issues by stabilizing the signal-to-noise ratio and reducing variance under specific conditions. The IWAE-STL method ignores score function terms, while IWAE-DREG removes them through a reparametrisation technique. The gradients in both methods are approximated due to intractable quantities, with biases discussed in Appendix A. The self-normalised importance sampling approximation \u03c0 \u03b8 \u03c6, z is used in optimization, with biases discussed in Appendix A. The lack of a joint objective for both \u03b8 and \u03c6 is a drawback of RWS. The ordered self-normalised importance weights are mainly supported on the two particles with the largest weights in high-dimensional settings. In high-dimensional settings, importance weights are mainly supported on two particles with the largest self-normalised weights. Adapting the proposal distribution in importance-sampling schemes may not always require minimizing the KL-divergence. Other techniques, such as minimizing the \u03c7 2 -divergence, can sometimes be preferable. In high-dimensional settings, importance weights are mainly supported on two particles with the largest self-normalised weights. Adapting the proposal distribution in importance-sampling schemes may not always require minimizing the KL-divergence. Another popular approach is based on minimizing the \u03c7 2 -divergence. The RWS-objective is slightly generalized as \u03b8 := arg max \u03b8 log Z \u03b8, \u03c6 := arg min \u03c6 D\u0192(\u03c0\u03b8 q \u03c6). The resulting algorithm is called adaptive importance sampling for learning (AISLE), which permits a straightforward derivation of robust \u03c6-gradient estimators. Optimization is performed via stochastic gradient-ascent. The unified framework of adaptive importance sampling for learning (AISLE) allows for robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e. Optimization is done through stochastic gradient-ascent, with the \u03b8-gradient being the same for all algorithms discussed. AISLE interprets the \u03b8-gradient as a self-normalized importance-sampling approximation of the gradient \u2207 \u03b8 log Z \u03b8. Integrals of the form \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) can be approximated using the vanilla Monte Carlo method with a bias of order O(K \u22121 ) and a standard deviation. The curr_chunk discusses the optimization of f-divergences in variational inference without relying on the knowledge of Z\u03b8. It mentions approximating integrals with self-importance sampling and additional importance-sampling approximations. The approximation has a bias of order O(K\u22121) and a standard deviation of order O(K\u22121/2). The curr_chunk discusses the reparametrised estimator and specific cases of optimization using self-importance sampling and reparametrisation. It shows how IWAE-STL can be derived from AISLE without the need for a multi-sample objective. The gradient of Equation (13) can be obtained through reparametrisation, leading to the derivation of IWAE-STL from AISLE without requiring a multi-sample objective. Proposition 1 establishes a theoretical basis for IWAE-STL, addressing issues highlighted in previous studies and showcasing its empirical performance. The breakdown of RWS in IWAE-STL's good empirical performance suggests it may not be due to RWS' lack of optimizing a joint objective. AISLE-KL reduces bias and variance by approximating the exact \u03c6-gradient. The \u03b1-divergence between distributions p and q can be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz with \u03ba = \u2212\u03b1 and f (y) = y \u03b1. The \u03b1-divergence between two distributions p and q is given by Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz with \u03ba = \u2212\u03b1 and f (y) = y \u03b1. Minimizing this divergence is important in importance sampling. AISLE-\u03b1-NOREP and AISLE-\u03b1 are methods for optimization, with the latter using reparametrisation. This demonstrates the derivation of IWAE-DREG. The text discusses the derivation of IWAE-DREG using AISLE-\u03b1 without the need for a multi-sample objective. It also highlights the scaling of the learning rate for IWAE or IWAE-DREG \u03c6-gradients. Additionally, it mentions the approximation for the 'exclusive' KL-divergence. The text discusses optimizing the 'exclusive' KL-divergence for faster convergence of \u03c6 in IWAE or IWAE-DREG. However, minimizing this divergence may impact learning of \u03b8 due to potential issues with importance weights. The adaptive-importance sampling paradigm of reweighted wake-sleep (RWS) is preferred over the multi-sample objective paradigm of importance weighted autoencoders (IWAEs) due to its ability to achieve the same goals while avoiding drawbacks. In the self-normalised importance-sampling approximation, the number of particles, K, can vary between two extremes to provide accurate approximations. The self-normalised importance-sampling approximation allows for varying the number of particles, K, to provide accurate approximations. As K approaches infinity, estimators become more accurate, while for K=1, they reduce to vanilla Monte Carlo approximations. This insight applies to various estimators, including the reparametrisation-free AISLE \u03c6 gradients. The self-normalised importance-sampling approximation with varying particles, K, can provide accurate estimations. For K=1, it reduces to vanilla Monte Carlo approximations. The small-K self-normalisation bias of reparametrisation-free AISLE \u03c6 gradients favors minimizing the exclusive KL-divergence. The use of IWAEs aims to reduce bias in the \u03b8-gradient relative to \u2207 \u03b8 log Z \u03b8 by using self-normalised importance-sampling with K > 1 particles. The error of importance-sampling approximations can be controlled by ensuring q \u03c6 is close to \u03c0 \u03b8. The family Q contains proposal distributions indexed by \u03c6, with two scenarios: 1. Sufficiently expressive Q, and 2. Flexible enough to contain a distribution q \u03c6 close to \u03c0. The family of proposal distributions can be divided into two scenarios: 1. When Q is sufficiently flexible and contains a distribution close to \u03c0 \u03b8, minimizing the exclusive KL-divergence yields well-behaved importance weights. 2. When Q is not flexible enough and all its members are far from \u03c0 \u03b8, minimizing the exclusive KL-divergence could lead to poorly-behaved importance weights. In Scenario 1, minimizing exclusive KL-divergence with a gradient-descent algorithm can be preferable for faster convergence. A smaller number of particles, K, may be better for some \u03c6-gradients due to self-normalisation bias outweighing standard deviation. Simply setting K = 1 for approximating \u03c6-gradients may not be ideal. In Scenario 1, minimizing exclusive KL-divergence with a gradient-descent algorithm can be preferable for faster convergence. Setting K = 1 for approximating \u03c6-gradients may not be optimal due to various reasons. In Scenario 1, minimizing exclusive KL-divergence with a gradient-descent algorithm can be preferable for faster convergence. Setting K = 1 for approximating \u03c6-gradients may not be optimal due to various reasons. The different \u03c6-gradient estimators are compared in the supplementary materials, including AISLE-KL-NOREP and AISLE-KL. These estimators provide gradients based on the KL-divergence with or without reparametrization. The gradient for AISLE can be based on different divergences such as KL-divergence and \u03c72-divergence, with or without reparametrization. These gradients do not require R1 but may not achieve zero variance even if q \u03c6 = \u03c0 \u03b8. The proportionality constant cancels when normalizing the gradients. The gradient approximations for IWAE and IWAE-DREG are computationally equivalent when normalizing the gradients. IWAE uses the reparametrization trick and its signal-to-noise ratio degenerates with K. IWAE-DREG is proportional to AISLE-\u03c72. RWS-DREG is a doubly-reparametrised gradient. The 'doubly-reparametrised' IWAE gradient from Tucker et al. (2019) is proportional to AISLE-\u03c72. The 'doubly-reparametrised' RWS \u03c6-gradient is parametrised by \u03b8 and factorises the joint law of observations and latent variables. The proposal distributions are fully factored Gaussians with parameters to optimize. The reparametrisation trick uses q( ) := N( ; 0, I). The proposal distributions are fully factored Gaussians with parameters to optimize, and the reparametrisation trick uses q( ) := N( ; 0, I). The model allows for correlated/dependent latent vectors z under the generative model, unlike previous benchmarks. In a more realistic scenario, the latent vectors z can be correlated under the generative model. The variational approximation, however, remains fully factored and may not capture uncertainty about the latent variables effectively. The 'score-function free' \u03c6-gradients achieve near-zero variance for proposal mean parameters when variance parameters are close to optimal values. The 'score-function free' \u03c6-gradients achieve near-zero variance for proposal mean parameters when variance parameters are close to optimal values. Additionally, the variance of the C-gradient portion also goes to zero as the model dimensions and number of particles vary. Further analysis of the benefits of reparametrisation-trick gradients in Gaussian settings is discussed in Xu et al. (2019). Empirical comparisons of algorithms are conducted for different numbers of particles and model dimensions, with each configuration repeated independently 100 times using new synthetic data sets. The study explores the optimization of \u03c6 for different model settings with varying numbers of particles and dimensions. Results show that for certain generative models, the variational approximation may not fully capture the latent variable dependence structure. The generative model is specified with a covariance matrix \u03a3 that does not fully mimic the dependence structure of the latent variables. The gradient-ascent algorithm is initialized with standard normal distribution values for \u03c6. Stochastic gradient-ascent and ADAM optimization methods are used with a total of 10,000 iterations. The covariance matrix \u03a3 is not diagonal, and logarithmic scaling is noted. The total number of iterations is 10,000; learning-rate parameters are i \u22121/2. The covariance matrix \u03a3 is not diagonal. Note the logarithmic scaling on the second axis."
}