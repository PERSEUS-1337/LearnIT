{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the importance of providing online explanations during execution to reduce human mental workload is emphasized. The challenge lies in the interdependence of different parts of an explanation, requiring a careful approach to generating online explanations. Three different implementations with varying online properties are presented based on a model reconciliation setting. Evaluation was done with human subjects in a planning competition domain and simulation with different problems across two domains. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective, but a good explanation should consider the differences between the human and the robot's model. This approach can help generate lucid explanations that are easily understood by the recipient. To address the challenge of generating explanations from the recipient's perspective, the agent should consider discrepancies between the human and its own model. Model reconciliation involves the robot explaining and adjusting its behavior to align with the human's expectations, ensuring a shared understanding. The model reconciliation process involves the robot adjusting its behavior to align with the human's expectations. Explanations should be provided in an online fashion to reduce the mental workload on the human. In this work, the argument is made for providing online explanations that intertwine with plan execution to reduce mental workload. Generating online explanations must consider dependencies between different parts to ensure smooth communication without cognitive dissonance. An example is given of two friends, Mark and Emma, planning a study session with breaks in between. Mark and Emma plan to study together for an exam. Mark breaks the session into two parts with lunch in between, while Emma prefers one continuous session. Mark doesn't reveal his plan to Emma but suggests lunch after studying for 60 minutes. He later explains the need for energy before lunch and a walk after. Mark and Emma go to the library to study for an exam. Mark suggests a lunch break after 60 minutes, explaining the need for energy. He also plans to take a walk after lunch, gradually revealing his reasoning to Emma to make it acceptable and understandable. The key is to explain minimally and only when necessary, spreading out information throughout the plan execution. In this paper, a new method for explanation generation called online explanation is developed, intertwining explanation with plan execution. It considers the mental workload of the receiver by breaking explanations into parts communicated at different times during the plan execution. Three different approaches for online explanation generation are implemented, each focusing on different properties. The paper introduces a new method called online explanation, which breaks explanations into parts communicated at different times during plan execution to reduce mental workload. Three approaches are implemented, focusing on matching plan prefixes, making the next action understandable, and matching robot's plan with optimal human plan. The approaches are evaluated with human subjects and in simulation. The paper introduces online explanation as a method to reduce mental workload by breaking explanations into parts communicated at different times during plan execution. Explainable AI is essential for human-AI collaboration, improving human trust and maintaining shared situation awareness. Explainable AI is crucial for human-AI collaboration, enhancing trust and shared situation awareness. The effectiveness of explainable agency is evaluated based on accurately modeling the human's perception of the AI agent. This model allows the agent to generate understandable motions, plans, and assistive actions, considering both cost and explicability. Additionally, the AI agent can signal its intention before execution. The model allows AI agents to substitute cost optimality with a metric that considers cost and explicability. AI agents can signal their intention before execution to search for additional context information for human understanding. Agents can also explain their behavior by generating explanations based on the recipient's perception model. In the context of AI agents, explanations can be generated based on the recipient's perception model to aid in human understanding. Online explanation generation is proposed for complex explanations, providing minimal information intertwined with plan execution. The idea behind online explanation generation is to provide a minimal amount of information that explains part of the plan currently of interest, intertwining explanation generation with plan execution. The problem is closely associated with planning problems, defined as a tuple (F, A, I, G) using PDDL, similar to STRIPS. Actions have preconditions, add and delete effects, with initial and goal states. The robot's plan to be explained is \u03c0 * I,G. The state of the world and actions (A) are used to change it. Actions have preconditions, add and delete effects. I, G are initial and goal states. The robot's plan (\u03c0 * I,G) must be optimal according to model reconciliation (M R), considering human's model (M H) expectations. Reconciliation occurs when robot's behavior matches human's expectations. Model reconciliation setting considers both M R and M H. Explanation generation in a model reconciliation setting involves updating the human's model (M H) to align with the robot's plan (\u03c0 * I,G) for optimal explanation. A mapping function converts a planning problem into a set of features, facilitating the explanation generation process. The explanation generation problem involves reconciling two models by updating the human's model to align with the robot's plan. A complete explanation is one that minimizes the cost difference between the human's expected plan and the robot's plan after model updates. A minimal complete explanation contains the fewest unit feature changes necessary for optimization. The online explanation generation approach aims to provide minimal information during plan execution to explain the part of the plan that is of interest and not explainable. It involves generating a set of sub-explanations at different steps in the plan to address the mental workload requirement of the human for understanding the explanation. Online Explanation Generation involves creating a set of sub-explanations during plan execution to match the human's expectation. Three approaches are discussed: Plan Prefix matching, Next Action matching, and any prefix matching. The planning process for Online Explanation Generation involves generating sub-explanations to match human expectations by considering model changes and plan prefixes. The challenge lies in ensuring that future model changes do not disrupt previously reconciled plan prefixes. The planning process for Online Explanation Generation involves generating sub-explanations to match human expectations by considering model changes and plan prefixes. To address the issue of future model changes disrupting previously reconciled plan prefixes, a search process from M R to M H is conducted to find the largest set of model changes that ensure the plan prefix remains unchanged after further sub-explanations. This process is illustrated in FIG1. An OEG-PP is a set of subexplanations (e k , t k ) where Prefix(\u03c0, t) returns the plan prefix up to step t k\u22121. E k represents e 1:k and \u03c0 H E k is the optimal plan created from M H E k. The optimal plan \u03c0HEk is created from MHEk by recursively generating sub-explanations. The search process starts from the robot model and stops when the plan prefixes for the updated human model and robot model match. This approach is more akin to MME BID6 but requires running the process multiple times. Our research process, similar to MME BID6, focuses on matching prefixes rather than the whole plan in one shot. This requires running the process multiple times but allows us to outperform MCE and MME in terms of computation. The dotted line marks the border of the maximum state space model modification in the robot model, reconciling the two models up to the current plan execution. It finds the largest set of model changes to match the plan using the corresponding model. The algorithm focuses on finding model changes to match the plan using the corresponding model. It ensures compatibility with the prefix of the plan for future steps through a recursive search procedure in the model space. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. It involves a model reconciliation procedure to find the largest set of model changes that can satisfy constraints. The goal is to ensure the robot and human plan have the same prefix during plan execution. Explanation generation ensures that the robot and human plans have the same prefix during execution. The robot only needs to reconcile between M R and M H to match the very next action in the plan, regardless of earlier actions. This approach considers the human's limited cognitive memory span and focuses on explaining differences in the very next action between the human plan and the robot's plan. The agent focuses on explaining the next action that differs between the most recent human plan and the robot's plan. The search is performed from M H \\M H for computational efficiency, explaining only the immediate next action that doesn't match without comparing the entire plan prefix. This process is similar to minimally monotonically explanation (MME) in BID6. The OEG approach focuses on reconciling differences between human and robot plans by explaining actions that do not match, using a search process similar to minimally monotonically explanation (MME) in BID6. The algorithm combines search from M H and M R for better performance, assuming a set of optimal plans for the robot. The OEG approach aims to reconcile differences between human and robot plans by ensuring that the robot's plan prefix matches the human's optimal plan, without the need for exhaustive plan generation. This is achieved through a compilation approach that transforms the human's model problem to always align with the robot's plan prefix. To ensure that a plan prefix in the robot's plan matches the human's model, a compilation approach is used to align the human's model problem with the robot's plan prefix. By adding predicates and prerequisites to consecutive actions in the plan, the compilation ensures that the plan prefix is always satisfied in the compiled model. The compilation approach aligns the human's model problem with the robot's plan prefix by adding predicates and prerequisites to consecutive actions. This process continues until an optimal human plan matches the robot's plan. The compilation approach aligns the human's model problem with the robot's plan until an optimal human plan exists that matches the robot's plan. The approach was evaluated for online explanation generation with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. The evaluation was done on ten different problems across the rover and barman domains, highlighting differences in information needed and computation time between the approaches. Our approach, MH, differs from MCE in terms of information needed and computation time. Evaluation was done on ten problems in rover and barman domains. Differences between MH and MR were tested by randomly removing preconditions. Human subject study aimed to confirm benefits of online explanation generation, hypothesizing it reduces mental workload and improves task performance. Evaluation with human subjects was conducted in a modified rover domain on Mars, focusing on exploring space, taking samples, images, and communicating results to the base station. In the rover domain on Mars, the goal is to explore space, take rock and soil samples, and communicate results to the base station. The rover must calibrate its camera, have empty storage for samples, and can only store one sample at a time. To take multiple samples, it must drop the current one. In the barman domain, the robot serves drinks using dispensers, glasses, and a shaker, with constraints on object grabbing. The robot in the barman domain serves drinks using dispensers, glasses, and a shaker. Constraints include grabbing one object at a time and ensuring glasses are empty and clean before filling them. Simulation results compare explanations in the rover and barman domains, showing differences in the number of shared model features between MCE and OEG-PP approaches. The comparison between MCE and OEG-PP approaches in generating explanations shows differences in the total number of model features shared. OEG focuses on minimal information per time step, leading to more information shared overall compared to MCE. OEG-AP considers all optimal plans, while OEG-NA and OEG-AP still show a distance from the robot's plan. The OEG-NA and OEG-AP approaches show a distance from the robot's plan due to considering all optimal human plans. The plan distance gradually decreases towards 0, suggesting a smoother adjustment for M H during execution and explanation. This is illustrated in FIG3. In our implementation, model updates are sorted based on feature size, with algorithms starting with smallest changes from the robot's side. Backtracking occurs if consistency check fails. This search process capitalizes on later information not affecting previous sub-explanations. A human study compared three approaches for online explanation generation. The study compared three approaches for online explanation generation using a human study with 3D simulation on Amazon Mechanical Turk. Explanations were given in plain English with GIF images depicting rover actions in a simulated scenario. In an experiment, human subjects acted as rover commanders in a 3D simulated scenario on Mars. They had to determine if the rover's actions were questionable, with explanations provided using OEG approaches or MCEs. Each subject had a 30-minute time limit and also completed spatial puzzles to increase cognitive demand. Certain information was deliberately removed from the scenario to observe the effect on mental workload. In an experiment, human subjects acted as rover commanders in a 3D simulated scenario on Mars. To observe the effect on mental workload, spatial puzzles were added as a secondary task. Certain information was deliberately removed from the scenario to create incorrect plans. This hidden information introduced differences between model reconciliation settings, requiring explanations to be provided. In an experiment, human subjects acted as rover commanders in a 3D simulated scenario on Mars. The robot provides explanations in different settings, with information shared at the beginning in one setting and broken up in another. Subjects are asked to determine if the robot's actions make sense. NASA Task Load standard questionnaire is given at the end of the study. The study involved human subjects acting as rover commanders in a 3D simulated scenario on Mars. Explanations were provided by the robot in different settings, with information shared at the beginning in one setting and broken up in another. Subjects evaluated the efficiency of different explanation approaches using the NASA Task Load Index (TLX) questionnaire, which assesses mental workload. The study assessed mental workload using the NASA Task Load Index questionnaire, specifically focusing on mental demand. 150 human subjects were recruited on MTurk, with criteria set for worker acceptance rate. After filtering out invalid responses, 94 valid responses were obtained across different settings. The age range of subjects was 18 to 70, with 29.8% being female. The study involved 94 valid responses from human subjects aged 18 to 70, with 29.8% being female. It examined how well subjects understood the robot's plan using different explanations and compared distances across five settings. The distance metric calculated the closeness of human understanding to the robot's plan. Overall, results showed that OEG approaches were more effective. The study compared OEG and MCE approaches in reducing human mental workload. OEG approaches showed better performance in NASA TLX measures and had fewer questionable actions, indicating higher trust in robots. The experiment found that OEG approaches create more temporal demand due to intertwining explanation with plan execution. The human study compared OEG and MCE approaches, showing that OEG approaches had fewer questionable actions and higher accuracy in identifying correct actions. OEG-AP had the least questionable actions and highest accuracy. There was a significant difference in mental workload between OEG approaches and MCEs. Time analysis showed varying task completion times for each category. In a pairwise comparison, OEGs and MCEs showed a significant difference in mental workload with an overall p-value of 0.0068. Time analysis revealed varying completion times for different categories. A novel approach for explanation generation was introduced to reduce mental workload in human-robot interaction by breaking down complex explanations into smaller parts. The key idea is to break down complex explanations into smaller parts and convey them online during human-robot interaction. Three different approaches were provided, focusing on generating easily understandable explanations intertwined with plan execution. Evaluation using simulation and human subjects showed improved task performance and reduced mental workload, a step towards achieving explainable AI."
}