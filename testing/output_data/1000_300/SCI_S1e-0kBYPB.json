{
    "title": "S1e-0kBYPB",
    "content": "To ensure widespread public acceptance of AI systems, we need methods to explain decisions made by black-box models like neural networks. Current explanatory methods face issues related to different explanation perspectives and lack of validation on complex models. A verification framework for explanatory methods is introduced to address these challenges. The verification framework introduced evaluates explanatory methods for neural networks under the feature-selection perspective, providing guarantees on inner workings. It aims to address issues with current explainers and offer a publicly available evaluation tool. A variety of post-hoc explanatory methods have been developed to explain black-box machine learning models. Two widely used perspectives on explanations are feature-additivity and feature-selection, which provide different explanations for individual predictions. In practice, explanatory methods adhering to different perspectives are being directly compared. For example, Chen et al. (2018) and Yoon et al. (2019) compare L2X, a feature-selection explainer, with LIME and SHAP, two feature-additivity explainers. These comparisons may not be coherent due to fundamentally different explanation targets. Current explanatory methods successfully point out biases but face limitations in certain scenarios. The current explanatory methods successfully identify biases but may have limitations in explaining less dramatic biases in neural networks. Evaluating explainers on complex neural networks often assumes that the target models behave reasonably. The current explanatory methods may have limitations in explaining biases in neural networks. Evaluating explainers on complex models assumes that the models behave reasonably, avoiding reliance on irrelevant correlations. A proposed framework generates evaluation tests for explanatory methods under the feature-selection perspective. The framework proposed addresses the issue of evaluating explanatory methods for biases in neural networks by generating tests under the feature-selection perspective. It involves identifying tokens with zero contribution and those relevant to model predictions, testing if explainers rank them correctly. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis, highlighting the need for further testing to determine the explainers' effectiveness in general. The framework proposed evaluates explanatory methods for biases in neural networks by generating tests under the feature-selection perspective. It tests if explainers rank tokens with zero contribution and those relevant to model predictions correctly. The framework was applied to three pairs for multi-aspect sentiment analysis, providing guarantees on the behavior of the target model during evaluation tests. The evaluation penalizes explainers only for errors produced under these guarantees, introducing a non-speculative and automatic evaluation test. The study introduces an automatic evaluation test for explanatory methods in neural networks, focusing on feature-selection explainers. LIME and SHAP perform better than L2X in most cases, with detailed reasons provided in Section 5. Error rates of these explainers are highlighted to raise awareness of potential failure modes. In Section 5, it is explained why LIME and SHAP outperform L2X in most cases. Error rates of these explainers are highlighted to show possible failure modes, such as predicting relevant tokens as having zero contribution. A generic evaluation methodology is provided for testing future explanatory methods under the feature-selection perspective. Instance-wise explanatory methods typically explain predictions based on input unit-features. The evaluation of instance-wise explanatory methods focuses on feature-based explanations, which can be either feature-additive or feature-selective. Other types of explanations include example-based methods that identify relevant instances in the training set influencing the prediction. In this work, the focus is on verifying feature-based explainers, which are the majority of current works. The evaluation of explainers involves testing on interpretable target models like linear regression and decision models. The evaluation of explainers involves testing on interpretable target models such as linear regression and decision trees. Evaluations are commonly performed using four types of setups: interpretable target models, synthetic setups, and others. The evaluation of explainers involves testing on interpretable target models like linear regression and decision trees. Evaluations are commonly done using setups such as controlled feature tasks and assuming reasonable behavior. Crowd-sourcing evaluation is often used to verify if the explainer's features align with the model's predictions. Neural networks may uncover unexpected artifacts, making crowd-sourcing evaluations unreliable for assessing explainer faithfulness to the model. Evaluating if explanations aid humans in predicting model behavior involves comparing different explainers based on human prediction accuracy after viewing their explanations. Our evaluation framework is fully automatic and aims to assess the faithfulness of explainers to neural network models. It differs from human-based evaluations and is similar to a sanity check introduced by Adebayo et al. (2018). The framework guarantees the inner-workings of the neural network model and tests if explainers provide different explanations for models trained on real data. The evaluation framework assesses the faithfulness of explainers to neural network models by testing if they provide different explanations for models trained on real data. It is more challenging than previous tests and requires a strong fidelity of the explainer to the target model. Many explanatory methods adhere to the perspective of feature-additivity, where the explanation of a prediction consists of contributions from each feature that approximate the prediction. The explanation of a prediction consists of contributions from each feature that approximate the prediction. LIME learns weights via linear regression on the neighborhood of the instance, while Shapley values from game theory provide feature-additive contributions that verify desired constraints. The contribution of each feature in the instance is an average of its contributions over a neighborhood. The contribution of each feature in an instance is an average of its contributions over a neighborhood. The choice of neighborhood is critical, and it is uncertain which neighborhood is best to use in practice. Feature-selection involves finding a subset of features that alone lead to a similar prediction as the original one. The explanation of a model f (x) involves a subset S(x) of features that lead to the same prediction. Different perspectives on this include maximizing mutual information between S(x) and the prediction. The number of important features per instance is often unknown in practice. In sentiment analysis, different perspectives aim to provide instance-wise explanations for a regression model. Real-world neural networks may heavily rely on specific tokens in the input, which may not always be accurate indicators for the target class. In sentiment analysis, neural networks may heavily rely on specific tokens in the input, which may not always accurately indicate the target class. The feature-additive perspective aims to provide an average explanation of the model on a neighborhood of the instance, while the feature-selective perspective aims to identify the pointwise features used by the model. The feature-additive and feature-selective perspectives in sentiment analysis provide insights into the model's behavior by explaining the importance of different tokens in the input. The two perspectives may offer different rankings of features, such as \"good\" and \"nice\", highlighting the varying explanations provided by each perspective. This distinction can be crucial in real-world use-cases, where one perspective may be preferred over the other. In the paper, a verification framework is proposed for the feature-selection perspective of instance-wise explanations using the RCNN model. The framework prunes the dataset to identify irrelevant and relevant features for each datapoint. Metrics are introduced to measure the ranking of tokens by explainers. The RCNN model introduced by Lei et al. (2016) is leveraged in this framework. The RCNN model, introduced by Lei et al. (2016), consists of a generator and an encoder instantiated with recurrent convolutional neural networks. The generator selects a subset of tokens from input text x, which is then passed to the encoder for making predictions. The training involves no direct supervision on subset selection, with supervision only on the final prediction. Regularizers were also used in the training process. The RCNN model consists of a generator and an encoder that make predictions based on a subset of tokens selected by the generator. Training involves no direct supervision on subset selection, with regularizers used to encourage specific behaviors. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability. The model may have learned an internal communication protocol for encoding information. The RCNN model may have learned an internal communication protocol that encodes information from non-selected tokens, known as a handshake. The goal is to eliminate handshakes by ensuring that non-selected tokens have zero contribution to the model's prediction. The RCNN model aims to eliminate handshakes by ensuring non-selected tokens have zero contribution to the prediction. The model's selection of tokens may not always imply a handshake, as some tokens may be non-selected without affecting the prediction significantly. The RCNN model prunes the dataset to retain instances where non-selected tokens have zero contribution to the prediction. Some tokens may be noise and selected to ensure a contiguous sequence, but not relevant to the prediction. The RCNN model prunes the dataset to ensure a contiguous sequence and penalizes disconnected tokens during training. To avoid penalizing explainers for not differentiating between noise and zero-contribution features, the dataset is further pruned to include at least one clearly relevant token for the prediction. This is determined by checking the absolute change in prediction when removing a token, with a threshold \u03c4. Selected tokens are then partitioned into clearly relevant tokens and the rest. The dataset is pruned to include at least one clearly relevant token for the prediction, determined by the absolute change in prediction when removing a token with a threshold \u03c4. Selected tokens are then partitioned into clearly relevant tokens and the rest, ensuring important tokens are ranked higher. The dataset is pruned to include only datapoints with at least one clearly relevant token per instance. Evaluation metrics focus on explainers providing rankings over features, ensuring important tokens are ranked higher than others. The evaluation metrics focus on explainers providing rankings over features, ensuring important tokens are ranked higher than others. Error metrics include the percentage of instances where the most important token provided by the explainer is among the non-selected tokens, instances where a non-selected token is ranked higher than a relevant token, and the average number of non-selected tokens ranked higher than any relevant token. In this work, the evaluation metrics focus on explainers providing rankings over features, ensuring important tokens are ranked higher than others. The framework is instantiated on the RCNN model trained on the BeerAdvocate corpus, consisting of human-generated multi-aspect beer reviews. The RCNN model aims to predict fractional ratings for appearance, aroma, and palate aspects. The study focuses on using an RCNN model to predict ratings for appearance, aroma, and palate aspects in human-generated beer reviews. Three separate RCNNs are trained for each aspect, with a threshold of 0.1 considered significant for prediction changes. The study uses an RCNN model to predict ratings for appearance, aroma, and palate aspects in beer reviews. A threshold of 0.1 is considered significant for prediction changes. Various statistics of the datasets are provided in Appendix A, including average review lengths and percentages of eliminated datapoints. Three popular explainers, LIME, SHAP, and L2X, are tested. The study evaluates three popular explainers: LIME, SHAP, and L2X, using default settings with some modifications. LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature-selection explainer. In practice, LIME and SHAP outperformed L2X on most metrics, even though L2X is a feature-selection explainer. L2X's limitation is the need to know the number of important features per instance, which is not practical in real-world scenarios. The average number of tokens highlighted by human annotators was used as a substitute for this in the evaluation. In Table 1, explainers like LIME and L2X often misidentify the most relevant feature as a token with zero contribution. SHAP performs better by placing fewer zero-contribution tokens ahead of relevant ones. In the evaluation dataset, LIME and L2X explainers show differences in the placement of zero-contribution tokens ahead of relevant ones. LIME has around 1 zero-contribution token ahead for the first two aspects and 9 tokens for the third aspect, while L2X places 3-4 zero-contribution tokens ahead for all three aspects. The rankings of explainers on an instance from the palate aspect are shown in Figure 4, with the heatmap indicating the ranking of tokens. The evaluation dataset compares LIME and L2X explainers in ranking tokens. Both explainers attribute importance to nonselected tokens, with differences in ranking. L2X prioritizes \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as most important, while LIME ranks \"mouthfeel\" and \"lacing\" first. The study highlights a key distinction between explanation perspectives. In this work, the authors introduce an evaluation test for post-hoc explanatory methods and present error rates for three popular explainers. The methodology is adaptable to various tasks and areas, offering guarantees on the behavior of real-world neural networks. The methodology presented is generic and can be applied to different tasks and areas, such as computer vision. The core algorithm of current post-hoc explainers is domain-agnostic, highlighting the fundamental limitations of these explainers. Statistics of the dataset are provided in Table 2, showing the number of instances retained and average review length. The statistics of the dataset in Table 2 show the number of instances retained, average review length, and average numbers of selected tokens with different effects on prediction. The percentage of instances eliminated due to potential handshake and further eliminated datapoints are also provided. The dataset statistics in Table 2 display the instances retained, average review length, and average selected tokens with varying effects on prediction. Instances were eliminated due to potential handshake, with further datapoints removed for lacking a selected token with a significant effect on prediction. The model's prediction consistency when eliminating non-selected tokens determines the presence of a handshake in the instance. The dataset statistics in Table 2 show instances retained, average review length, and selected tokens affecting prediction. Instances were removed for potential handshake, with further data points eliminated for lacking significant token effects. Model prediction consistency determines handshake presence. No handshake in instance x means zero contribution from tokens in Nx. Proof concludes with Equation 11 condition satisfied. LIME: dark yellow color, fruity smell, no distinct taste. The beer poured a dark yellow color with a lot of head, smelling like fruit with no distinct taste. It had a slight warming effect and was better than most American lagers. The bottle was brown and resembled a \"grolsch\" bottle. The beer had a dark yellow color with a lot of head, smelling like fruit with no distinct taste. It had a slight warming effect and was better than most American lagers. The bottle was brown and resembled a \"grolsch\" bottle. The beer has a fruity taste with low alcohol content, giving a slight warming sensation. It is smoother than most American lagers and easy to drink quickly."
}