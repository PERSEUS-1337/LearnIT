{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters while improving performance. This arrangement also introduces additional regularization. The branches are tightly coupled by averaging their log-probabilities, promoting better representations and learning. This approach, termed \"coupled ensembles,\" is applicable to various neural network architectures. The paper explores a new architecture for deep convolutional networks called \"coupled ensembles,\" where parallel branches at the global network level reduce parameters and improve performance. This approach can be applied to various neural network architectures. Results show improved error rates on CIFAR-10, CIFAR-100, and SVHN tasks compared to traditional DenseNet-BC networks. The design of early convolutional architectures involved choices of hyper-parameters like filter size and number of filters. The current trend follows a template with fixed filter size of 3x3, down-sampling by half using maxpool or strided convolutions, and doubling feature maps after each down-sampling. State-of-the-art models like ResNet and DenseNet use this philosophy, with the addition of skip-connections. A new element called \"coupled ensembling\" decomposes the network into branches, achieving comparable performance. The proposed template introduces \"coupled ensembling\" by decomposing the network into branches, achieving comparable performance with fewer parameters. It is shown that splitting parameters among branches is more effective than a single branch. Combining activations through an arithmetic mean improves performance on CIFAR and SVHN datasets. Further ensembling of coupled ensembles enhances performance even more. The paper introduces \"coupled ensembling\" by decomposing the network into branches, achieving comparable performance with fewer parameters. Combining activations through an arithmetic mean significantly improves performance on CIFAR and SVHN datasets. Further ensembling of coupled ensembles leads to more improvement. The network architecture proposed has similarities with Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, but with differences in the coupled ensemble networks. The paper introduces \"coupled ensembling\" by decomposing the network into branches, achieving comparable performance with fewer parameters. The network architecture proposed has similarities with Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, but with differences in the coupled ensemble networks. The coupled ensemble networks differ in training approach, parameter budget allocation, activation combination, and input usage compared to existing models. The curr_chunk discusses modifications in multi-branch architectures for vision applications, focusing on grouped convolutions and template building blocks. In contrast to previous approaches, the proposed modification is at the global model level, offering a generic restructuring of the CNN architecture. The curr_chunk proposes a generic modification of CNN structure at the global model level, using replicated \"element blocks\" as parallel branches. Unlike Shake-Shake regularization, this method maintains performance with the same hyper-parameters as the base model. The curr_chunk discusses the benefits of ensembling neural networks to improve performance by combining outputs from multiple trainings with different error distributions. It also contrasts a generic rearrangement method for CNN structures with a technique involving parallel paths in a ResNet that requires local modifications. Ensembling neural networks can improve performance by combining outputs from multiple trainings with different error distributions. The proposed model architecture involves parallel branches trained jointly, similar to ResNet and Inception modules but at the global network level. \"Arranging\" parameters into parallel branches can increase performance. The proposed model architecture involves parallel branches trained jointly, similar to ResNet and Inception modules but at the global network level. \"Arranging\" parameters into parallel branches can increase performance. Ensembling approach on checkpoints during training process leads to higher performance with same training time budget, but increases model size and prediction time. The approach aims to either keep model size constant and improve performance, or obtain same performance with smaller model size. Our approach aims to improve performance while keeping the model size constant or achieving the same performance with a smaller model size. The model comprises multiple branches, each using DenseNet-BC or ResNet with pre-activation as element blocks. The branches are combined using a fuse layer by averaging their individual log probabilities over target classes. In our approach, we use ResNet with pre-activation as element blocks and a fuse layer to combine branches. The classification task involves samples associated with one class from a finite set. The neural network models output a score vector of the same dimension as the target classes, usually followed by a fully connected (FC) layer and SoftMax (SM) layer for probability distribution. Neural network models output a score vector of the same dimension as target classes, followed by a fully connected (FC) layer and SoftMax (SM) layer for probability distribution. The differences among recent network architectures lie in what is present before the last FC layer. Fusion in ensemble models involves computing individual predictions separately for each model instance. The \"element block\" takes an image as input and produces a vector of N values, parametrized by tensor W. Fusion in ensemble models involves averaging individual predictions from separately trained model instances. Implementing a \"super-network\" with parallel branches and an AVG layer is not common due to GPU memory constraints. Alternatively, the AVG layer can be placed after the last FC layer and before the SM layer for a \"factorized\" approach. The model consists of parallel branches producing score vectors for target categories. These vectors are fused through a \"fuse layer\" during training to make a single prediction. Three options are explored for combining score vectors: FC average, LSM average, and LL average. Averaging FC layer activations is equivalent to averaging log-probabilities for inference. The model uses multiple branches to predict target classes, with score vectors fused through a \"fuse layer\" during training. Averaging the log-probabilities of the target categories leads to improved performance with lower parameters. The parameter vector W is a concatenation of element block parameter vectors. The architecture is evaluated on CIFAR-10 and CIFAR-100 datasets. The proposed architecture uses \"element blocks\" for parameters, evaluated on CIFAR-10, CIFAR-100, and SVHN datasets. Images are normalized and standard data augmentation is applied during training on CIFAR datasets. The hyperparameters are set according to the original descriptions of the \"element block\" used. During training on CIFAR datasets, standard data augmentation is used, including random horizontal flips and random crops. A dropout ratio of 0.2 is applied when training DenseNet on SVHN. Testing is done after normalizing the input in the same way as during training. Error rates are given in percentages and correspond to an average of the last 10 epochs. DenseNet-BC's PyTorch implementation was used, and all execution times were measured using a single NVIDIA 1080Ti GPU with micro-batch 2. Experiments in sections 4.3 and 4.4 were conducted on the CIFAR-100 dataset. The DenseNet-BC model was used for experiments on the CIFAR-100 dataset with specific configurations. Results comparing the proposed branched architecture with an ensemble of independent models are presented in Table 1. The error rate from averaging predictions of 4 identical models trained separately is shown, highlighting the effectiveness of the jointly trained branched architecture. Table 1 presents the results of two cases, showing that a jointly trained branched configuration has a lower test error compared to a single branch model with a similar number of parameters. The error from the multi-branch model is considerably lower, indicating the effectiveness of the branched configuration as the number of parameters increase. The multi-branch model has lower error compared to a single branch model, showing the effectiveness of the branched configuration as parameters increase. Section 4.5 analyzes the relation between the number of branches and model performance, evaluating different fusion combinations in a branched model with e = 4. Experiments were conducted to evaluate the best training and prediction fusion combinations in a branched model with e = 4. Table 1 compares Coupled Ensembles of DenseNet-BCs with different \"fuse layer\" combinations against a single branch model, showing the top-1 error rate on the CIFAR-100 test set. Performance metrics include architecture details, number of branches, and different \"fuse layer\" choices during training and inference. The performance of models under different \"fuse layer\" operations for inference is shown in Table 1. Observations include the branched model with e = 4 and Avg. LSM for the \"fuse layer\" performing similarly to a DenseNet-BC model with significantly more parameters. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with significantly more parameters. Coupled ensembles with LSM fusion result in lower error rates for \"element blocks\" trained jointly, indicating better representations and complementary feature learning. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal. Updating all branches to be consistent with each other provides a stronger gradient signal during training. Ensemble combinations outperform single branch networks, with a significant reduction in error rate when using multiple branches. However, training with Avg. FC may not perform well due to unrelated FC instances. The Avg. FC training with Avg. SM prediction shows some improvement but is still not optimal due to the non-linearity of the SM layer distorting the FC average. In the following experiments, Avg. FC prediction performs better than Avg. SM prediction due to the FC values transmitting more information. The optimal number of branches for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100 with a parameter budget of 0.8M. In this section, experiments investigate the optimal number of branches for a given model parameter budget using DenseNet-BC on CIFAR-100 with a parameter budget of 0.8M. The results are shown in table 3, indicating the performance for different configurations of branches, depth, and growth rate. DenseNet-BC parameter counts are quantified based on the values of L, k, and e, which is crucial for moderate size models like the 800K one. The experiments investigate the optimal number of branches for DenseNet-BC on CIFAR-100 with a parameter budget of 0.8M. Model configurations with parameters just below the target were selected for fair comparison. The optimal configuration for the 800K model is e = 3, L = 70, k = 9, resulting in a decrease in error rate from 22.87 to 21.10. Using 2 to 4 branches shows a significant performance gain over the single branch case. The experiments on DenseNet-BC with a parameter budget of 0.8M showed that using 2 to 4 branches resulted in a significant performance gain over the single branch case. However, using 6 or 8 branches performed worse. The model's performance is robust to slight variations in parameters, but the increased performance comes with longer training and prediction times due to smaller values of k. The experiments showed that using 2 to 4 branches in DenseNet-BC with a parameter budget of 0.8M resulted in a performance gain. However, using 6 or 8 branches performed worse. The increased performance comes with longer training and prediction times due to smaller values of k. Coupled ensembles were evaluated against existing models, including DenseNet-BC and ResNet BID8 with pre-activation as the element block. Performance of coupled ensembles was compared to current state-of-the-art models. The experiments compared the performance of coupled ensembles with single models, showing that coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperformed single branch models. Different network sizes for DenseNet-BC were considered, ranging from 0.8M to 25.6M parameters. The trade-off between depth L and growth rate k was found to not be critical for a given parameter budget. Our experiments compared the performance of coupled ensembles with single models, showing that coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperformed single branch models. Different network sizes for DenseNet-BC were considered, ranging from 0.8M to 25.6M parameters. The trade-off between depth L and growth rate k was not critical for a given parameter budget. Interpolating between extreme cases for depth L and growth rate k was done on a log scale. The choice between the number of branches e, depth L, and growth rate k for a fixed parameter budget was not critical as long as e \u2265 3 (or even e \u2265 2 for small networks). The coupled ensemble approach with DenseNet-BC models outperformed single models, with error rates comparable to state-of-the-art implementations. The larger models achieved error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. Only the Shake-Shake S-S-I model BID5 performed slightly better on CIFAR 10. The coupled ensemble approach with DenseNet-BC models outperformed single models, achieving error rates comparable to state-of-the-art implementations. However, the approach is limited by GPU memory and training time constraints, with a maximum of 25M parameters. Further improvements were explored through classical ensembling based on independent trainings. The coupled ensemble approach with DenseNet-BC models showed significant performance improvements compared to single models, but faced limitations in GPU memory and training time. To further enhance performance, ensembling was done with four large coupled ensemble models, resulting in significant gains by fusing two models. These ensembles outperformed all state-of-the-art implementations, including other ensemble-based approaches. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" that act as standalone CNN models. These blocks are connected via a \"fuse layer\" and their score vectors are averaged during training and testing. The ensembles of coupled ensemble networks outperform all state-of-the-art implementations, including other ensemble-based approaches. The proposed approach involves using multiple \"element blocks\" as standalone CNN models, connected via a \"fuse layer\" to improve performance. Score vectors are averaged during training and testing, leading to better results compared to a single branch configuration. However, this comes with a slight increase in training and prediction times due to sequential processing of branches. The increase in training and prediction times for the proposed approach is mainly due to sequential processing of branches during forward and backward passes. Smaller branch sizes reduce data parallelism efficiency on GPUs, but this effect is less pronounced for larger models. To address this, data parallelism can be extended to branches through parallel implementation of multiple 2D convolutions or by spreading branches over multiple GPUs. Preliminary experiments on ImageNet show that coupled ensembles have lower error rates compared to single branch models with the same parameter budget. When multiple GPUs are used, branches can be spread over them. Preliminary experiments on ImageNet show that coupled ensembles have lower error rates compared to single branch models with the same parameter budget. Figure 3 illustrates the common structure of test and train versions of networks used as element blocks. Figure 4 demonstrates how an averaging layer can be placed in the test version after the last FC layer and before the SM layer. In the train version, the averaging layer can be placed after the last FC layer, after the SM layer, or after the LL layer. Model instances do not need to share the same architecture. In the train version, the averaging layer can be placed after the last FC layer, after the LSM layer, or after the LL layer. Parameter vectors define each branch, with a global network concatenating all parameters. Dedicated scripts are used for splitting parameters in coupled networks for training and prediction. The global network architecture is determined by hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. Parameter vectors define each branch, with a dedicated script used for splitting in coupled networks for training and prediction. The global network architecture is determined by hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. For larger models, data batches are split into \"micro-batches\" to accommodate batch size limitations. Gradient accumulation and averaging over micro-batches are used to approximate processing data as a single batch, with BatchNorm layer adjustments for normalization during the forward pass. The BatchNorm layer adjusts the gradient for micro-batches, using batch statistics for normalization during the forward pass. To optimize throughput, parameter updates are done with batch gradients while forward passes use micro-batches. Memory requirements in single branch cases depend on network depth and batch size, with the micro-batch \"trick\" used to adjust memory needs while maintaining default batch size. In single branch cases, memory needs depend on network depth and batch size, with the micro-batch \"trick\" used to adjust memory requirements. The multi-branch version requires more memory only if branch width is reduced. Hyper-parameter search experiments showed that reducing both width and depth was the best option. In practice, for \"full-size\" experiments with 25M parameters, training was done within 11GB memory of GTX 1080 Ti using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch ones. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but does not significantly increase speed or improve performance. Test time equivalence between FC average and logsoftmax was observed for coupled ensembles with two branches only. The experiments conducted on coupled ensembles with two branches showed significant performance gains over single-branch architectures of comparable size. The study also evaluated variations in depth and growth rate, finding stable performance across different parameter settings. Validation results suggested that a specific combination of parameters (L = 82, k = 8, e = 3) would be the most effective on the test set. The experiments on coupled ensembles with two branches showed performance gains over single-branch architectures. The (L = 70, k = 9, e = 3) combination appeared slightly better, but not statistically significant. Comparison of parameter usage and performance with meta learning techniques revealed issues with reproducibility and statistical significance. Variations in performance were identified across different experiments using Torch7 and PyTorch frameworks. The observed difference in performance measures across experiments can be attributed to various sources of variation, including the underlying framework used (Torch7 and PyTorch), random seed for network initialization, CuDNN non-determinism during training, fluctuations in batch normalization, and the choice of model instance. These factors contribute to fluctuations in performance even when using the same tools and settings. During training, the influence of learning rate, SGD momentum, and weight decay remains consistent even when set to 0. The choice between the model obtained after the last epoch or the best performing model can impact evaluation measures due to random initialization variations. Despite efforts to design and train neural networks properly, different random seeds may lead to different local minima, resulting in a small dispersion in performance. The dispersion in performance due to different random seeds complicates comparisons between models, making differences less significant. Statistical tests may not help as even models with the same seed can show significant differences. Experiments show a small dispersion in a moderate scale model, but larger models may not allow for many trials. Experiments on a moderate scale model show dispersion in performance due to different random seeds, making comparisons challenging. Larger models may not allow for many trials. Different effects were quantified on DenseNet-BC with L = 100, k = 12 on CIFAR 100 using Torch7 and PyTorch with the same or different seeds. Performance measures included error rates at the last epoch, average of last 10 epochs, and lowest error rate over all epochs. The study compared performance measures of models using different random seeds and implementations in Torch7 and PyTorch. Results showed no significant difference between the two implementations or using the same seed versus different seeds. The study found no significant difference between Torch7 and PyTorch implementations or using the same seed versus different seeds. Results also showed that there was no way to exactly reproduce results due to dispersion observed with the same seed. Additionally, there was no significant difference between the means computed on the single last epoch versus the last 10 epochs. The standard deviation of measures was slightly smaller when computed on the last 10 epochs, as expected. The mean of measures on the last 10 epochs is consistently smaller, reducing fluctuations due to batch normalization and random fluctuations. The method proposed ensures reproducibility and fair comparisons by choosing the minimum error measure. The method proposed ensures reproducibility and fair comparisons by choosing the minimum error measure from all models computed during training. This approach avoids bias in absolute performance estimation and recommends using the error rate at the 10 last iterations for more stable results. In experiments, using the error rate at the last 10 iterations is recommended for stable results. Different values can be used, but using the average of error rates from the last 10 epochs is more robust. For SVHN experiments, the last 4 iterations were used due to a smaller number of bigger epochs. These observations highlight the importance of choosing the right error rate measurement for accurate performance estimation. In experiments, using the error rate at the last 10 iterations is recommended for stable results. For SVHN experiments, the last 4 iterations were used. The principle of using the average error rate from the last epochs leads to more robust and conservative results. Comparisons between single-branch and multi-branch architectures show a clear advantage for multi-branch networks under a constant parameter budget. However, multi-branch networks currently have longer training times than single-branch networks. The training time of multi-branch networks is longer than single-branch networks, but there are ways to reduce it. The study compares different options to improve multi-branch architectures within a constant training time budget. Results for CIFAR 10 and 100 are shown, with statistics from 5 runs for each configuration. The baseline single branch DenseNet-BC L = 190, k = 40, e = 1 has a training time of about 80 hours, while the corresponding multi-branch baseline is DenseNet-BC L = 106, k = 33, e = 4. The study compares different options to improve multi-branch architectures within a constant training time budget. Results for CIFAR 10 and 100 are shown, with statistics from 5 runs for each configuration. Options include reducing training epochs, depth, and parameter count to match the single-branch baseline. Results are also shown for parameter budgets divided by 2 and 3.7, with slightly worse performance than the full multi-branch baseline. In this section, the performance of single branch models and coupled ensembles is compared in a low training data scenario. DenseNet-BC L = 88, k = 20, e = 4 performs better than the single-branch baseline with reduced parameters and training time. The study explores different options to enhance multi-branch architectures within a constant training time budget, showing results for CIFAR 10 and 100 datasets. In a low training data scenario, a comparison was made between a single branch model and a multi-branch coupled ensemble on two datasets: STL-10 and a 10K balanced random subset of CIFAR-100. Results showed that coupled ensembles outperformed the single branch model within a fixed parameter budget. Preliminary experiments on ILSVRC2012 also supported this finding. Experiments were conducted on 256\u00d7256 images due to storage constraints, with data augmentation involving random flips and crops of size 224\u00d7224. A DenseNet-169-k32-e1 single-branch model was compared to a coupled ensemble DenseNet-121-k30-e2, showing significant improvement in results. Future experiments with full-sized images and increased data augmentation are planned."
}