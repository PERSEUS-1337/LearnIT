{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, showing improved performance by incorporating ontological information. Humans can recognize various sounds in their environment, which can be categorized into more abstract classes. Ontologies are defined for sound event classification datasets but are underutilized in current neural network models. Ontologies are formal representations of domain knowledge through categories and relationships that can provide structure to training data and neural network architecture. They are defined for sound event classification datasets but are rarely utilized in neural network models. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Taxonomies in sound event classifiers are defined by nouns or verbs, such as animal containing dog and cat. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Hierarchical relations in classifiers offer benefits like allowing back-off to more general categories, disambiguating acoustically similar classes, and penalizing classification differently based on super classes. Ontology-based network architectures have shown improved performance in sound event classification by disambiguating acoustically similar classes and penalizing classification differently based on super classes. This approach has been successfully applied in computer vision and music, but is not commonly used in sound event classification. Authors have demonstrated improved performance in topic classification by using ontology-based network architectures, which include adding intermediate layers to model the transformation from super classes to sub classes. This approach reduces overfitting in training data and allows for class disambiguation. The proposed framework deals with ontological information using deep learning architectures and describes the type of ontologies considered and their implications. The framework presented deals with ontological information using deep learning architectures. It includes assumptions, a Feed-forward model with constraints, and ontology-based embeddings using Siamese Neural Networks. The framework is designed to utilize ontology structure and model neural network architectures, specifically focusing on ontologies with two levels but can be generalized to more levels. Training data is considered in the form of {(x 1 , y 1 ), ..., (x n , y n )}, where x i \u2208 X. The framework presented focuses on utilizing ontological information in deep learning architectures. It can be generalized to more than two levels of ontologies. The training data consists of audio representations associated with labels from the ontology. Each class at a certain level is mapped to one element at the next level in a hierarchical manner. The framework utilizes ontological information in deep learning architectures, mapping classes hierarchically. Each element in C1 is related to one element in C2. By inferring labels, a probabilistic formulation can estimate p(y2|x) by computing p(y1|x) and summing corresponding values. The framework utilizes ontological information in deep learning architectures to estimate p(y2|x) by computing p(y1|x) and summing corresponding values. It is important to consider using knowledge to relate different classes in y1 during model training for improved performance, especially in predicting classes y2. The proposed framework is used to design ontology-based neural network architectures, introducing the ontological layer that leverages the ontology structure. The proposed framework introduces an ontological layer in neural network architectures to design ontology-based models. The Feed-forward Network (FFN) with Ontological Layer consists of a base network, an intermediate vector z, and two outputs for each ontology level. The base network learns weights for audio features x to generate probability vectors for classes C1 and C2. The ontological layer refines predictions for classes in C2 based on the output from C1. Once trained, the FFN can predict classes in both C1 and C2 for any input x. The ontological layer in the neural network architecture refines predictions for classes in C2 based on the output from C1. Equation 3 describes how the ontological layer is used, with M defining the weights of a standard layer connection. The model is trained using a gradient-based method to minimize the loss function L. The ontological layer M defines weights for a standard layer connection in the neural network architecture. The model is trained using a gradient-based method to minimize the loss function L, a convex combination of two categorical cross-entropy functions. The goal is to create embeddings that preserve the ontological structure using a Siamese neural network (SNN). The ontology-based embeddings were learned using a Siamese neural network (SNN) to preserve the ontological structure. The SNN architecture includes a Feed-forward Network with an Ontological Layer, where samples of the same class are closer while samples of different classes are separated. The twin networks share weights and learn simultaneously, with ontological embeddings used to compute a Similarity metric (Euclidean Distance). The Feed-forward Model with Ontological layer uses ontological embeddings to compute a Similarity metric. The distance between embeddings indicates the difference between samples in terms of ontology. Output probabilities are generated for different levels based on the distance between samples. Training involves providing pairs of audio examples and minimizing using a gradient-based method. The Feed-forward Model with Ontological layer uses ontological embeddings to compute a Similarity metric and generate output probabilities for different levels. Evaluating sound event classification performance of ontological-based neural network architectures involves datasets, ontologies, baseline and proposed architectures, and classification performance at different hierarchy levels. The dataset for Making Sense of Sounds Challenge 2 - MSoS aims to classify the highest level classes in its taxonomy with two levels in the ontology. Audio files are sourced from Freesound database, ESC-50 dataset, and Cambridge-MT Multitrack. The ontology used in the study has two levels, with the highest level containing 5 classes. Audio files were sourced from Freesound, ESC-50 dataset, and Cambridge-MT Multitrack. The development dataset consists of 1500 audio files divided into five categories, while the evaluation dataset has 500 files. All files are in the same format and 5 seconds long, with 80% used for training and tuning parameters, 10% for testing. The official blind evaluation set consisted of 500 files across 5 classes. Urban Sounds - US8K dataset was used for evaluation. The dataset for urban sounds classification consists of 8,732 audio files with 10 classes. Files are 5 seconds long, sourced from Freesound, and have a single-channel 44.1 kHz, 16-bit .wav format. Training and tuning parameters were done on 9 folds, with one fold for testing. The dataset for urban sounds classification consists of 8,732 audio files with 10 classes. Files are in a single-channel 44.1 kHz, 16-bit .wav format. Training and tuning parameters were done on 9 folds, with one fold for testing. Audio recordings were represented using state-of-the-art Walnet features BID1 and transformed via a convolutional neural network (CNN) with a feed-forward multi-layer perceptron network architecture. The model consists of 4 layers: input layer (1024), 2 dense layers (512 and 256), and output layer (128). Dense layers use Batch Normalization, dropout rate of 0.5, and ReLU activation function. Parameters were tuned in the Net box and for transforming z into p(y 1 |x). Baseline models were considered for different data sets, without ontological information. Base Network Architecture with an added output layer for level 1 or level 2. Training the Feed-forward model with Ontological Layer using \u03bb = 1 is equivalent to level 1. Loss function for level 2 is not considered with \u03bb = 1. The architecture includes an output layer for level 1 or level 2. Training the model with \u03bb = 1 is equivalent to level 1. Baseline models were evaluated for MSoS and US8K datasets in both levels. The baseline performance for MSoS level 2 was 0.81. Different values of \u03bb were tested to analyze the ontological layer's utility, showing an increase in performance. The architecture was validated by training models with different values of \u03bb, showing that values other than 0 and 1 improved performance. The ontological layer had a significant impact on classification, with the best results achieved using \u03bb = 0.8 for MSoS dataset and \u03bb = 0.7 for US8K dataset. The improvements over baseline models were 5.4% and 6% for MSoS, and 2.5% and 0.2% for US8K. The best performance was achieved using \u03bb = 0.7, with an accuracy of 0.82 and 0.86 for level 1 and 2 respectively. The ontology-based embeddings resulted in tighter and better-defined clusters for sound event classification. The ontology-based embeddings led to improved cluster definition for sound event classification. The architecture in Section 2.3 was tested to evaluate their performance, with t-SNE plots illustrating clustering at different levels. Walnet audio features were processed, and a Siamese neural network was trained with different super and sub class pairs to generate the embeddings. The SNN was trained for 50 epochs using the Adam algorithm, with hyper-parameter tuning for optimal performance. Testing various numbers of input pairs showed that 100,000 pairs yielded the best results. The SNN was trained with 100,000 pairs for optimal performance. Different lambda values were used for classifiers in level 1 and 2, affecting overall performance. Results showed accuracy performance of MSoS and US8K in levels 1 and 2. The architecture outperformed the baseline but slightly underperformed without embeddings, showcasing better grouping with ontology-based embeddings. The architecture's performance with ontology-based embeddings was better than the baseline but slightly underperformed without embeddings. The t-SNE plots showed clustered groups of level 2 classes, with tighter clusters from ontology-based embeddings. However, the US8K dataset had limited performance due to a similar number of sub classes and super classes. The number of sub classes was similar to the number of super classes, with 10 sub classes for 4 classes. In comparison, the MSoS data set had 97 sub classes and 5 classes. The contribution of the ontology was negligible when the ratio between sub classes and classes was not large. Two approaches were used in the Making Sense of Sounds Challenge, with the Feed-forward Network with Ontological Layer achieving 0.88 accuracy and using ontological embeddings achieving 0.89 accuracy, both outperforming the baseline. The paper proposed a framework for designing neural networks for sound event classification using hierarchical ontologies, showing two methods to incorporate structure into deep learning models without adding more parameters. Neural networks were used for sound event classification with hierarchical ontologies. Two methods were presented to integrate structure into deep learning models without adding parameters. A Feed-forward Network with an ontological layer related predictions across hierarchy levels, while a Siamese neural Network computed ontology-based embeddings. Results from datasets and challenges showed improvements over baselines, indicating potential for further exploration of ontologies in sound event classification."
}