{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where the source and target domains may not share the same categories. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), a novel approach that incorporates category-agnostic clusters in the target domain to improve domain adaptation. SE-CC is a novel architecture for domain adaptation that utilizes category-agnostic clusters in the target domain to improve generalization in both closed-set and open-set scenarios. Clustering is used to reveal the underlying data space structure in the target domain, and mutual information maximization enhances the learned representation. Extensive experiments on Office and VisDA datasets validate the effectiveness of SE-CC. SE-CC is a novel architecture for domain adaptation that enhances the learned representation with mutual information maximization. Extensive experiments on Office and VisDA datasets show superior results compared to state-of-the-art approaches, addressing the challenge of domain shift in vision technologies. Unsupervised domain adaptation leverages labeled source samples and unlabeled target samples to generalize a target model. Existing models struggle in open-set scenarios due to the assumption of shared categories between domains, hindering generalization to distinguish unknown classes. In open-set domain adaptation, distinguishing unknown target samples from known ones is a challenge. One approach is to use an additional binary classifier to assign known/unknown labels to target samples, discarding unknown samples during adaptation. This helps in learning a hybrid network for both closed-set and open-set domain adaptation. In open-set domain adaptation, distinguishing unknown target samples from known ones is a challenge. One approach is to use clustering to explicitly model the diverse semantics of both known and unknown classes in the target domain. By decomposing all target samples into clusters, the learnt category-agnostic clusters convey discriminative knowledge specific to the target domain, improving domain adaptation performance. In open-set domain adaptation, clustering is used to distinguish unknown target samples from known ones. The category-agnostic clusters convey discriminative knowledge specific to the target domain, improving domain adaptation performance. The Self-Ensembling with Category-agnostic Clusters (SE-CC) includes an additional clustering branch to refine representations and preserve the target domain's structure. The Self-Ensembling with Category-agnostic Clusters (SE-CC) introduces a clustering approach to decompose target samples into category-agnostic clusters. A clustering branch in the student model predicts cluster assignment distributions, minimizing KL-divergence to preserve the target domain's structure. The SE-CC framework utilizes KL-divergence to model the mismatch between estimated cluster assignment distribution and inherent cluster distribution, enforcing the learnt feature to preserve data structure in the target domain. Additionally, mutual information is maximized among input feature map, output classification distribution, and cluster assignment distribution in the student model to enhance feature representation. The framework is jointly optimized for unsupervised domain adaptation, focusing on learning transferrable features in CNNs through Maximum Mean Discrepancy (MMD) to achieve domain invariant representation. The SE-CC framework utilizes KL-divergence to model the mismatch between estimated cluster assignment distribution and inherent cluster distribution, enforcing the learnt feature to preserve data structure in the target domain. In contrast, unsupervised domain adaptation focuses on learning transferrable features in CNNs through Maximum Mean Discrepancy (MMD) to achieve domain invariant representation. This involves integrating MMD into CNNs, incorporating a residual transfer module, and utilizing a domain discriminator to encourage domain confusion across different domains. Ganin & Lempitsky (2015) use a gradient reversal algorithm to enforce domain invariance in representation learning. Open-set domain adaptation addresses scenarios with new and unknown classes in the target domain. Panareda Busto & Gall (2017) and Saito et al. (2018b) propose methods to handle open-set scenarios by distinguishing known and unknown classes in target samples. Baktashmotlagh et al. (2019) also contribute to this area. In the context of domain adaptation, various methods have been proposed to handle scenarios with unknown classes in the target domain. Saito et al. (2018b) utilize adversarial training to separate target samples of unknown classes. Baktashmotlagh et al. (2019) factorize source and target data into shared and private subspaces to model known and unknown target samples. Clustering is used to decompose unlabeled target samples into category-agnostic clusters for further analysis. Self-ensembling loss aligns classification predictions between teacher and student. Clustering decomposes unlabeled target samples into category-agnostic clusters, integrated into Self-Ensembling for closed-set and open-set scenarios. A clustering branch in the student infers cluster assignment distribution for each target sample, enforcing feature representation to preserve target domain structure. Student's feature representation is enhanced by maximizing mutual information among feature map, classification, and cluster assignment distributions. KL-divergence is used to enforce feature representation preservation in the target domain. The student's feature representation is improved by maximizing mutual information among its feature map, classification, and cluster assignment distributions. SE-CC leverages category-agnostic clusters for representation learning, aiming to preserve the target data structure during domain adaptation for effective alignment of sample distributions within known and unknown classes. The SE-CC model integrates category-agnostic clusters into domain adaptation to preserve target data structure, align sample distributions, and enhance representation learning. This approach maximizes mutual information among input features, clusters, and class probability distributions for both closed-set and open-set scenarios. The SE-CC model integrates category-agnostic clusters into domain adaptation for closed-set and open-set scenarios. It aims to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown samples. The goal of open-set domain adaptation is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown samples. Self-Ensembling builds upon the Mean Teacher for semi-supervised learning by encouraging consistent classification predictions between teacher and student models under small perturbations of input images. The self-ensembling loss penalizes the difference between classification predictions of teacher and student models for augmented target samples. The student is trained using gradient descent, while the teacher's weights are updated as an exponential moving average. Additionally, unsupervised conditional entropy loss is adopted to train the student's classification branch, aiming to move decision boundaries away from high-density regions in the target domain. The Self-Ensembling approach incorporates unsupervised conditional entropy loss to train the student's classification branch, aiming to move decision boundaries away from high-density regions in the target domain. The training loss includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. Open-set domain adaptation is challenging as it requires classifying both inliers and outliers into known and unknown classes, with a typical approach being to learn a binary classifier for each target sample. To address the challenge of open-set domain adaptation, a binary classifier is typically used to classify target samples as known/unknown classes. However, this oversimplifies the problem when unknown samples span multiple classes. To improve this, clustering is used to model diverse semantics in the target domain, creating category-agnostic clusters integrated into Self-Ensembling for domain adaptation. An additional clustering branch aligns cluster assignments with inherent cluster distribution, enforcing domain-invariant feature representations. In Self-Ensembling, an additional clustering branch aligns cluster assignments with inherent cluster distribution to create domain-invariant feature representations for known and unknown classes in the target domain. Clustering using k-means decomposes unlabeled target samples into category-agnostic clusters, revealing the underlying structure tailored to the target domain. In Self-Ensembling, clustering with k-means decomposes target samples into category-agnostic clusters, revealing the underlying structure tailored to the target domain. Target samples are represented as output features of pre-trained CNNs for clustering. The clusters are periodically refreshed but do not significantly impact the results. The inherent cluster distribution of each target sample is measured through softmax over cosine similarities with all clusters. The clustering branch in the student model predicts the distribution over category-agnostic clusters for cluster assignment of target samples. It measures the inherent cluster distribution of each target sample through cosine similarities with cluster centroids. The centroid of each cluster is the average of all samples in that cluster. The clustering branch in the student model predicts cluster assignment distribution for target samples based on input features. It uses a modified softmax layer to infer cluster assignments and is trained with supervision from inherent cluster distribution, measured using KL-divergence Loss. The clustering branch in the student model predicts cluster assignment distribution for target samples using a modified softmax layer and is trained with supervision from inherent cluster distribution, measured by KL-divergence Loss. The KL-divergence loss is minimized to enforce the learnt representation to preserve the data structure of the target domain and be more discriminative for both unknown and known classes. Inter-cluster relationships are incorporated into the loss as a constraint to preserve inherent relations among cluster assignment parameter matrices. The goal is to ensure that the cluster assignment parameter matrices of semantically similar clusters are similar. The student model in SE-CC produces classification and cluster assignment distributions for target samples using a multi-task paradigm. Mutual Information Maximization is utilized to enhance the learned target feature in an unsupervised manner by maximizing the mutual information among input features and output distributions. This approach aims to tune the feature's suitability for downstream tasks by leveraging global/local mutual information. The MIM module in the student model aims to estimate and maximize the local and global mutual information between input features, output distributions, and cluster assignments. Global Mutual Information is encoded from the output feature map of the last convolutional layer and concatenated with the conditioning classification for enhanced feature tuning. The global Mutual Information is extracted from the output feature map and combined with classification and cluster assignment distributions. It is then fed into a discriminator to determine alignment with the given distributions. The discriminator consists of three fully-connected networks with nonlinear activation, outputting a probability score. The global Mutual Information is estimated using a Jensen-Shannon MI estimator with a softplus function. Local Mutual Information is also utilized by replicating and concatenating feature maps for spatial locations. The local Mutual Information discriminator is constructed with three stacked convolutional layers to discriminate input local features based on classification and cluster assignment distributions. The final objective for the Mutual Information Matching (MIM) module is a combination of global and local Mutual Information estimations. The local Mutual Information discriminator is constructed with three stacked convolutional layers to discriminate input local features based on classification and cluster assignment distributions. The final objective for the Mutual Information Matching (MIM) module is a combination of global and local Mutual Information estimations, balanced with a tradeoff parameter \u03b1. The overall training objective of the SE-CC integrates various losses on source and target data, including cross entropy loss, unsupervised self-ensembling loss, conditional entropy loss, KL-divergence loss, and local & global Mutual Information estimation. The SE-CC method is empirically verified through experiments on the VisDA dataset, which involves synthetic-real image transfer. The training domain consists of synthetic images from 3D CAD models, while the validation domain includes real images from COCO and the testing domain comprises video frames from YTBB. The source domain is the synthetic images, and the target domain is the COCO images for evaluation in open-set adaptation. For open-set adaptation, the COCO images in the validation domain are used as the target for evaluation. Known classes in the source and target domains are set as 12 classes, background classes in the source as 33 unknown classes, and other COCO categories in the target as unknown classes. The known-to-unknown ratio in the target domain is 1:10. Three metrics - Knwn, Mean, and Overall - are used for evaluation. ResNet152 is utilized as the backbone for CNNs in both closed-set and open-set scenarios. For closed-set adaptation, ResNet152 is used as the backbone for CNNs. In open-set domain adaptation on Office, different models' performances are compared. A variant of SE-CC is included for fair comparison with AODA, which learns a classifier without unknown source samples. The results consistently indicate the effectiveness of the approach. Our SE-CC model outperforms other closed-set and open-set adaptation methods on various transfer directions, especially on challenging domain transfers like D \u2192 A and W \u2192 A. The approach leverages category-agnostic clusters to create domain-invariant features for known classes while effectively distinguishing target samples. The key advantage of exploiting category-agnostic clusters for open-set domain adaptation is highlighted. By aligning data distributions between source and target domains, RTN and RevGrad outperform Source-only. Open-set adaptation techniques (AODA, ATI-\u03bb, and FRODA) excel by rejecting unknown target samples and aligning data distributions for inliers. SE-CC model outperforms other methods on challenging domain transfers. Our SE-CC model, utilizing category-agnostic clusters as a constraint for feature learning and alignment, outperforms other state-of-the-art techniques in both open-set and closed-set domain adaptation scenarios. The results demonstrate the effectiveness of excluding unknown target samples and aligning data distributions for improved performance. Our SE-CC model achieves better performance in closed-set domain adaptation by exploiting the data structure in the target domain through category-agnostic clusters. Ablation study shows that Conditional Entropy and KL-divergence Loss contribute to improving overall performance by driving classifier decision boundaries and aligning cluster assignment distribution, respectively. The SE-CC model improves performance in closed-set domain adaptation by utilizing category-agnostic clusters in the target domain. Conditional Entropy (CE) enhances classifier performance, with a mean accuracy increase from 65.2% to 66.3%. KL-divergence Loss (KL) aligns cluster assignment distribution, resulting in a 3.0% performance gain, while Mutual Information Maximization (MIM) further enhances feature suitability for downstream tasks. The SE-CC model utilizes category-agnostic clusters in the target domain for domain adaptation, improving performance by 4.2% in Mean metric. The approach focuses on separating unknown target samples from known ones and integrating category-agnostic clusters into Self-Ensembling. The study focuses on domain adaptation by separating unknown target samples from known ones and integrating category-agnostic clusters into Self-Ensembling. Clustering is used to decompose target samples into clusters, and a clustering branch is added to align cluster assignments. Mutual information is utilized to enhance learned features. Experimental results on Office and VisDA datasets show performance improvements in both open-set and closed-set adaptation tasks. The implementation of SE-CC utilizes information from input features, classification, and clustering branches to enhance learned features. Experiments on Office and VisDA datasets confirm performance improvements compared to state-of-the-art techniques. The framework includes global and local mutual information estimation, with specific settings for learning rate, batch size, training iterations, and feature dimensions. The implementation of SE-CC utilizes input features, classification, and clustering branches to enhance learned features. Experiments on Office and VisDA datasets show performance improvements. Specific settings for learning rate, batch size, training iterations, and feature dimensions are detailed in Table 6 for open-set and closed-set adaptation tasks. The number of clusters is determined using the Gap statistics method, with fixed parameters \u03bb 1 = 10 and tuned parameters \u03bb 2, \u03bb 3, \u03bb 4, and \u03b1. The design of loss function in the clustering branch is compared using KL-divergence, L 1, and L 2 distance. Evaluation of Clustering Branch: Comparing KL-divergence with L1 and L2 distance shows KL-divergence as a better measure of mismatch. Evaluation of Mutual Information Maximization: Different variants of MIM module in SE-CC are evaluated by estimating mutual information between input feature and outputs. The MIM module in SE-CC, including CLS, CLU, and CLS+CLU, improves performance by exploiting mutual information between input features and classification/clustering branch outputs. CLS and CLU slightly enhance performance, while CLS+CLU shows a larger boost by combining outputs from both branches. This demonstrates the benefit of leveraging mutual information among input features and combined outputs for classification and clustering tasks. SE helps align distributions of different domains for domain invariance. In the MIM module of SE-CC, the combined outputs of classification and cluster assignment tasks are used to align source and target distributions for domain invariance. While SE aligns distributions, SE-CC preserves the underlying target data structure to separate unknown target samples from known samples, improving recognition accuracy."
}