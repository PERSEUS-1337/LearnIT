{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions but current training techniques require fully-observed samples. Learning implicit generative models with only partial, noisy observations is challenging. A new method called AmbientGAN is proposed for training Generative Adversarial Networks (GANs) using lossy measurements. This method shows promising results on benchmark datasets with qualitative and quantitative improvements. AmbientGAN is a new method for training Generative Adversarial Networks (GANs) using lossy measurements. It shows substantial qualitative and quantitative improvements on benchmark datasets, with models achieving higher inception scores. This approach addresses the challenge of training generative models with noisy or incomplete samples, allowing for effective modeling of complex data distributions. This work introduces AmbientGAN, a method for training GANs using lossy measurements to overcome the challenge of training generative models with noisy or incomplete samples. The approach allows for effective modeling of complex data distributions by distinguishing real measurements from simulated measurements of generated images. AmbientGAN is a method for training GANs using lossy measurements to construct generative models from noisy observations. It distinguishes real measurements from simulated measurements of generated images, demonstrating effectiveness on various datasets and measurement models. The approach can generate high-quality samples even from low-dimensional projections with significant information loss. Theoretical results show that noisy measurements of images can uniquely determine the original image distribution, leading to the need for a generative model that matches the true distribution in a GAN game. Empirical work explores measurement models without guarantees, showcasing results on various datasets. In empirical work, measurement models without guarantees are explored, showcasing results on datasets like celebA. In one scenario, randomly placed occlusions are set to zero, improving sample quality in GAN training. In another scenario, learning from noisy, blurred images from celebA dataset is considered, resulting in better samples with GAN training. Learning a GAN on images denoised by Wiener deconvolution leads to poor sample quality, while our models can produce cleaner samples. Additionally, AmbientGAN recovers a lot of the underlying structure in generative models trained on 2D images from the MNIST dataset using 1D projections. AmbientGAN is effective in recovering underlying structure in generative models trained on 2D images from the MNIST dataset using 1D projections. Different approaches exist for constructing neural network based implicit generative models, including autoregressive and adversarial methods. The adversarial framework is powerful for modeling complex data distributions like images, video, and 3D models. Generative models have various applications, including solving ill-posed inverse problems and generating synthetic data. Generative models like GANs are used for various applications, including solving inverse problems and generating realistic synthetic data. Different approaches, such as training generators against discriminators on low-dimensional projections, have been explored to improve stability. Other works focus on translating images between domains and creating 3D shapes from 2D projections. The AmbientGAN framework is effective in recovering underlying structures in generative models trained on 2D images. Our work involves using discriminators on low-dimensional data projections to enhance stability. It is related to a study where 3D shapes are generated from 2D projections. The AmbientGAN framework is utilized, with notation for real and generated distributions, underlying space, and measurements. Lossy measurements are taken from a real distribution over R^n, with each measurement being an output of a stochastic measurement function parameterized by \u03b8. Our task is to create an implicit generative model of a distribution p_r_x using a set of IID realizations from the distribution p_r_y. The main idea is to combine the measurement process with parameterized measurement functions to achieve this goal. The main idea is to combine the measurement process with adversarial training to learn a generator that can sample from a distribution p_r_x using a dataset of measurements from p_r_y. The generator aims to produce objects X_g close to the desired distribution p_r_x, by simulating random measurements and using a discriminator to distinguish real from fake measurements. In the GAN setting, the goal is to generate objects X_g similar to the desired distribution p_r_x using a dataset of measurements from p_r_y. Random measurements are simulated on X_g, and a discriminator distinguishes real from fake measurements. The AmbientGAN objective involves a quality function q(x) and requires the measurement function f_\u03b8 to be differentiable. The AmbientGAN objective involves a quality function q(x) and requires the measurement function f_\u03b8 to be differentiable. The model uses feedforward neural networks for G and D, making it end-to-end differentiable and trainable with a gradient-based GAN training procedure. Stochastic gradients are computed by sampling Z, \u0398, and Y r in each iteration for parameter updates in G and D. The approach is compatible with GAN improvements and can easily incorporate them. The AmbientGAN learning framework is compatible with various GAN improvements and can incorporate additional information like per sample labels. Different measurement models are used for theoretical and empirical results, primarily focusing on 2D images. The framework is versatile and can be applied to other data formats and measurement models as well. The AmbientGAN learning framework is versatile and can be used with various GAN improvements and additional information like per sample labels. Different measurement models tailored for 2D images are considered, including Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, and Extract-Patch. In the context of the AmbientGAN learning framework, different measurement models for 2D images are explored, such as Block-Patch, Keep-Patch, Extract-Patch, Pad-Rotate-Project, PadRotate-Project-\u03b8, and Gaussian-Projection. These models involve various techniques like setting patches to zero, extracting random patches, rotating images, and projecting onto Gaussian vectors. The chosen angle is included in measurements along with projection values in Gaussian-Projection. It is shown that the true underlying distribution can be recovered for certain measurement models. The unique distribution consistent with observed measurements is invertible, providing a consistency guarantee with the AmbientGAN training procedure. Lemma 5.1 provides a consistency guarantee with the AmbientGAN training procedure by assuming uniqueness of the true underlying distribution given the measurement distribution. The following theorems demonstrate this assumption holds under various measurement models, allowing recovery of the true distribution with the AmbientGAN framework. The AmbientGAN framework can recover the true underlying distribution under various measurement models, such as Gaussian-Projection, Convolve+Noise, and Block-Pixels. The required conditions are easily satisfied for Gaussian blurring with additive noise. The framework also assumes a finite discrete set of pixel values, which is common in practical scenarios. Additionally, a sample complexity result is provided for learning distributions in the AmbientGAN framework. The AmbientGAN framework can recover the true underlying distribution under various measurement models like Gaussian-Projection, Convolve+Noise, and Block-Pixels. The framework assumes a finite discrete set of pixel values. A sample complexity result is provided for learning distributions in this framework, with three datasets used for experiments: MNIST for handwritten digits, CelebA for face images of celebrities, and an aligned and cropped version of CelebA with 64x64 RGB images. For experiments, three datasets were used: MNIST for handwritten digits, CelebA for face images of celebrities, and CIFAR-10 for images from 10 classes. Different generative models were used for each dataset, with specific architectures detailed in the appendix. The second model used in the experiments is an unconditional Wasserstein GAN with gradient penalty (WGANGP) following a specific architecture. Different models were used for the MNIST, celebA, and CIFAR-10 datasets, each with their own discriminator architectures. Baseline approaches were implemented to evaluate the AmbientGAN framework's performance. The fully connected discriminator architectures for the MNIST and celebA datasets were 25-25-1 and 100-100-1, respectively. Baseline approaches were implemented to assess the AmbientGAN framework's performance, including an \"ignore\" baseline and a stronger baseline leveraging invertible measurement functions. The \"ignore\" baseline and a stronger baseline using invertible measurement functions were implemented to evaluate the AmbientGAN framework. Despite violations of assumptions in the measurement models, an approximation of an inverse function was used to train a generative model. By \"unmeasuring\" the measurements, an estimate of the original data was obtained to train the generative model and approximate the true distribution. To obtain approximate inverse functions for different measurement models, various methods were used. For Block-Pixels measurements, blurring the image or using total variation inpainting was employed. Convolve+Noise measurements were approximated using Wiener deconvolution. Block-Patch measurements utilized Navier Stokes based inpainting. However, obtaining an approximate inverse function for other measurement models remains unclear. For Block-Patch measurements, the Navier Stokes based inpainting method BID2 is used to fill in zero pixels. Inverting other measurement models to obtain an approximate inverse function remains unclear. Inverting Pad-Rotate-Project measurements is challenging due to the lack of information about \u03b8. Only results with AmbientGAN models are reported in this subset of experiments. Inverting Pad-Rotate-Project measurements is difficult due to the lack of information about \u03b8. Results with AmbientGAN models are reported in this subset of experiments. Samples generated by baselines and our models are presented, showing the impact of the measurement process on sample quality. Additional results on MNIST, celebA, and CIFAR-10 are provided in the appendix. The results show that our models outperform baselines in generating high-quality images from heavily degraded measurements. Different measurement models such as Convolve+Noise, Block-Patch, and Keep-Patch are evaluated on celebA and CIFAR-10 datasets with DCGAN, demonstrating the effectiveness of our approach in creating coherent images. Our models can create coherent faces by observing parts of one image at a time. Two measurement models are used, with one model learning rotation and reflection while the other produces upright digits. The generated images may have lesser visual quality, but the method shows the ability to produce images. Our method demonstrates the ability to produce images of digits using only 1D projections, despite lower visual quality. The model trained on celebA dataset shows a crude outline of a face, emphasizing the challenge of learning complex distributions with 1D projections. Inception scores are used to quantify the generative model quality in the AmbientGAN framework. The study evaluates distribution recovery and training methods for GANs, using Inception scores to measure generative model quality. Different models were trained on CIFAR-10 and MNIST datasets, with varying pixel blocking probabilities. The results show how the model performance changes with increasing pixel blocking. Inception scores were computed to evaluate model performance with increasing pixel blocking probabilities. AmbientGAN models outperformed baseline models as noise levels increased, maintaining a high inception score. The AmbientGAN models outperform baseline models as noise levels increase, maintaining high inception scores. The Pad-Rotate-Project model performs poorly with an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements achieves a score of 8.12. The vanilla GAN model trained with fully-observed samples achieves a score of 8.99, showing that the second model trained on 1D projections comes close to fully-observed performance. The second model trained on 1D projections achieves a performance close to the fully-observed vanilla GAN model with an inception score of 8.99. The total variation inpainting method is slow and performs similarly to the unmeasured-blur baseline on MNIST. Inpainting baselines are not run on the CIFAR-10 dataset. The plots show the superiority of the approach over baselines, with the inception score plotted against training iterations. Generating models typically require a large dataset, but the study demonstrates how to relax this requirement. Our approach demonstrates the ability to learn a distribution from incomplete, noisy data, enabling the construction of new generative models without the need for a high-quality dataset. The lemma outlines the relationship between data distribution, parameter distribution, and measurement distribution in the context of the vanilla GAN model. The lemma explains how a unique probability distribution can match all 1D marginals in the context of the vanilla GAN model. This distribution enables learning from incomplete, noisy data to construct new generative models without requiring a high-quality dataset. By the Cramer-Wold theorem, any sequence of random vectors matching 1D marginals converges to the true distribution. A unique distribution can match all 1D marginals from Gaussian projection measurements. If the support of the Fourier transform of the convolution kernel and noise distribution is empty, there exists a unique distribution inducing the measurement distribution. The probability density functions are denoted by p with the variable name. There is a bijective map between X and Z, with a continuous transformation. The pdfs of X and Z are related through the Jacobian of h. The pdf of Y, a sum of two random variables, is a convolution of individual pdfs. Taking the Fourier transform, a reverse map is obtained. The reverse map from the measurement distribution to the sample distribution uniquely determines the true underlying distribution, as stated in Theorem 1. In the discrete setting, the empirical version of the vanilla GAN objective is defined for a dataset of measurement samples. The optimal discriminator for this objective is determined by the empirical distribution of samples. The empirical version of the vanilla GAN objective is defined for measurement samples. The optimal discriminator is determined by the empirical distribution of samples. The optimal generator must satisfy the empirical distribution. Theorem 5.4 states that in a Block-Pixels measurement model, if the probability of blocking a pixel is less than 1, then a certain result holds. Theorem 5.4 states that in a Block-Pixels measurement model, if the probability of blocking a pixel is less than 1, then there is a unique distribution that can be recovered from measurements using a transition matrix A. The distribution over measurements can be written in terms of a transition matrix A, which guarantees recoverability if A is invertible. The sample complexity is determined by the minimum eigenvalue magnitude of A. For Block-Pixels measurement, images are divided into classes based on blocking probabilities. In Block-Pixels measurement, images are divided into classes based on the number of zero pixels. The transition matrix is lower triangular due to the blocking probabilities. Each pixel is blocked independently with a probability p, ensuring that every image has at least (1 - p) n chance of being unaffected by the measurements. The transition matrix in Block-Pixels measurement is lower triangular with diagonal entries representing eigenvalues. Each pixel is blocked independently with probability p, ensuring a minimum eigenvalue of (1 - p) n. The DCGAN model on MNIST follows a specific architecture with a noise input of 100 dimensions sampled uniformly. The DCGAN model on MNIST uses a noise input of 100 dimensions sampled uniformly. The generator has two linear layers and two deconvolutional layers, while the discriminator has two convolutional layers and two linear layers. Batch-norm is used in both. The WGANGP model on MNIST has a generator with a latent vector of 128 dimensions and one linear layer followed by three deconvolutional layers. The discriminator has three convolutional layers and one linear layer, without batch-norm. The unconditional DCGAN model on celebA follows a similar architecture. The ACWGANGP model on CIFAR-10 uses a latent vector of 128 dimensions sampled from a standard Gaussian distribution. The generator consists of a linear layer followed by three residual blocks, each containing conditional batch normalization and nonlinearity operations. The ACWGANGP model on CIFAR-10 uses a latent vector of 128 dimensions sampled from a standard Gaussian distribution. The generator has a linear layer followed by three residual blocks, each containing conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator consists of one residual block with two convolutional layers followed by three residual blocks and a final linear layer. The analysis and experiments assumed known parameter distribution for simulating the stochastic measurement process, but now consider the case where the parameter distribution is only approximately known. In the experiment, the AmbientGAN approach is tested for robustness to mismatches in parameter distribution of the measurement function using the Block-Pixels measurement model on the MNIST dataset. Pixels are blocked with a probability of 0.5 to create a dataset for training AmbientGAN models with varying blocking probabilities. The AmbientGAN approach is tested for robustness to parameter distribution mismatches using the Block-Pixels measurement model on the MNIST dataset. Generative models have shown improvement in sensing over sparsity-based approaches. The learned generator from AmbientGAN captures the data distribution well, as evidenced by the peak in the inception score at p = 0.5. The GAN trained with a blocking probability of 0.5 is used for compressed sensing. Using the AmbientGAN trained with corrupted samples, we tested its performance for compressed sensing on the MNIST dataset. Comparing reconstruction error vs the number of measurements, we observed a reduction similar to Lasso, indicating the effectiveness of AmbientGAN for this task."
}