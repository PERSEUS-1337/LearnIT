{
    "title": "BJE-4xW0W",
    "content": "We introduce causal implicit generative models (CiGMs) for sampling from observational and interventional distributions using adversarial training. The generator architecture is structured based on a causal graph for conditional and interventional sampling of face images with binary feature labels. Two new conditional GAN architectures, CausalGAN and CausalBEGAN, are proposed for generating images conditioned on binary labels. The neural network is consistent with the causal graph between labels. Two new conditional GAN architectures, CausalGAN and CausalBEGAN, are proposed for generating images conditioned on binary labels. The architectures can sample from observational and interventional image distributions, even for interventions not in the dataset. GANs are successful in training implicit generative models. Generative adversarial networks (GANs) are neural models that can sample from high-dimensional distributions without explicit parameterization. GANs consist of a generator network that produces samples from a noise vector and a discriminator network that distinguishes between generated and real samples. The goal is for the generator to convince the discriminator that its samples are real. GANs have been successful in generating samples like images and videos. Generative adversarial networks (GANs) aim to maximize the discriminator's loss by generating samples that mimic real data distributions. GANs have been successful in generating samples such as images and videos. An extension of GANs involves sampling from class conditional data distributions by providing class labels to the generator. Various neural network architectures have been proposed for this task. CausalBEGAN can sample from joint and interventional distributions, showcasing differences in data distribution probabilities. In this paper, the focus is on extending previous work on conditional image generation by capturing the dependence between labels and the causal effect between labels. Conditional image generation is viewed as a causal process where labels determine the image distribution. The generator maps labels to images non-deterministically, following the causal graph \"Labels cause the Image\" denoted by L \u2192 I. The generator in conditional image generation is a non-deterministic mapping from labels to images, following the causal graph \"Labels cause the Image\" denoted by L \u2192 I. Causal models allow sampling from different distributions based on conditional relationships between labels, such as Gender (G) causing Mustache (M). Causal models allow sampling from conditional and interventional distributions. Interventions fix the value of a variable in a causal graph, affecting the descendants but not the ancestors. Intervening on Mustache = 1 does not change the distribution of Gender. Samples illustrate this concept on Bald and Mustache variables. In this work, causal implicit generative models (CiGM) are proposed to sample from joint, conditional, and interventional probability distributions. The true causal graph is assumed to be known, and GANs are used to train CiGM when the generator structure follows the causal graph. Wasserstein GAN (WGAN) is employed to train a CiGM for binary image labels, as part of a two-step procedure for training CiGM for images and labels. GANs are utilized to train causal implicit generative models, specifically using Wasserstein GAN (WGAN) for binary image labels. Two novel conditional GANs, CausalGAN and CausalBEGAN, are proposed for the second step of training. The optimal generator of CausalGAN can sample from true conditional distributions. Combining CausalGAN with a CiGM on labels results in a CiGM for both labels and images. Adversarial training is used to structure the generator architecture based on the causal graph to train a CiGM. WGAN is shown to effectively learn a CiGM for binary labels. The focus is on conditional and interventional sampling of images. We propose a two-stage procedure to train a CiGM for binary labels and images using a novel conditional GAN architecture and loss function. Additionally, we introduce CausalBEGAN, an extension of BEGAN that produces high-quality images capturing image labels. Our framework is evaluated on labeled data. Using the motivations from BEGAN, a \"margin of margins\" term is introduced in CausalBEGAN, which generates high-quality images capturing image labels. The CiGM training framework is evaluated on labeled CelebA data, showing that CausalGAN and CausalBEGAN can produce label-consistent images even for interventions not seen during training. Various conditional GAN architectures like CGAN and ACGAN have been proposed in previous studies. In BID10, ACGAN is introduced where the discriminator estimates labels instead of receiving them as input. BID15 proposes InfoGAN to maximize mutual information between inputs and images. BiGAN and ALI extend GAN by learning a mapping to a latent space. CoGAN learns a joint distribution over images and binary labels. In BID18, BiGAN and ALI extend the GAN framework by learning a mapping to a latent space. CoGAN learns a joint distribution over images and binary labels, while SD-GAN splits the latent space into \"Identity\" and \"Observation\" portions. BID0 uses CGAN with a one-hot encoded vector for age intervals. CausalBEGAN is an extension of BEGAN that accepts labels. CoGAN and SD-GAN struggle to extend to more than two labels. Generative models have applications in compressed sensing, with guarantees for recovering a vector close to the output of a trained model. Recent attention is on using causal principles in deep learning and deep learning techniques for causal inference. GAN layers are connected to structural equation models, with CGAN used to learn causal direction between variables in datasets. In recent research, the connection between GAN layers and structural equation models has led to the use of CGAN to determine causal direction between variables. Other studies have explored using neural networks to identify causal relationships in image class labels and have introduced causal regularization for training predictive causal models. Additionally, there is a growing interest in utilizing neural networks for learning causal graphs. In recent research, neural networks are being used to learn causal graphs and identify causal relationships in images without using labels. This involves mimicking structural equations through neural connections to represent causal models using directed acyclic graphs. Pearl's framework of structural causal models (SCMs) is utilized, where X causing Y implies the existence of a function f and an unobserved random variable E, independent from X, determining the value of Y based on X and E. Assumption 3 states that X causes Y through a function f and an unobserved random variable E. The causal graph is represented as X \u2192 Y, with parents of a node X_i in the graph indicating the causes of that variable. A structural causal model consists of functions, random variables, and exogenous variables. A structural causal model (SCM) is defined by a tuple containing functions, random variables, and exogenous variables. The causal graph represents the relationships between variables, with interventions denoted as changes to the underlying causal mechanism. An intervention in a structural causal model changes the causal graph by disconnecting nodes from their parents. The post-interventional distribution can be calculated from the Bayesian network. After an intervention in a structural causal model, the post-interventional distribution can be calculated from the Bayesian network by factorizing the observational distribution. Identifying the true causal graph for a set of variables without experiments or additional assumptions is generally not possible due to multiple causal graphs that can produce the same joint probability distribution. This paper assumes the causal graph is given and focuses on learning a causal model with structural equations. This paper focuses on learning a causal model with structural equations assuming the causal graph is given. Bayesian networks can be used to sample from correct observational distributions, and causal implicit generative models are proposed to sample from both observational and interventional distributions. Causal implicit generative models extend Bayesian networks by allowing sampling from interventional distributions. Generative adversarial networks can train these models using a causal graph structure. Feedforward neural networks represent functions fX, fY, fZ, with independent noise terms (NX, NY, NZ) satisfying joint independence conditions. See FIG3 for the architecture. The feedforward neural networks in the causal graph architecture represent functions fX, fY, fZ with independent noise terms (NX, NY, NZ). Gaussian distributed variables can be used for the noise terms. Two causal models with the same observational distribution have the same interventional distributions for any intervention. Two causal models with the same observational distribution have the same interventional distributions for any intervention. A feedforward neural network is tied to a causal graph, where Z = {Z1, Z2, ..., Zm} are mutually independent random variables. Causal implicit generative models are defined using a feedforward neural network G with output DISPLAYFORM7 for the causal model DISPLAYFORM8. Causal implicit generative models are defined as feedforward neural networks consistent with a causal graph. Adversarial training is proposed for training CiGMs using samples from a joint distribution. In the context of image generation with binary labels, a CausalGAN architecture is focused on, where a causal controller is used as a pretrained model for image labels. The CausalGAN architecture divides the task of learning a CiGM into two subtasks: training a generative model for labels and then for images conditioned on the labels. The architecture and loss function ensure that the generator outputs label-conditioned image distributions. The CausalGAN architecture introduces a new model called Causal Controller for generating label-conditioned images based on a joint distribution assumption. The generator is designed to sample from a discrete label distribution for binary labels. The Causal Controller network is structured to produce labels sequentially according to the causal graph. To sample from a discrete label distribution, WGAN is employed instead of standard GAN training. A new conditional GAN architecture is designed to generate images based on the labels of the Causal Controller. The gradient constraint is replaced by a penalty term in the loss. A new conditional GAN architecture is designed to generate images based on the labels of the Causal Controller. Two separate labeler neural networks, Labeler and Anti-Labeler, are used to estimate labels. The generator aims to produce realistic images and images consistent with the labels. The CausalGAN generator produces images by competing with the discriminator, minimizing Labeler loss, and maximizing Anti-Labeler loss to avoid mode collapse. The Anti-Labeler network is crucial for preventing the generator from outputting only a few typical faces for a fixed label combination. The Anti-Labeler loss prevents label-conditioned mode collapse by discouraging the generator from outputting typical faces for fixed label combinations. Using Anti-Labeler aids faster convergence and is effective for rare label combinations. Results for a single binary label are presented, with potential extension to more labels. The data distribution between images and labels (P r) and the joint distribution between generator labels and images (P g) are analyzed assuming a perfect scenario. The CausalGAN model includes mappings for the generator, discriminator, Labeler, and Anti-Labeler. The generator loss function contains label loss terms, GAN loss, and an additional loss term from the discriminator. The optimal generator outputs the class conditional image distribution with the added loss term. The Anti-Labeler solves an optimization problem for a fixed generator, while the Labeler also solves an optimization problem. The CausalGAN model includes mappings for the generator, discriminator, Labeler, and Anti-Labeler. The optimal generator outputs the class conditional image distribution for multiple binary labels. The Anti-Labeler, Labeler, and discriminator each solve specific optimization problems. The best CausalGAN generator samples from the class conditional image distribution when the Causal Controller samples from the true label distribution. The proof shows the optimal operation of label distribution, discriminator, and labeler networks in a conditional generative adversarial network architecture. The optimal discriminator behavior is characterized, along with the generator loss when all components are at their best. The generator minimizes its loss by sampling from class conditional distributions. The appendix contains Proposition 2, Lemma 1, and Lemma 2. The generator minimizes the loss by sampling from class conditional distributions. The global minimum of the virtual training criterion is achieved when the generator output has the same distribution as the class conditional image distribution. The two-stage procedure can train a causal implicit generative model for any causal graph where the Image variable is a sink node. The corollary discusses a causal implicit generative model for a causal graph, where a generator can sample from image distribution based on given labels. The objective is to extend this to multiple binary labels. Theorem 2 shows that the minimizer of the model samples from class conditional distributions given d labels. The proposed alternative architecture extends the single binary label setup by using cross entropy loss terms for each label, requiring the Labeler and Anti-Labeler to have only d outputs. While this ensures the generator captures each label's posterior distribution, it does not guarantee the class conditional distributions will be true to the data distribution. The generator captures each label's posterior distribution, but it may not ensure the class conditional distributions align with the data distribution. However, for joint distributions where labels are solely determined by the image, the joint label posterior will reflect the data distribution. Causal implicit generative models can also sample from counterfactual distributions with known exogenous noise terms. Generative models can sample from counterfactual distributions with known exogenous noise terms by conditioning on an event and sampling from the push-forward of posterior distributions. A simple extension of BEGAN involves feeding image labels to the generator using a Labeler network for interventional sampling. In interventional sampling, the Causal Controller is used to produce labels. The architecture modifications include a Labeler network that labels real images well and generated images poorly. Margin modifications are motivated by observations that a better trained Labeler is necessary for meaningful gradients for label quality. Training CausalGAN and CausalBEGAN on the CelebA Causal Graph involves using margin terms and a margin of margins term comparing the first two margins. The image quality is high, and training CausalGAN and CausalBEGAN on the CelebA Causal Graph involves using margin terms. The dataset used satisfies the condition, with observations suggesting a margin term and a necessary margin of margins term. The Causal Controller results are discussed, and the relationship between Male and Mustache in the CelebA Causal Graph is explored. The top row of images shows both males and females with mustaches, while the bottom row shows only male images. The generator never sees the label combination {Male = 0, Mustache = 1} during training. Our generative model can sample from interventional distributions with provable guarantees. Top: Intervene Narrow Eyes=1, Bottom: Only male images sampled from P(.|Mustache = 1). Our generative model can sample from interventional distributions with provable guarantees. Intervening on Narrow Eyes = 1 increases the proportion of smiling images in the dataset. The generative model can produce samples that differ from training data, illustrated with CausalGAN and CausalBEGAN models. Supported by NSF Grants, the research focuses on structural causal models and interventional distributions. NVIDIA Corporation and ONR N000141512009 focus on structural causal models, which consist of functions, random variables, exogenous variables, and a probability distribution. The causal graph is a directed acyclic graph representing the relationships between variables in a Bayesian network. The model assumes causal sufficiency. The text discusses Bayesian networks, causal sufficiency, and interventional distributions. It mentions that under the causal sufficiency assumption, interventional distributions for causal Bayesian networks can be calculated directly from conditional probabilities and the causal graph. The joint data distribution over a single binary label and image is denoted as P r (l, x), while the joint distribution over the binary label fed to the generator and the image produced by the generator is denoted as P g (l, x). In this section, we use P r (l, x) for the joint data distribution over a single binary label l and the image x. P g (l, x) is used for the joint distribution over the binary label l fed to the generator and the image x produced by the generator. Proposition 2 states that for fixed G, the optimal discriminator D is given by a certain formula. The optimal Labeler is defined as having D LR (x) = P r (l = 1|x), and a similar lemma exists for the Anti-Labeler. The proof for the optimal discriminator involves maximizing a certain formula. The lemma for the Anti-Labeler is similar to that of the Labeler. The definition assumes causal sufficiency and a product distribution over exogenous variables. The complete graph \"cG1\" is formed with added edges, and the reversed graph rcG1 is utilized. Theorem 1 defines C(G) as the generator loss when discriminator, Labeler, and Anti-Labeler are considered. The global minimum of the virtual training criterion C(G) is achieved when the generator output has the same distribution as the class conditional image distribution. The relations between the Labeler, Anti-Labeler, and discriminator are defined, leading to the Kullback-Leibler divergence. The relations between the Labeler, Anti-Labeler, and discriminator are defined by Prop 2, Lemma 1, and Lemma 2. The Kullback-Leibler divergence is minimized when the image and label distributions are equal. A causal implicit generative model for the causal graph D is consistent with the graph, and a generator can sample from the image distribution based on given labels. The concatenated generator neural network in a conditional GAN is consistent with the causal graph D, assuming perfect sampling from true label and image distributions. This allows sampling from true observational and interventional distributions, making it a causal implicit generative model for graph D. The concatenated model serves as a causal implicit generative model for graph D, allowing sampling from true observational and interventional distributions. Modifications are needed for multiple binary labels, as each labeler can only learn about the posterior probabilities for each label, which may not be sufficient for the generator to learn the correct joint distribution. Two solutions are proposed to address this issue. Two solutions are proposed to address the issue of multiple binary labels. The first solution involves each labeler estimating the probability of each label combination, while the second solution involves using labelers to estimate the probabilities of each label individually. The extension and results are presented, with a lemma stating the optimum labeler with respect to loss. The optimum Labeler with respect to loss in (12) has D * LR (x)[j] = P r (l = j|x). The Labeler loss can be written as DISPLAYFORM3 where L x is the discrete random variable such that P(L x = j) = P r (l = j|x). The loss is lower bounded by \u2212H(L x. The optimum Labeler network gives the posterior probability of a label combination based on the observed image. The loss function is lower bounded by \u2212H(L x) and can be minimized by ensuring the two random variables have the same distribution. The Anti-Labeler network solves a different optimization problem. The softmax function can satisfy the constraint of coordinates summing to 1. The Anti-Labeler network's optimization problem involves P g (x|l = j) and \u03c1 j = P(l = j). The optimum Anti-Labeler has D *LG (x)[j] = P g (l = j|x). The generator's optimization problem involves DISPLAYFORM6. The optimal generator samples from class conditional image distributions. The generator, Labeler, and Anti-Labeler work together to optimize the distribution of samples. The optimal generator samples from class conditional image distributions based on label combinations. The global minimum of the virtual training criterion is achieved when the generator's output matches the true joint label distribution. The CausalGAN architecture with d labels assumes a deterministic relationship between images and labels in the dataset. This ensures that the generator's output matches the true joint label distribution, optimizing the distribution of samples. Theoretical guarantees for the implemented CausalGAN architecture with d labels are provided under the assumption of a deterministic relationship between images and labels in the dataset. This assumption ensures that the global optimal generator samples from the class conditional distributions. The Anti-Labeler and Labeler optimization problems are solved for fixed generator and discriminator configurations. The generator aims to optimize the virtual training criterion by ensuring that the generated distribution matches the real distribution for all labels and overall. However, this does not guarantee that the generated distribution samples from the class conditional image distributions. The generator aims to match the real distribution for all labels, but this doesn't ensure sampling from class conditional image distributions. The assumption that the image determines all labels is crucial for correct conditional sampling. The lemma states that a discrete joint probability distribution with all marginal distributions as kronecker delta functions is the product of these marginals. The joint probability distribution is zero everywhere except at specific points. The lemma states that a joint probability distribution is zero everywhere except at specific points. This contradicts the assumption that the distribution is zero everywhere except at (u1, u2, ..., un), where it should be 1. Applying the lemma on the conditional distribution P_g(l1, l2, ..., ld | x) shows that the marginals are kronecker delta functions. In this section, a simple extension of BEGAN is proposed where image labels are fed to the generator. The optimum generator samples from class conditional image distributions, as shown by the chain of equalities and the proof based on Bayes' rule and kronecker delta functions. In this section, a new loss and margins are introduced to extend BEGAN by feeding image labels to the generator. The proposed approach aims to utilize label gradients effectively when image quality is high. The proposed approach introduces a new loss and margins to extend BEGAN by incorporating image labels into the generator. This aims to effectively utilize label gradients for high-quality images. The formulation includes a margin-coefficient tuple to address the use of margins critical in the BEGAN formulation. The generator in the BEGAN model aims to minimize two loss terms, incorporating a margin-coefficient tuple to utilize label gradients for high-quality images. A new margin of margins term is introduced to encourage the generator to incorporate label loss effectively when the image quality margin is large compared to the label margin. In the BEGAN model, a new margin of margins term is introduced to optimize label gradients for high-quality images. The extension maintains a monotonically decreasing scalar to track gradient descent convergence, and explores training causal implicit generative models with distinct causal graphs. In investigating causal implicit generative models, the behavior and convergence are studied when the true data distribution comes from different causal graphs. Synthetic data with three features {X, Y, Z} generated from various causal models are analyzed for convergence. The joint distribution is compared to the true joint using total variation distance, structured according to different causal graphs like \"line\" X \u2192 Y \u2192 Z, \"collider\" X \u2192 Y \u2190 Z, and \"complete\" X \u2192 Y \u2192 Z, X \u2192 Z. The convergence behavior of causal implicit generative models is studied using synthetic data generated from different causal graphs. The joint distribution is compared to the true joint using total variation distance, structured according to line, collider, or complete graphs. Results are presented for generators with varying layers and no knowledge of causal structure. The convergence behavior of causal implicit generative models is studied using synthetic data generated from different causal graphs. The generator distribution's convergence is analyzed based on the structured causal graphs, such as line, collider, or complete graphs. The performance of the models with varying layers and no prior knowledge of causal structure is evaluated. In exploring the convergence behavior of causal generative models, different graph structures were analyzed. The best performance was seen with line and complete graphs in the generator architecture. Fully connected networks with 3 layers performed well, but those with 5 and 10 layers did not. Using the wrong Bayesian network, like a collider, resulted in worse performance. Surprisingly, a fully connected generator with 3 and 5 layers showed the best performance for the collider graph. The number of layers is crucial for achieving optimal performance in causal generative models. Using the wrong Bayesian network, such as a collider, leads to inferior results. Surprisingly, a fully connected generator with 3 and 5 layers performs best for the collider graph. Different graph structures, like line and complete graphs, impact convergence behavior differently. Fully connected networks with 3 layers excel in the complete graph scenario. Line and collider graphs perform poorly and show no convergence behavior. The effect of using the wrong causal graph on an artificially generated dataset is evaluated. Data is generated using the causal graph X1 \u2192 X2 \u2192 X3. Different generator causal graphs are compared, showing that using the correct graph gives the closest approximation. The importance of graph structure on convergence behavior is highlighted. The CelebA dataset experiments involve causal graphs to learn distributions accurately. The CelebA Causal Graph (G1) is used, with a completed version cG1. The ordering in cG1 determines causal relationships, such as Male causing Smiling. The importance of using the correct graph is highlighted for accurate scatter plot representation. The CelebA dataset uses causal graphs G1 and cG1 to learn distributions accurately. In cG1, Male causes Smiling due to the ordering. Comparing distributions in TAB0 shows that cG1 approximates the true distribution better than G1. Despite inaccuracies, both G1 and cG1 avoid outputting {Female, Mustache}. The CelebA dataset uses causal graphs G1 and cG1 to learn distributions accurately. Despite inaccuracies, both graphs avoid outputting {Female, Mustache}. Wasserstein GAN ensures convergence in distribution of the Causal Controller output. A modified version of Wasserstein GAN with a penalized gradient is used to demonstrate good convergence. The CelebA dataset uses causal graphs G1 and cG1 to accurately learn distributions for all labels. The Wasserstein Causal Controller shows good convergence in training, ensuring marginal distributions are within a small neighborhood of the true values. The Wasserstein Causal Controller demonstrates strong convergence in training, with 96% of samples appearing in a small neighborhood of 0 or 1. Total variational distance decreases to 0 for CelebA Causal Graph, completion, and reversed completion. The total variational distance (TVD) shows strong convergence in training for CelebA Causal Graph (G1), completion (cG1), and reversed completion (rcG1). TVD decreases to 0 for cG1 and rcG1, while G1 asymptotes to around 0.14, indicating incorrect conditional independence assumptions. CausalGAN results are presented in FIG3, showing the impact of intervening vs conditioning on wearing lipstick in CelebA Causal Graph. Intervening vs Conditioning on Wearing Lipstick in CelebA Causal Graph: When intervening on Wearing Lipstick=1, both males and females are shown, but when conditioning on Wearing Lipstick=1, only female images are displayed due to the dataset distribution. Similarly, for Narrow Eyes, intervening or conditioning on it does not affect the probability of Smiling=1. In CelebA Causal Graph, intervening on Narrow Eyes=1 does not affect the probability of Smiling=1, but conditioning on Narrow Eyes=1 increases the proportion of smiling images. CausalBEGAN is trained on CelebA dataset using CelebA Causal Graph, with a pretrained Causal Controller. Removing the margin of margins in the model deteriorates image quality for rare labels. The need for the margin of margins in the CausalBEGAN model was empirically justified by training with c3=1, showing deteriorated image quality for rare labels. The difference between interventional and conditional sampling for labels like Bald was illustrated, with examples in CelebA Causal Graph. Conditioning on Bald=1 resulted in only male images being sampled. The conditional distribution of images in CelebA Causal Graph was demonstrated by conditioning on Bald=1, resulting in only male images being sampled. Additionally, simulations for CausalGAN were provided to show conditional image generation properties by sweeping a single label from 0 to 1 while keeping other inputs fixed. In this section, additional simulations for CausalGAN and CausalBEGAN are provided. Figures show conditional image generation properties and image diversity. The impact of the third margin term in CausalBEGAN is discussed, along with the extension of a scalar \"M\" that decreases monotonically during training. In this section, the setup in CausalBEGAN allows for the definition of a scalar \"M\" that decreases monotonically during training. An extension called M complete is introduced, preserving these properties. The conditional image generation properties of CausalBEGAN are demonstrated using label sweeps, showing the generator learns a discrete function with respect to its label input parameters. Additionally, a random sampling of 256 images is shown to examine mode collapse and image diversity. In this section, the results of training an implicit causal generative model for labels and images are presented. The approach treats the image as part of the causal graph, with attempts made to jointly train labels and images. Implementation details for CausalGAN and CausalBEGAN are discussed, highlighting differences between theory and practice. The implementation details of the Wasserstein Causal Controller for generating face labels are explained in this section. The total variation distance (TVD) is used as a metric to evaluate the success of the models. The gradient term as a penalty is estimated by evaluating the gradient at points interpolated between real and fake batches. The Wasserstein approach allows training the Causal Controller to output discrete labels, although rounding them before passing to the generator still provides benefits. The Wasserstein approach enables training the Causal Controller to output discrete labels, with benefits seen from rounding before passing to the generator. The generator architecture is based on a causal graph, using neural networks and uniform noise as exogenous variables. Training involves Wasserstein discriminator updates and stochastic gradient descent. The model is built on DCGAN, with modifications for the Causal GAN framework. Our Causal GAN framework extends the net-based implementation of generative adversarial networks by incorporating Labeler networks and a Causal Controller network. We make 6 generator updates for each discriminator update on average and update the discriminator and labeler networks concurrently. The loss terms are modified to accommodate d-dimensional label vectors, with the Labeler and Anti-Labeler loss terms averaged for each label. The architecture of the Causal GAN framework includes Labeler networks and a Causal Controller network. The loss terms are adjusted for d-dimensional label vectors, with the order of terms in the cross entropy expressions swapped for sharper images during training. In the supplementary material, the implementation of CausalBEGAN involves swapping the order of terms in cross entropy expressions for labeler losses, using few parameter tunings, and setting learning rates and update frequencies. Customized margin learning rates are also utilized to reflect the generator's response behavior. The implementation of CausalBEGAN involves customized margin learning rates \u03bb 1 = 0.001, \u03bb 2 = 0.00008, \u03bb 3 = 0.01 to reflect the generator's response behavior. Comparing CausalGAN with and without Anti-Labeler network shows that using Anti-Labeler allows for faster convergence and provides more diverse images for very rare labels."
}