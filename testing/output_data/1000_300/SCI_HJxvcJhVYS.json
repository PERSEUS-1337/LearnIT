{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A proposed method uses Bayesian optimization to approximate these distributions through Stein variational gradient descent on Gaussian process model estimates. Initial results show promise for likelihood-free inference in reinforcement learning environments. The method uses Stein variational gradient descent on Gaussian process model estimates to approximate posterior distributions for parameter estimation in physical systems. It addresses challenges in likelihood-free inference for robotics and reinforcement learning due to limited simulations. Recent methods aim to improve efficiency in simulations for robotics and reinforcement learning by using conditional density estimators or sequentially learning approximations to the likelihood function. Gutmann and Corander (2016) propose an active learning approach using Bayesian optimization to reduce the number of simulator runs. Corander (2016) developed an active learning approach using Bayesian optimization to propose parameters for simulations, reducing the number of simulator runs. The approach combines variational inference methods with Bayesian optimization, using a Thompson sampling strategy to refine variational approximations to a black-box posterior. Parameters for new simulations are proposed by running Stein variational gradient descent over samples from a Gaussian process. The approach also includes a method to optimally subsample variational approximations for batch evaluations of simulator models. The approach combines variational inference methods with Bayesian optimization to estimate a distribution q approximating a posterior distribution p(\u03b8|y) over simulator parameters \u03b8. It uses a black-box approach to minimize the discrepancy between q and the target p, solving it via Stein variational gradient descent over samples from a Gaussian process. The optimization approach aims to find the optimal q by minimizing the discrepancy with the target p using kernelized Stein discrepancy. A black-box method is used, incorporating a GP model for likelihood approximation, Thompson sampling for candidate selection, and kernel herding for parameter sampling. By directly learning q through Stein variational gradient descent, the need for gradients of the target distribution is bypassed. The text discusses using Stein variational gradient descent (SVGD) to learn q directly, bypassing the need for gradients of the target distribution. A Gaussian Process (GP) is used to model the synthetic likelihood function, making the simulations-observations discrepancy easier to evaluate. The choice of discrepancy function for experiments is detailed in Section 3, with additional background on the Kernelized Stein Discrepancy (KSD) provided in the appendix. The text discusses using Gaussian Process (GP) for approximating the simulations-observations discrepancy \u2206 \u03b8, enabling the application of SVGD in the Bayesian Optimization (BO) loop. Candidate distributions q n are selected using Thompson sampling from the GP posterior, which accounts for uncertainty in the model. Thompson sampling has been successfully applied in BO problems for selecting point candidates \u03b8 \u2208 \u0398. Thompson sampling involves sampling functions from the GP posterior to account for uncertainty in the model. For models with finite feature maps like SSGPs, weights are sampled from a multivariate Gaussian to constitute a sample from the posterior. The acquisition function is defined based on this sampling approach. SVGD represents the variational distribution as a set of particles that are optimized through perturbations using the SSGP kernel. Thompson sampling involves sampling functions from the GP posterior to address uncertainty. SVGD optimizes particles using the SSGP kernel to update the GP model with samples from a distribution. Thompson sampling uses GP posterior sampling for uncertainty, while SVGD optimizes particles with SSGP kernel for GP model updates from a distribution. The large number of particles in q is improved by representing it with M particles to explore the posterior surface efficiently. Subsampling query parameters from q is done to run the simulator with fewer expensive simulations. Kernel herding is used to select a subset of query parameters for simulations, minimizing error using maximum mean discrepancy. In the case of SSGPs, the algorithm uses GP posterior kernel to choose informative samples for the model. The GP posterior kernel encodes information for selecting informative samples. The DBO algorithm is summarized in Algorithm 1 and evaluated in synthetic data scenarios against MDNs. The method is tested on OpenAI Gym's cart-pole environment with fixed physics parameters. The experiment evaluates a method using MDNs learned from a dataset of parameters and simulator outputs. The discrepancy between real and estimated statistics is calculated. Results show the method recovers the target system's posterior curve and provides better approximations than MDNs. The DBO algorithm outperforms in terms of overall approximation. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations than MDN. Results suggest DBO is more sample-efficient for inferring parameters in reinforcement learning. Future work includes scalability and theoretical analysis. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations than MDN. Results suggest DBO is more sample-efficient for inferring parameters in reinforcement learning. Future work includes scalability and theoretical analysis. The method allows for fast incremental updates of the GP posterior with time complexity O(M^2). Gijsberts and Metta (2013) suggest using Cholesky factors to update the GP posterior efficiently with time complexity O(M^2), which is independent of the number of data points N."
}