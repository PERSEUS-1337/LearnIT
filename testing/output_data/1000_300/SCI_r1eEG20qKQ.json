{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks are adapted by fitting approximations to the best-response function. Scalable best-response approximations are constructed by modeling the best-response as a single network with gated hidden units. This model is fitted using a gradient-based hyperparameter optimization algorithm. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training without the need to differentiate the training loss. It outperforms other hyperparameter optimization methods on large-scale deep learning problems, adapting hyperparameter schedules dynamically. Self-Tuning Networks (STNs) update hyperparameters online during training, outperforming other methods on large-scale deep learning problems. Popular hyperparameter optimization approaches include grid search, random search, and Bayesian optimization, but they pose hyperparameter tuning as a black-box problem and require many training runs. Hyperparameter optimization can be formulated as a bilevel optimization problem. Hyperparameter optimization can be formulated as a bilevel optimization problem, aiming to solve for the best-response function w * (\u03bb) to minimize validation loss. Approximating w * with a parametric function offers speed-ups over black-box methods. Approximating the best-response function w * with a parametric function offers speed-ups over black-box methods. A scalable approximation\u0175 \u03c6 is proposed to directly approximate w * using a parametric function. This involves jointly optimizing \u03c6 and \u03bb, updating \u03c6 to approximate w * in a neighborhood around the current hyperparameters, and then updating \u03bb using\u0175 \u03c6 as a proxy for w *. The challenge lies in constructing a compact approximation for the weights of a neural network, which involves modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters. Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering advantages over other optimization methods. They can be easily implemented by replacing existing modules in deep learning libraries with \"hyper\" counterparts. This ensures that computational effort is not wasted fitting hyperparameters, as they are adapted online. Self-Tuning Networks (STNs) update hyperparameters online during training, outperforming fixed settings. STN training algorithm does not require differentiating loss with respect to hyperparameters, allowing tuning of discrete hyperparameters. Empirical evaluation on large-scale deep-learning problems shows improved performance. Bilevel optimization involves solving upper and lower-level problems simultaneously. These problems are used in various fields and can be applied to machine learning tasks like hyperparameter optimization and GAN training. In economics, bilevel optimization models leader/follower dynamics and has applications in machine learning for hyperparameter optimization, GAN training, meta-learning, and neural architecture search. Bilevel problems are NP-hard, leading to focus on linear, quadratic, and convex functions. This study aims to find local solutions in nonconvex, differentiable, and unconstrained settings. The study focuses on finding local solutions in nonconvex, differentiable, and unconstrained settings. It aims to solve a problem using a gradient-based algorithm, specifically looking at the limitations of simultaneous gradient descent and proposing the use of the best-response function for a more principled approach. The study proposes using the best-response function for a more principled approach to solving a problem in nonconvex, differentiable, and unconstrained settings. It involves minimizing Eq. 5 using gradient descent on F* with respect to \u03bb, assuming unique optimum w*(\u03bb) for Problem 4b. Sufficient conditions for differentiability and uniqueness of w* are discussed. The gradient of F* can be decomposed into direct and response gradients, with the direct gradient reflecting the upper-level objective's reliance on \u03bb and the response gradient showing how the lower-level parameter responds to changes in the upper-level parameter. This decomposition can help stabilize optimization in bilevel problems. Including the response gradient in optimization can stabilize the process by converting the bilevel problem into a single-level one. This ensures a conservative gradient vector field and can lead to fruitful algorithms in practice. Gradient-based hyperparameter optimization methods aim to approximate the best-response or its Jacobian, but they may struggle with discrete hyperparameters and stochastic elements. Gradient-based hyperparameter optimization methods aim to approximate the best-response w * or its Jacobian \u2202w * /\u2202\u03bb, which can be computationally expensive and struggle with discrete and stochastic hyperparameters. Lorraine & Duvenaud (2018) proposed a global approximation algorithm that approximates w * as a differentiable function\u0175 \u03c6 with parameters \u03c6, allowing for more efficient optimization. Gradient-based hyperparameter optimization methods aim to approximate the best-response w * or its Jacobian \u2202w * /\u2202\u03bb. If the distribution p(\u03bb) is fixed, gradient descent with respect to the hypernetwork \u03c6 minimizes a specific objective function. Local approximation techniques are used to model w * in a neighborhood around the current upper-level parameter \u03bb, using a factorized Gaussian noise distribution. An alternating gradient descent scheme is employed to update the parameters \u03c6 and \u03bb iteratively. The approach involves minimizing an objective by perturbing the upper-level parameter \u03bb and updating \u03c6 and \u03bb alternately. It has shown success with L2 regularization on MNIST but its applicability to different regularizers or larger problems is uncertain. Challenges include determining the neighborhood size defined by \u03c3 for training \u03c6 and adapting to discrete and stochastic hyperparameters. A memory-efficient best-response approximation \u0175 \u03c6 is proposed for large neural networks, supported by analysis of simpler scenarios. The approach involves constructing a memory-efficient best-response approximation for large neural networks, which can adapt to discrete and stochastic hyperparameters. This method allows for automatic adjustment of the neighborhood scale and updates hyperparameters online during training, resulting in Self-Tuning Networks (STNs). The best-response for a layer's weight matrix and bias is approximated as an affine transformation of the hyperparameters. The architecture involves an affine transformation of hyperparameters for weight and bias computation in neural networks. It is memory-efficient and tractable, enabling parallelism and independent perturbation of hyperparameters in batches. The architecture allows for parallel computation of hyperparameters in batches, improving sample efficiency. It involves an affine transformation of hyperparameters for weight and bias computation in neural networks, enabling memory-efficient and tractable operations. In this section, a model is presented where the best-response function is represented by a linear network with Jacobian norm regularization. The network's hidden units are modulated based on hyperparameters, using a 2-layer linear network to predict targets from inputs. The model uses a squared-error loss with an L2 penalty on the Jacobian, with a penalty weight \u03bb. The theorem states the conditions for the network's weights, and the implementation of y(x; w*(\u03bb)) is discussed. Theorem 2 discusses the implementation of y(x; w*(\u03bb)) using a regular network with sigmoidal gating of hidden units. This architecture simplifies the best-response function approximation for deep, nonlinear networks, especially for a narrow range of hyperparameter values. The best-response function can be approximated by an affine function for a small range of hyperparameter values, replacing sigmoidal gating with linear gating. Using an affine approximation and minimizing the objective yields the correct best-response Jacobian, ensuring convergence to a local optimum in gradient descent. The effect of the sampled neighborhood on approximating the best-response function is crucial. If the neighborhood is too small, the approximation may not match the exact best-response or its gradient. If the neighborhood is too wide, the approximation may not be flexible enough to model the best-response accurately. The scale of the hyperparameter distribution controls the flexibility of the approximation, with entries needing to be balanced to capture the best-response locally. Adjusting the scale of the hyperparameter distribution during training is crucial for capturing the best-response function accurately. By varying the scale parameter \u03c3 based on the sensitivity of the upper-level objective, we can ensure that the model captures the shape locally around the current hyperparameter values. This adjustment is similar to a variational inference objective, with an entropy term weighted by \u03c4 to enlarge the entries of \u03c3. The objective is to interpolate between variational optimization and variational inference by balancing the first term to avoid heavy entropy penalties. Performance evaluation is done at the deterministic current hyperparameter \u03bb 0. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot handle, such as discrete or stochastic hyperparameters. Hyperparameters are represented as \u03bb \u2208 R n and mapped to a constrained space using the function r. Training and validation losses, L T and L V, are functions of hyperparameters and parameters, denoted by f (\u03bb, w) = L T (r(\u03bb), w) and F (\u03bb, w) = L V (r(\u03bb), w). The STN training algorithm can handle discrete hyperparameters by using a gradient descent scheme to update \u03c6 for training steps and \u03bb and \u03c3 for validation steps. The algorithm is detailed in Algorithm 1 and can be implemented in code. The non-differentiability of r due to discrete hyperparameters is not an issue, and the derivative of the loss function can be estimated using the reparametrization trick. To estimate the derivative of E \u223cp( |\u03c3) [f (\u03bb + ,\u0175 \u03c6 (\u03bb + ))] with respect to \u03c6, the reparametrization trick can be used. Two cases are considered for discrete hyperparameter \u03bb i: Case 1 involves regularization schemes where L V and F do not directly depend on \u03bb i, allowing the reparametrization gradient. Case 2 deals with L V explicitly relying on \u03bb i, requiring the REINFORCE gradient estimator BID15. Certain hyperparameters like the number of hidden units in a layer affect validation loss and need this approach. The reparametrization trick can be used to estimate the derivative of the expectation with respect to \u03bb i. Hyperparameters like the number of hidden units directly impact validation loss and require specific gradient estimators. Self-tuning CNNs and LSTMs were created using this method, showing improved performance compared to fixed hyperparameter values. STNs were tested on CIFAR-10 and PTB datasets, outperforming traditional hyperparameter optimization methods. STNs outperform traditional hyperparameter optimization methods on CIFAR-10 and PTB datasets by dynamically adjusting hyperparameters during training. An ST-LSTM discovered a schedule for output dropout that achieved better results than a fixed dropout rate found through grid search. This improvement is attributed to the schedule, not regularizing effects or limited capacity. The best output dropout rate (0.68) was found through a grid search, resulting in 82.58 vs 85.83 validation perplexity. The improved performance is attributed to the schedule rather than regularizing effects or limited capacity. STNs outperformed standard LSTM with perturbations in dropout rate. The STN discovered an optimal output dropout schedule, leading to improved performance on language modeling and image classification tasks. Training a standard LSTM with the same schedule showed comparable results to the STN, highlighting the importance of the hyperparameter schedule. The STN discovered an optimal output dropout schedule for improved performance on language modeling and image classification tasks. It implements a curriculum by gradually increasing the dropout rate, leading to better generalization. The ST-LSTM was evaluated on the PTB corpus, showing the importance of hyperparameter schedule. The ST-LSTM was evaluated on the PTB corpus using a 2-layer LSTM with 650 hidden units per layer and 650-dimensional word embeddings. 7 hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. LSTM tuning resulted in the best performance with a fixed perturbation scale of 1 for the hyperparameters. Additional details can be found in Appendix D. STNs were compared to grid search. The ST-LSTM was tuned with a fixed perturbation scale of 1 for hyperparameters. STNs outperformed other methods in achieving lower validation perplexity more quickly. Different forms of dropout were observed in the schedules found by STNs for each hyperparameter. Some forms of dropout are used at different stages of training, including input, hidden, output, embedding, and weight dropout. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture and tuning 15 hyperparameters. STNs were compared to grid search, random search, and Bayesian optimization. In total, 15 hyperparameters were considered, including data augmentation parameters. STNs outperformed grid search, random search, and Bayesian optimization in finding better hyperparameter configurations in less time. The hyperparameter schedules found by STNs are shown in FIG3. Bilevel Optimization is discussed in BID10, with a common approach for linear, quadratic, or convex problems. When objectives/constraints are linear, quadratic, or convex, a common approach involves replacing the lower-level problem with its KKT conditions as constraints for the upper-level problem. In the unrestricted setting, the work resembles trust-region methods, which approximate the problem locally using a simpler bilevel program. Hypernetworks, first considered by Schmidhuber, are functions mapping to neural net weights. Various forms of predicting weights in CNNs have been developed, with Ha et al. using hypernetworks to generate weights for modern CNNs and RNNs. Hypernetworks are used to predict weights in CNNs and RNNs. Two main approaches for gradient-based hyperparameter optimization are discussed, one involving gradient descent steps and the other using the Implicit Function Theorem. The Implicit Function Theorem is used to derive \u2202w * /\u2202\u03bb(\u03bb 0 ) under certain conditions for hyperparameter optimization in neural networks. Different approaches have been used for hyperparameter optimization in log-linear models, kernel selection, and image reconstruction. Both approaches face challenges with certain hyperparameters and become expensive as the number of descent steps increases. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. The next hyperparameter to train on is chosen by maximizing an acquisition function that balances exploration and exploitation. Training each model to completion can be avoided by making assumptions on learning curve behavior. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. The next hyperparameter to train on is chosen by maximizing an acquisition function that balances exploration and exploitation. Training each model to completion can be avoided by making assumptions on learning curve behavior. Model-free approaches like grid search and random search are also used for hyperparameter optimization. Model-free approaches for hyperparameter optimization include grid search and random search. BID3 prefers random search over grid search. Successive Halving and Hyperband extend random search by adaptively allocating resources using multi-armed bandit techniques. These methods do not consider problem structure and ignore gradient information. However, they are easy to parallelize and perform well in practice. Population Based Training (PBT) involves training a population of networks in parallel, periodically evaluating performance, and replacing under-performing networks with better ones while copying and perturbing hyperparameters for training new network clones. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units. This allows for gradient-based optimization to tune various regularization hyperparameters, including discrete ones, leading to the discovery of hyperparameter schedules that outperform fixed hyperparameters. Self-Tuning Networks (STNs) optimize hyperparameters by scaling and shifting hidden units, leading to better generalization performance than fixed hyperparameters. STNs offer a path towards automated hyperparameter tuning for neural networks. The CIFAR Canadian AI Chairs program provides support for optimizing hyperparameters using the best-response of parameters. The hyperparameter gradient is a sum of direct and response gradients of validation losses. Lemma 1 shows that the Jacobian of the function decomposes as a block matrix, with the Hessian being positive definite and invertible. The Implicit Function Theorem guarantees the existence of a unique continuously differentiable function for optimizing hyperparameters. The Hessian is positive definite and invertible at a point, leading to the existence of a unique continuously differentiable function for optimization. This function is the unique solution to the problem, following second-order optimality conditions. The data matrix is denoted as X, with associated targets t. The SVD decomposition of X is represented by orthogonal matrices U and V, and a diagonal matrix D. The SVD decomposition of the data matrix X is represented by orthogonal matrices U and V, and a diagonal matrix D. The function y(x; w) is simplified by setting u = s Q, leading to a constant Jacobian. The optimal solutions for regularized and unregularized least-squares linear regression are given by specific equations. The SVD decomposition of the data matrix X is represented by orthogonal matrices U and V, and a diagonal matrix D. The optimal solutions for regularized and unregularized least-squares linear regression are given by specific equations. The unregularized version of Problem 19 involves finding the change-of-basis matrix Q 0 and solving for s 0 to obtain u *. \"Best-response functions\" Q * (\u03bb) and s * (\u03bb) are chosen to meet certain criteria, with specific computations shown for a quadratic function f. The quadratic function f is represented by matrices A, B, C, d, and e. By setting the derivative equal to 0 and using second-order sufficient conditions, we find the optimal solution. The function \u0175 \u03c6 (\u03bb) = U \u03bb + b is defined, and after simplification, we get equation 36. Expectations are simplified using linearity and properties of the Trace operator. The expressions are simplified using linearity of expectation and properties of the Trace operator. Matrix-derivative equalities are used to differentiate f and find the best-response Jacobian. The approximate best-response is obtained as the first-order Taylor series of w* about \u03bb0. Model parameters were updated, but hyperparameters were not. Training was terminated at this point. The model parameters were updated without updating hyperparameters. Training was stopped when the learning rate fell below 0.0003. Variational dropout was tuned for the input, hidden state, and output of the LSTM. Embedding dropout was also tuned to remove certain words from sequences. DropConnect was used to regularize the hidden-to-hidden weight matrix. DropConnect was used to zero out weights in the weight matrix, operating directly on weights rather than activations. A single DropConnect rate was sampled per mini-batch. Activation regularization (AR) penalizes large activations, while temporal activation regularization (TAR) is a slowness regularizer. Scaling coefficients \u03b1 and \u03b2 were tuned for AR and TAR. Hyperparameter ranges for baselines were [0, 0.95] for dropout rates and [0, 4] for \u03b1 and \u03b2. ST-LSTM had all dropout rates and coefficients initialized to 0.05. Additional details on CNN experiments were presented. For CNN experiments, 20% of training data was held out for validation. Baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128. Learning rate decayed by 10 when validation loss didn't decrease for 60 epochs. Training ended if learning rate fell below 10^-5 or validation loss didn't decrease for 75 epochs. Hyperparameter search spaces for baselines were defined for dropout rates. ST-CNN's elementary parameters were trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128. The hyperparameters for the ST-CNN were optimized using Adam with a learning rate of 0.003. The training schedule alternated between the best-response approximation and hyperparameters, following the same schedule as the ST-LSTM. The model parameters had five epochs of warm-up, during which the hyperparameters remained fixed. Cutout length was limited to {0, 24} and the number of cutout holes to {0, 4}. An entropy weight of \u03c4 = 0.001 was used in the entropy regularized objective. The ST-CNN model parameters had a warm-up period with fixed hyperparameters. Entropy weight was set to \u03c4 = 0.001. Cutout length was restricted to {0, 24} and cutout holes to {0, 4}. Dropout rates and data augmentation noise parameters were initialized to 0.05. The model showed robustness to hyperparameter initialization, with low regularization aiding optimization in the early epochs. Curriculum learning (BID2) is related to hyperparameter schedules. Curriculum learning (BID2) is a continuation method that optimizes non-convex functions by solving a sequence of functions ordered by increasing difficulty. Hyperparameter schedules can implement a form of curriculum learning by gradually increasing parameters like dropout over time to make the learning problem more challenging. This approach is hypothesized to aid optimization and improve generalization. Hyperparameter schedules implement a form of curriculum learning by gradually increasing dropout over time to make the learning problem more difficult. Grid searches help understand the effects of different hyperparameter settings, showing that greedy schedules can outperform fixed values. Validation perplexity is measured for various dropout combinations, with smaller values initially performing better and larger rates becoming optimal with more training epochs. At the start of training, small dropout values yield the best validation loss, while larger dropout rates are more effective with more epochs. A grid search for output dropout values showed that a schedule with varying dropout rates outperformed fixed values, leading to better generalization. Using small dropout initially results in a rapid decrease in validation perplexity, while larger dropout later improves overall performance. The schedule achieves a fast decrease in validation perplexity by using small dropout values at the start of training, and better overall validation perplexity by using larger dropout later. PyTorch code listings for constructing ST-LSTMs and ST-CNNs are provided, along with optimization steps for the training and validation sets."
}