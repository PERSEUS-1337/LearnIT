{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. A method using \\textit{M-of-N} rules was proposed to map the complexity/accuracy landscape of rules describing hidden features in a Convolutional Neural Network (CNN). Experiments show an optimal trade-off between comprehensibility and accuracy, with rules in the first and final layers being highly explainable. The experiments in this paper reveal an optimal trade-off between comprehensibility and accuracy in rule extraction from deep networks. Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on the feasibility of explainable Artificial Intelligence and the value of decompositional knowledge extraction. Knowledge extraction aims to enhance the explainability of neural networks by revealing the implicit knowledge learned in their weights. This involves translating trained neural networks into symbolic rules or decision trees, similar to those in symbolic AI and logic programming. Various rule extraction algorithms have been developed over the years to achieve this goal. Rule extraction techniques have been developed over the years to translate trained neural networks into symbolic rules or decision trees, aiming to enhance their explainability by revealing implicit knowledge learned in their weights. The complexity of extracted rules remains a major issue, especially due to the distributed representations found in neural networks. The difficulty in extracting knowledge from neural networks lies in their distributed representations, where important concepts are not represented by single neurons but by patterns of activity across many neurons. This challenges traditional symbolic knowledge extraction methods, leading to suggestions for adopting distillation techniques instead. Distillation is seen as a way to improve robustness, although its effectiveness has been questioned. Other practical approaches focus on guaranteeing network behavior or using visualizations rather than attempting to fully explain the black box. In this paper, a method is developed to examine the explainability of latent variables in neural networks by using rule extraction. The approach involves searching for M-of-N rules describing a latent variable and evaluating the error and complexity of each rule. By analyzing various error/complexity trade-offs, a rule extraction landscape is created to show the relationship between rule complexity and accuracy in capturing network behavior. The method is applied to a 4-layer CNN trained on fashion MNIST, revealing varying levels of accuracy in extracted rules across different layers. The study explores the explainability of latent variables in neural networks through rule extraction. It reveals varying accuracy levels in extracted rules across different layers of a 4-layer CNN trained on fashion MNIST. The discovery of an 'ideal point' on the rule extraction landscape indicates an optimal M-of-N rule for each latent variable, with accuracy depending on the variable being described. Rules extracted from convolutional layers without complexity penalties are more complex compared to those from fully connected layers. The first and final layers yield rules with near 0% error, while the second and third layers struggle to achieve less than 15% error. The study discusses the extraction of rules from neural networks, focusing on varying accuracy levels across different layers of a 4-layer CNN trained on fashion MNIST. Rules with low error rates were found in the first and final layers, while the second and third layers struggled to achieve less than 15% error. The extraction process involved defining accuracy and complexity for M-of-N rules and presenting experimental results before concluding. Previous algorithms for knowledge extraction, such as KBANN, used a decompositional approach to extract symbolic rules from hidden variables. Artificial Neural Networks (KBANN) used hidden variable weights to extract symbolic rules known as M-of-N rules. More recent algorithms select M-of-N rules based on input units as concepts, focusing on maximum information gain with respect to the output. These methods treat the model as a black box for rule extraction without explaining latent variables. Other extraction methods combine pedagogical and decompositional approaches, while some opt for visually oriented techniques. The curr_chunk discusses the use of a black box as an oracle for data generation and various methods for rule extraction from neural networks. Decompositional techniques are used to explain features of deep networks, but the complexity of extracted rules can be challenging for human understanding. The curr_chunk discusses the potential of rule extraction from deep networks, highlighting the challenge of complex rules for human comprehension. It suggests that some layers of a deep network may have explainable rules, offering insight into network behavior and latent feature disentanglement. The curr_chunk explains the formal definition of logical rules in logic programming, where a rule is an implication of the form A \u2190 B. It also discusses how rules can be used to explain neural networks by referring to the states of neurons. The chunk emphasizes that a single conjunctive rule may not fully describe a latent variable in a neural network due to the various input configurations that can activate a neuron. In neural networks, a latent variable is often not fully described by a single conjunctive rule due to the different input configurations that can activate a neuron. M-of-N rules, which require only M variables in the body to be true for some specific value of M < N, offer a more compact and general representation reflecting the input/output dependencies of neurons. M-of-N rules provide a compact and general representation of input/output dependencies in neural networks. They are a subset of propositional formulas and can be viewed as 'weightless perceptrons'. By setting biases and weights accordingly, M-of-N rules can be encoded in neural networks. This paper aims to bring M-of-N rules back into focus for knowledge extraction. This paper discusses the use of M-of-N rules in neural networks for knowledge extraction. The rules involve setting biases and weights to encode input/output dependencies. Continuous activation values require choosing splitting values for defining literals. Information gain is used to select splits for explaining target neurons. The input literals are generated from the inputs to the target neuron by choosing splits for each input. The text discusses using M-of-N rules in neural networks for knowledge extraction, focusing on information gain to select splits for target neurons. Each target literal in a layer has its own set of input literals, maximizing information gain. The goal is to extract rules that balance comprehensibility and accuracy, defining accuracy in terms of a soundness measure. In rule extraction from neural networks, the focus is on maximizing information gain for target neurons by selecting optimal splits. The goal is to extract rules that balance comprehensibility and accuracy, defining accuracy in terms of a soundness measure. The discrepancy between the output of the rules and the network is measured to evaluate performance. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), measured relative to a maximum complexity. The maximum complexity is calculated based on the number of possible input variables. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), measured relative to a maximum complexity. In experiments, complexity is normalized by taking the logarithm of the maximum complexity. An example is given with a perceptron and a specific rule. A loss function for a rule is defined as a weighted sum with a parameter determining the trade-off between soundness and complexity. A brute force search procedure is used to determine the relationship between different values of the parameter. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), measured relative to a maximum complexity. A loss function for a rule is defined as a weighted sum with a parameter determining the trade-off between soundness and complexity. Using a brute force search procedure, the relationship between the allowed complexity of a rule and its maximum accuracy is explicitly determined. Neurons with multiple inputs generate splits for each neuron using a technique to create sets of literals. The search for M-of-N rules minimizes L(R) by reordering variables based on weight magnitude. A search procedure relies on variable ordering to find the best rule. Neurons with n inputs have O(2^n) possible rules, making exhaustive search impractical. The text discusses the challenge of generating M-of-N rules for neurons with multiple inputs. It suggests ordering literals by weight to reduce the search space from exponential to polynomial. The assumption is that accurate rules use literals corresponding to neurons with the strongest weights. This approach is supported by experimental results, despite the computational complexity involved. The algorithm for rule extraction was implemented in Spark and run on IBM cloud services to reduce computational complexity. The accuracy of extracted rules was measured using examples from the training set, with a focus on the network's learned behavior. Running the search in parallel allowed for mapping the accuracy/complexity graph for 50 hidden neurons in a few hours. The network was trained on a better representation of its behavior. Running the search in parallel mapped the accuracy/complexity graph for 50 hidden neurons. Using 1000 examples for accuracy calculation, the extraction process for the first hidden feature in the CNN was demonstrated. The optimal splitting value of each neuron was found by computing information gain, with neuron 96 having the maximum information gain. Neuron 96, with an information gain of 0.015 at split value 0.0004, corresponds to a 5x5 image patch at (3, 12). Input splits are defined based on maximum information gain with respect to variable H. M-of-N rules are extracted to explain H with different error/complexity tradeoffs. Increasing complexity penalty yields three rules visualized in Figure 1. The M-of-N rules are extracted to explain variable H with different error/complexity tradeoffs. Increasing the complexity penalty results in three rules, with the most complex being a 5-of-13 rule with 0.025 error. A mild penalty leads to a simpler 3-of-4 rule with 0.043 error, while a heavy penalty produces a trivial 1-of-1 rule with 0.13 error. The technique of extracting M-of-N rules to explain variable H with different error/complexity tradeoffs is demonstrated on the DNA promoter dataset. Training a feed forward network with a single hidden layer of 100 nodes reveals an exponential relationship between complexity and error in the first layer. In the output layer, the rule 1-of-{H39, H80} achieves 100% fidelity to the network. The rules extracted from the input layer show varying complexity and error tradeoffs, such as a 64-of-119 rule for variable H39 with one incorrect classification. The hidden layer is defined by information gain with respect to the output. Each literal in the 1-of-2 rule can be described with an M-of-N rule from the input layer. Rules like 64-of-119 for variable H39 result in one incorrect classification, while rules like 32-of-61 for variable H80 have no incorrect classifications. Errors propagate through layers when rules don't perfectly approximate each layer. To replace a network with hierarchical rules, a single set of splits for each layer must be chosen by moving down one layer at a time and selecting input splits based on the previous layer. In order to replace a network with hierarchical rules, a single set of splits for each layer must be chosen by moving down one layer at a time and selecting input splits based on the information gain against all configurations of the new output layer. Conducting experiments layer by layer independently provides an idealized complexity/error curve for rule extraction with M-of-N rules. Testing layerwise rule extraction on a basic CNN trained on fashion MNIST in tensorflow revealed the network's architecture and performance. The layerwise rule extraction search was tested on a basic CNN trained on fashion MNIST in tensorflow. The CNN had a standard architecture with convolutional and max pooling layers, followed by a fully connected layer. Rules were extracted and tested on random inputs from the fashion MNIST training data, with limitations on the complexity of rules for efficiency. The output layer was tested with one-hot neurons using the rule searching procedure. The layerwise rule extraction search was conducted on a basic CNN trained on fashion MNIST in tensorflow. The search involved testing 50 randomly chosen features in the third layer, limiting the search to rules with 1000 literals or less. The output layer was tested with 10 one-hot neurons, each undergoing the rule searching procedure. The search procedure was repeated for 5 different values of \u03b2, resulting in 5 sets of extracted rules with varying error/complexity trade-offs. The extracted rules from each layer showed different complexity/error trade-offs, with the first and final layers achieving near-zero error, while the second and third layers had a similar accuracy/complexity trade-off. The complexity/error trade-off for extracted rules from each layer varies. The first and final layers achieved near-zero error, while the second and third layers showed a similar accuracy/complexity trade-off. The optimal trade-off is not solely based on the number of input nodes, as seen in the performance comparison between the second and third layers. There is a critical point where error increases rapidly as complexity penalty rises. The final layer provides more accurate rules with less complexity compared to the first layer. The results show a critical point where error increases rapidly as complexity penalty rises, indicating a 'natural' set of rules for explaining latent features. This paper integrates rule complexity into the extraction algorithm, unlike current methods that do not explicitly consider complexity in optimization. The paper integrates rule complexity into the extraction algorithm, highlighting the importance of empirical evaluation for validation. It discusses the limitations and potential of rule extraction algorithms, showing that while some layers may have complex rules with a 15% error rate, others have simple rules with near 0% error. This suggests that selective use of decompositional algorithms depending on the layer being explained is necessary. The black box problem of neural networks has hindered their integration into society, leading to a growing need for explainability. Knowledge extraction from neural networks has had mixed success, with most large networks remaining difficult to interpret. Critics argue that decompositional rule extraction is unfeasible due to the distributed nature of neural networks. The distributed nature of neural networks makes rule extraction difficult. A novel search method for M-of-N rules was applied to explain latent features of a CNN, showing that explanations can be simplified without reducing accuracy. Rule extraction may not adequately describe all latent variables, but can be a useful tool for network examination. The distributed nature of neural networks makes rule extraction challenging. While rule extraction may not fully describe all latent variables, simplifying explanations without reducing accuracy can be a useful tool for network examination. Further research is needed to explore the impact of different factors on the accuracy and interpretability of rule extraction."
}