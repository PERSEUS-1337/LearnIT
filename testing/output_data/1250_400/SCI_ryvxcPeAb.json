{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, where small perturbations can deceive models and transfer across different systems, enabling attacks on black-box systems. In this work, the adversarial perturbation is decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. A new approach using noise reduced gradient (NRG) to craft adversarial examples enhances transferability significantly, especially for low-capacity models. These findings offer insights for constructing successful adversarial examples and designing defense strategies against black-box attacks in the era of large neural network models applied in real-world applications. Designing effective defense approaches against black-box attacks is crucial as large neural network models are increasingly used in real-world applications like speech recognition and computer vision. Adversarial examples can manipulate inputs to produce incorrect outputs, posing a challenge for understanding and defending against them. The transferability of adversarial examples across different models allows for attacks on black-box systems. Adversarial vulnerability was first studied as an optimization problem solved by box-constraint L-BFGS, highlighting the need for effective defense strategies. The transferability of adversarial examples can be leveraged to attack black-box systems. Various methods like FGSM, DeepFool, and ensemble-based approaches have been proposed for crafting effective adversarial attacks. High-confidence adversarial examples that are strongly misclassified by the original model have stronger transferability. In BID8, ensemble-based approaches were proposed for effective black-box attacks. BID3 showed that high-confidence adversarial examples have stronger transferability. Various defense mechanisms have been introduced, such as defensive distillation by BID12 and adversarial training by BID5. BID9 used image transformation to mitigate adversarial perturbations. Some works focused on detecting adversarial examples, but they can be easily overcome by designing stronger attacks. This work explains the transferability of adversarial examples and enhances black-box attacks by decomposing adversarial perturbations into two components. In this work, the transferability of adversarial examples is explained, and black-box attacks are enhanced by decomposing adversarial perturbations into model-specific and data-dependent components. The data-dependent component mainly contributes to transferability across different models. A noise-reduced gradient (NRG) method is proposed to construct adversarial examples, showing promising results on the ImageNet validation set. The proposed noise-reduced gradient (NRG) method enhances black-box attacks by utilizing the data-dependent component of the gradient. It significantly increases the success rate of attacks on the ImageNet validation set. Model-specific factors like capacity and accuracy influence the success rate, with higher accuracy and lower capacity models showing stronger attack capabilities. This phenomenon is explained by transferability and provides guidance for attacking unseen models. In this work, the focus is on studying adversarial examples in deep neural networks, where high dimensionality makes the model vulnerable to adversarial perturbations. These perturbations are small and imperceptible changes to input data, resulting in misclassification by the model. Different types of attacks, such as non-targeted and targeted attacks, are discussed, with the latter aiming to produce a specific wrong label specified by the adversary. In a black-box attack setting, the adversary has no knowledge of the target model and cannot query it directly. Instead, they create adversarial examples on a local model trained on a similar dataset and use these to fool the target model. This type of attack is known as a black-box attack, as opposed to a white-box attack where the target is the source model itself. Crafting adversarial perturbations involves optimizing a loss function to measure the discrepancy between prediction and ground truth. Crafting adversarial perturbations involves optimizing a loss function to measure the discrepancy between prediction and ground truth. BID2 introduced a loss function that manipulates the output logit directly, adopted by many works. Ensemble-based approaches suggest using a large ensemble of source models to improve the strength of adversarial examples. The measurement of distortion commonly uses \u221e and 2 norms, with human eyes being the ideal metric, although difficult to quantify. Ensemble-based approaches suggest using a large ensemble of source models to improve the strength of adversarial examples. The objective for non-targeted attacks involves ensemble weights and normalized-gradient based optimizers. The Fast Gradient Based Method aims to solve attacks with one-step iterations, showing good transferability despite not being optimal. The Fast Gradient Based Method (FGBM) involves one-step iterations using normalized gradient vectors for attacks. It is shown to be fast and has good transferability. The Iterative Gradient Method performs normalized-gradient ascent for k steps, with different gradient choices for different attacks. Transferability between models is crucial for black-box attacks and defenses. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Research suggests that transferability comes from the similarity between decision boundaries of source and target models, especially in the direction of transferable adversarial examples. Models with high performance on the same dataset learn a similar function on the data manifold, but their behavior off the data manifold can differ due to architecture and random initializations. This hints at decomposing perturbations into on and off data factors. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Models with high performance on the same dataset learn a similar function on the data manifold, but their behavior off the data manifold can differ due to architecture and random initializations. Perturbations can be decomposed into data-dependent and model-specific components, with the former contributing more to transferability between models. The adversarial perturbation crafted from model A can mislead both model A and B by attacking the inter-class area. Decomposing the perturbation into data-dependent and model-specific components reveals that enhancing the data-dependent component increases success rates of black-box attacks. The NRG method reduces model-specific noise to achieve this. The NRG method aims to enhance the data-dependent component of adversarial attacks by reducing model-specific noise. By applying local averaging to the noisy model-specific information, the noise-reduced gradient (NRG) captures more data-dependent information than the ordinary gradient \u2207f. Visualizations show that larger sample sizes lead to smoother and more data-dependent gradients, with m = 100 capturing semantic information effectively. The noise-reduced gradient (NRG) method enhances adversarial attacks by reducing model-specific noise, capturing more data-dependent information than the ordinary gradient \u2207f. Visualizations demonstrate that larger sample sizes result in smoother and more data-dependent gradients, with m = 100 effectively capturing semantic information. The noise-reduced iterative sign gradient method (nr-IGSM) is proposed, utilizing NRG to drive the optimizer towards more data-dependent solutions. The noise-reduced gradient (NRG) method, known as nr-FGSM for k = 1, \u03b1 = \u03b5, enhances adversarial attacks by reducing model-specific noise. The noise-reduced version is used for q-attack, and a general optimizer can have a noise-reduced counterpart. The effectiveness of NRG is analyzed using classification models trained on the ImageNet dataset. The dataset used is the ImageNet ILSVRC2012 validation set with 50,000 samples. Pre-trained models provided by PyTorch are used for the experiments. For targeted attack experiments, various pre-trained models from PyTorch are utilized. The models include resnet, vgg, densenet, alexnet, and squeezenet. The performance of targeted attacks is evaluated based on Top-1 and Top-5 success rates. The cross entropy is used as the loss function, and distortion is measured using \u221e norm and scaled 2 norm. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with fast gradient based methods like FGSM and IGSM. Results show that nr-FGSM consistently outperforms original FGSM in both blackbox and white-box attacks, indicating the superiority of noise-reduced gradient for enhancing transferability. The noise-reduced gradient technique, when combined with fast gradient methods like FGSM and IGSM, shows superior performance in both blackbox and white-box attacks. Results indicate that nr-FGSM outperforms original FGSM consistently, highlighting the effectiveness of noise-reduced gradients for enhancing transferability. Additionally, nr-IGSM generates adversarial examples that transfer more easily than IGSM, suggesting that noise-reduced gradients guide the optimizer towards more data-dependent solutions. Large models, such as resnet152, are found to be more robust to adversarial transfer compared to smaller models. The noise-reduced gradient (NRG) guides the optimizer towards data-dependent solutions, with large models like resnet152 being more robust to adversarial transfer. Observations show that model-specific components contribute to transfer across models with similar architectures. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet. This contradicts previous claims, with higher confidence adversarial examples more likely to transfer to the target model. Inappropriate hyperparameter choices may lead to underfitting in the source model. The inappropriate choice of hyperparameters, such as \u03b1 = 1 and k = min(\u03b5 + 4, 1.24\u03b5) in BID6, leads to underfitting in the source model. When attacking the alexnet as a target model, IGSM overfits more than FGSM, resulting in a lower fooling rate. The noise reduced gradient technique removes model-specific information from gradients, leading to better cross-model generalization. This approach is applied to ensemble-based methods for improved performance. The NRG method regularizes the optimizer by removing model-specific information from gradients, improving cross-model generalization. 1,000 images are used for evaluation due to computational costs. Non-targeted attacks using FGSM and IGSM are tested, with IGSM showing saturated success rates. For targeted attacks, generating adversarial examples with specific labels is challenging. Single-model approaches are ineffective for targeted attacks. Targeted adversarial examples are sensitive to the step size. Targeted adversarial examples are challenging to generate due to the need for a large step size in optimization procedures. The NRG method outperforms normal methods significantly in both targeted and non-targeted attacks, as shown in Table 3. In targeted and non-targeted attacks, NRG methods outperform normal methods by a large margin. Sensitivity of hyper parameters m and \u03c3 is explored for black-box attacks using the nr-FGSM approach. Larger m leads to higher fooling rates, while an optimal value of \u03c3 is crucial for best performance. In this experiment, the optimal value of \u03c3 varies for different source models, being about 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is explored, which is crucial for real-world applications. Destruction rate is used to quantify the influence of transformations on adversarial examples surviving in the physical world. The study explores the influence of image transformations on adversarial examples' survival in the physical world. Densenet121 and resnet34 models are used, with four transformations considered: rotation, Gaussian noise, Gaussian blur, and JPEG compression. Results show that NRG-based methods generate more robust adversarial examples compared to vanilla methods. Decision boundaries of different models are analyzed to understand the superior performance of NRG-based methods. In this section, decision boundaries of different models are studied to understand the performance of NRG-based methods. Resnet34 is the source model, and nine target models are considered. The direction of sign \u2207f is found to be as sensitive as sign (\u2207f \u22a5 ) for resnet34, but other target models are more sensitive along sign \u2207f. This aligns with previous arguments. The direction of sign \u2207f is sensitive for the source model resnet34, but other target models are more sensitive along sign \u2207f. Removing \u2207f \u22a5 penalizes the optimizer along the model-specific direction, preventing overfitting. Big models have larger distances for adversarial transfer, making them more robust. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well. In experiments, it was observed that different models exhibit varying performances in attacking the same target model. The study aims to understand why big models are more robust than small models. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well. The research focuses on finding the principle behind this phenomenon to guide the selection of a better local model for generating adversarial examples. The study compares the attack capabilities of different models like vgg19 bn and resnet152 using FGSM and IGSM attacks. Results show that models with smaller test errors and lower capacities have stronger attack capabilities. This is attributed to the transferability of adversarial examples. In this study, it is shown that models with lower capacity and higher test accuracy are more effective in black-box attacks. Adversarial perturbations consist of model-specific and data-dependent components, with the latter contributing more to transferability. The proposed noise-reduced gradient (NRG) based methods for crafting adversarial examples are shown to be more effective than previous methods. The noise-reduced gradient (NRG) based methods are proposed for crafting adversarial examples, which are more effective than previous methods. Models with lower capacity and higher test accuracy are better for black-box attacks. Future research will focus on combining NRG-based methods with adversarial training to defend against black-box attacks and learning stable features for transfer learning. The success rates of targeted black-box attacks using IGSM depend on hyperparameters like the number of iterations and step size. Incorporating the NRG strategy for transfer learning can reduce model-specific noise and improve accuracy. The success rates of targeted black-box attacks using IGSM depend on hyperparameters like the number of iterations and step size. Optimal step size is crucial, with both too large and too small sizes harming attack performance. Small step sizes with a large number of iterations may lead to worse performance due to overfitting. The experiment on MNIST dataset confirms that model redundancy influences attack capability. Low-capacity models have stronger attack capability than large-capacity models. The results show that more iterations with a large step size prevent overfitting and encourage exploration of model-independent areas, leading to better performance in attacks. The experiment on MNIST dataset confirms that model redundancy influences attack capability. Low-capacity models have stronger attack capability than large-capacity models. Attacks are reported in TAB5, showing that the low-capacity model has much stronger attack capability than large-capacity. Top-1 success rates of FGSM and IGSM attacks against resnet152 for various models are shown in Figure 9."
}