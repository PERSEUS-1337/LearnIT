{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a crucial aspect of intelligence, aiding in flexibility and creativity. Neuromorphic computing chips, utilizing analog circuits, mimic the brain but current neural networks do not consider their uncertainty. A proposed uncertainty adaptation training scheme improves neural network performance on uncertain chips, aligning with results on original platforms. Fuzziness and stochasticity are two types of uncertainties in intelligent systems, aiding in human thinking processes. The essence of uncertainty reasoning in human thinking activities is crucial for intelligence. Fuzziness and stochasticity are two types of uncertainties in intelligent systems, aiding in efficient processing of information and enabling creativity. While most AI systems lack these characteristics, utilizing 8-bit integers instead of 32-bit or 64-bit floating numbers for weights and activations has shown promise in various applications. The 8-bit integer has shown promise in various applications, addressing issues in AI systems. Methods like network quantization and Bayesian network are used. Neuromorphic computing chips supplement uncertainty in DNN. Nanotechnology devices and crossbar structure chips have developed significantly. The crossbar structure is efficient for VMM with Ohms law and Kirchhoffs law. Nonvolatile memory devices provide additional storage capability. Neuromorphic computing chips with crossbar structure and nanoscale nonvolatile memory devices have become more efficient for AI applications. The computing in memory architecture relieves memory bottleneck and improves energy and area efficiency. Uncertainty is an important feature of these chips that is not fully utilized. The uncertainty in neuromorphic computing chips arises from fuzziness caused by analog to digital converters (ADCs) and stochasticity induced by NVM devices. The stochasticity of NVM devices is due to random particle movement, leading to varied conductance and different output currents even with the same voltage applied. The stochasticity of NVM devices is caused by random particle movement, resulting in varied conductance and different output currents. A training scheme is proposed to utilize this stochasticity to enhance the performance of neuromorphic computing chips. Various types of NVM devices exist, each with different intrinsic physical mechanisms leading to varying levels of stochasticity. The Gaussian distribution is used to model device stochasticity, although it may not perfectly fit all types of devices. The stochasticity of NVM devices is modeled using a Gaussian distribution, with the mean representing the conductance value of the stable state. The variance is typically correlated to the mean, and the standard deviation is assumed to be linearly related to the mean. Conductances below a minimum value are cut off, ensuring all devices reach a minimum conductance level. The conductance of NVM devices is modeled using a Gaussian distribution, with a linear correlation between standard deviation and mean. Conductances below a minimum value are cut off to ensure all devices reach a minimum level. Writing the conductance of each device is essential for neuromorphic computing applications, involving mapping weights to device conductance levels. Mapping weights to device conductance levels is crucial for using neuromorphic computing chips in AI applications. The mapping process involves scaling weights into the working range of device conductance and expressing weights as the difference between two device conductances. To achieve higher energy efficiency, lower conductances are preferred. However, accuracy in writing conductance is hindered by device stochasticity and circuit fuzziness. The mapping algorithm aims to utilize the entire conductance working range, but variations in conductance manipulation and inaccurate measurements pose challenges. Conductance values are determined by dividing read current by applied voltage, which is influenced by device stochasticity. The conductance value in neuromorphic computing chips is affected by device stochasticity and circuit fuzziness, leading to inaccurate measurements. A model using Gaussian distribution is proposed to describe the fuzziness, with a constant \u03b2 representing the level of device fuzziness. Uncertainty can impact the performance of a well-trained DNN programmed into the chip, decreasing classification accuracy. The uncertainty in neuromorphic computing chips, influenced by device stochasticity and circuit fuzziness, can decrease the accuracy of a well-trained DNN. However, using the uncertainty adaptation training scheme (UATS) can alleviate this issue and even improve accuracy by guiding neural networks to handle uncertainty during training. The stochasticity model introduces randomness in the feed forward process, using a sample of random variable w s instead of weight w. This approach helps neural networks learn to deal with uncertainty effectively. The uncertainty adaptation training scheme (UATS) introduces randomness in the feed forward process by using a sample of random variable w s instead of weight w. This approach helps neural networks learn to handle uncertainty effectively by replacing weights with random variables after every k epochs of training. Additionally, the loss function is calculated based on the average output of n FF processes with the same input batch, improving network performance evaluation under uncertainty. The uncertainty adaptation training scheme (UATS) introduces randomness in the feed forward process by using random variables instead of weights. The loss function is calculated based on the average output of n FF processes with the same input batch. Multiple models and datasets were evaluated, including the MNIST dataset with different models like MLP and CNN. Training was done with varying levels of uncertainty using different models. The experiment used different uncertainty levels to test MLP and CNN models. Without UATS, uncertainty increased test errors for both models. LeNet-5 had the best performance but was most affected by uncertainty. UATS was then used to tune weights and retrain models, showing improved performance. The study validated the effectiveness of UATS in improving model accuracies and reducing uncertainty levels in both retraining and fine-tuning experiments. Results showed that UATS outperformed traditional methods, even achieving lower error rates with proper hyperparameters. Additionally, UATS was successfully applied to a more complex DNN model on the CIFAR-10 dataset, further demonstrating its power in enhancing model performance. The study demonstrated that UATS improved model accuracies and reduced uncertainty levels in experiments with a more complex DNN model. Results showed that UATS outperformed traditional methods, achieving lower error rates with proper hyperparameters. Additionally, UATS was effective in enhancing model performance on the CIFAR-10 dataset. The study explored different distributions to model device stochasticity, including Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli distributions. Despite significant differences in device behavior, network performance using each distribution with the same mean and variance was similar. The computation intensity of UATS may be high due to the need for a large number of random numbers, but methods exist to reduce this requirement. The computation intensity of UATS may be high due to the requirement of a large number of random numbers. Methods to reduce this requirement include sampling weights for inputs or batches instead of every VMM, and using the uncertainty model of VMM results instead of weights, which can accelerate simulation speed while achieving similar results."
}