{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks. This paper discusses novel observations about dropout in DNNs with ReLU activations, leading to the proposed method \"Jumpout\" for better training of local linear models. Jumpout is a new method that samples the dropout rate using a decreasing distribution, trains local linear models at each data point, and normalizes the dropout rate adaptively. It rescales outputs for a better trade-off between variance and mean of neurons, improving performance on various datasets without significant additional costs. Dropout is a technique used in deep learning to prevent overfitting by randomly setting hidden neuron activations to 0. However, it has drawbacks such as the need to tune dropout rates for optimal performance. Dropout rates should ideally be tuned separately for each layer and during different training stages. In deep learning, dropout is used to prevent overfitting by randomly deactivating hidden neurons. Dropout rates need to be tuned for optimal performance, ideally separately for each layer and during different training stages. However, in practice, a single dropout rate is often used for all layers to reduce computation. This fixed rate may limit the generalization of the model to noisy samples with a specific expected amount of perturbation, potentially hindering improvements in generalization. Dropout in deep learning is used to prevent overfitting by randomly deactivating hidden neurons. However, using a constant dropout rate for all layers may limit generalization to noisy samples. Dropout is also incompatible with batch normalization, which is essential for stable training and convergence in modern DNN architectures. This incompatibility often leads to dropout being dropped in favor of batch normalization. The proposed \"jumpout\" is a modified version of dropout aimed at overcoming its drawbacks, especially when used with batch normalization in modern DNN architectures. The approach is motivated by observations on how dropout improves generalization performance for DNNs with ReLU activations. Applying dropout randomly changes the ReLU activation patterns, altering the underlying polyhedral structure and linear models, leading to improved training. DNN with ReLU uses piecewise linear functions with different linear models for data points in various polyhedra defined by the activation patterns. Dropout randomly changes activation patterns, affecting polyhedral structure and linear models, improving generalization. However, a fixed dropout rate results in a typical number of units dropped out, smoothing linear models over typical distances. This may hinder achieving local optimization goals. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, ensuring a higher probability of smaller dropout rates. This leads to smoother polyhedra at closer distances and a decrease in smoothing as points move farther away. Additionally, the fraction of activated neurons can vary across layers and training stages, affecting the effective dropout rate. In jumpout, the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent neural deactivation rates. This addresses the incompatibility issue between dropout and BN by rescaling outputs to maintain variance. This allows for the benefits of both dropout and BN in training a DNN without inconsistency. Jumpout, a new approach to address the fixed dropout rate problem, randomly generates a 0/1 mask over hidden neurons similar to dropout. It shows almost the same memory and computation costs as dropout but consistently outperforms it on various tasks. Other methods like \"standout\" and BID24 have also been proposed to adaptively change dropout rates for different layers and training stages. Jumpout is a new approach that adjusts dropout rates based on ReLU activation patterns, without relying on additional trained models. It introduces minimal computation and memory overhead and can easily be integrated into existing model architectures. Additionally, BID19 proposed Gaussian dropout as a faster convergence optimization method. In addition to Gaussian dropout, recent variants of dropout include Swapout and Fraternal Dropout, which aim to improve neural network architectures. Variational dropout, introduced by BID9, connects global uncertainty with adaptive dropout rates for each neuron. BID11 extended variational dropout to reduce gradient estimator variance and achieve sparse dropout rates. These modifications to dropout do not require extra training or optimization. Jumpout is a new variant of dropout that aims to improve neural network architectures without requiring extra training or optimization costs. It can be applied along with other dropout variants and targets different problems of dropout. The formalization of a feed-forward deep neural network is discussed, focusing on the network's output prediction and hidden nodes. The formalization of a feed-forward deep neural network discusses the network's output prediction and hidden nodes, including the representation of DNN architectures used in practice. It covers fully-connected networks, bias terms, convolution operators, sparse weight matrices, pooling operations, and activation functions. The weight matrix resulting from applying a convolutional filter is sparse with tied parameters. Average-pooling is a linear operator represented as matrix multiplication, while max-pooling acts as an activation function. A residual network block can be represented by appending an identity matrix to retain input values. For ReLU activation functions, the DNN can be written as a piecewise linear function. The linear model in a given data point x is formed by combining activation patterns with weight matrices. ReLU activation sets units to 0 or preserves values, leading to a linear model without ReLU functions. The gradient \u2202x represents the weight vector of the linear model associated with activation patterns on all layers. This analysis focuses on DNNs with ReLU activations. The linear model in Eqn. 2 is associated with activation patterns on all layers for a data input x, defining a convex polyhedron. The focus is on DNNs with ReLU activations for their computational efficiency and performance. Dropout improves generalization by considering local linear models and their nearby convex polyhedra. Three modifications to dropout lead to Jumpout, enhancing DNN performance by preventing neuron co-adaptation. The text discusses three modifications to dropout, explaining how it improves DNN performance by preventing neuron co-adaptation and promoting diversity. Dropout creates smaller networks during training and acts as an ensemble during testing, reducing variance. The text also explores how dropout smooths local linear models in DNNs with ReLU activations. Dropout improves generalization performance in DNNs by smoothing local linear models. The input space is divided into convex polyhedra, where each data point behaves as a linear model. Training samples are dispersed among polyhedra, leading to distinct local linear models for each data point. Nearby polyhedra may result in significantly different linear models due to activation patterns and weight matrices. Dropout in DNNs involves consecutively multiplying weight matrices with activation patterns. Differences in activation patterns can lead to varied linear models, affecting generalization ability. To address issues with dropout, a dropout rate is sampled from a truncated half-normal distribution to ensure smoothness and stability in model performance. To improve generalization performance in DNNs, a Gaussian-based dropout rate distribution is utilized. The dropout rate is sampled from a Gaussian distribution with mean zero and truncated to ensure it falls within specified limits. This approach encourages smoother generalization by assigning higher probabilities to smaller dropout rates, leading to better contributions to local linear models. The standard deviation parameter controls the amount of generalization enforcement. The Gaussian-based dropout rate distribution encourages smooth generalization performance in DNNs by assigning higher probabilities to smaller dropout rates. This promotes contributions to local linear models of closer polyhedra, improving network performance. Tuning dropout rates separately for each layer is ideal but computationally expensive, leading to the common practice of using a single global dropout rate, which may not be optimal due to varying proportions of active neurons in different layers. To optimize dropout rates in deep neural networks, it is important to consider the varying proportions of active neurons in different layers. Instead of using a single global dropout rate, normalizing the dropout rate by the fraction of active neurons in each layer can help achieve a more consistent activation pattern and improve smoothing encouragement during training. To better control dropout behavior across different layers and training stages, the dropout rate is normalized by q + j, resulting in a more consistent activation pattern. This allows for precise tuning of the dropout rate as a single hyper-parameter. The compatibility issue between dropout and batch normalization arises from the variance difference between training and test phases, impacting the predictability of deep neural networks. The variance difference between training and test phases can lead to unpredictable behavior in deep neural networks when using batch normalization layers. One approach to address this is by combining dropout layers with batch normalization layers, where the dropout rate is normalized by q + j to control behavior across different layers and training stages. This allows for precise tuning of the dropout rate as a single hyper-parameter. The text discusses the impact of dropout layers on batch normalization in deep neural networks. It explains how dropout affects the mean and variance of neurons during training, leading to inconsistencies between training and testing phases. To address this, the output of neurons should be rescaled to counteract dropout's effects on mean and variance scales. Rescaling factors are suggested to recover the original scales of the mean and variance. The text discusses rescaling factors to recover the original scales of mean and variance in neurons affected by dropout layers during training. Rescaling factors are suggested to address inconsistencies between training and testing phases caused by dropout's impact on mean and variance scales. The rescaling factors (1 \u2212 p) \u22120.75, (1 \u2212 p) \u22121, and (1 \u2212 p) \u22120.5 are proposed to address the variance shift issue caused by dropout in neural networks. The rescaling factor (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling. Depending on the magnitude of the mean E(y), different rescaling factors are recommended to maintain the variance. In practice, using (1 \u2212 p) \u22120.75 is suggested as a trade-off point between (1 \u2212 p) \u22121 and (1 \u2212 p) \u22120.5. The rescaling factor (1 \u2212 p) \u22120.75 is proposed as a trade-off point between (1 \u2212 p) \u22121 and (1 \u2212 p) \u22120.5 to address the variance shift issue caused by dropout in neural networks. It shows a good balance between mean and variance rescaling, making both consistent for cases with and without dropout. Using dropout with batch normalization can potentially improve performance, with larger dropout leading to more improvement. The proposed \"Jumpout\" layer for DNN with ReLU combines three modifications to improve dropout performance. It samples from a decreasing distribution for random dropout rates, leading to better performance compared to original dropout with increasing rates. Jumpout is a modification to the original dropout technique for DNNs with ReLU activation. It samples from a decreasing distribution for random dropout rates, adapts the dropout rate based on the number of active neurons, and scales the outputs differently during training. It requires a main hyper-parameter \u03c3 and two auxiliary truncation hyperparameters (p min , p max ) to control the dropout rate distribution. Jumpout has three hyperparameters: the main hyper-parameter \u03c3 controls the standard deviation of the half-normal distribution, while the auxiliary truncation hyperparameters (p min , p max ) bound the samples. Setting p min = 0.01 and p max = 0.6 has shown consistent performance across datasets and models. The input h j is considered as the features of layer j for one data point, and the estimation of q + j can be done separately for each data point or averaged over the mini-batch. Utilizing the latter option provides comparable performance with less computation and memory usage. Jumpout has a similar memory cost to original dropout, with minimal computation required for counting active neurons and sampling from the distribution. Jumpout, a dropout alternative, offers comparable performance with less computation and memory usage. It is applied to various DNN architectures on benchmark datasets, showing promising results. Jumpout, a dropout alternative, outperforms dropout on various DNN architectures across different datasets. The experiments include ResNet-20 on Fashion-MNIST, WideResNet-16-8 on SVHN and STL10, and ResNet-18 on ImageNet. Starting from a pre-trained model on ImageNet, jumpout consistently shows better performance compared to dropout. Jumpout, a dropout alternative, consistently outperforms dropout on various DNN architectures and datasets. Experimental results show that jumpout achieves significant improvements, even on datasets with high test accuracy. A thorough ablation study confirms the effectiveness of each modification, with combining all three modifications (jumpout) resulting in the best performance. The ablation study confirms the effectiveness of each modification, with combining all three resulting in the best performance. Jumpout shows advantages over dropout in early learning stages and reaches good accuracy faster. Further improvements are seen when adding modifications together. The study compares Jumpout and dropout techniques, showing Jumpout reaches final performance earlier. A rescaling factor is applied to y with dropout. The network used is \"CIFAR10(s)\". Plots demonstrate the mean and variance ratios with dropout. (1 \u2212 p) \u22120.75 provides a good balance between mean and variance rescaling."
}