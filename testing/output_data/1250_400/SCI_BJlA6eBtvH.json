{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a major challenge for neural networks in dynamic data environments. A Differentiable Hebbian Consolidation model is proposed to address this issue by adding a rapid learning component to fixed parameters, allowing retention of learned representations for longer periods. This approach is evaluated on various benchmarks, including imbalanced Permuted MNIST dataset. Our proposed model integrates task-specific synaptic consolidation methods to reduce forgetting in dynamic environments. It outperforms comparable baselines on various benchmarks, including an imbalanced variant of Permuted MNIST. Human intelligence's ability to adapt in dynamic environments is challenging to embed into artificial intelligence. Machine learning models face non-stationarity during deployment, leading to performance degradation when exposed to distributional changes. In real-world applications, machine learning models face non-stationarity where data distributions change over time, leading to performance degradation known as catastrophic forgetting. Continual learning aims to adapt to new tasks without forgetting previous ones, enabling scalable and efficient models over long timescales. This poses a challenge for deep neural networks that require independent and identically distributed samples from a stationary training distribution. Continual learning in machine learning systems faces challenges such as concept drift, imbalanced class distributions, and the stability-plasticity dilemma. This dilemma requires a balance between integrating new knowledge (plasticity) and preserving existing knowledge (stability). Synaptic plasticity in biological neural networks is crucial for learning and memory. Continual learning in machine learning systems involves balancing plasticity (integrating new knowledge) and stability (preserving existing knowledge). Synaptic plasticity in biological neural networks is essential for learning and memory. Two major theories for continual learning include synaptic consolidation to preserve important synaptic parameters and the complementary learning system theory for storing high-level structural information. Recent work on differentiable plasticity has shown that neural networks can be trained with \"fast weights\" that change quickly based on input representations, allowing for reactivation of long-term memory traces. This approach leverages Hebbian learning rules and stochastic gradient descent to optimize both \"slow weights\" for long-term memory and the amount of plasticity in synaptic connections. Differentiable Hebbian Consolidation 1 model extends work on differentiable plasticity to task-incremental continual learning. It adapts quickly to changing environments and consolidates previous knowledge by adjusting the plasticity of synapses. This model modifies the traditional softmax layer and augments the slow weights for improved performance. The Differentiable Hebbian Consolidation 1 model adapts to changing environments and consolidates previous knowledge by adjusting synapse plasticity. It modifies the softmax layer by adding plastic weights for rapid adaptation and memory consolidation. The model combines task-specific synaptic consolidation approaches to overcome catastrophic forgetting. The model unifies core concepts from Hebbian plasticity, synaptic consolidation, and CLS theory to enable rapid adaptation to new data while leveraging compressed episodic memories. It is tested on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. The Imbalanced Permuted MNIST problem is introduced, showing that plastic networks with task-specific synaptic consolidation outperform those with uniform plasticity. Hebbian learning is a major theory explaining continuous learning in humans, attributing learning and memory to weight plasticity and activity-dependent synaptic plasticity. Hebbian learning theory suggests that learning and memory are due to synaptic plasticity, with correlated activation strengthening connections between neurons. Recent meta-learning approaches incorporate fast weights for one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed using non-trainable Hebbian learning-based associative memory, while Rae et al. (2018) introduced a Hebbian Softmax layer. Munkhdalai & Trischler (2018) and Rae et al. (2018) introduced models with fast weights for one-shot and few-shot learning. Munkhdalai & Trischler used non-trainable Hebbian learning-based associative memory, while Rae et al. proposed a Hebbian Softmax layer. Miconi et al. (2018) suggested differentiable plasticity for optimizing synaptic connections in neural networks, mainly demonstrated on RNNs for pattern memorization tasks and maze exploration. These approaches were focused on meta-learning and not continual learning challenges. Our work introduces a method for training neural networks using fast weights in the FC layer with DHP, updating only the softmax output layer parameters for fast learning and knowledge preservation. To overcome catastrophic forgetting, we employ Task-specific Synaptic Consolidation and CLS Theory, creating a dual memory system where the neocortex learns structured representations while the hippocampus stores new instances rapidly. The CLS Theory involves a dual memory system where the neocortex learns structured representations while the hippocampus stores new instances rapidly. Regularization strategies inspired by task-specific synaptic consolidation help prevent catastrophic forgetting by estimating the importance of each synapse and adjusting plasticity dynamically during learning tasks. Regularization strategies like Elastic Weight Consolidation (EWC) and online methods adjust plasticity dynamically to prevent forgetting of important parameters learned from previous tasks. EWC computes importance using Fisher information matrix values, while online variants improve scalability by controlling computational costs. Our work draws inspiration from CLS theory, a computational framework for representing memories with a dual memory system. Various approaches based on CLS principles involve pseudo-rehearsal, exact or episodic replay, and generative methods. Our work is focused on neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting. Previous research has shown how synaptic connections can have fixed weights for long-term knowledge storage and fast-changing weights for temporary memory. Neuroplasticity techniques inspired by CLS theory aim to alleviate catastrophic forgetting by utilizing slow and fast weights in synaptic connections. Recent research includes replacing soft attention mechanisms with fast weights in RNNs, Hebbian Softmax layer, augmenting slow weights with fast weights matrix, differentiable plasticity, and neuromodulated differentiable plasticity. These methods are designed for rapid learning on simple tasks. The goal of the study is to evaluate differentiable plasticity for continual learning, without comparing it to neuroplasticity-inspired CLS methods designed for meta-learning. These methods focus on rapid learning on simple tasks or meta-learning over various tasks, with the Hebbian Softmax layer adjusting parameters based on an annealing schedule. However, in continual learning setups, the fast weights memory storage may become ineffective when the network learns from a large number of examples per class on each task. The study aims to metalearn a local learning rule for fast weights in continual learning scenarios. In continual learning setups, the fast weights memory storage may become ineffective as the network learns from a large number of examples per class on each task. The goal is to metalearn a local learning rule for the fast weights via the fixed (slow) weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component with a scaling parameter \u03b1 and Hebbian traces accumulating mean hidden activations. In continual learning setups, the network learns from a large number of examples per class on each task. The Hebbian traces accumulate mean hidden activations of the final hidden layer for each target label in the mini-batch. The post-synaptic activations of neurons are computed using a softmax function to obtain predicted probabilities. The \u03b7 parameter dynamically learns how quickly to acquire new experiences into the plastic component. The network parameters are optimized by gradient descent as the model is trained sequentially on different tasks. The model in continual learning setups optimizes network parameters through gradient descent as it learns tasks sequentially. It updates Hebbian traces during training and uses them for predictions at test time. Hidden activations accumulate directly into the softmax output layer weights for better performance. During training, Hebbian traces are updated to learn tasks continually, while at test time, the most recent Hebb traces are used for predictions. The model accumulates hidden activations into softmax output layer weights for improved initial representations and long-term retention. Fast learning with a plastic weight component enhances test accuracy, with selective consolidation into a stable component to protect old memories. DHP Softmax simplifies implementation without requiring additional space or computation, allowing scalability with increasing tasks. The DHP Softmax model enables learning to remember by modeling plasticity over various timescales to form a neural memory. It is simple to implement, scales easily with tasks, and quickly stores memory traces for recent experiences without interference. The Hebbian update visualizes the accumulation of hidden activations into compressed episodic memory traces. The DHP Softmax model facilitates learning by storing memory traces for recent experiences without interference. It accumulates hidden activations into compressed episodic memory traces, improving learning of rare classes and binding class labels to deep representations. Hebbian Synaptic Consolidation regularizes the loss and updates synaptic importance parameters online. Incorporating Hebbian Synaptic Consolidation, the model updates synaptic importance parameters online to regularize the loss and improve learning without interference. Task-specific consolidation approaches are adapted, focusing on slow weights of the network to prevent catastrophic forgetting of consolidated classes. The plastic component of the softmax layer allows for optimization of connection plasticity, reducing interference during gradient descent. Our model incorporates a plastic component in the softmax layer to prevent catastrophic forgetting of consolidated classes. It is compared to other methods like Online EWC, SI, and MAS on various benchmarks. The model's capacity is increased by adding plastic weights, and an extra set of slow weights is added to ensure a fair evaluation. The model is tested on Permuted MNIST, Split MNIST, Vision Datasets Mixture benchmarks, and introduces the Imbalanced Permuted MNIST problem. Our model, with plastic weights to prevent forgetting, was tested on various benchmarks including Permuted MNIST and Split MNIST. Evaluation was based on classification accuracy for all tasks trained so far, measuring memory retention and flexibility. Forgetting was assessed using the backward transfer metric, with positive values indicating improvement on previous tasks. The model was compared to Online EWC, SI, and MAS methods for task-specific consolidation. In a benchmark testing different consolidation methods, neural networks were trained sequentially on tasks with varying input distributions, causing concept drift. The model used a multi-layered perceptron with specific hyperparameters and a fixed random permutation of MNIST pixels. Plastic components were not regularized, and the model was compared to Online EWC, SI, and MAS methods. The study focused on concept drift in sequential task learning using a multi-layered perceptron network with specific hyperparameters. The network with DHP Softmax showed improved performance in alleviating catastrophic forgetting compared to a baseline network. Task-specific consolidation methods further enhanced performance, maintaining higher test accuracy throughout sequential training. The study compared the performance of a network with DHP Softmax to a baseline network using task-specific consolidation methods. Results showed that DHP Softmax maintained higher test accuracy during sequential task learning. An ablation study examined structural parameters and Hebb traces, showing that synaptic plasticity increased initially to acquire new information but then decayed to prevent interference between learned representations. The study showed that synaptic plasticity increases initially to acquire new information but then decays to prevent interference between learned representations. The network leverages the structure in the plastic component, with gradient descent and backpropagation used for meta-learning. An Imbalanced Permuted MNIST problem was introduced to address class imbalance and concept drift in predictive performance. The Imbalanced Permuted MNIST problem introduces imbalanced class distributions for each task, aiming to address class imbalance and concept drift in predictive performance. DHP Softmax achieves 80.85% accuracy after learning 10 tasks sequentially, showing a significant 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer, resulting in improved performance. The DHP Softmax with MAS achieves a 0.04 decrease in BWT, with an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The MNIST dataset was split into 5 binary classification tasks, each with disjoint output spaces. Using an MLP network with two hidden layers, DHP Softmax alone achieves 98.23% test accuracy, a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation further enhances performance. In previous experiments, different values of \u03b7 had similar final test performance after learning 5 tasks. DHP Softmax alone achieved 98.23% test accuracy, a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and improved average test accuracy across all tasks. The continual learning was performed on 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 using a CNN architecture. In previous experiments, different values of \u03b7 had similar final test performance after learning 5 tasks. DHP Softmax alone achieved 98.23% test accuracy, a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and improved average test accuracy across all tasks. The datasets are zero-padded to be of size 32\u00d732 and replicated 3 times to create grayscale images with 3 channels. A CNN architecture similar to previous studies was used, with an initial \u03b7 parameter value of 0.0001. The network was trained with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. Results showed that DHP Softmax plus MAS decreased BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax outperformed other methods with an average test performance of 81.75% and a BWT of -0.04 after learning all five tasks. The addition of compressed episodic memory in the softmax layer through DHP alleviates catastrophic forgetting in continual learning environments. It allows new information to be learned without interference, improving generalization across experiences. The \u03b1 parameter in the plastic component scales the magnitude of plastic connections, balancing between protecting old knowledge and acquiring new information quickly. The neural network with DHP Softmax shows significant improvement across all benchmarks compared to traditional softmax layers. The plastic component in the neural network with DHP Softmax learns to adjust the plastic connections, balancing between preserving old knowledge and acquiring new information quickly. This approach outperforms traditional softmax layers and does not require additional hyperparameters. The model's flexibility allows for Hebbian Synaptic Consolidation using EWC, SI, or MAS to alleviate catastrophic forgetting after learning multiple tasks. DHP Softmax combined with SI performs best on Split MNIST and 5-Vision Datasets. The model combines DHP Softmax with Hebbian Synaptic Consolidation methods like EWC, SI, or MAS to mitigate catastrophic forgetting when learning multiple tasks sequentially. DHP Softmax with SI shows superior performance on Split MNIST and 5-Vision Datasets, while combining DHP Softmax with MAS yields better results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The model's use of Hebbian plasticity leads to lower negative BWT and higher average test accuracy, indicating improved continual learning and memory retention. Our model with Hebbian plasticity consistently exhibits lower negative BWT and higher average test accuracy across all benchmarks, enabling neural networks to learn continually and remember distant memories. This allows for reduced catastrophic forgetting when learning from sequential datasets in dynamic environments. Continual synaptic plasticity plays a key role in learning from limited labelled data and adapting at long timescales, opening new investigations into gradient descent optimized Hebbian consolidation for learning and memory in DNNs. Continual learning trains a model on a sequence of tasks with task-specific loss terms to prevent forgetting. The model learns an approximated mapping to the true underlying function and can handle different classes in each task. Experiments were conducted on Nvidia GPUs, training the network on a sequence of tasks with mini-batches. The network is trained on a sequence of tasks with task-specific loss terms to prevent forgetting. Experiments were conducted on Nvidia GPUs with mini-batches and optimized using plain SGD. Hyperparameters were set for different consolidation methods in experiments. In MNIST experiments, regularization hyperparameters were set for task-specific consolidation methods. A grid search was performed to find the best hyperparameter combination for each method. Training samples were artificially removed from each class in the Imbalanced Permuted MNIST problem. In the Imbalanced Permuted MNIST problem, training samples were artificially removed from each class based on random probabilities. The distribution of classes in each dataset for tasks 1 to 10 is shown in Table 2. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. A grid search was conducted to find the best hyperparameter combinations for each method. In Online EWC, \u03bb values ranged from 50 to 1000, in SI from 0.1 to 3.0, and in MAS from 0.01 to 2.0. Random probabilities were maintained across all experiments. For the Split MNIST experiments, hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. A grid search was performed to determine the best hyperparameter combinations for each method using a 5 task binary classification sequence.\u03bb values ranged from 1 to 2000 for Online EWC, from 0.1 to 5.0 for SI, and from 0.01 to 5.0 for MAS. The network was trained on a sequence of 5 tasks with mini-batches of size 64. In synaptic consolidation methods, a grid search was conducted for Online EWC, SI, and MAS using different \u03bb values. The network was trained on a sequence of 5 tasks with mini-batches of size 64 and optimized using plain SGD. The Vision Datasets Mixture benchmark includes tasks from MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets. The notMNIST dataset contains font glyphs for letters 'A' to 'J', FashionMNIST has 10 clothing categories, and SVHN includes digits '0' to '9' from Google Street View images. The CNN architecture used in the benchmark problems includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations. A multi-headed approach was employed due to different class definitions between datasets. The model has a trainable parameter \u03b7 for this benchmark. The CNN architecture in the benchmark includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling. A multi-headed approach was used due to different class definitions between datasets. The model has a trainable \u03b7 parameter for each connection in the final output layer, improving stability and convergence. Different \u03b7 values for each connection allow for modulation of plasticity when learning new experiences. Hyperparameters for the 5-Vision Datasets Mixture experiments include regularization parameters \u03bb for Online EWC, SI, and MAS methods, with specific values set for each method. In SI, the damping parameter \u03be was set to 0.1. The hyperparameters for the 5-Vision Datasets Mixture experiments include regularization parameters \u03bb for Online EWC, SI, and MAS methods. Different values were tested for each method through random search. A sensitivity analysis was conducted on the Hebb decay term \u03b7, showing its impact on test performance after learning tasks sequentially. The effect of initial \u03b7 values on test performance for Permuted MNIST and Imbalanced Permuted MNIST benchmarks was illustrated in Figure 4. The sensitivity analysis was conducted on the Hebb decay term \u03b7, showing its impact on test performance after learning tasks sequentially for different benchmarks. Setting \u03b7 to low values led to the best performance in alleviating catastrophic forgetting. The average test accuracy across 5 trials for MNIST-variant benchmarks was presented in Table 4. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to a neural network's final output layer through plastic connections. This implementation is highlighted for its simplicity using popular ML frameworks. Zenke et al. (2017b) discussed the impact of the Hebb decay term \u03b7 on test performance after learning tasks sequentially for different benchmarks. Low values of \u03b7 were found to alleviate catastrophic forgetting, with the average test accuracy across 5 trials for MNIST-variant benchmarks presented in Table 4. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to a neural network's final output layer through plastic connections. It emphasizes simplicity using popular ML frameworks. The model significantly outperforms Finetune on class-incremental learning tasks, sometimes even outperforming training from scratch. Test accuracies from CIFAR-10 and CIFAR-100 splits are reported after learning tasks sequentially."
}