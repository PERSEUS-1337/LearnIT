{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features from images to improve CNN robustness. A robust edge detection approach called Robust Canny is also introduced to reduce sensitivity to adversarial perturbations. Comparisons with a simplified version called EdgeNetRob show that EdgeGANRob improves model accuracy without sacrificing robustness. EdgeNetRob boosts model robustness but reduces clean model accuracy. EdgeGANRob enhances clean model accuracy without compromising robustness. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to adversarial examples, where imperceptible perturbations can manipulate predictions. Data poisoning or backdoor attacks manipulate training data to reduce model generalization accuracy. CNNs tend to learn surface statistical regularities instead of high-level abstractions, leading to a lack of generalization under distribution shifting. Improving the general robustness of Deep Neural Networks (DNNs) remains a challenge. Recent studies explore the vulnerability of CNNs to adversarial examples, attributing it to non-robust but highly-predictive features. They suggest training classifiers on \"robust features\" that are insensitive to perturbations. Additionally, human recognition relies on global object shapes, while CNNs are biased towards local patterns. This bias potentially contributes to CNN's lack of robustness under distribution shifting. The bias towards local features in CNNs contributes to their vulnerability to adversarial examples and distribution shifting. Human recognition relies on global object shapes, while CNNs are biased towards local patterns. To improve CNN robustness, researchers propose focusing on global shape structures, specifically edges. The paper proposes EdgeGANRob, a new approach to enhance CNN robustness by leveraging global shape structure through edge detection. This approach, illustrated in Figure 1, utilizes EdgeNetRob to extract structural information from images and train the classifier solely based on shape information, reducing reliance on texture/color for predictions. The unified framework, EdgeGANRob, utilizes EdgeNetRob to extract structural information through edge detection and train CNNs based on shape information, improving robustness. A robust edge detection algorithm, Robust Canny, significantly enhances EdgeGANRob's robustness against attacks. However, EdgeNetRob's focus on shape information reduces clean accuracy, leading to the development of EdgeGANRob, which incorporates a generative model to restore texture/color information based on edge images. The EdgeGANRob method improves CNNs' robustness by extracting edge/structure information and refilling texture/colors using a generative model. A robust edge detection approach, Robust Canny, reduces sensitivity to adversarial perturbations, enhancing defense against attacks. Visualization results can be found on the provided website. The proposed Robust Canny edge detection approach aims to enhance defense against adaptive evasion attacks by reducing the sensitivity of edge detectors to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial robustness against various attack scenarios. Defense methods based on adversarial training are highlighted, with a focus on addressing gradient obfuscation as a common pitfall. The state-of-the-art defense methods against adaptive attacks are based on adversarial training. Gradient obfuscation is identified as a common pitfall, and defense methods should be evaluated against customized white-box attacks. Distribution shifting is common in real-world applications, and CNNs tend to learn superficial statistical cues. Methods to robustify CNNs include penalizing predictive power of local representations and evaluating on various patterns. Benchmark datasets are proposed for evaluating model robustness under common perturbations. Backdoor attacks are also discussed. The text discusses methods to robustify CNNs by penalizing the predictive power of local representations and evaluating on different patterns. It also mentions benchmark datasets for evaluating model robustness and the concept of backdoor attacks. Additionally, the connection between recognition robustness and robust features in image recognition is highlighted. The work discusses the connection between recognition robustness and robust features in image recognition. It proposes using edge as a robust feature and introduces a new classification pipeline called EdgeGANRob. The method extracts edge/structure features from an image, reconstructs it using a GAN, and then feeds it into a classifier. EdgeGANRob is a pipeline that extracts edge features from images, reconstructs them using a GAN, and feeds them into a classifier. The simplified backbone procedure, EdgeNetRob, consists of two stages: extracting edge maps and training a classifier based on these maps. This approach aims to make the classifier less sensitive to local textures by focusing solely on edges. EdgeNetRob focuses on making CNNs less sensitive to local textures by forcing decisions based on edges. However, this simplicity leads to degraded performance on clean test data. To address this, EdgeGANRob fills edge maps with texture/colors to improve accuracy. The robustness of this system depends on the edge detector used, motivating the proposal of a robust edge detection algorithm named Robust Canny. The text discusses the importance of a robust edge detection algorithm in the context of improving the accuracy of recognition tasks. It highlights the vulnerability of existing edge detection algorithms to attacks and proposes a new algorithm called Robust Canny. The algorithm aims to enhance the robustness of traditional methods like Canny by truncating noisy pixels in its intermediate stages. The proposed Robust Canny consists of 6 stages, including noise reduction and gradient computation. The proposed Robust Canny edge detector improves the robustness of the traditional Canny algorithm by truncating noisy pixels in its intermediate stages. The 6 stages of Robust Canny include noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The Robust Canny edge detector enhances the traditional Canny algorithm by truncating noisy pixels in intermediate stages. It includes noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The algorithm adjusts parameters like standard deviation of gaussian filter and thresholds to improve robustness. In addition to masking, the parameters of Canny (e.g. \u03c3, thresholds \u03b8 l , \u03b8 h) impact edge map quality. Larger \u03c3 and higher thresholds improve robustness but may reduce clean accuracy. Careful parameter selection is crucial for a robust edge detector. The training of a Generative Adversarial Network (GAN) in EdgeGANRob is described, focusing on generating color images from edge maps using the image-to-image translation framework (pix2pix). The inpainting GAN is trained in two stages, following a common setup with specific objective functions. The inpainting GAN in EdgeGANRob is trained in two stages using the pix2pix framework. The first stage involves training a conditional GAN with an objective function including adversarial and feature matching losses. In the second stage, the GAN is fine-tuned along with a classifier to improve accuracy on generated RGB images. This method aims to enhance robustness against adversarial attacks, distribution shifting, and backdoor attacks. EdgeGANRob does not include classification loss to focus on generating more realistic images, improving robustness under adversarial attack, distribution shifting, and backdoor attack scenarios. It leverages edge features to enhance model generalization and shape structure preservation, making it less sensitive to distribution changes during testing. In backdoor attacks, EdgeGANRob aims to prevent models from being tricked into predicting a certain class by focusing on specific patterns in the training data. EdgeGANRob focuses on shape structure to mitigate distribution changes during testing and prevent backdoor attacks by removing malicious patterns through edge extraction. The robustness of the method is evaluated against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. EdgeNetRob, a variant without inpainting GAN, shows unique advantages in certain settings and is considered an independent robust recognition method for comparison with EdgeGANRob. The study evaluates the robustness of EdgeNetRob against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. The choice of datasets is justified by the limitations of MNIST and CIFAR-10 datasets. The evaluation is conducted using \u221e adversarial perturbation constraints with a standard perturbation budget. We evaluate our methods using \u221e adversarial perturbation constraints on Fashion MNIST and CelebA datasets. The evaluation includes standard perturbation budgets and robustness to white-box attacks using the BPDA attack. We compare the robustness of three edge detection methods: RCF, Canny, and Robust Canny. A comparison of edge detection methods (RCF, Canny, Robust Canny) is conducted, with a focus on their robustness against adversarial attacks. Results on Fashion MNIST and CelebA datasets show that EdgeNetRob and EdgeGANRob perform slightly worse in clean accuracy but outperform adversarial training with = 8 in terms of robustness. EdgeNetRob and EdgeGANRob show a slight drop in clean accuracy compared to the baseline model but outperform adversarial training with = 8 in terms of robustness. EdgeGANRob has higher clean accuracy than EdgeNetRob on the CelebA dataset, highlighting the importance of using GANs on complex datasets. Both models remain robust against strong adaptive attacks, with EdgeNetRob being more time-efficient as it does not use adversarial training. Test accuracy under different attack iterations is shown in Figure 3, and the models demonstrate generalization ability under distribution shifting. EdgeNetRob does not use adversarial training, leading to time efficiency. Test accuracy under various attack iterations is shown in Figure 3. The method is tested for generalization ability under distribution shifting using perturbed Fashion MNIST and CelebA datasets with different patterns. Comparison with state-of-the-art method PAR shows significant improvements in accuracy for negative color, radial kernel, and random kernel patterns. Visualization results are provided in Appendix D, with overall results shown in Table 3. Our method EdgeNetRob significantly improves accuracy on various patterns without adversarial training. We demonstrate its effectiveness as a defense against backdoor attacks by embedding invisible watermarks in images. The poisoning ratio and attack pairs are specified for Fashion MNIST and CelebA datasets. Comparisons with baseline methods are made to showcase the advantages of our approach. Our method EdgeNetRob improves accuracy on different patterns without adversarial training, serving as a defense against backdoor attacks by embedding invisible watermarks in images. Attack pairs and poisoning ratios are specified for Fashion MNIST and CelebA datasets, with comparisons to baseline methods showing the effectiveness of our approach. The results in Table 4 and Table 5 demonstrate high poisoning accuracy on both datasets, with EdgeGANRob achieving better clean accuracy. Our method EdgeNetRob improves accuracy by embedding invisible watermark patterns in images, serving as a defense against backdoor attacks. EdgeGANRob achieves better clean accuracy and utilizes an inpainting GAN for improved results. The approach combines robust edge features with a generative adversarial network to enhance model robustness and generalization under distribution shifting. Resizing images to 128 \u00d7 128 using bicubic interpolation and using 10% of total images as test data are part of the data pre-processing steps. The study focuses on using shape information to enhance model robustness against backdoor attacks. Data pre-processing involves resizing images to 128 \u00d7 128 and normalizing data. Different attack methods like PGD and CW are evaluated with specific parameters. Robust Canny is used for evaluating adversarial robustness with hyper-parameters chosen using the validation set. For CW attack evaluation, 1,000 images are randomly sampled due to high computational complexity. Robust Canny hyper-parameters for Fashion MNIST and CelebA datasets are reported. In white-box attack scenarios, backpropagation through non-differentiable transformations is necessary for constructing adversarial samples. The Backward Pass Differentiable Approximation (BPDA) technique can be used to replace non-differentiable transformations for stronger attacks. In white-box attack scenarios, backpropagation through non-differentiable transformations is necessary for constructing adversarial samples. The Backward Pass Differentiable Approximation (BPDA) technique can be used to replace non-differentiable transformations for stronger attacks. Athalye et al. (2018) demonstrate the use of BPDA to construct adversarial examples by replacing transformations with differentiable approximations. To achieve a stronger attack, a differentiable approximation of the Robust Canny algorithm is found by breaking the transformation into two stages: C1 and C2. C2 is a non-differentiable operation where the output is a masked version of the input. To obtain a differentiable approximation of R-Canny for BPDA, the mask is assumed to be constant, allowing backpropagation through C1 but not through M. The text discusses the use of Backward Pass Differentiable Approximation (BPDA) in constructing adversarial samples by replacing non-differentiable transformations with differentiable approximations. The approach involves breaking the transformation into two stages, C1 and C2, with C2 being a non-differentiable operation. The mask is assumed to be constant for a differentiable approximation of R-Canny, allowing backpropagation through C1 but not through M. Visualizations and results of backdoor attacks on Fashion MNIST are also presented."
}