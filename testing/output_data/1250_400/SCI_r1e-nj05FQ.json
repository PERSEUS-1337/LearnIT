{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is a key feature in the natural world, where organisms collaborate despite individual incentives. The study focuses on intertemporal social dilemmas (ISDs) and uses MARL and natural selection to show how cooperation can be learned. A modular architecture for deep reinforcement learning agents is introduced to support multi-level selection, with results in challenging environments interpreted in the context of cultural and ecological evolution. Nature exhibits cooperation at various scales, from microscopic interactions to species-wide societies. An innovative modular architecture for deep reinforcement learning agents supports multi-level selection, with results in challenging environments interpreted in the context of cultural and ecological evolution. Cooperation in nature is prevalent despite individual incentives, with various mechanisms such as kin selection, reciprocity, and group selection playing a role. The emergence of cooperation among self-interested agents is a significant topic in multi-agent deep reinforcement learning, formalized as intertemporal social dilemmas (ISDs) to generalize matrix game social dilemmas to Markov settings. In multi-agent deep reinforcement learning, cooperation among self-interested agents is a key topic. Social dilemmas involve a trade-off between collective welfare and individual utility. Evolutionary theory predicts that agents tend to converge to defecting strategies instead of achieving the collectively optimal outcome. Various solutions have been proposed to encourage cooperation, including opponent modeling, long-term planning, and intrinsic motivation functions. These approaches contrast with end-to-end model-free learning algorithms, which have shown greater generalization abilities. Evolutionary principles can be applied to remove hand-crafted intrinsic motivation in multi-agent deep reinforcement learning. Evolution has been used for optimizing hyperparameters, black-box optimization, evolving neuroarchitectures, regularization, loss functions, behavioral diversity, and reward functions. However, success is not guaranteed in the ISD setting. Evolutionary simulations of predator-prey dynamics have used enforced subpopulations to evolve populations of neurons for neural networks. Evolutionary simulations of predator-prey dynamics have used enforced subpopulations to evolve populations of neurons for neural networks in the ISD setting. The proposed system distinguishes between fast learning and slow evolution processes, with intrinsic motivation modeled as an additional reward term in a neural network genotype. Evolution bridges the intertemporal social dilemma by integrating intrinsic rewards over two distinct time-scales. The intrinsic motivation is modeled as an additional reward term in a neural network genotype to bridge the intertemporal social dilemma. Evolutionary dynamics are structured to evolve individual intrinsic reward weights, leading to altruistic behavior. A \"Greenbeard\" strategy is implemented for assortative matchmaking, but it has limitations in explaining cooperation in all taxa and is not a general method for multi-agent reinforcement learning. The shared reward network evolution approach is introduced as an alternative to assortative matchmaking for multi-agent reinforcement learning. Agents consist of policy and reward network modules that evolve separately, with the reward network determining collective returns for the group. This method addresses limitations of assortative matchmaking and is inspired by ideas from the theory of multi-level selection. In multi-agent reinforcement learning, the policy and reward network modules evolve separately. The fitness for the policy network is based on individual rewards, while the fitness for the reward network is based on collective returns. This approach prevents overfitting and may explain the evolutionary origin of social inductive biases. Different parameters were explored, including environments, reward network features, matchmaking, and reward network evolution. The study focuses on Markov games within a MARL setting. The study explores different parameters in multi-agent reinforcement learning, including environments, reward network features, matchmaking, and reward network evolution. It focuses on intertemporal social dilemmas in Markov games within a MARL setting, where selfish actions have negative group impacts over time. Two dilemmas are considered, one involving collecting apples in a field with a decreasing respawn rate linked to aquifer cleanliness. In the Cleanup game, agents collect apples with a respawn rate tied to aquifer cleanliness, leading to a dilemma of cleaning for no reward. In the Harvest game, agents face a dilemma between quickly harvesting all apples and depleting the resource for lower long-term yield. In the Harvest game, agents face a dilemma between quickly harvesting all apples and depleting the resource for lower long-term yield. The reward for players is a combination of extrinsic and intrinsic rewards, calculated using a neural network with evolved parameters based on fitness. The intrinsic reward in the Harvest game is calculated using a neural network with evolved parameters based on fitness. The reward is an aggregate social preference across features and is transformed from a player-specific quantity. Each agent has access to the same set of features, with the exception of its own demarcated feature. The features are based on recently received or expected future rewards, and social preferences should not be overly influenced by the precise temporal alignment of rewards in Markov games. In Markov games, social preferences should not be influenced by the precise temporal alignment of rewards. Two ways of aggregating rewards are considered, using off-policy importance weighted actor-critic and a neural network architecture with intrinsic and extrinsic value heads. The retrospective method derives intrinsic reward from past rewards, while the prospective variant derives it from expected future rewards. The architecture for Markov games includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. Intrinsic reward is derived from past rewards in the retrospective method, while the prospective variant uses expected future rewards. Training involved a population of 50 agents with policies, sampled 5 players for each of 500 arenas running in parallel, and used a distributed asynchronous training framework. In BID27, distributed asynchronous training in multi-agent environments, including population-based training (PBT), was conducted with a population of 50 agents. 5 players were sampled for each of 500 arenas running in parallel, with episode trajectories lasting 1000 steps. Agents were updated using V-Trace, and the policy network parameters were inherited in a Lamarckian fashion. Agents were allowed to observe their last actions and rewards as input to the LSTM in the neural network. The objective function comprised the value function gradient, policy gradient, and reward network evolution. In BID26, agents were allowed to observe their last actions, intrinsic rewards, and extrinsic rewards as input to the LSTM in the neural network. The objective function included the value function gradient, policy gradient, and entropy regularization. Evolution was based on a fitness measure calculated as a moving average of total episode return. Matches were determined through random matchmaking or assortative matchmaking based on recent cooperativeness metrics. Matches were determined through random matchmaking or assortative matchmaking based on recent cooperativeness metrics, with cooperative matchmaking grouping agents based on their level of cooperativeness. This ensured that highly cooperative agents played with others of similar rank, while defecting agents played with other defectors. Cooperative metric-based matchmaking was only used with individual reward networks or no reward networks. In contrast to cooperative matchmaking based on cooperativeness metrics, the study focused on evolving reward networks separately within their own population. This approach allowed for independent exploration of hyperparameters and forced reward networks to generalize to a wide range of policies. Multiple policy networks were paired with the same reward network in a shared reward network setup. Fitness determined the evolution of policy network weights and optimization-related hyperparameters. In a shared reward network setup, multiple policy networks were paired with the same reward network. The evolution of policy network weights and optimization-related hyperparameters was determined by fitness based on individual agent return. The reward network parameters evolved based on total episode return across the group of co-players, addressing the tension in Intrinsic Social Dilemmas (ISDs) and promoting social cooperation through communication evolution. Multiple agents sharing components in a social setting proved to be critical. Shared reward networks in Intrinsic Social Dilemmas (ISDs) promote social cooperation through communication evolution. Multiple agents sharing components in a social setting proved to be critical. Adding reward networks over social features benefits when players have separate networks evolved selfishly. In experiments comparing random and assortative matchmaking with PBT and reward networks using retrospective social features, it was found that individual reward network agents did not perform significantly better than PBT on Cleanup and only moderately better on Harvest. Adding reward networks over social features had little benefit if players had separate networks evolved selfishly. Assortative matchmaking with individual reward networks showed very high performance, indicating that conditioning internal rewards on social features and a preference for cooperative agents to play together were key. Shared reward network agents performed as well as assortative matchmaking, even with random matchmaking, suggesting that agents did not necessarily need immediate access to honest signals of other agents' cooperativeness to resolve the dilemma. Agents with the same intrinsic reward function, evolved according to collective episode return, performed as well as assortative matchmaking and handcrafted inequity aversion intrinsic reward from BID25, even with random matchmaking. The retrospective variant of reward network evolution outperformed the prospective variant, which resulted in worse performance and more instability. Various social outcome metrics were plotted to capture agent behavior complexities, showing that having no reward network impacted sustainability. The text discusses the impact of reward networks on agent behavior. Sustainability, equality, and tagging metrics were used to analyze the effects of different reward network types. The study found that having no reward network led to unsustainable behavior, while different types of reward networks affected equality and tagging behavior. The retrospective shared reward networks were found to be the most effective in resolving issues. The study found that different reward network types influenced agent behavior, with a higher propensity for tagging seen with prospective or individual reward networks compared to retrospective shared reward networks. The final weights of the retrospective shared reward networks varied for different games, suggesting varying social preferences were needed. Cleanup required a simpler reward network, while Harvest needed a more complex one to prevent over-exploitation. Random matchmaking led to arbitrary positive values in the first layer weights. In Harvest, a more complex reward function was needed to prevent over-exploitation of apples by other agents. The first layer weights tended to take on arbitrary positive values due to random matchmaking. Real environments do not provide scalar reward signals, so organisms have developed internal drives based on primary or secondary goals. Implementing natural selection did not lead to cooperation, but assortative matchmaking generated cooperative behavior when honest signals were present. A new evolutionary paradigm based on shared reward networks promotes cooperation in general situations. The emergence of cooperation was not achieved through natural selection but was successful with assortative matchmaking when honest signals were available. A new multi-level evolutionary paradigm based on shared reward networks promotes cooperation in general situations by addressing the intertemporal choice problem and mitigating the social dilemma through mechanisms like competitive altruism and inequity aversion. Humans cooperate more readily when communication is possible. The shared reward network evolution model promotes cooperation through mechanisms like competitive altruism and inequity aversion. Humans cooperate more readily when communication is possible, as seen in laboratory experiments. Modularity in evolution can be observed in various forms, such as free-living microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids for cooperation in social dilemmas. The curr_chunk discusses the spread of cultural norms and the emergence of cooperation through alternative evolutionary mechanisms like kin selection and reciprocity. It suggests exploring how these mechanisms affect a reward network and the evolutionary origins of social biases. Additionally, it proposes studying an assortative matchmaking model and combining an evolutionary approach with multi-agent communication to understand cooperative behaviors like cheap talk. The curr_chunk discusses the setup for games involving agents in a partially observable environment with specific actions and rewards. Training was conducted through joint optimization of network parameters and evolution in the standard PBT setup. The Cleanup game involves punishing free-riders and has an additional action for cleaning waste. Training was done via joint optimization of network parameters and hyperparameters/reward network parameters using SGD and evolution in the standard PBT setup. Gradient updates were applied for every trajectory up to 100 steps with a batch size of 32. Optimization was done via RMSProp with specific parameters, and hyperparameters were evolved using PBT with genetic algorithms. Mutation rates were applied for evolving hyperparameters. The hyperparameters were evolved using a mutation rate of 0.1, with perturbations for entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 10 6 agent steps was implemented before evolution to ensure accurate fitness assessment."
}