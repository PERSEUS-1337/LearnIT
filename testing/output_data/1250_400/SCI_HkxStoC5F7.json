{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning. It extends existing probabilistic interpretations of meta-learning and introduces \\Versa{}, which uses a flexible amortization network for few-shot learning. This network outputs a distribution over task-specific parameters in a single forward pass, eliminating the need for optimization at test time. The paper introduces ML-PIP, a framework for meta-learning approximate probabilistic inference for prediction. \\Versa{} achieves state-of-the-art results on benchmark datasets, handling arbitrary shots and classes at train and test time. The approach is demonstrated through a few-shot ShapeNet view reconstruction task, showcasing its adaptability to new datasets at test time. In this paper, a framework called ML-PIP is developed for meta-learning approximate probabilistic inference for prediction. The framework extends existing interpretations of meta-learning to cover various methods, including gradient-based meta-learning, metric-based meta-learning, amortized MAP inference, and conditional probability modeling. It leverages shared statistical structure between tasks, shares information between tasks for learning and inference, and enables fast learning for a wide range of tasks. In this paper, a new method called VERSA is proposed for fast learning in multi-task and transfer learning. VERSA substitutes optimization procedures with forward passes through inference networks at test time, resulting in faster performance. It can handle various tasks and learning settings efficiently, setting new state-of-the-art results in standard benchmarks. The VERSA framework includes a multi-task probabilistic model and a method for meta-learning probabilistic inference. It utilizes shared parameters for all tasks and task-specific parameters. The joint probability of outputs and task-specific parameters for T tasks is given by a directed graphical model with shared parameters. The standard multi-task directed graphical model employs shared parameters for all tasks. The goal is to meta-learn fast and accurate approximations to the posterior predictive distribution for unseen tasks. Point estimates are used for shared parameters, while distributional estimates are used for task-specific parameters. The probabilistic solution to few-shot learning involves forming the posterior distribution over task-specific parameters and computing the posterior predictive. The model uses shared parameters for all tasks and approximates the posterior predictive distribution for unseen tasks. Point estimates are used for shared parameters, while distributional estimates are used for task-specific parameters. The probabilistic solution to few-shot learning involves forming the posterior distribution over task-specific parameters and computing the posterior predictive. The framework approximates the posterior predictive distribution by an amortized distribution using a feed-forward inference network. Our framework approximates the posterior predictive distribution by an amortized distribution q \u03c6 (\u1ef9|D), using a feed-forward inference network with parameters \u03c6. The approximate posterior predictive distribution is constructed by amortizing the approximate posterior q \u03c6 (\u03c8|D) and may require additional approximation like Monte Carlo sampling. The goal is to meta-learn the approximate posterior predictive distribution by minimizing the KL-divergence between the true and approximate posterior predictive distribution averaged over tasks. The training method described involves meta-learning the approximate posterior predictive distribution to minimize the KL-divergence between the true and approximate posterior predictive distribution. By selecting a task at random, sampling training data, forming the posterior predictive, and computing the log-density, accurate prediction is supported through approximate inference. The framework is grounded in Bayesian decision theory and aims to optimize parameters that best approximate the posterior predictive distribution. The training procedure involves selecting a task at random, sampling training data, forming the posterior predictive, and computing the log-density to optimize parameters for accurate prediction through approximate inference. The framework is grounded in Bayesian decision theory and focuses on minimizing the KL-divergence between true and approximate posterior predictive distributions. The training procedure focuses on minimizing KL-divergence between true and approximate posterior predictive distributions. It involves end-to-end stochastic training with shared parameters and optimizing predictive performance through episodic train/test splits. The objective does not require an explicit prior distribution over parameters. The Meta-Learning Probabilistic Inference for Prediction (ML-PIP) approach involves episodic train/test splits and approximating integrals using Monte Carlo samples. The learning objective implicitly specifies the prior distribution over parameters. The framework supports versatile learning for rapid and flexible inference, unifying existing approaches. The system supports rapid and flexible inference by using deep neural networks for amortized posterior distribution. Design choices enable flexibility in processing sets of variable size for tasks like few-shot image classification. The system utilizes deep neural networks for amortized posterior distribution, allowing for flexible processing of sets of variable size in tasks like few-shot image classification. The probabilistic model is inspired by previous work and features a shared feature extractor feeding into task-specific linear classifiers. Amortization is proposed to model the distribution over weight matrices in a context-independent manner, enabling metalearning without specifying the number of few-shot classes ahead of time. The proposed method involves specifying weight vectors in a context-independent manner for metalearning systems that output large matrices. Amortization is used to map image/angle examples to stochastic inputs, reducing the number of learned parameters. End-to-end training is employed, backpropagating through the inference network to construct the classification. In our implementation, end-to-end training is used, backpropagating through the inference network to construct weight vectors and biases for each class. The classification matrix is created by performing feed-forward passes through the network. Context-independent inference is approximated and justified theoretically and empirically, addressing limitations of naive amortization. The context-independent approximation in few-shot learning addresses limitations of naive amortization by reducing the number of parameters needed for inference, allowing meta-training with varying numbers of classes, and accommodating different class numbers at test-time. This approach is applied to few-shot image reconstruction tasks involving complex output spaces, where a generative model uses a latent vector and angle representation to produce images at specified orientations. Our generative model uses a latent vector and angle representation to output images with specified orientations. The generator network's parameters are global, while the latent inputs are task-specific. A Gaussian likelihood in pixel space is used for the generator outputs, with a sigmoid activation to ensure output means between zero and one. An amortization network processes image representations, concatenates them with view orientations, and produces a distribution over latent vectors. This process is illustrated in FIG5. The text discusses ML-PIP, a meta-learning approach that unifies various methods including gradient-based and metric-based variants. It highlights the use of point estimates for task-specific parameters and compares previous approaches. The process involves instance-pooling and producing a distribution over vectors. The text discusses semi-amortized inference in neural networks, focusing on task-specific parameters and gradient ascent for training. It compares this approach to Model-agnostic meta-learning and emphasizes the use of predictive KL for update choices. VERSA is highlighted for its distributional nature and the avoidance of back-propagation during training. Amortization in neural networks involves predictive KL for training and recovering test-train splits. VERSA is distributional and simplifies inference by treating local and global parameters. Metric-Based Few-Shot Learning uses amortized point estimates for task-specific parameters. Amortized MAP inference is also discussed. The predictive distribution in VERSA recovers prototypical networks using Euclidean distance. BID43 proposed predicting weights from activations for online and transfer learning tasks. VERSA's amortization function is more flexible, supporting multi-task learning. VERSATILE (VERSA) goes beyond point estimates with its amortization network, supporting full multi-task learning. It employs end-to-end training and shares information between tasks. The ML-PIP training procedure establishes a strong connection to neural processes. Comparing to Variational Inference (VI), VERSA optimizes the Monte Carlo approximated free-energy with conceptual differences and regularization. In addition to the conceptual difference from ML-PIP, VERSA optimizes the Monte Carlo approximated free-energy with regularization. It significantly improves few-shot classification and is evaluated on various tasks, including toy experiments, Omniglot, miniImageNet datasets, and ShapeNet objects. In Section 5.3, VERSA's performance on a one-shot view reconstruction task with ShapeNet objects is examined. Data is generated from a Gaussian distribution with varying means across tasks. The inference network is introduced for amortizing inference, and the model is trained with Adam using mini-batches of tasks. The approximate posterior distributions inferred for unseen test sets by the trained amortization networks are shown in Fig. 4. The true posterior over \u03c8 is Gaussian with a mean dependent on the task, inferred by the amortization networks. Evaluation on few-shot classification tasks like Omniglot and miniImageNet shows accurate recovery of posterior distributions despite minimizing predictive KL divergence. VERSA's performance is compared to previous work, following established experimental protocols. The experimental protocol for few-shot classification tasks like Omniglot and miniImageNet involves training with episodic learning using specific architectures. Results are compared with competitive approaches in a tabular format, excluding those using pre-training or residual networks to assess the learning algorithm's quality independently. VERS achieves new state-of-the-art results on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks using convolution-based network architecture and end-to-end training. It also performs competitively on other benchmarks. VERS achieves state-of-the-art results on various benchmarks, including 5-way -1-shot miniImageNet and 5-way -5-shot Omniglot. VERSA outperforms amortized VI by adapting only the top-level classifier weights, showing improved performance and faster posterior formation. The amortization network is used for both amortized VI and non-amortized VI, with the latter showing improved performance but slower posterior formation. VERSA demonstrates flexibility by retaining high accuracy across varying numbers of classes and shots during testing. VERSATILE (VERSA) retains high accuracy across different test-time conditions, demonstrating flexibility and robustness. It outperforms MAML in speed and accuracy on ShapeNetCore v2 BID5 dataset, showing a 5\u00d7 speed advantage and 4.26% better accuracy. The dataset consists of 37,108 objects from 12 categories, with 70% for training, 10% for validation, and 20% for testing. For experiments, 12 large object categories are used, totaling 37,108 objects. The dataset is split for training, validation, and testing. Each object has 36 views generated for evaluation. VERSA is compared to a C-VAE using episodic training. VERSA produces sharper and more detailed images compared to C-VAE, capturing correct object orientation. VERSATM outperforms C-VAE in generating detailed and sharp images of unseen objects, accurately capturing object orientation. Quantitative comparison in Table 2 shows VERSA's superiority in mean squared error and structural similarity index. With an increase in shots to 5, VERSA's performance improves further. ML-PIP is introduced as a probabilistic framework for meta-learning, unifying various meta-learning methods. ML-PIP is a probabilistic framework for meta-learning that unifies various methods and suggests alternative approaches. Building on ML-PIP, VERSA is a few-shot learning algorithm that achieves state-of-the-art performance on challenging tasks like 1-shot view reconstruction. Prototypical Networks perform better when trained on a higher \"way\" than that of testing. Prototypical Networks perform better when trained on a higher \"way\" than that of testing. A new inference framework based on Bayesian decision theory is presented, providing a recipe for making predictions for unknown test variables by combining information from observed training data and a loss function. This framework separates test and training data, aligning with recent episodic approaches to training. The text discusses a derivation of a stochastic variational objective for meta-learning probabilistic inference grounded in Bayesian inference and decision theory. It introduces Distributional BDT for returning a full predictive distribution over unknown test variables. Amortized variational training is used to make quick predictions at test time by minimizing average expected loss over tasks. The optimal predictive distribution is found through amortized variational training, where shared variational parameters are learned to make quick predictions at test time. The variational parameters are optimized by minimizing the expected distributional loss across tasks, without the need for computing the true predictive distribution. This approach emphasizes the meta-learning aspect of the procedure. The text discusses stochastic approximation in meta-learning by sampling tasks and partitioning data into training and test sets. It focuses on log-loss functions and the approximation of predictive distributions. The approach emphasizes meta-learning and the optimization of variational parameters for quick predictions at test time. The text discusses the approximation of predictive distributions using density ratio estimation in the context of meta-learning and stochastic approximation. It emphasizes the construction of estimators for conditional densities for each class independently, similar to training a naive Bayes classifier. The text discusses the construction of estimators for conditional densities for each class independently in the context of meta-learning and stochastic approximation. It details a simple experiment to evaluate the validity of the context-independent inference assumption by randomly generating tasks and performing free-form variational inference on the weights for each task. The text examines the assumption on the amortization network by randomly generating tasks and performing free-form variational inference on the weights for each task. It includes a 5-way classification experiment on the MNIST dataset, achieving 99% accuracy on test examples. The optimized weights cluster according to class in 2-dimensional space. After achieving 99% accuracy on test examples, the optimized weights cluster by class in 2-dimensional space. Some overlap exists between classes, and for tasks with classes '1' and '2', class '2' weights are located away from their cluster. This suggests that class-weights are typically independent of the task, but in cases where classes from similar regions appear, the inference procedure may adjust weights to different regions. A VI-based objective is derived for the probabilistic model, with an \"amortized\" VI parameterized by a neural network. The inference procedure adjusts class weights to different regions of space when classes from similar regions appear. A VI-based objective is derived for the probabilistic model, with an \"amortized\" VI parameterized by a neural network. The objective function remains the same for both \"amortized\" and \"non-amortized\" VI. An evidence lower bound (ELBO) is expressed for a single task, and a stochastic estimator is derived to optimize it by sampling training tasks and integrating over parameters. In this section, comprehensive details on few-shot classification experiments using the Omniglot dataset are provided. The dataset consists of 1623 handwritten characters from 50 alphabets, with 20 instances each. The training, validation, and test sets are split randomly, resulting in 4400 training, 400 validation, and 1292 test classes for C-way, k c -shot classification. Training is done episodically, with each iteration involving a batch of one. The training, validation, and test sets for few-shot classification experiments on the Omniglot dataset consist of 1100, 100, and 423 characters respectively, with rotations of 90 degrees. Augmented data results in 4400 training, 400 validation, and 1292 test classes. Training is episodic, with each iteration involving a batch of tasks where C classes are randomly selected. The validation set is used to monitor progress, and the final evaluation is done on 600 randomly selected tasks from the test set. The models are trained using the Adam BID25 optimizer with a constant learning rate, and different models are trained for varying iterations based on the number of classes and shots. The Adam BID25 optimizer with a constant learning rate is used for training models on k c character instances for both training and testing. Different models are trained for varying iterations based on the number of classes and shots. The miniImageNet dataset consists of 60,000 color images divided into 100 classes, with training, validation, and test class splits defined. Training proceeds episodically, similar to Omniglot, with specific training configurations for different models. The Adam BID25 optimizer is used with a Gaussian form for q and L = 10 \u03c8 samples. Different training iterations are used for 5-way -5-shot and 5-way -1-shot models with specific learning rates. Neural network architectures for feature extractor \u03b8, amortization network \u03c6, and linear classifier \u03c8 are detailed. The feature extraction network is similar to BID54. The amortization network provides Gaussian parameters for \u03c8 weight distributions. The local-reparameterization trick is used for sampling. The feature extraction network is shared with the amortization network. The feature extraction network is shared with the amortization network to reduce parameters. Different architectures are used for miniImageNet and few-shot learning. Batch Normalization and dropout are utilized throughout the process. The Shared Feature Extraction Network (\u03b8) consists of multiple convolutional layers with dropout and pooling operations. The network is used in conjunction with the Linear Classifier (\u03c8) for training on the ShapeNetCore v2 dataset, which includes 12 object categories and over 37,000 objects for training, validation, and testing. The Linear Classifier (\u03c8) is trained on a dataset of 37,108 objects, with 70% for training, 10% for validation, and 20% for testing. Each object has 36 views, with training done episodically on randomly selected views. An amortization network is used to generate 36 views for evaluation. The system's performance is assessed using quantitative metrics on the test set. The amortization network generates 36 views of each object for evaluation. Network architectures for the encoder, amortization, and generator networks are described in Tables E.2 to E.4. Training uses the Adam BID25 optimizer with a learning rate of 0.0001 and 24 tasks per batch for 500,000 iterations. The ShapeNet Encoder Network (\u03c6) has specific layers and output sizes."
}