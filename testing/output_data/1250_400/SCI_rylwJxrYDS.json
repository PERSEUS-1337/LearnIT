{
    "title": "rylwJxrYDS",
    "content": "We propose vq-wav2vec to learn discrete representations of audio segments through a self-supervised context prediction task. The algorithm uses gumbel softmax or online k-means clustering to quantize dense representations, enabling the application of NLP algorithms. Experiments show BERT pre-training achieves state-of-the-art results on TIMIT phoneme classification and WSJ speech recognition. Learning discrete speech representations has gained recent interest, with approaches including autoencoding and self-supervised learning of continuous speech representations. This paper combines these approaches by learning discrete speech representations through a context prediction task. In this paper, the vq-wav2vec encoder learns discrete representations of speech through a context prediction task, enabling the application of NLP algorithms to speech data. The algorithm utilizes gumbel softmax or online k-means clustering for quantization, leading to state-of-the-art results in phoneme classification and speech recognition tasks. The vq-wav2vec algorithm learns discrete representations of audio segments using Gumbel-Softmax and online k-means clustering. These representations are then input into a Deep Bidirectional Transformer (BERT) and outperform other methods on TIMIT and WSJ benchmarks. The discretization of audio allows for the application of NLP algorithms to speech data, such as using a sequence to sequence model for speech recognition. The WAV2VEC model learns audio representations through a self-supervised context-prediction task. It uses convolutional neural networks to produce representations for each time step and aggregates them to distinguish future samples. The model is trained to minimize contrastive loss and optimize step-specific affine transformations. The vq-wav2vec approach learns vector quantized representations of audio data using a future time-step prediction task. It optimizes step-specific affine transformations and uses a contrastive loss for different step sizes. BERT is a pre-training approach for NLP tasks that uses a transformer encoder model for text representation. The vq-wav2vec approach learns vector quantized representations of audio data through a future time-step prediction task. It utilizes convolutional networks for feature extraction and aggregation, along with a quantization module to build discrete representations. The model predicts missing tokens and determines if text passages are from the same document. The vq-wav2vec approach utilizes an aggregator to optimize context prediction tasks and replaces original representations with a fixed size codebook. It employs Gumbel-Softmax for selecting discrete codebook variables and performs multiple vector quantizations to prevent mode collapse. The model uses a linear layer and ReLU for outputting logits, with the largest index chosen at inference. The vq-wav2vec approach utilizes Gumbel-Softmax for selecting codebook variables and optimizing context prediction tasks. It employs vector quantization to prevent mode collapse and uses a linear layer for outputting logits. The model selects the largest index at inference and optimizes future time step prediction loss instead of autoencoder reconstruction loss. The codebook variable representation is chosen based on Euclidean distance, and gradients for the encoder network are obtained through back-propagation. The vq-wav2vec approach uses vector quantization to prevent mode collapse and selects codebook variables based on Euclidean distance. Gradients for the encoder network are obtained through back-propagation, and the final loss includes terms for future prediction tasks and moving codebook vectors closer to the encoder output. To address mode collapse, a new strategy of independently quantizing is described. The text describes a strategy to independently quantize partitions of the encoder feature vector z, similar to product quantization. This approach results in larger dictionaries and improved downstream performance. The feature vector is organized into multiple groups and represented by integer indices, allowing for the use of two vector quantization approaches. The codebook can be initialized in two ways, either sharing variables across groups or keeping them separate. The text discusses the use of VQ approaches for quantizing encoder feature vectors into groups, with the option to share or not share codebook variables. Training a vq-wav2vec model allows for discretizing audio data for use in algorithms like BERT pre-training, improving speech recognition by feeding representations into an acoustic model. Recent BERT training advancements focus on masked input token prediction. The text discusses using BERT pre-training with a vq-wav2vec model to improve speech recognition. They modify BERT training by masking spans of consecutive speech tokens to make prediction harder and show improved accuracy. Models are evaluated on TIMIT and Librispeech datasets. The study evaluates the performance of vq-wav2vec and BERT models on TIMIT and Wall Street Journal datasets for speech recognition. The models are trained on Librispeech data and undergo ablations on a subset for evaluation. The vq-wav2vec/wav2vec models have 34 \u00d7 10 6 parameters with specific architecture details. The vq-wav2vec/wav2vec models used in the study have 34 \u00d7 10 6 parameters with specific architecture details. The encoder has 8 layers with 512 channels each, kernel sizes, and strides, while the aggregator has 12 layers with skip connections. Training involves context prediction loss, warming up, and batch size considerations. The learning rate is adjusted during training, with a batch size of 10 and models trained on 8 GPUs. A smaller model is used for experiments on the 100h Librispeech subset. Gumbel-Softmax models are utilized with specific parameters and training details, resulting in unique codeword combinations. After training on 960h of Librispeech, the model is left with 13.5k unique codeword combinations. The VQ auxiliary loss is balanced with \u03b3 = 0.25. BERT base models have specific architecture details and are trained on 128 GPUs. BERT small models are used for ablations with a smaller setup. The model is trained on 128 GPUs with a batch size of 3072 tokens per GPU, totaling 393k tokens. A smaller setup with model dimension 512, FFN size 2048, 8 attention heads, and dropout 0.05 is used for ablations. Models are trained for 250k updates with a batch size of 2 examples per GPU. Different language models are evaluated on the WSJ speech recognition benchmark. A vq-wav2vec model is trained on Librispeech, then used to estimate a BERT model, followed by training a wav2letter acoustic model on WSJ. The vq-wav2vec model is trained on Librispeech and used to estimate a BERT model. A wav2letter acoustic model is then trained on WSJ using either the BERT or vq-wav2vec representations. Results show that vq-wav2vec with BERT training achieves a new state of the art of 2.34 WER on nov92. Table 1 demonstrates that vq-wav2vec combined with BERT training achieves a new state of the art WER of 2.34 on nov92. Gumbel-Softmax with vq-wav2vec uses a limited set of codewords, enabling training of BERT models with a small vocabulary. Comparison between Gumbel-Softmax and k-means for vector quantization is also discussed, along with results on TIMIT phoneme recognition in terms of PER. Table 3 presents TIMIT phoneme recognition results in terms of phoneme error rate (PER) for various models, including vq-wav2vec with Gumbel-Softmax and k-means clustering, both with and without BERT pre-training. The study shows that Gumbel-Softmax is more accurate than k-means without BERT, but the differences diminish with BERT. Additionally, the large codeword model significantly reduces the performance gap to the original wav2vec model. In experiments with vq-wav2vec and BERT, a new state-of-the-art phoneme error rate of 11.67 PER was achieved, showing a 21% error reduction compared to previous results. Training a standard sequence-to-sequence model on discretized speech data also yielded promising results, although not as good as the current state-of-the-art due to the lack of data augmentation. Additionally, the compression capabilities of vq-wav2vec on audio data were investigated. The study evaluated vq-wav2vec's compression of audio data by training models with varying numbers of groups and variables to measure accuracy on phoneme recognition. Different compression rates were tested, ranging from 0.53 kbit/s to 33.03 kbit/s. The quantization module was placed after the aggregator module, and models were trained on the 100h clean Librispeech subset. Various lossy compression algorithms were used as baselines for comparison. The study evaluated vq-wav2vec's compression of audio data by training models with varying numbers of groups and variables to measure accuracy on phoneme recognition. Different compression rates were tested, ranging from 0.53 kbit/s to 33.03 kbit/s. The quantization module was placed after the aggregator module, and models were trained on the 100h clean Librispeech subset. Various lossy compression algorithms were used as baselines for comparison. Acoustic models on vq-wav2vec achieved the best results across most bitrate settings, showing a trade-off between bitrate and TIMIT accuracy. Masking entire spans of tokens performed significantly better than individual tokens, and BERT training on discretized audio data was robust to masking large parts of the input. vq-wav2vec is a self-supervised algorithm that quantizes unlabeled audio data, making it suitable for algorithms requiring discrete data. Table 5a demonstrates that masking entire spans of tokens outperforms individual tokens, while BERT training on discretized audio data is robust to masking large parts of the input (Table 5b). The vq-wav2vec algorithm quantizes unlabeled audio data, improving performance on WSJ and TIMIT benchmarks by leveraging BERT pre-training. Future work includes exploring algorithms requiring discrete inputs for audio data and finetuning the pre-trained model to output transcriptions. Additionally, the relationship between variables and groups is investigated, showing the benefit of multiple groups over a single group with many variables."
}