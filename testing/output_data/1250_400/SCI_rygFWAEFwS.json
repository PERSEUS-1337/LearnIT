{
    "title": "rygFWAEFwS",
    "content": "We propose Stochastic Weight Averaging in Parallel (SWAP) to accelerate DNN training by using large mini-batches to compute an approximate solution quickly and refining it through averaging weights of multiple models computed independently and in parallel. The resulting models generalize well and are produced in a substantially shorter time, demonstrated on CIFAR10, CIFAR100, and ImageNet datasets. Increasing mini-batch size with available computational resources accelerates DNN training by producing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss per iteration. The use of larger mini-batches in DNN training accelerates the process by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss per iteration. However, there is a maximum batch size that can lead to worse generalization performance, limiting the effectiveness of large-batch training strategies. Stochastic Weight Averaging (SWA) is a method that addresses this issue by producing models with good overall scaling behavior. Stochastic Weight Averaging (SWA) addresses the issue of worse generalization performance with large batch sizes by averaging weights of models sampled from training runs. SWAP is derived from SWA, where multiple independent SGD sequences are generated and averaged to achieve similar generalization performance. This method allows for models trained with small-batches to perform comparably to those trained solely with small-batches. The SWAP algorithm accelerates DNN training by utilizing compute resources efficiently. It achieves generalization performance comparable to models trained with small-batches in a similar time frame as models trained with large-batches. SWAP has been successful in reducing training times for efficient models and outperforming the state of the art in certain image classification tasks. The impact of training batch size on generalization performance remains unknown. The SWAP algorithm accelerates DNN training by efficiently utilizing compute resources, reducing training times for efficient models and outperforming the state of the art in certain image classification tasks. The impact of training batch size on generalization performance is still unknown, with theories suggesting larger batch sizes may lead to sharper global minima. In (Dinh et al., 2017) and (Li et al., 2018), authors discuss transforming minimizers and the impact of weight-decay. (McCandlish et al., 2018) predict a critical batch size for accuracy in image classification. (Hoffer et al., 2017) argue that larger batch sizes imply fewer model updates, affecting generalization performance. In the context of batch size and model training, larger batch sizes lead to fewer model updates, impacting generalization performance and optimization processes. Training with large batches for longer periods can improve generalization performance but takes more time. There is a critical batch size for convergence in convex functions, and adaptive batch size methods exist but may require extensive tuning. Methods exist for adaptive batch sizes in model training, but they often require extensive tuning and may not effectively utilize computational resources. Local SGD and Post-local SGD are distributed optimization algorithms that trade off gradient precision with communication costs, resulting in better generalization and significant speedups compared to training with large batches. SWAP, similar to Post-local SGD, averages models after a certain number of iterations to improve performance. Stochastic weight averaging (SWA) is a method that improves model generalization by averaging models from later stages of training. SWA has been effective in various domains such as deep reinforcement learning, semisupervised learning, Bayesian inference, and low-precision training. The SWA strategy differs from Post-local SGD in the timing of model averaging, suggesting different optimization mechanisms in deep neural networks. Stochastic Weight Averaging (SWA) improves model generalization by averaging models from later training stages. SWA has been effective in deep reinforcement learning, semisupervised learning, Bayesian inference, and low-precision training. SWAP algorithm involves three phases: initial training with large mini-batch updates, independent refinement of model copies with smaller batch size and lower learning rate, and averaging weights for final output. The SWAP algorithm consists of three phases: initial training with large mini-batch updates, independent refinement of model copies with smaller batch size and lower learning rate, and averaging weights for final output. No synchronization between workers is required in the first phase, and stopping early is recommended to prevent optimization from getting stuck. In the second phase, small-batch training is performed independently and simultaneously by each worker, resulting in different models. The SWAP algorithm involves three phases: initial training with large mini-batch updates, independent refinement with smaller batch size and lower learning rate, and averaging weights for final output. In the small-batch phase, workers produce different models due to stochasticity, with the averaged model outperforming individual models. The SWAP algorithm consists of three phases: large-batch training with synchronized models, small-batch training with independent models, and averaging weights for final output. The test accuracy is computed by averaging independent models. Visualization of the mechanism behind SWAP involves plotting error on a plane containing outputs of different phases. Training and testing errors for the CIFAR10 dataset are shown in Figure 2, with 'LB' representing phase one, 'SGD' representing a single worker after phase two, and 'SWAP' as the final output. The SWAP algorithm involves three phases: large-batch training with synchronized models, small-batch training with independent models, and averaging weights for final output. In Figure 2, training and testing errors for the CIFAR10 dataset are plotted, showing 'LB' from phase one, 'SGD' from a worker after phase two, and 'SWAP' as the final output. The model in phase 2 moved to a different side of the basin, while 'SWAP' ended up closer to the center, resulting in lower error. Figure 3 illustrates the positions of 'SGD1', 'SGD2', 'SGD3' in relation to the training error basin, with 'SWAP' closer to the center. In Figure 3, worker points 'SGD1', 'SGD2', 'SGD3' are shown in relation to the training error basin, with 'SWAP' closer to the center. The change in topology causes worker points to have higher testing errors compared to 'SWAP'. The authors argue that in later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. The weight iterates in SGD behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. The distribution is centered at the local minimum, with a covariance that grows proportionally with the learning rate and inversely proportional to the batch size. Sampling weights from different SGD runs can generate independent samples from the stationary distribution. SWA and SWAP have an advantage over SGD in terms of cosine similarity. In this section, the performance of SWAP is evaluated for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. The best hyper-parameters were found using grid searches. Training was done using mini-batch. The cosine similarity between gradient descent direction and the output of SWAP decreases as training progresses, indicating faster progress towards the center of the basin. In this section, SWAP's performance for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated. The experiments involved grid searches for hyper-parameters, training with mini-batch SGD, data augmentation with cutout, and the use of a custom ResNet 9 model. The experiments were conducted on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for computation distribution. SWAP phase one had 4096 samples per batch across 8 GPUs, while phase two involved 8 workers with one GPU each. The experiment with large-batches had 4096 samples per batch across 8 GPUs and ran for 150 epochs. The study evaluated SWAP's performance for image classification on CIFAR10, CIFAR100, and ImageNet datasets. Different batch sizes and GPU configurations were used in the experiments. Results showed improved test accuracies after model averaging. The study evaluated SWAP's performance for image classification on CIFAR10, CIFAR100, and ImageNet datasets. Results showed significant improvement in test accuracies after averaging the models. Training with small-batches achieved higher testing accuracy than training with large-batches but took longer. SWAP terminated in comparable time to large-batch runs and achieved accuracies on par with small batch training. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch reached state-of-the-art training speeds for CIFAR10 in 27 seconds. SWAP with 8 Tesla V100 GPUs accelerates ImageNet model training by modifying batch sizes and learning rates. Small-batch experiments run for 28 epochs on 8 GPUs, while large-batch experiments on 16 GPUs double batch size and learning rates. SWAP phase 1 uses large-batch settings for 22 epochs, and phase 2 runs on 2 workers with 8 GPUs each for 6 epochs. Doubling batch size initially reduces accuracies, but SWAP recovers generalization performance with faster training times. Results are compiled in Table 3, showing improved efficiency over multiple runs. SWAP accelerates ImageNet model training by modifying batch sizes and learning rates on 8 Tesla V100 GPUs. Doubling batch size initially reduces accuracies, but SWAP recovers generalization performance with faster training times. Results are compiled in Table 3, showing improved efficiency over multiple runs. Comparing SWAP with SWA on the CIFAR100 dataset, similar models are sampled for both algorithms. SWAP accelerates ImageNet model training by adjusting batch sizes and learning rates on 8 Tesla V100 GPUs. Results show improved efficiency over multiple runs. Comparing SWAP with SWA on CIFAR100 dataset, similar models are sampled for both algorithms. SWA was unable to improve the lower training accuracy of large-batch training runs. The study evaluates the impact of SWA on large-batch training runs. SWA did not improve the lower training accuracy of large-batch runs. Small-batch SWA required more time to compute models but reached test accuracy. The learning rate schedule is illustrated in Figure 6b. Small-batch SWA and SWAP are compared in terms of accuracy and training time. SWA achieves better accuracy than SWAP but requires significantly more training time. SWAP achieves a test accuracy of 79.11% in less time by relaxing constraints and increasing the phase two schedule. The algorithm SWAP improves model generalization by averaging weights of models trained with large and small mini-batches. It achieves a test accuracy of 79.11% in 241 seconds, 3.5x faster than previous methods. This approach is novel and effective in image classification datasets like CIFAR10, CIFAR100, and ImageNet. By refining models with large-batch runs using SWA or SWAP, good generalization performance can be achieved in image classification datasets like CIFAR10, CIFAR100, and ImageNet. Visualizations show that averaged weights are closer to the center of a training loss basin. The transition point between large-batch and small-batch training is a key hyperparameter that requires further exploration in future work. Our method requires choosing the transition point between large-batch and small-batch training through grid search. Future work will focus on a principled method for this. SWAP can be used with other optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad. Parameters used in experiments were obtained through independent grid searches. Momentum and weight decay constants were kept constant for all CIFAR experiments. Stopping accuracy of 100% indicates maximum epochs were used."
}