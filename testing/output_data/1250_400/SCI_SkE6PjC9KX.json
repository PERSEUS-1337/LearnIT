{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions. NPs efficiently fit data with linear complexity and can model a wide range of conditional distributions. However, they suffer from underfitting issues. Attention mechanisms are incorporated into NPs to improve prediction accuracy, training speed, and expand the range of functions that can be modeled. Neural Processes (NPs) improve prediction accuracy, training speed, and expand function modeling range by learning to map input-output pairs to a distribution over regression functions. They offer an efficient method with linear prediction complexity and can capture co-variability in outputs given inputs. Neural Processes (NPs) offer an efficient method to model a distribution over regression functions with linear prediction complexity. They can predict the distribution of an arbitrary target output based on a set of context input-output pairs of any size, allowing them to model data generated from a stochastic process. However, NPs and Gaussian Processes (GPs) have different training regimes, with NPs trained on multiple realizations of a stochastic process and GPs trained on observations from a single realization. NPs may underfit the context set, leading to inaccurate predictive means and overestimated variances. Neural Processes (NPs) may underfit the context set, resulting in inaccurate predictive means and overestimated variances. The encoder aggregates the context set to a fixed-length latent summary, which can act as a bottleneck in learning relevant information for target predictions. The encoder in Neural Processes (NPs) may cause underfitting by acting as a bottleneck in learning relevant information for target predictions. To address this, Attentive Neural Processes (ANPs) use differentiable attention to attend to relevant contexts while maintaining permutation invariance. This mechanism is inspired by Gaussian Processes (GPs) where the kernel measures similarity between points to prevent underfitting. Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by using differentiable attention to focus on relevant contexts for target predictions while maintaining permutation invariance. ANPs show enhanced expressiveness, better reconstruction of contexts, and faster training compared to NPs. The NP model is for regression functions mapping input x to output y, with conditional distributions that can model multiple observed contexts and targets in a way that is invariant to ordering. The NP defines conditional distributions for multiple observed contexts and targets, invariant to ordering. It uses a deterministic function to aggregate context pairs and models likelihood with a Gaussian factorised across targets. The latent variable NP model includes a global latent variable to address uncertainty in predictions. The NP model defines conditional distributions for observed contexts and targets, using a deterministic function to aggregate context pairs and a Gaussian likelihood. It incorporates a global latent variable to address prediction uncertainty, modeled by a factorised Gaussian parametrised by specific properties. The model can be defined using just the deterministic path, just the latent path, or both, with the most expressive setup being the combination of both paths. The NP model learns to reconstruct targets by maximizing ELBO for a subset of contexts and targets using the reparametrisation trick. The model incorporates both deterministic and latent paths, allowing for a wide range of conditional distributions to be learned. The NP model learns a wide family of conditional distributions by randomly choosing contexts and targets at each training iteration. It offers scalability, flexibility, and permutation invariance, but lacks consistency in contexts. Maximum-likelihood learning minimises the KL between conditional distributions. The NP model approximates consistent data-generating processes by minimizing KL divergence. An attention mechanism computes weights for key-value pairs based on a query, allowing for permutation invariance. This mechanism has been successfully applied in various areas of Deep Learning. The attention mechanism computes weights for key-value pairs based on a query, allowing for permutation invariance. This mechanism has been successfully applied in various areas of Deep Learning, such as handwriting generation, neural machine translation, natural language processing, and image modeling. Differentiable addressing mechanisms learned from data have been used in these applications. Multihead attention is a parametrised extension of dot-product attention, where keys, values, and queries are linearly transformed for each head. This architecture allows the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. The multihead architecture in attentive neural processes applies self-attention to context points to compute representations of each (x, y) pair. The self-attention helps model interactions between context points and encode relations between them. Higher order interactions are modeled by stacking self-attention mechanisms. The self-attention mechanism in the model allows each query to focus on relevant context points for prediction, enhancing the representation of interactions between them. The latent path preserves global dependencies in the stochastic process, while the deterministic path captures local structures. The decoder remains unchanged, with a shared context. The attention mechanism in the model enhances interactions between context points for prediction. The latent path captures global dependencies, while the deterministic path models local structures. The decoder is updated with query-specific representation. The computational complexity increases due to self-attention, but most computations are done in parallel. The computational complexity of the NP with attention increases due to self-attention, but most computations are done in parallel. Training time for ANPs remains comparable to NPs, with ANPs learning significantly faster despite being slower at prediction time. The (A)NP learns a stochastic process and should be trained on multiple functions. In this study, realisations are drawn from a data generating stochastic process, with random points selected as targets and contexts to optimize loss. The experiments use a decoder architecture with 8 heads for multihead, focusing on 1D function regression on synthetic GP data. The (A)NPs are trained on data from a Gaussian Process, exploring fixed and randomly varying hyperparameters of the kernel. The number of contexts and targets are randomly chosen at each iteration, with x-values drawn uniformly at random. The use of cross-attention in the deterministic path is explored, without self-attention for this simple 1D data. The number of contexts and targets are randomly chosen at each iteration, with x-values drawn uniformly at random in [-2, 2]. Cross-attention is explored in the deterministic path for 1D data without self-attention. ANP shows faster learning and lower reconstruction error compared to NP, especially with dot product and multihead attention mechanisms. Computational times for Laplace and dot-product ANP are similar to NP, while multihead ANP takes around twice the time. The bottleneck size in NP affects underfitting behavior. The bottleneck size in the deterministic and latent paths of the NP affects underfitting behavior. Increasing the bottleneck size helps achieve better reconstructions, but there is a limit to improvement. Using ANPs has significant benefits over simply increasing the bottleneck size in NPs. Visualizing the learned conditional distribution shows that the predictive mean of the NP underfits the context. Laplace exhibits similar behavior. The learned conditional distribution for a qualitative comparison of attention mechanisms in NPs is visualized in FIG2. The predictive mean of the NP underfits the context, with Laplace attention showing similar behavior. Dot-product attention provides more accurate predictive means for context points. Dot-product attention outperforms Laplace attention due to computed similarities in a learned representation space. Multihead attention appears to improve predictions. The Laplace attention is based on L1 distance in the x-coordinate domain, while dot-product attention outperforms it but displays non-smooth predictions. Multihead attention helps smooth out interpolations and improves reconstruction of contexts and prediction of targets. The (A)NP is more expressive than the NP, as shown in a toy Bayesian Optimization experiment. The ANP is more expressive than the NP and can learn a wider range of functions. A proof-of-concept experiment shows the utility of sampling entire functions from the ANP and accurate context reconstructions. Image data regression is explored using the ANP on MNIST and CelebA datasets with self-attentional layers in the encoder. See Appendices C and D for details and results analysis. The ANP is trained on MNIST and CelebA datasets with self-attentional layers in the encoder. Three different models are compared: NP, Multihead ANP, and Stacked Multihead ANP. Results show Stacked Multihead ANP outperforms NP in accuracy for full image predictions. The Stacked Multihead ANP outperforms the NP in accuracy for full image predictions, providing diverse and accurate reconstructions compared to the NP. Attention helps achieve crisper inpaintings, enhancing the ANP's ability to model less smooth 2D functions. The diversity in faces and digits obtained with different z values supports the claim that z can model the global structure of the image. The model generalizes well even with a small number of context points, showing its ability to predict accurately with limited training data. The Multihead ANP model shows improved context reconstruction error and NLL for target points compared to NP. Stacked self-attention enhances crispness and global coherence in image predictions. Each head of Multihead ANP in CelebA focuses on different regions, with varying attention weights. The Multihead ANP model in CelebA shows different heads focusing on various regions with distinct attention weights. The model can map images from one resolution to another by predicting pixel intensities in a continuous space. The Multihead ANP model in CelebA can map images from one resolution to a higher resolution by predicting pixel intensities in a continuous space. The reconstructions of ANPs are accurate, allowing for reliable mappings between different resolutions. The model trained on 32 \u00d7 32 images can map low resolutions (4 \u00d7 4 or 8 \u00d7 8) to realistic 32 \u00d7 32 outputs with some diversity. The ANP model, trained on 32 \u00d7 32 images, can map low resolutions (4 \u00d7 4 or 8 \u00d7 8) to realistic 32 \u00d7 32 outputs with diversity. It can also map to higher resolutions, such as 256 \u00d7 256, producing sharp high-resolution images with internal representations of features like eyes. The ANP model can map low resolutions to realistic 32 \u00d7 32 outputs with diversity and can also produce sharp high-resolution images up to 256 \u00d7 256, showcasing features like eyes. The flexibility of the ANP in modeling conditional distributions is highlighted through image applications, although it is not intended to replace state-of-the-art algorithms for image inpainting or super-resolution. The use of attention in NPs is motivated by parallels with GP kernels. The work on NPs in Gaussian Processes, Meta-Learning, conditional latent variable models, and Bayesian Learning has been extensively discussed in previous works. Attention in NPs is motivated by parallels with GP kernels, where both measure similarity between points. The use of attention in an embedding space is related to Deep Kernel Learning, but learning is still done in a GP framework by maximizing the marginal likelihood. Comparing the training regimes of GPs and NPs is challenging due to their differences, but one possibility is to learn the GP via the training regime of NPs. Comparing the methods of Gaussian Processes (GPs) and Neural Processes (NPs) is challenging. GPs rely on kernel choice for predictive uncertainties, while NPs learn uncertainties directly from data. GPs have the advantage of consistent stochastic processes with exact closed-form expressions for predictions. Variational Implicit Processes (VIP) are related to NPs, approximating processes and posteriors with GPs. Meta-Learning NPs focus on few-shot learning. Variational Implicit Processes (VIP) are related to Neural Processes (NPs) and approximate processes and posteriors with Gaussian Processes (GPs). Meta-Learning NPs focus on few-shot learning and use attention for tasks in Meta-RL. Few-shot density estimation using attention has also been explored extensively in various works. Few-shot density estimation using attention has been explored in various works, including the Neural Statistician and the Variational Homoencoder. Multitask learning in Gaussian Processes literature has also been addressed. Generative Query Networks are models for spatial prediction, with a special case corresponding to Neural Processes. Generative Query Networks (GQN) are models for spatial prediction that render a scene frame given a viewpoint. Attention is applied to context frames in the task of 3D localization. Augmented Neural Processes (ANPs) improve prediction accuracy by resolving underfitting issues. Future work includes incorporating cross-attention and training ANPs on text data. One way to enhance ANPs is by incorporating cross-attention and introducing a global latent, similar to the Neural Statistician model. ANPs could be trained on text data to fill in missing information stochastically. The Image Transformer (ImT) has similarities with ANPs in how they predict target pixels using context pixels. By replacing the MLP in the decoder of ANPs with self-attention, a model similar to ImT can be created for arbitrary pixel orderings. Adding self-attention in the decoder of ANPs can increase their expressiveness, but the ordering and grouping of targets will become crucial. In contrast to the original ImT, ANPs are planned to have self-attention in the decoder to extend their expressiveness. The targets will influence each other's predictions, making ordering and grouping important. Architectural details of NP and Multihead ANP models for regression experiments are shown in Figure 8. The models use relu non-linearities except for the final layer. Latent path outputs parameterize distributions for z and y, with different functions for each. Multihead cross-attention is used in 1D regression experiments, while a different form is used in 2D regression experiments. The decoder outputs parameters for the distribution of y, using the sigmoid function \u03c3. Multihead cross-attention is used in regression experiments, with self-attention modules stacked in the 2D Image regression experiments. Different kernel hyperparameters are used for fixed and random cases. In the 2D Image regression experiments, stacking more layers did not result in significant improvements. Different kernel hyperparameters were used for fixed and random cases, with a batch size of 16. The Adam Optimiser with a fixed learning rate was used, and one sample of q(z|s C ) was used to estimate the loss. The Multihead ANP model showed closer predictions to the oracle GP compared to the NP model, but still underestimated the predictive variance. In Figure 9, the Multihead ANP model's predictions are closer to the oracle GP compared to the NP model but still underestimate the predictive variance. Variational inference used in learning the ANP may lead to this underestimation. The conditional distributions show non-smooth behavior for dot-product attention, possibly due to collapsing to a local minimum, resulting in good reconstructions but poor interpolations between context points. The attention collapses to a local minimum in learning, leading to good reconstructions but poor interpolations between context points. The KL term in NP loss differs between fixed and random GP kernel hyperparameters, with the model using latents to model uncertainty in the stochastic process. The attention in the model gives a non-zero KL and uses latents to model uncertainty in the stochastic process. ANPs are used for Bayesian optimization by considering previous function evaluations as context points. Results show that NP with multihead attention has the smallest simple regret, approaching the oracle GP. Thompson sampling is used with NP models, showing smallest simple regret for multihead attention compared to ANP and oracle GP. Cumulative regret decreases rapidly for multihead, utilizing previous function evaluations effectively. Under-exploration initially lowers cumulative regret due to smaller uncertainties of ANP away from context. Random pixels in images are selected as targets and contexts for training. MNIST and CelebA datasets use batch size of 16 for training. In 1D experiments, random pixels in images are chosen as targets and contexts for training. MNIST and CelebA datasets use a batch size of 16 for training. The stacked self-attention architecture is similar to the Image Transformer BID21, without Dropout or positional embeddings. The NP overestimates predictive variance, as shown in the plot of standard deviation. The NP overestimates predictive variance in the CelebA dataset, but the Stacked Multihead ANP improves results significantly by reducing uncertainty in reconstructions. Different heads play varying roles in predicting targets, even when disjoint from the context. The Stacked Multihead ANP improves predictive variance in the CelebA dataset by reducing uncertainty in reconstructions. Different heads play varying roles in predicting targets, even when disjoint from the context."
}