{
    "title": "S1HlA-ZAZ",
    "content": "We introduce an end-to-end trained memory system inspired by Kanerva's sparse distributed memory. The memory has a robust distributed reading and writing mechanism and is analytically tractable for optimal compression. Formulated as a hierarchical conditional generative model, it combines top-down memory and bottom-up perception to produce observation codes. Empirical results show improved generative models on Omniglot and CIFAR datasets compared to Differentiable Neural Computers (DNC). Our memory model has greater capacity and is easier to train, addressing the challenge of efficiently using memory in neural networks. Recent work in machine learning has explored new ways to enhance neural networks with fast memory stores, but the efficient use of memory remains a challenge. Different models like DNCs, Matching Networks, and Neural Episodic Controller have different approaches to memory storage, with some storing embeddings directly while others summarize datasets. The goal is to have large memories that can capture details of past experiences efficiently. The Neural Statistician BID7 summarizes datasets by averaging embeddings, potentially losing information. Associative memory architectures like the Hopfield Net BID14 store patterns in low-energy states, but are limited by recurrent connections. The Boltzmann Machine BID1 introduces latent variables but has slow reading and writing mechanisms. Kanerva's sparse distributed memory model BID15 allows fast reads and writes, dissociating capacity from input dimensionality. In this paper, a conditional generative memory model inspired by Kanerva's sparse distributed memory is presented. The model generalizes Kanerva's original model through learnable addresses and reparametrized latent variables. By exploiting the analytic tractability of the memory model, a Bayesian memory update rule is derived to effectively trade-off preserving old content and storing new content. The resulting hierarchical generative model has a memory-dependent prior that quickly adapts to new data, providing top-down knowledge in addition to bottom-up perception. The proposed memory architecture extends the variational autoencoder (VAE) by incorporating an adaptive memory store. It offers a novel way of enriching priors in VAE-like models through adaptive memory, enabling effective online distributed writing for compression and storage of complex data. The model includes a generative model with a memory-dependent prior that adapts quickly to new data, providing top-down knowledge alongside bottom-up perception. The architecture utilizes parameterized multivariate Gaussian distributions for the generative and inference models. The proposed memory architecture extends the VAE by incorporating an adaptive memory store, enriching priors in VAE-like models through adaptive memory. It includes a generative model with a memory-dependent prior that adapts quickly to new data. The model utilizes parameterized multivariate Gaussian distributions for the generative and inference models. The objective of training a VAE is to maximize its log-likelihood by jointly optimizing parameters for a variational lower-bound of the likelihood. The proposed memory architecture extends the VAE by incorporating an adaptive memory store, enriching priors in VAE-like models through adaptive memory. It includes a generative model with a memory-dependent prior that adapts quickly to new data. The model utilizes parameterized multivariate Gaussian distributions for the generative and inference models. The objective of training is the expected conditional log-likelihood. The model introduces the concept of an exchangeable episode and formulates memory-based generative models by maximizing the mutual information between the memory and the episode to store. The proposed memory architecture extends the VAE by incorporating an adaptive memory store, enriching priors in VAE-like models through adaptive memory. It includes a generative model with a memory-dependent prior that adapts quickly to new data. The model utilizes parameterized multivariate Gaussian distributions for the generative and inference models. The objective of training is the expected conditional log-likelihood. The model introduces the concept of an exchangeable episode and formulates memory-based generative models by maximizing the mutual information between the memory and the episode to store. This scenario is a general and principled way of formulating memory-based generative models, with the memory being a random matrix with a matrix variate Gaussian distribution. The proposed memory architecture extends the VAE by incorporating an adaptive memory store, enriching priors in VAE-like models through adaptive memory. It includes a generative model with a memory-dependent prior that adapts quickly to new data. The model utilizes parameterized multivariate Gaussian distributions for the generative and inference models. The distribution BID11 involves matrices R, U, and V to model covariance between rows and columns of M. The addressing variable y t computes weights for memory access, with a learned projection transforming it into a key vector. The weights across rows of M are computed using a vector w t. The proposed memory architecture extends the VAE by incorporating an adaptive memory store with a memory-dependent prior that adapts quickly to new data. It includes a generative model with a learned projection transforming the addressing variable into a key vector. The weights across rows of the memory matrix are computed using a vector. The addressing variable is used to compute weights controlling memory access, and a learned projection transforms it into a key vector. The prior for the code z t has a memory-dependent prior, resulting in a richer marginal distribution due to its dependence on memory and the addressing variable. The proposed memory architecture extends the VAE by incorporating an adaptive memory store with a memory-dependent prior that adapts quickly to new data. The prior for the code z t has a memory-dependent prior: DISPLAYFORM4 whose mean is a linear combination of memory rows, with the noise covariance matrix fixed as an identity matrix by setting \u03c3 2 = 1. This results in a much richer marginal distribution, dependent on memory and the addressing variable DISPLAYFORM5. In the hierarchical model, M is a global latent variable capturing statistics of the entire episode, while local latent variables y t and z t capture local statistics for data x t within an episode. The reading inference model factorizes the approximated posterior distribution using conditional independence, refining the prior distribution with additional evidence from x t. The proposed memory architecture extends the VAE by incorporating an adaptive memory store with a memory-dependent prior that adapts quickly to new data. The posterior distribution q \u03c6 (z t |x t , y t , M ) refines the prior distribution p \u03b8 (z t |y t , M ) with additional evidence from x t. Balancing old and new information is crucial in memory updating, which can be optimized through Bayes' rule. Memory writing is interpreted as inference, computing the posterior distribution of memory p(M |X). Batch and online inference methods are considered for updating memory. The approximated posterior distribution of memory is obtained by using one sample of y t, x t to approximate the intractable integral. The proposed memory architecture extends the VAE by incorporating an adaptive memory store with a memory-dependent prior that adapts quickly to new data. Memory writing is interpreted as inference, computing the posterior distribution of memory p(M |X). The posterior of memory p \u03b8 (M |Y, Z) is analytically tractable, and its parameters R and U can be updated accordingly. The posterior of memory p \u03b8 (M |Y, Z) is analytically tractable, with parameters R and U updated using Bayes' rule in a linear Gaussian model. The prior parameters R 0 and U 0 are trained through back-propagation to learn the general dataset structure, while the posterior adapts to features in each episode. The update rule's main cost is inverting \u03a3 z with O(T 3 ) complexity, but this can be reduced by on-line updating. Updating using the entire episode at once is equivalent to iteratively performing one-sample updates for all observations. To train the model, a variational lower-bound of the conditional likelihood J is optimized, similar to standard VAEs. The complexity of inverting \u03a3 z is O(T 3 ), but can be reduced with on-line updating. Intermediate updates can be done with mini-batches of size between 1 and T. The storage and multiplication of the memory's row-covariance matrix U has a complexity of O(K 2 ), which can be reduced by restricting the covariance to diagonal. Future work includes investigating low-rank approximation of U for better cost-performance balance. To train the model, a variational lower-bound of the conditional likelihood J is optimized, similar to standard VAEs. The model uses a mean-field approximation for memory efficiency and exploits the analytical tractability of the Gaussian distribution for reading and writing operations. The memory learns useful representations without relying on complex addresses, penalizing deviation from the memory-based prior. The model utilizes an iterative reading mechanism similar to Kanerva's sparse distributed memory, improving denoising and sampling by repeatedly feeding back the reconstruction. This process benefits from knowledge about memory, suggesting the use of q \u03c6 (y t |x t , M ) for addressing. The iterative reading mechanism in the model improves denoising and sampling by repeatedly feeding back the reconstruction. It utilizes q \u03c6 (y t |x t , M ) for addressing, approximating intractable posteriors efficiently. Future research will aim to better understand this process. Our model implements local coupling between x t and y t well, allowing iterative sampling to converge to the true posterior q \u03c6 (y t |x t , M ). The model architecture remains consistent across experiments with Omniglot and CIFAR datasets, with variations in filter numbers, memory size, and code size. The Adam optimizer was used with minimal tuning. Experiments report the variational lower bound divided by episode length for comparison with existing models. Initial testing was done on the Omniglot dataset, known for its diverse hand-written character classes. Our model was tested on the Omniglot dataset with 1623 classes and 20 examples per class. We used a 64 \u00d7 100 memory M and a 64 \u00d7 50 address matrix A. Each \"episode\" consisted of 32 randomly sampled images without class labels. The variational lower bound was optimized using Adam with a learning rate of 1 \u00d7 10 \u22124. We also tested our model on the CIFAR dataset, using convolutional coders with 32 features. Our model was tested on the Omniglot dataset with 1623 classes and 20 examples per class, using a 64 \u00d7 100 memory M and a 64 \u00d7 50 address matrix A. Each \"episode\" consisted of 32 randomly sampled images without class labels. The variational lower bound was optimized using Adam with a learning rate of 1 \u00d7 10 \u22124. Testing on the CIFAR dataset involved using convolutional coders with 32 features, a code size of 200, and a 128 \u00d7 200 memory with a 128 \u00d7 50 address matrix. The model was tested in an unsupervised setting, with comparisons made to a baseline VAE model. The Kanerva Machine outperformed the VAE in terms of negative variational lower bound, reconstruction loss, and KL-divergence. The model learned to use memory effectively, leading to a more informative prior. This rich prior was achieved with a lower KL-divergence for y t compared to the VAE. Our VAE model learned to use memory effectively, resulting in a rich prior with a near-zero KL-divergence for z t. The reduction in KL-divergence was crucial for improving sample quality, leading to a negative log-likelihood of \u2264 112.7 at the end of training. This performance is comparable to IWAE training results but inferior to the Kanerva Machine's conditional NLL of 68.3. The VAE model achieved a negative log-likelihood of \u2264 112.7 at the end of training, showing improvement in sample quality. The model's performance is comparable to IWAE training but not as good as the Kanerva Machine's conditional NLL of 68.3. The incorporation of adaptive memory into generative models led to a significant reduction in KL-divergence and improved results. The reconstruction of inputs and weights in memory slots is shown in Figure 3, with denoising through iterative reading. The model can generate batches of images with many classes and samples, improving sample quality with each iteration. Samples from the model reflect the conditioning patterns' statistics, with minimal changes after the 6th iteration. The VAE and Kanerva Machine samples were compared in FIG1, showing improved sample quality in consecutive iterations for the Kanerva Machine. The approach did not apply to VAEs due to their lack of structure. Samples from CIFAR dataset were also compared, with VAE samples being blurred and lacking structure, while Kanerva Machine samples had clear local structures. The Kanerva Machine samples show clear local structures in consecutive iterations, unlike the blurred and structure-lacking samples from the VAE. The model can recover original images from corrupted inputs through iterative reading, showcasing interpretability of internal representations in memory. Linear interpolations between address weights are meaningful, allowing for the examination of internal representations. The text chunk discusses the meaningful linear interpolations between address weights in memory representations. It compares the training curves of DNC and Kanerva Machine, highlighting differences in sensitivity to initialization and error rates. Additionally, it compares the test variational lower-bounds of DNC and Kanerva Machine in different scenarios. The text chunk compares the Differentiable Neural Computer (DNC) and Kanerva Machine in terms of training performance and sensitivity to hyperparameters. The DNC showed sensitivity to hyperparameters and random initialization, while the Kanerva Machine was more robust. The Kanerva Machine is easier to train compared to the DNC, as it performs well with batch sizes between 8 and 64 and learning rates between 3 \u00d7 10 \u22125 and 3 \u00d7 10 \u22124. It trained fastest with batch size 16 and learning rate 1 \u00d7 10 \u22124, converging below 70 test loss with all configurations. The model's capacity was analyzed by comparing it to the DNC in terms of storing and retrieving patterns from large episodes. Both models can exploit redundancy in episodes with fewer classes, resulting in lower reconstruction losses. The Kanerva Machine is a memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model. It generalizes well to larger episodes and outperforms the DNC in terms of variational lower-bound. The model removes the assumption of a uniform data distribution by training a generative model to learn the observed data distribution. This allows for retrieving unseen patterns through sampling from memory. Our model generalizes Kanerva's memory model to continuous, non-uniform data while maintaining an analytic form of Bayesian inference. It integrates with deep neural networks and quickly adapts to new data, unlike other models that do not update memory following learning. Our model efficiently updates memory by storing information in a compressed form using statistical regularity in images, learned addresses, and Bayes' rule. Unlike other models, our model quickly adapts to new data and integrates with deep neural networks. Our model efficiently updates memory by taking advantage of statistical regularity in images, learned addresses, and Bayes' rule. It employs an exact Bayes' update-rule without compromising the flexibility and expressive power of neural networks. The model combines classical statistical models and neural networks for novel memory models in machine learning. Kanerva's sparse distributed memory is reviewed, characterized by distributed reading and writing operations with fixed addresses pointing to modifiable memory. Kanerva's memory model features fixed addresses pointing to modifiable memory, with inputs compared through Hamming distance. Addresses are selected based on a threshold, and patterns are stored by adding them to memory. Selected memory contents are summed for reading. The memory model by Kanerva involves selecting addresses based on Hamming distance, storing patterns by adding them to memory, and summing selected memory contents for reading. Kanerva's model operates sparsely and distributedly, allowing for correct retrieval even with over-written content. However, the model's application is limited by the assumption of a uniform and binary data distribution. The Kanerva memory model relies on Hamming distance for address selection and sparse, distributed operations for pattern storage and retrieval. However, its application is hindered by the assumption of a uniform and binary data distribution. In contrast, our model uses a convolutional encoder to convert input images into 2C embedding vectors, allowing for more efficient representation in high-level neural network implementations optimized for floating-point numbers. The model consists of 3 consecutive blocks with convolutional layers and ResNet blocks, followed by a linear projection to a 2C dimensional vector. Adding noise to the input helps stabilize training. Different likelihood functions are used for different datasets. The model is compared with a differentiable neural computer using the same interface for memory. The model uses Bernoulli likelihood for Omniglot and Gaussian likelihood for CIFAR. To prevent Gaussian likelihood collapsing, uniform noise is added to CIFAR images during training. The differentiable neural computer (DNC) is wrapped with the same interface as Kanerva memory for fair comparison. DNC's reading and writing stages are separated in experiments to avoid interference with memory. A 2-layer MLP with ReLU nonlinearity is used as the controller instead of LSTM. The DNC model uses a 2-layer MLP controller with 200 hidden neurons and ReLU nonlinearity to avoid interference with its external memory. To prevent confusion in auto-encoding, the controller output bypassing the memory is removed. The focus is on memory performance, with experiments showing the importance of covariance between memory rows. The DNC model uses a 2-layer MLP controller with 200 hidden neurons and ReLU nonlinearity. Experiments highlight the importance of covariance between memory rows. Different models with full and diagonal covariance matrices show varying test loss patterns. The bottom-up stream in the model compensates for memory by directly sampling z t from p \u03b8 (z t |y t , M ). Training on CIFAR data shows similar trends in negative variational lower bound and KL-divergence as in Omniglot training. The DNC model uses a 2-layer MLP controller with 200 hidden neurons and ReLU nonlinearity. Experiments highlight the importance of covariance between memory rows. Training on CIFAR data shows similar trends in negative variational lower bound and KL-divergence as in Omniglot training. The difference in KL-divergence significantly influences sample quality, with the advantage of the Kanerva Machine over the VAE increasing. The model described in the paper works well using samples from q \u03c6 (z t |x t ) for writing to the memory and mean-field approximation during reading. The DNC model uses a 2-layer MLP controller with 200 hidden neurons and ReLU nonlinearity. The model described in the paper works well using samples from q \u03c6 (z t |x t ) for writing to the memory and mean-field approximation during reading. Properties of matrix variate Gaussian distribution are used to update rules in eq. 9 to 11. An alternative method is described that fully exploits the analytic tractability of the Gaussian distribution for reading and writing to the memory."
}