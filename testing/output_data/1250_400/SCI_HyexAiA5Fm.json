{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are neural generative models successful in modeling high-dimensional continuous measures. A scalable method for unbalanced optimal transport (OT) is presented using the generative-adversarial framework. The problem involves learning a transport map and scaling factor to push a source measure to a target measure in a cost-optimal way. The algorithm for solving this problem is based on stochastic alternating gradient updates, similar to GANs, and is applied to population modeling. The problem of unbalanced optimal transport involves finding a cost-optimal way to transform one measure to another using mass variation and transport. Classical optimal transport focuses on pushing a source to a target distribution without allowing for mass variations, while modern approaches use the Kantorovich formulation for optimal probabilistic coupling between measures. Regularizing the objective with an entropy term allows for more efficient solution using the Sinkhorn algorithm. Stochastic methods based on the dual objective have been proposed for the continuous setting. Optimal transport methods involve coupling measures efficiently using linear programming and entropy regularization. Stochastic methods based on the dual objective have been proposed for continuous settings. Transport maps can be learned using generative models like GANs for applications in image translation, natural language translation, domain adaptation, and biological data integration. Optimal transport methods involve coupling measures efficiently using linear programming and entropy regularization. Stochastic methods based on the dual objective have been proposed for continuous settings. GANs have been used for tasks like image translation, natural language translation, domain adaptation, and biological data integration. Variants of GANs have been employed to tackle translation, domain adaptation, and biological data integration challenges. These methods aim to enforce correspondence between original and transported samples, but struggle with handling mass variation. Extensions of optimal transport theory have been proposed to address unbalanced masses in measures. Scaling algorithms have been developed to approximate solutions for unbalanced optimal transport. The theory of optimal transport has been extended to handle unbalanced masses, with scaling algorithms developed for approximating solutions. These algorithms have been used in various applications such as computer graphics, tumor growth modeling, and computational biology. However, current methods cannot explicitly model mass variation between continuous measures. Inspired by the success of GANs, a novel framework for unbalanced optimal transport that directly models mass variation is presented. The novel framework presented aims to address the limitations of current methods in modeling mass variation for unbalanced optimal transport. It proposes a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport. The methodology is demonstrated using datasets like MNIST and USPS handwritten digits. The methodology presented offers a scalable solution for solving the optimal-entropy transport problem, utilizing convex conjugate representation of divergences. It is demonstrated with various datasets including MNIST, USPS handwritten digits, CelebA, and a single-cell RNA-seq dataset from zebrafish embryogenesis. Additionally, a new scalable method is proposed in the Appendix for unbalanced optimal transport in the continuous setting. Optimal transport (OT) involves transporting measures in a cost-optimal way, with Monge formulating it as a search over deterministic transport maps. The Kantorovich OT problem is a convex relaxation of the Monge problem, formulating OT as a search over probabilistic transport plans. This approach considers conditional probability distributions as stochastic maps from X to Y, offering a \"one-to-many\" version of the deterministic transport maps. The Kantorovich OT problem is a convex relaxation of the Monge problem, representing OT as a search over probabilistic transport plans. Introducing entropic regularization simplifies the dual optimization problem, solvable efficiently with the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans handling continuous measures. Unbalanced OT formulations extending classical OT to handle mass variation exist, based on optimal-entropy transport formulations obtained by relaxing marginal constraints using divergences. In this section, a new algorithm for unbalanced optimal transport is proposed, allowing for mass variation between continuous measures. The algorithm directly models mass variation and can be applied in high-dimensional spaces, filling a gap in practical algorithms for unbalanced optimal transport. The proposed algorithm for unbalanced optimal transport allows for mass variation between continuous measures in high-dimensional spaces. It directly models mass variation and aims to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. The algorithm for unbalanced optimal transport models mass variation between measures in high-dimensional spaces. It aims to learn a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal way. This approach is more suitable for practical problems, such as in cell biology, where one cell can give rise to multiple cells in a target population. The unbalanced Monge optimal transport model uses a stochastic map and scaling factor to transform a source measure to a target measure. It can address class imbalances by adjusting the scaling factor to balance the classes. The unbalanced Monge optimal transport model addresses class imbalances by adjusting a scaling factor to balance classes. The optimization challenge lies in satisfying constraints, leading to a relaxation using a divergence penalty. This relaxation transforms the problem into the Monge-like version of optimal-entropy transport, specifying a joint measure gamma. The main difference lies in the search space, as not all joint measures can be specified by the choice of parameters. The optimization challenge in unbalanced Monge optimal transport model is addressed by adjusting a scaling factor to balance classes. Constraints are relaxed using a divergence penalty, transforming the problem into the Monge-like version of optimal-entropy transport. The search space differs as not all joint measures can be specified by parameters like (T, \u03be). The support of \u03b3 is restricted to supp(\u00b5) \u00d7 Y in the asymmetric Monge formulation. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem under certain conditions. The joint measure specified by a minimizer of L \u03b6 k \u03c8 converges weakly to the joint measure specified by a minimizer of L(\u00b5, \u03bd) if L(\u00b5, \u03bd) < \u221e. The transport map and scaling factor can be learned using stochastic gradient methods in unbalanced Monge optimal transport. The optimization procedure involves minimizing the divergence term with an adversary function, parameterized with neural networks, in an adversarial game setting similar to GAN training. The optimization procedure involves minimizing the divergence term with an adversary function, parameterized with neural networks in an adversarial game setting similar to GAN training. The objective is to transport points from X to Y by generating T(x, z) and determining the importance weight of each point. Cost functions encourage finding the most cost-efficient strategy. Further practical considerations for implementation and training are discussed in the Appendix. The probabilistic Monge-like formulation is similar to the Kantorovich-like entropy-transport problem in theory but results in different numerical methods in practice. Algorithm 1 learns a transport map and scaling factor parameterized by neural networks for scalable optimization. The non-convex optimization problem may not guarantee the global optimum, unlike the convex optimization problem solved by the scaling algorithm of BID8. The proposed stochastic method in the Appendix generalizes the approach for handling transport between continuous measures and overcomes scalability limitations. However, the output is in the form of a less interpretable dual solution compared to Algorithm 1. It is unclear how to obtain a scaling factor or a stochastic transport map for generating samples outside the target dataset. The proposed method in the Appendix generalizes the approach for handling transport between continuous measures and overcomes scalability limitations. However, the output is in the form of a less interpretable dual solution compared to Algorithm 1. It is unclear how to obtain a scaling factor or a stochastic transport map for generating samples outside the target dataset. The problem of learning a scaling factor that \"balances\" measures \u00b5 and \u03bd also arises in causal inference, where the goal is to eliminate selection biases in the inference of treatment effects. Algorithm 1 performs unbalanced optimal transport on MNIST data to eliminate selection biases in treatment effect inference. It applies to modified MNIST datasets with class distribution changes to mimic population shifts. The class imbalance between source and target datasets is reflected in Algorithm 1, which models growth or decline of different classes in a population. The scaling factor learned by Algorithm 1 can be used to transport the source distribution to the target distribution, showing reweighting during unbalanced optimal transport. This approach is applied from the MNIST dataset to the USPS dataset to model the evolution of distributions. The unbalanced optimal transport from the MNIST dataset to the USPS dataset is illustrated in a schematic. Using Algorithm 1, the evolution of the MNIST distribution to the USPS distribution is modeled, with the Euclidean distance as the transport cost. The unbalanced transport is visualized in FIG1, showing how MNIST digits change in prominence in the USPS dataset. Despite the imperfect measure of correspondence, many MNIST digits retain their likeness during the transport. The study compared MNIST and USPS datasets using unbalanced OT model. MNIST digits with higher scaling factors appeared brighter and covered more pixels, similar to USPS digits. Algorithm 1 was applied on CelebA dataset to model transformation from young to aged faces using VAE. The study utilized a variational autoencoder on the CelebA dataset to model the transformation of a population from young to aged faces using unbalanced optimal transport. The transported faces generally retained key features, with some exceptions like gender swaps. Young faces with higher scaling factors were found to be significantly enriched for males. The study used a variational autoencoder on the CelebA dataset to model the transformation of young to aged faces, revealing a gender imbalance with more males in aged faces. The research also discussed lineage tracing in zebrafish embryogenesis, highlighting unbalanced distributions between different developmental stages. In zebrafish embryogenesis, lineage tracing between different developmental stages is of interest. Algorithm 1 was applied to single-cell gene expression data from blastulation to gastrulation stages. Cells with higher scaling factors were enriched for genes related to mesoderm development. The blastula stage cells with higher scaling factors were compared to the rest using gene expression analysis, revealing upregulated genes associated with mesoderm development. A stochastic method for unbalanced OT based on the regularized dual formulation is presented, which includes a strongly convex regularization term for encouraging high entropy transport plans. The regularized dual formulation for unbalanced optimal transport includes a strongly convex regularization term that encourages high entropy transport plans. The dual problem is solved using functions u and v, which can be parameterized with neural networks for optimization using stochastic gradient descent. This approach is a generalization of classical optimal transport to unbalanced scenarios. The dual solution learned from Algorithm 2 in the context of unbalanced optimal transport can be used to reconstruct the primal solution, indicating the amount of mass transported between points in X and Y. The marginals of the transport map are not necessarily \u00b5 and \u03bd, allowing for implicit mass variation in the problem. The primal solution \u03b3 * can be reconstructed based on the dual solution learned from Algorithm 2 in unbalanced optimal transport. \u03b3 * indicates the mass transported between points in X and Y, with marginals that may not be \u00b5 and \u03bd, allowing for implicit mass variation in the problem. Additionally, an \"averaged\" deterministic mapping from X to Y can be learned using \u03b3 * through the barycentric projection T : X \u2192 Y. The primal solution \u03b3 * from Algorithm 2 in unbalanced optimal transport can be reconstructed to indicate mass transported between points in X and Y, with marginals that may not be \u00b5 and \u03bd. The formulations are equivalent when the search space is restricted to joint measures specified by (T, \u03be). Lemma 3.3 formalizes the relation between the formulations, showing that L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) and L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) for any solution (T, \u03be) and \u03b3. The disintegration theorem states that there exists a family of measurable functions that satisfy certain relations, leading to the conclusion that the optimal entropy-transport formulations are equivalent. This implies that the optimal entropy-transport results can be derived from the analysis of (6). The proof of Proposition B.1 shows that under certain conditions, the joint measure specified by any minimizer of the optimal entropy-transport is unique. This uniqueness is guaranteed by the existence of minimizers for both W c1,c2,\u03c8 (\u00b5, \u03bd) and L \u03c8 (\u00b5, \u03bd), with the marginals uniquely determined for any solution \u03b3 of W c1,c2,\u03c8 (\u00b5, \u03bd) when \u03c8 \u221e = \u221e. Theorem 3.3 of BID27 states that W c1,c2,\u03c8 (\u00b5, \u03bd) has a minimizer, which implies the existence of a minimizer for L \u03c8 (\u00b5, \u03bd). Uniqueness is guaranteed when \u03c8 \u221e = \u221e, with marginals uniquely determined for any solution \u03b3 of W c1,c2,\u03c8 (\u00b5, \u03bd). The product measure generated by the minimizers of L \u03c8 (\u00b5, \u03bd) is unique. L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd for certain cost functions and divergences. Theoretical analysis shows that for an appropriate choice of divergence penalty, solutions of the relaxed problem converge to solutions of the original problem. The convergence is proven through pointwise convergence of the equality constraint and boundedness of the sequence of minimizers. The convergence of solutions from the relaxed problem to the original problem is proven through pointwise convergence of the equality constraint and boundedness of the sequence of minimizers. The convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem.\u03b3 is a minimizer ofW c1,c2,\u03b9= (\u00b5, \u03bd) as shown by the proof of Lemma 3.3. In this section, the convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem. Lemma B.2 states conditions for non-negative finite measures P, Q, and provides a simple proof. The optimal f over the support of Q is obtained when dP dQ belongs to the subdifferential of \u03c8 * (f ). The text discusses the proof of Lemma B.2 and the choice of cost functions for a min-max problem in generative modeling. It outlines conditions for well-posed problems and suggests using specific cost functions for transport and mass adjustment. The text discusses the choice of cost functions for a min-max problem in generative modeling, emphasizing the importance of selecting a convex function that vanishes at 1. It also mentions how any \u03c8-divergence can be used to train generative models to match a generated distribution to a true data distribution. The text discusses the importance of selecting a convex function that vanishes at 1 for cost functions in generative modeling. It explains how \u03c8-divergence can be used to train generative models to match distributions, with a specific example illustrating divergence minimization when P and Q are not probability measures. The text discusses the importance of selecting a convex function that vanishes at 1 for cost functions in generative modeling. It explains how \u03c8-divergence can be used to train generative models to match distributions, with a specific example illustrating divergence minimization when P and Q are not probability measures. The choice of activation layers for neural networks is also discussed to ensure mapping to the correct range. In practice, the choice of activation layers for neural networks is crucial to ensure mapping to the correct range. For example, using a final layer with a sigmoid function can map pixel brightness to the range (0, 1), while a softplus function can map scaling factor weight to the range (0, \u221e)."
}