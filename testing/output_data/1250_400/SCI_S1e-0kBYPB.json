{
    "title": "S1e-0kBYPB",
    "content": "In this work, the authors highlight issues with current explanatory methods for AI systems, particularly in explaining decisions of black-box models like neural networks. They point out that different explanatory perspectives lead to varying instance-wise explanations and that current explainers are mainly validated on simple models, not real-world neural networks. To address this, they introduce a verification framework based on a complex neural network architecture trained on a real-world task, providing guarantees on its inner workings. The authors introduce a verification framework for explanatory methods under the feature-selection perspective, based on a non-trivial neural network architecture trained on a real-world task. They aim for this framework to provide an off-the-shelf evaluation when the feature-selection perspective on explanations is needed. Various post-hoc explanatory methods have been developed to explain black-box machine learning models, with two widely used perspectives: feature-additivity and feature-selection. The curr_chunk discusses two widely used perspectives on explanations in machine learning models: feature-additivity and feature-selection. These perspectives lead to fundamentally different explanations, especially when explaining predictions on a single input. The comparison of explanatory methods adhering to different perspectives may not be coherent due to their different explanation targets. The strengths and limitations of these two perspectives are also discussed. The curr_chunk discusses the challenges of evaluating explanatory methods for neural networks, especially when the target model has less dramatic biases. It highlights the difficulty of understanding the decision-making process of neural networks and the need to assume that target models behave reasonably when evaluating explainers. The curr_chunk proposes a framework for evaluating explanatory methods for neural networks by generating evaluation tests under the feature-selection perspective. It addresses the challenge of assuming target models behave reasonably and highlights the unreliability of penalizing explainers for pointing to insignificant tokens. The curr_chunk introduces a framework for generating evaluation tests for explanatory methods in neural networks. It focuses on identifying tokens that have zero contribution to the model's prediction and testing explainers for critical failures. The framework is applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. It is emphasized that the test is not sufficient for general conclusions about explainers' performance. The framework introduced focuses on generating evaluation tests for explainers in neural networks, testing for critical failures. It evaluates L2X and popular explainers like LIME and SHAP, finding that the latter perform better on the test. The study compares LIME and SHAP explainers with L2X, finding that the former perform better. The error rates of these methods are highlighted, showing potential failures in explanations. The evaluation test will be released for community use, with a generic methodology that can be applied to other research areas. The evaluation test for explanatory methods under the feature-selection perspective is generic and can be applied to various research areas. Feature-based explainers can be categorized into feature-additive and feature-selective types. Other types include example-based and human-level explanations. In this work, the focus is on verifying feature-based explainers, which are the majority of current works. Evaluations commonly performed include testing explainers on interpretable target models like linear regression and decision trees. The faithfulness of explainers to the target model is still an open question. Three common types of evaluations for explainers include testing on interpretable target models like linear regression and decision trees, synthetic setups with controlled important features, and assuming a reasonable behavior for high-performing target models. The evaluation of explainers involves testing on interpretable target models, synthetic setups with controlled features, and assuming reasonable behavior for high-performing models. Evaluations include assessing if explanations align with model predictions and testing if explanations help humans predict model behavior accurately. Our evaluation framework for explainers is fully automatic and focuses on a non-trivial neural network model with guaranteed inner-workings. It differs from previous methods that require significant human effort and expense. The evaluation framework for explainers focuses on a non-trivial neural network model with guaranteed inner-workings. It is more challenging than previous methods and requires a stronger fidelity of the explainer to the target model. Explanatory methods adhere to the perspective of feature-additivity, where the explanation of a prediction consists of contributions from each feature that approximate the prediction. Many methods follow this perspective, such as LIME and methods unified by Lundberg & Lee. The explanation of a prediction involves contributions from each feature that approximate the prediction. Methods like LIME and those unified by Lundberg & Lee adhere to this feature-additivity perspective. Shapley values from game theory provide feature contributions by considering all subsets of features in x that do not include the feature i. The contribution of each feature is an average over a neighborhood of the instance, with the choice of neighborhood being critical according to Laugel et al. (2018). The explanation of a prediction involves contributions from each feature that approximate the prediction. Methods like LIME and those unified by Lundberg & Lee adhere to this feature-additivity perspective. Shapley values from game theory provide feature contributions by considering all subsets of features in x that do not include the feature i. The contribution of each feature is an average over a neighborhood of the instance, with the choice of neighborhood being critical according to Laugel et al. (2018). However, the choice of neighborhood for perturbations is still an open question, as different perspectives exist on how to select the most relevant features for explanation. L2X (Chen et al., 2018) maximizes mutual information between S(x) and prediction, assuming known important features per instance, which is not always true. Instance-wise explanations for sentiment analysis regression model provided in Figure 1 to understand differences in perspectives. Real-world neural networks may heavily rely on specific tokens in input, not always indicators for correct target class. Neural networks may heavily rely on specific tokens in input, not always indicators for the correct target class. Feature-additive and feature-selective perspectives provide different explanations of model behavior. The feature-additive and feature-selective perspectives offer different explanations of a model's behavior, focusing on neighborhood instances and pointwise features, respectively. The ranking of important features can vary between instances, highlighting the need to choose the appropriate perspective based on the use-case. A verification framework for the feature-selective perspective is proposed, leveraging the RCNN model architecture. In the paper, a verification framework is proposed for the feature-selection perspective of instance-wise explanations, utilizing the RCNN model architecture introduced by Lei et al. (2016). The framework involves pruning the dataset to identify irrelevant and relevant features for each datapoint, and introducing metrics to evaluate the ranking of these features by explainers. The RCNN model consists of a generator and an encoder, both implemented with recurrent convolutional neural networks. The RCNN model, introduced by Lei et al. (2016), comprises a generator and an encoder implemented with recurrent convolutional neural networks. The generator selects a subset of tokens from input text x using a Bernoulli distribution, which is then passed to the encoder for making predictions. Training involves joint training of the generator and encoder with supervision only on the final prediction, using regularizers to encourage the selection of shorter sub-phrases and fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. The RCNN model, introduced by Lei et al. (2016), involves a generator and an encoder that select tokens from input text using a Bernoulli distribution. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. The model aims to eliminate instances that contain \"handshakes\" where non-selected tokens have zero contribution to the final prediction. The RCNN model aims to eliminate instances with \"handshakes\" where non-selected tokens have zero contribution to the prediction. The proof is in Appendix B. In an example, selecting \"very\" yields a score of 1, but selecting only \"very\" results in a score of 0.5. Non-selected tokens are considered irrelevant or zero-contribution. Retaining only relevant tokens helps prune the dataset. The dataset is pruned to retain instances where selected tokens are clearly relevant for the prediction, ensuring non-selected tokens have no contribution. Noise tokens like \"the\" or \"a\" may be selected to maintain a contiguous sequence, but at least one selected token must be relevant to avoid penalizing explainers. The dataset is pruned to ensure that selected tokens are clearly relevant for the prediction, distinguishing between noise and zero-contribution features. Tokens are further partitioned into clearly relevant tokens and those for which relevance is uncertain. The dataset is pruned to ensure selected tokens are relevant for prediction, distinguishing between noise and zero-contribution features. Tokens are further partitioned into clearly relevant and uncertain relevance. The procedure does not provide an explainer but guarantees that all tokens in N x are ranked lower than any token in SR x. The first most important token has to be in S x. The text discusses the evaluation of explainers that rank features, focusing on error metrics related to the importance of tokens in instances. The explainers are evaluated based on the percentage of instances where the most important token is not selected, instances where non-selected tokens are ranked higher than relevant tokens, and the average number of non-selected tokens ranked higher than relevant tokens. The study evaluates explainers that rank features based on error metrics related to token importance. Metrics include instances where non-selected tokens are ranked higher than relevant ones, the percentage of errors in explanations, and the number of zero-contribution features ranked higher than relevant ones. The framework is applied to the RCNN model trained on the BeerAdvocate corpus, consisting of human-generated beer reviews with three aspects: appearance, aroma, and palate. The study analyzed human-generated beer reviews with three aspects: appearance, aroma, and palate. The RCNN model was used to predict ratings rescaled between 0 and 1. Three separate RCNNs were trained for each aspect independently. A threshold of 0.1 was chosen to identify clearly relevant tokens for prediction. Various statistics of the datasets were provided in the appendix. The study analyzed human-generated beer reviews using the RCNN model to predict ratings rescaled between 0 and 1. A threshold of 0.1 was chosen to identify clearly relevant tokens for prediction. Various statistics of the datasets were provided in the appendix, including average lengths of reviews and percentages of eliminated datapoints for evaluation. Three popular explainers, LIME, SHAP, and L2X, were tested with default settings for text explanations. The study tested LIME, SHAP, and L2X explainers with default settings for text explanations. LIME and SHAP outperformed L2X in most metrics, despite L2X being a feature-selection explainer. L2X's limitation is the need to know the number of important features per instance, which is usually unknown in practice. The study used the average number of tokens highlighted by human annotators as a proxy for this number. In a study comparing LIME, SHAP, and L2X explainers for text explanations, LIME and SHAP outperformed L2X in most metrics. L2X's limitation is the need to know the number of important features per instance, which is usually unknown in practice. The study used the average number of tokens highlighted by human annotators as a proxy for this number. L2X ranked zero-contribution tokens higher than relevant features in some cases, with SHAP performing better in this aspect. The study compared LIME, SHAP, and L2X explainers for text explanations. SHAP placed fewer zero-contribution tokens ahead of relevant ones compared to L2X. The explainers tended to attribute importance to nonselected tokens. The study compared LIME, SHAP, and L2X explainers for text explanations. Both explainers tended to attribute importance to nonselected tokens. L2X prioritized \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as most important tokens, while LIME and SHAP ranked \"mouthfeel\" and \"lacing\" from N x as the top two. The evaluation test introduced offers guarantees on the behavior of real-world neural networks. The curr_chunk discusses an evaluation test for post-hoc explanatory methods in neural networks, highlighting errors in popular explanatory methods and the potential for adaptation to other tasks. The core algorithm in current explainers is domain-agnostic. The evaluation test for post-hoc explanatory methods in neural networks identifies errors in popular explainers and emphasizes their domain-agnostic nature. Statistics of the dataset are provided in Table 2, showing the number of instances retained and average lengths of reviews. Percentages of eliminated instances and datapoints are also discussed. The text discusses the evaluation test for post-hoc explanatory methods in neural networks, focusing on identifying errors in explainers. It explains the process of determining potential handshakes in the dataset and how the model's prediction is affected by eliminating certain tokens. The goal is to determine if there was a handshake in the instance based on the model's prediction when eliminating non-selected tokens. The text discusses evaluating post-hoc explanatory methods in neural networks by identifying errors in explainers. It explains determining potential handshakes in the dataset and how model predictions are affected by eliminating tokens to detect handshakes. The goal is to determine if there was a handshake in the instance based on the model's prediction when non-selected tokens are removed. The curr_chunk describes a beer with a dark fizzy yellow color, fruity aroma, and smooth taste with slight warming. The beer is better than most American lagers and comes in a nice brown \"grolsch\" like bottle. The beer in the curr_chunk is described as having a dark fizzy yellow color, fruity aroma, and smooth taste with slight warming. It is better than most American lagers and comes in a nice brown \"grolsch\" like bottle."
}