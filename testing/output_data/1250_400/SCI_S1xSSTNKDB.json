{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a balanced face image dataset with 108,501 images representing 7 race groups was created. Models trained on this dataset showed improved accuracy across different race and gender groups. Commercial computer vision APIs were also compared for balanced accuracy. The dataset is more accurate on novel datasets and consistent across race and gender groups. Various large-scale face image datasets have fostered research in automated face detection, alignment, recognition, generation, modification, and attribute classification. These systems have been applied in security, medicine, education, and social sciences. Existing public face datasets are biased towards Caucasian faces, with other races like Latino being underrepresented. This bias can lead to models that do not apply to all subpopulations and raise ethical concerns about fairness in automated systems. This issue has been highlighted in recent machine learning and AI literature, with criticism directed at commercial computer vision systems for their asymmetric accuracy across different groups. The existing bias in public face datasets towards Caucasian faces has raised ethical concerns about fairness in automated systems. Commercial computer vision systems have been criticized for their asymmetric accuracy across different demographic groups, with better performance on male and light faces. To address this bias, a novel face dataset with a balanced race composition of 108,501 facial images has been proposed. The proposed novel face dataset contains 108,501 facial images with a balanced race composition, including White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino groups. The dataset aims to mitigate race bias in existing face datasets and shows improved performance on novel data across racial groups. The new dataset includes diverse racial groups and aims to reduce bias in existing face datasets. It enhances performance on novel data across different races, expanding the applicability of computer vision methods to various fields. Face attribute recognition is crucial for various computer vision tasks and must perform well across different gender and race groups to avoid bias. Instances of racial bias in technology, such as Google Photos mistaking African American faces for Gorillas, highlight the importance of ensuring fairness in machine learning systems. The public trust in the machine learning and computer vision research community has been affected by incidents of racial bias, such as Google Photos mistaking African American faces for Gorillas and Nikon's cameras prompting Asian users about blinking. These incidents often lead to service termination or dropping sensitive output categories. Commercial providers have stopped offering race classifiers, and face attribute recognition is used for demographic surveys in marketing and social science research to understand human social behaviors and demographic backgrounds. Social scientists use off-the-shelf tools and commercial services to infer demographic attributes from images and analyze behaviors, including demographic analyses of social media users. Social scientists utilize off-the-shelf tools and commercial services to analyze demographic attributes and behaviors from images of people. Algorithmic fairness and dataset biases are increasingly important in AI and machine learning communities, focusing on producing fair outcomes regardless of race or gender. In this paper, the focus is on balanced accuracy in attribute classification, specifically regarding race and gender independence. Research in fairness aims to ensure fair outcomes regardless of protected attributes like race or gender. Studies in algorithmic fairness have focused on auditing bias in datasets, improving datasets, and designing better algorithms. The main task in this paper is gender classification from facial images, highlighting biases in commercial gender classification systems. Biased results may stem from skewed datasets or underlying associations between scene and race. The study focuses on gender classification from facial images and addresses biases in commercial systems, particularly towards dark-skinned females. Biases may arise from skewed datasets or associations between scene and race. The paper aims to mitigate existing limitations by collecting more diverse face images from non-White race groups, improving generalization performance to novel datasets. The dataset includes Southeast Asian and Middle Eastern races, enhancing representation in large-scale face image datasets. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is defined based on physical traits, while ethnicity is based on cultural similarities. Latino is considered a race in this dataset, and subgroups like Middle Eastern and Southeast Asian are further divided. The aim is to provide more diverse face images from non-White race groups to mitigate biases in commercial systems. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Latino is considered a race, not just an ethnicity, based on facial appearance. Subgroups like Middle Eastern and Southeast Asian are also included. Skin color is used as a proxy for racial or ethnicity grouping in some studies, but it has limitations due to lighting conditions affecting skin color. The Pilot Parliaments Benchmark dataset used skin color as a proxy for racial or ethnicity grouping, but it has limitations due to lighting conditions affecting skin color. Skin color is one-dimensional and cannot differentiate between many race groups, so race is explicitly annotated by human annotators. Skin color is also used, measured by Individual Typology Angle, to complement race categorization. The use of race and skin color in face datasets is discussed, with a focus on minimizing selection bias and maximizing diversity. Existing datasets sourced from public figures may have biases, such as age and attractiveness, leading to quality bias. Web search queries for specific demographics may also result in stereotypical faces. The goal is to reduce filtering bias and ensure diversity and coverage in dataset selection. The dataset was created by detecting faces from the Yahoo YFCC100M dataset without preselection to minimize selection bias and maximize diversity. The dataset is smaller but more balanced on race, with an incremental increase in size to ensure representation of different demographics. Demographic compositions of each country were estimated to adjust the number of images for each race, preventing dominance by the White race. After estimating demographic compositions of each country, the dataset was adjusted to prevent dominance by the White race. Faces were annotated with race, gender, and age group using Amazon Mechanical Turk, with three workers assigned to each image for accuracy. If there was disagreement among workers, the image was reassigned to another set of workers. The minimum face size detected was 50 by 50 pixels, allowing for recognizable attributes and robust classifiers. Only images with \"Attribution\" and \"Share Alike\" Creative Commons licenses were used for the dataset. Creative Commons licenses were used for the dataset, allowing derivative work and commercial usages. Amazon Mechanical Turk was utilized to annotate race, gender, and age group for each face, with three workers assigned per image. Ground-truth values were determined if two or three workers agreed, otherwise, the image was reassigned. Annotations were refined by training a model and manually verifying annotations that differed from model predictions. The race composition of datasets was measured, with most datasets biased towards the White race, especially those focusing on celebrities or politicians. Race labels were annotated for 3,000 random samples from datasets without race annotations. Most face attribute datasets, particularly those featuring celebrities or politicians, show bias towards the White race. Gender balance in datasets ranges from 40%-60% male ratio. Model performance comparison was conducted using ResNet-34 architecture trained on different datasets. Face detection was done using dlib's CNN-based face detector, and attribute classification was run on each face using PyTorch. Comparison was made with UTKFace, LFWA+, and CelebA datasets, with detailed characteristics in Table 1. CelebA was used only for gender classification due to the lack of race annotations. In comparing datasets for face attribute evaluation, UTKFace and LFWA+ with race annotations were used, while CelebA was only utilized for gender classification due to the absence of race annotations. FairFace defined 7 race categories but focused on 4 races for comparison. Cross-dataset classifications were performed using models trained from these datasets, showing varying performance across subpopulations. The accuracy of each model generally aligned with the dataset it was trained on. The study compared face attribute evaluation datasets, including UTKFace and LFWA+ with race annotations, and CelebA for gender classification. Results showed varying performance across subpopulations, with models generally performing best on the dataset they were trained on. The model achieved highest accuracy on some variables in the LFWA+ dataset, which is the most biased, while being close to the leader in other cases. The study also tested the generalization performance on three novel datasets collected from different sources, including geo-tagged Tweets from four countries. The study measured model effectiveness on diverse races using test datasets from geo-tagged Tweets, media photographs, and a protest dataset. Faces were sampled from four countries and media outlets, as well as from a dataset collected for a protest activity study. The study analyzed model performance on diverse races using test datasets from geo-tagged Tweets, media photographs, and a protest dataset. Faces were sampled from various countries and media outlets, as well as from a dataset collected for a protest activity study. The FairFace model outperformed other models for race, gender, and age classification on novel datasets. The FairFace model outperforms other models for race, gender, and age classification on novel datasets, even when trained with fewer images. It also produces more consistent results across different race groups compared to other datasets. The FairFace model achieves the lowest maximum accuracy disparity for gender classification across different demographic groups, with less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White. Other models show biases towards certain gender and race categories. The LFWA+ model exhibits a strong bias towards the male category, while the CelebA model tends to be biased towards the female category due to dataset composition. FairFace achieves less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White for gender classification. Other models show biases towards the male class and perform inaccurately on the female and non-White groups. The gender performance gap is largest in LFWA+. Recent work suggests that unbalanced representation in training data may be the cause of asymmetric gender biases in computer vision services. Data diversity is further investigated using t-SNE visualization of facial embeddings. The dataset characteristics were investigated to measure data diversity, showing that FairFace contains non-typical examples with loosely separated race groups. The diversity of faces in different datasets was measured by examining pairwise distances between faces. UTKFace dataset focuses on local clusters of individuals, while FairFace contains non-typical examples with loosely separated race groups. Pairwise distance distributions show that UTKFace has tightly clustered faces, LFWA+ has diverse faces despite majority being white, and FairFace falls in between. This diversity is influenced by the training data used for face embeddings. The FairFace dataset is diverse in terms of race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement compared to previous datasets. It was used to test gender classification APIs, with 7,476 random samples ensuring equal representation across race, gender, and age groups. Children under 20 were excluded due to ambiguity in gender determination. The experiments were conducted in August 2019. The experiments conducted in August 2019 used 7,476 random samples from FairFace, ensuring equal representation across race, gender, and age groups. Children under 20 were excluded due to gender ambiguity. Gender classification accuracies of tested APIs, including Microsoft, Face++, IBM, and Amazon, were evaluated. Table 6 displays gender classification accuracies of tested APIs, with Amazon Rekognition detecting all faces. Mis-detections were considered in two accuracy sets, showing a preference for male category. Dark-skinned females had higher error rates, with exceptions like Indians. The study found that gender classifiers still favor males, with dark-skinned females experiencing higher error rates. However, there were exceptions, such as Indians being classified more accurately by some APIs. The research also highlighted the impact of face detection on gender bias, with Microsoft's model failing to detect many male faces. The paper introduced a new face image dataset balanced on race, gender, and age, showing improved classification performance on non-White faces from diverse sources like Twitter and online newspapers. The study introduced a novel face image dataset balanced on race, gender, and age, derived from the Yahoo YFCC100m dataset. This dataset improves classification performance on non-White faces from various sources like Twitter and online newspapers. It aims to address algorithmic fairness in AI systems and can be used for training new models and verifying balanced accuracy of existing classifiers. Algorithmic fairness is crucial in AI system design as they are integrated into various societal aspects. Large image datasets enhance computer vision accuracy but face transparency concerns. A new dataset aims to uncover and address race and gender bias in computer vision for better societal acceptance."
}