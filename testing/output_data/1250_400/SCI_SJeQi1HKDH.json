{
    "title": "SJeQi1HKDH",
    "content": "In this work, the social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, leading to improved performance and diverse behaviors. Reinforcement Learning involves learning through interaction with the environment to maximize rewards, inspired by cognition and animal studies. Biodiversity and skill development are essential for evolution and continuation. The paradigm of Reinforcement Learning involves learning through interaction with the environment to maximize rewards. Behavioral diversity is crucial for species evolution, and previous works have focused on encouraging diversity in RL through designing rich environments or motivating agents to explore beyond just maximizing rewards. In this work, the focus is on improving the diversity of RL agents while maintaining their ability to solve tasks. The concept of social influence is introduced to encourage agents to differentiate their actions from others. This approach draws inspiration from animal society dynamics and aims to increase behavioral diversity in RL. The learning scheme in this work focuses on maximizing rewards while encouraging agents to differentiate their actions from others. Social influence is implemented as a motivation for social uniqueness, leading to a constrained optimization problem. A policy distance metric is defined to compare agent similarity, and an optimization constraint is developed for immediate feedback. Interior Policy Differentiation (IPD) is proposed as a solution to promote diversity in RL agents. The curr_chunk discusses the use of social influence in learning, introducing constraints to encourage agents to perform differently from others. It compares different methods like VIME and curiosity-driven methods to tackle sparse reward problems in reinforcement learning. In contemporary RL algorithms, an intrinsic reward term based on information gains is added to encourage exploration. Various methods like Random Network Distillation and Competitive Experience Replay define intrinsic rewards based on prediction errors or state coincidence. These approaches combine external rewards from environments with intrinsic rewards from different heuristics, posing a challenge in balancing the trade-off between them. The Task-Novelty Bisector (TNB) learning method aims to optimize the balance between external rewards and intrinsic rewards by updating the policy in the direction of the angular bisector of the two gradients. However, this joint optimization approach requires additional computation expenses due to the need for extra intrinsic reward functions and evaluation of state or policy novelty. The Distributed Proximal Policy Optimization (DPPO) method enables agents to learn complex locomotion skills in diverse environments. Different RL algorithms may converge to different policies for the same task, with policy gradient algorithms tending to converge to the same local optimum while off-policy and value-based algorithms learn sophisticated strategies. In contrast to previous findings on RL algorithms converging to different policies, this paper focuses on learning diverse policies through a single algorithm while avoiding local optima. The approach involves maintaining model uncertainty using an ensemble of deep neural networks and defining a metric to measure policy differences. The learned policies are denoted as {\u03b8i} where \u03b8i represents parameters of the i-th policy. The metric should satisfy identity, symmetry, and triangle inequality properties. The learned policies are denoted as {\u03b8i} where \u03b8i represents parameters of the i-th policy. A metric space is defined as an ordered pair (M, d) where d is a metric on M satisfying identity, symmetry, and triangle inequality properties. The Total Variance Divergence is used to measure the distance between policies, making (\u0398, D \u03c1 T V) a metric space. The goal is to maximize the uniqueness of a new policy by maximizing U(\u03b8|\u0398 ref). In order to improve sample efficiency, the method proposes approximating a certain fixed behavior policy \u03b8 to maximize the uniqueness of a new policy. This involves calculating D \u03c1 T V(\u03b8i, \u03b8j) based on Monte Carlo estimation, which presents challenges in continuous state spaces due to the difficulty in obtaining enough samples efficiently. The method proposes approximating a fixed behavior policy \u03b8 to maximize policy differentiation. This involves calculating D \u03c1 T V(\u03b8i, \u03b8j) based on Monte Carlo estimation, with challenges in continuous state spaces. The domain of possible states must be similar between policies for this approximation to hold. The method aims to maximize policy differentiation by approximating a fixed behavior policy \u03b8. It involves calculating D \u03c1 T V(\u03b8i, \u03b8j) using Monte Carlo estimation, with challenges in continuous state spaces. The objective function considers reward from the primal task and policy uniqueness to improve behavioral diversity among agents. The objective is to maximize policy differentiation by considering reward from the primal task and policy uniqueness. Previous approaches directly combine these rewards but face challenges in selecting the weight parameter \u03b1 and formulating the intrinsic reward. To address this, inspiration is drawn from social uniqueness to improve agent performance. To address challenges in selecting the weight parameter \u03b1 and formulating intrinsic reward, inspiration is drawn from social uniqueness to improve agent performance. The multi-objective optimization problem is transformed into a constrained optimization problem with a penalty method, replacing the constrained optimization problem with a penalty term and penalty coefficient. The difficulty lies in selecting \u03b1, which is tackled by the Task Novel Bisector (TNB) heuristic approximation. In this work, the authors propose solving the constrained optimization problem using Interior Point Methods (IPMs) instead of the Task Novel Bisector (TNB) approach. The use of IPMs involves reformulating the problem into an unconstrained form with a barrier term in the objective. However, directly applying IPMs can be computationally challenging and numerically unstable, especially when the weight parameter \u03b1 is small. In our proposed RL paradigm, the learning process is influenced by peers, allowing us to bound collected transitions within the feasible region. By terminating new agents that step outside this region, we ensure that all valid samples collected during training are unique. This eliminates the need to deliberate between intrinsic and extrinsic rewards, resulting in a new policy with sufficient uniqueness. During training, valid samples are kept within the feasible region to ensure uniqueness. This eliminates the need to balance intrinsic and extrinsic rewards, leading to a more robust learning process. The approach, named Interior Policy Differentiation (IPD) method, is demonstrated on MuJoCo-based environments. Experiments show that policies can differ significantly. The Interior Policy Differentiation (IPD) method is applied to locomotion environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3. The method aims to improve behavior diversity by generating uniqueness through stochasticity in training processes. Comparison with TNB and WSR approaches is also conducted. Further implementation details are available in Appendix D. The proposed method, compared with TNB and WSR approaches, focuses on combining task goals and uniqueness motivation. The uniqueness metric is used directly in policy learning without reshaping. Experimental settings involve training 10 different policies sequentially to be unique. Qualitative results are shown in Fig.2, depicting agent motion with highlighted frames. The experimental results in Fig. 3 demonstrate the uniqueness and performance of the proposed method compared to other approaches. The method outperforms in Hopper and HalfCheetah environments, while showing improvement in Walker2d. However, none of the methods surpass the performance of PPO. Detailed comparison on task-related rewards is presented in Table 1. Our proposed method outperforms other methods in improving policy uniqueness in Walker2d. However, none of the methods can surpass the performance of PPO. Detailed comparisons on task-related rewards and success rates are provided in Table 1. Figures in Appendix C depict the performance of trained policies and their reward curves. Success rate analysis shows that unique behavior is not achieved at the expense of performance. Our method consistently outperforms the PPO baseline in training, ensuring improved performance in environments like Hopper and HalfCheetah. It prevents policies from getting stuck in local minima, encouraging exploration of different action patterns for enhanced performance. Our method prevents policies from falling into the same local minimum, encouraging exploration of different action patterns for improved performance in environments like HalfCheetah. The environment of HalfCheetah lacks an explicit termination signal, leading to random actions at the start of the learning process. In our learning scheme, agents receive termination signals from peers to avoid random actions and high control costs. The learning process involves imitating previous policies to terminate early and then exploring new behaviors for higher rewards. As social influence grows, finding unique policies becomes more challenging. Performance changes under different levels of social influence are analyzed in a ablation study. In our learning scheme, agents receive termination signals from peers to avoid random actions and high control costs. As social influence grows, finding unique policies becomes more challenging. The results of an ablation study on performance changes under different levels of social influence are shown in Fig. 4. A new approach, Interior Policy Differentiation (IPD), is developed to motivate RL to learn diverse strategies inspired by social influence. IPD can help agents avoid local minimum and facilitate implicit curriculum learning in certain cases. Our proposed method, Interior Policy Differentiation (IPD), draws insights from Interior Point Methods to learn various well-behaved policies and avoid local minimum. Experimental results show that IPD can be interpreted as implicit curriculum learning in some cases. Comparison with other methods shows that careful hyper-parameter tuning and reward shaping are necessary to balance uniqueness and task performance. The calculation of DTV involves removing Gaussian noise in PPO policies. In the implementation details, the deterministic part of policies is used for DTV calculation in PPO. MLP with 2 hidden layers is employed for actor models, with varying hidden unit numbers based on task requirements. Training timesteps are fixed for different tasks, and policy uniqueness can be controlled by adjusting the constraint threshold. In the experiments, training timesteps are fixed at 1M for Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The constraint threshold in the proposed method controls policy uniqueness, with larger thresholds leading to more distinct behaviors. Cumulative uniqueness is used as a constraint instead of forcing every action to be different. Different threshold values are tested, affecting agent performance as shown in Fig. 9 and Table 2. In the experiments, training timesteps are fixed at 1M for Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The constraint threshold in the proposed method controls policy uniqueness, with larger thresholds leading to more distinct behaviors. Cumulative uniqueness is used as a constraint instead of forcing every action to be different. Different threshold values are tested, affecting agent performance. The implementation of Eq. (7) does not force every single action of a new agent to be different from others, focusing on long-term differences. Constraints can be applied after the first t timesteps for similar starting sequences. The WSR, TNB, and IPD methods correspond to three approaches in constrained optimization problems. Eq. (9) is considered with a more concise notion, simplifying the optimization of policy based on batches of trajectory samples with stochastic gradient descent. The optimization of policy in constrained problems is simplified by considering Eq.(9) with a concise notion. The Penalty Method handles constraints by incorporating them into a penalty term, while the Feasible Direction Method finds a direction that satisfies the constraints. The TNB method selects a direction using the bisector of gradients. The final solution heavily depends on the selection of a fixed weight term \u03b1. The Feasible Direction Method (FDM) considers constraints by finding a direction that satisfies them. The TNB method selects a direction using the bisector of gradients and relies heavily on the selection of a fixed weight term \u03b1. The optimization result in TNB is influenced by the shape of g, which can be improved by using a barrier term with a small positive number \u03b1. The objective with the barrier term gets closer to the primal objective as \u03b1 approaches zero. In practice, a sequence of decreasing \u03b1 values is chosen to improve the optimization process. The Feasible Direction Method (FDM) considers constraints by finding a direction that satisfies them, while the TNB method relies on a fixed weight term \u03b1. The optimization result in TNB is influenced by the shape of g, which can be improved by using a barrier term with a small positive number \u03b1. The solution of the objective with the barrier term gets closer to the primal objective as \u03b1 decreases. To address computational challenges, a more natural approach is to bound collected transitions within the feasible region by terminating new agents that step outside it during training. This ensures uniqueness in the new policy obtained at the end of training. During training, valid samples are kept within the feasible region to ensure uniqueness in the new policy obtained. This eliminates the need to balance intrinsic and extrinsic rewards, leading to a more robust learning process without objective inconsistency. The pseudo code of IPD based on PPO includes additions to the primal PPO algorithm."
}