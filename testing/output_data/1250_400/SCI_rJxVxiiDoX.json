{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Combining quantization-aware training with weight matrix factorization reduces model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training is used to optimize small-footprint low-power keyword spotting models by considering quantized weights in full precision representation. This approach enables successful training of 8 bit and 4 bit quantized models. Dynamic quantization is employed for DNN weight matrices, with shifts and scales calculated independently column-wise. The keyword spotting system uses dynamic quantization for DNN weight matrices, with shifts and scales calculated independently column-wise. The accuracy loss due to quantization is addressed through quantization-aware training. The experiments focus on the keyword 'Alexa' using a 500 hrs far-field corpus for training and a 100 hrs dataset for evaluation. Training is done in 3 stages, with a small ASR DNN pre-trained in the first stage. The training process involves using GPU-72 for distributed DNN training in 3 stages. A small ASR DNN is pre-trained with 3 hidden layers of 128 units. The performance of quantized models is evaluated, showing an improvement in AUC. DET curves for different quantized models are compared."
}