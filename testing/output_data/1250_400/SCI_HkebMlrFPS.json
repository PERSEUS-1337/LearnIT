{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets and outperform strong baselines on various NLP tasks. The new approach introduces multi-mode codebook embeddings for phrases and sentences, which outperform strong baselines on various NLP tasks such as unsupervised phrase similarity, sentence similarity, hypernym detection, and extractive summarization. These models learn representations from co-occurrence statistics in raw text without supervision, addressing the limitations of single-mode embeddings. The curr_chunk discusses word sense induction methods and multi-mode word embeddings to represent target words in a distributional semantic space. It compares these approaches to topic modeling like LDA and highlights the challenges in extending multi-mode representations to arbitrary sequences like phrases or sentences. The curr_chunk discusses the challenges of extending multi-mode representations to phrases or sentences due to efficiency issues in clustering-based approaches. It introduces a compositional model that predicts cluster centers' embeddings from word sequences. The curr_chunk introduces a neural encoder and decoder model to predict cluster centers' embeddings from word sequences, addressing the challenge of efficiently representing phrases or sentences with multi-mode representations. The neural encoder and decoder model aims to compress redundant parameters in local clustering problems by learning a mapping between target sequences and cluster centers. This allows for direct prediction of cluster centers using a single forward pass of the neural network during testing. The model uses a nonnegative and sparse coefficient matrix to match predicted cluster centers with observed word embeddings, enabling joint training of the entire model. Experiments demonstrate the model's ability to capture the compositional meanings of words in unsupervised settings. The proposed model uses word embeddings and coefficient matrices to train the model jointly and capture compositional meanings of words in unsupervised tasks. It outperforms baseline methods in phrase similarity and can measure asymmetric relations like hypernymy. The multimode representation is superior to single-mode alternatives, especially in sentence representation as shown in extractive summarization experiments. The training setup, objective function, and architecture are detailed in sections 2.1, 2.2, and 2.3 respectively. The training setup, objective function, and architecture of the model are detailed in sections 2.1, 2.2, and 2.3. The model represents sentences using codebook embeddings and aims to reconstruct co-occurring words while avoiding common topics. The input sequence is reconstructed by considering neighboring words within a fixed window size. The model reconstructs input sequences by considering neighboring words within a fixed window size, aiming to cluster words that could potentially occur beside the sequence. The training signal differs for sentences and phrases, requiring separate models for each representation. The focus is on predicting possibly co-occurring words based on similar sequences rather than actual occurrences in the corpus. The model focuses on predicting co-occurring words based on similar sequences rather than actual occurrences in the corpus. It considers word order information in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in a pre-trained word embedding space. The model predicts cluster centers of input sequences using a neural network model with a fixed number of clusters. The reconstruction loss in k-means clustering in the word embedding space is calculated using a permutation matrix. Non-negative sparse coding is adopted to relax constraints on coefficient values. The neural network model uses a permutation matrix to match cluster centers and co-occurring words, allowing for arbitrary order generation. Non-negative sparse coding is adopted to relax constraints on coefficient values, leading to diverse cluster centers. The NNSC loss is smoother and easier to optimize for neural networks compared to kmeans loss, resulting in better capturing of conditional co-occurrence distribution. The proposed method encourages predicted clusters to play different roles in reconstructing word embeddings. The reconstruction error is defined with a hyper-parameter \u03bb controlling sparsity. The approach efficiently minimizes L2 distance in a pre-trained embedding space and uses convex optimization for estimating coefficients. This allows for end-to-end training while preventing the neural network from predicting the same global topics regardless of input. The method involves estimating M Ot as a constant and back-propagating gradients to the neural network for end-to-end training. The loss function is defined based on co-occurring words in a sequence. The approach is a generalization of Word2Vec, encoding compositional meaning and decoding multiple embeddings. The neural network architecture is similar to a seq2seq model, with an encoder transforming input sequences into contextualized embeddings. The decoder outputs a sequence of embeddings instead of words, eliminating the need for discrete decisions. The decoder in the neural network model outputs a sequence of embeddings instead of words, eliminating the need for discrete decisions. Different linear layers are used to make different codebook embeddings capture different aspects. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation, but does not significantly affect phrase representation. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation, but does not significantly affect phrase representation. The framework is flexible, allowing for the replacement of encoder and decoder with other architectures. Cluster centers predicted by the model summarize the target sequence well, with more codebook embeddings capturing semantic facets of a phrase or sentence. The model predicts cluster centers in Table 1 to summarize target sequences well, using codebook embeddings to capture semantic facets. Evaluating topics conditioned on input sequences is challenging, but codebook embeddings can enhance unsupervised semantic tasks. GloVe embeddings are used for sentence and phrase representation, with models trained on Wikipedia 2016 without the need for additional resources like PPDB. Noun phrases are considered in experiments, with boundaries extracted using POS tags and spaCy for sentence detection. Our models do not require additional resources like PPDB or multi-lingual resources for training. The models are compared with baselines using raw text and sentence/phrase boundaries, making them practical for domains with low resources such as scientific literature. The transformers in our models have a dimensionality of 300, matching the GloVe embedding size. Despite training on a single GPU within a week, the models tend to underfit the data due to their small size. Comparing with BERT is challenging as BERT's masked language modeling loss preserves more syntax information, producing effective pretrained embeddings for downstream tasks. Our models underfit the data after a week, making fair comparison with BERT difficult. BERT is trained on a masked language modeling loss, preserving syntax information and producing effective pretrained embeddings. BERT uses a word piece model to alleviate the out-of-vocabulary problem. Semeval 2013 and Turney 2012 are standard benchmarks for evaluating phrase similarity. BiRD and WikiSRS contain ground truth phrase similarities. The task involves evaluating phrase similarity using benchmarks like BiRD and WikiSRS. Semeval 2013 aims to distinguish similar phrase pairs from dissimilar ones. Turney's method identifies the most similar unigram to a query bigram. Two scoring functions measure phrase similarity, one based on contextualized word embeddings and the other on reconstruction error from normalized codebook embeddings. Our method, labeled as Ours Emb, computes cosine similarity between phrase embeddings using transformer encoder. We calculate symmetric distance SC by comparing reconstruction error from normalized codebook embeddings of two phrases. Performance comparison with 5 baselines including GloVe Avg, Word2Vec Avg, BERT CLS, BERT Avg, and FCT LM Emb is presented in Table 2. SemEval 2013 and Turney have training and testing split for evaluating phrase similarity. Our models significantly outperform baselines in 4 datasets, especially in Turney. Non-linearly composing word embeddings improves performance compared to linear methods like GloVe and Word2Vec. Ours (K=1) performs slightly better than Ours (K=10), supporting the idea that multi-mode embeddings may not always enhance performance. Despite this, Ours (K=10) still shows strong performance compared to baselines. The study compares the performance of different models in word similarity benchmarks and sentence similarity tasks. The results show that the number of clusters does not significantly affect similarity performance, making it easier to select the optimal number of clusters in practice. Additionally, the models are evaluated on a sentence similarity task called STSB Low, where their performance is compared with other methods like word mover's distance and cosine similarity. In addition to BERT CLS, BERT Avg, and GloVe Avg, the method is compared with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). A weighting method proposed by Arora et al. (2017) is applied, with \u03b1 set to 10^-4 in the STS benchmark. Post-processing techniques from Arora et al. (2017) are also utilized, including GloVe SIF and GloVe Prob_avg. The study suggests that considering the embeddings of words is crucial for performance. The study compares different methods including GloVe SIF and GloVe Prob_avg for measuring sentence similarity. It highlights the importance of considering word embeddings in addition to sentence embeddings for accurate representation. The method uses multi-facet embeddings to estimate word importance in predicting co-occurring words. The importance weighting is computed using cosine similarity between words and predicted codebook embeddings. The importance of word embeddings is highlighted in measuring sentence similarity. A method using multi-facet embeddings estimates word importance by computing cosine similarity between words and predicted codebook embeddings. The proposed attention weighting boosts performance in averaging-based methods, especially in STSB Low scenarios. The proposed attention weighting in Ours (K=10) improves performance compared to Ours (K=1), especially in STSB Low scenarios. A variant of the method using a bi-LSTM encoder and LSTM decoder performs worse than the transformer alternative. The model is applied to HypeNet for unsupervised hypernymy detection, showing the effectiveness of ignoring the order of co-occurring words in the loss function. The model is applied to HypeNet for unsupervised hypernymy detection, based on the assumption that co-occurring words of a phrase are often less related to some of its hyponyms. Our asymmetric scoring function outperforms baselines in detecting hypernyms, with Ours (K=1) performing similarly to Ours (K=10). The objective is to discover a summary A with normalized embeddings that best reconstructs the distribution of word embeddings in the document. The extractive summarization method aims to generate multiple codebook embeddings to represent each sentence in the document. By selecting sentences greedily to optimize the summarization equation, the model compares different ways of modeling sentence aspects. Our model generates multiple codebook embeddings to represent each sentence in the document, comparing different ways of modeling sentence aspects. The results on the testing set of CNN/Daily Mail are compared using F1 of ROUGE in Table 5. The study compares different methods for selecting sentences in document summarization, focusing on unsupervised approaches. Results show that predicting more aspects leads to better performance, with a cluster number of K=100 yielding the best results. Larger clusters demonstrate improved performance in selecting 3 sentences for summarization. In evaluating unsupervised sentence embeddings, larger cluster numbers (K) lead to better results, with K=100 performing the best after selecting 3 sentences. The method allows for setting a large K to address computational challenges. Neural networks have been used to discover coherent topics, but the focus here is on efficiently finding different sets of topics on small word subsets. Sparse coding in word embedding space is utilized to model multiple aspects of a word. Neural networks are used to efficiently discover different sets of topics on small word subsets. Sparse coding in word embedding space models multiple aspects of a word. Parameterizing word embeddings with neural networks helps test hypotheses and save storage space. Representing words as single or multiple regions in Gaussian embeddings captures asymmetric relations. Challenges include designing a neural decoder for sets and computing distance loss after matching elements. Chamfer distance is a popular loss function widely used in auto-encoders. One challenge is designing a neural decoder for sets and computing distance loss after matching elements. Popular loss functions like Chamfer distance are used in auto-encoders. Various methods for achieving permutation invariance in neural networks are discussed, including removing predicted elements from the ground truth set, beam search, and predicting permutations using different models like CNNs, transformers, or reinforcement learning. The goal is to efficiently predict a set of elements. The goal is to efficiently predict a set of clustering centers that can reconstruct observed instances. A neural encoder models the target sequence's meaning, while a neural decoder predicts codebook embeddings as sentence representations. A non-negative sparse coefficient matrix dynamically matches predicted embeddings to observed words, allowing the decoder to predict clustering centers with any permutation. The models can learn interpretable clustering. The proposed models use a non-negative sparse coefficient matrix to match predicted codebook embeddings to observed words, allowing the neural decoder to predict clustering centers with any permutation. These models outperform BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings perform best for sequences with many aspects, while single-facet embeddings perform similarly well for sequences with one aspect. Future work aims to generate multi-facet embeddings for both phrases and sentences and apply the method to other unsupervised learning tasks. In the future, the plan is to train a single model to generate multi-facet embeddings for phrases and sentences, and evaluate it as a pre-trained embedding approach for supervised or semi-supervised settings. The model is kept simple to converge training loss quickly, without fine-tuning hyper-parameters. The transformer architecture and most hyper-parameters are similar to BERT, with a sparsity penalty weight of 0.4. The sentence size is limited to 50 words, with a maximum of 30 co-occurring words. The number of dimensions in transformers is set to 300. The penalty weight on the coefficient matrix \u03bb is set to 0.4. The maximal sentence size is 50 words, with a maximum of 30 co-occurring words. The number of dimensions in transformers is 300. For sentence representation, the number of transformer layers on the decoder side is 5 with a dropout on attention of 0.1 for K = 10, and 1 for K = 1. For phrase representation, the number of transformer layers on the decoder side is 2 with a dropout on attention of 0.5. The window size d t is 5. Hyperparameters are determined by the validation loss of the self-supervised co-occurring word reconstruction task. The number of codebook embeddings K is chosen based on training data performance. The number of codebook embeddings K is determined by training data performance, with larger K needing longer training time. Skip-thoughts use a hidden embedding of 600 and are retrained in Wikipedia 2016 for 2 weeks. The model has fewer parameters than BERT base and uses less computational resources. Comparison with BERT Large is presented in Table 6, 7, and 8, showing BERT Large usually performs better. In comparison with BERT Large, our method outperforms in most cases, especially in phrase similarity tasks. BERT Large excels in similarity tasks but struggles in hypernym detection. The performance gains of BERT in similarity tasks suggest that training a larger model could be beneficial. Our method, despite being smaller, performs better, possibly due to BERT's training method being less effective for short sequences like phrases. In Section 3.4, comparisons were made between different summarization methods when using the same number of sentences. The performance of W Emb (*) methods may suffer due to selecting shorter sentences. Our method (K=100) outperformed W Emb (GloVe) and Sent Emb (GloVe) when summaries were of similar length. Additionally, W Emb (*) generally outperformed Sent Emb (*) in similar length summaries, although this comparison may not be entirely fair. In comparing different summarization methods with the same number of sentences, it was found that W Emb (*) methods may struggle with shorter sentences. Our method (K=100) performed better than W Emb (GloVe) and Sent Emb (GloVe) for similar length summaries. W Emb (*) generally outperformed Sent Emb (*) in similar length summaries, although this comparison may not be entirely fair. When aiming to maximize the ROUGE F1 score with a fixed summary length, Ours (K=100) is the best choice for summaries less than 50 words, while W Emb (BERT) is better for longer summaries. Combining our method with BERT may lead to improved performance. BERT base model combined with the method discussed may offer promising results for improving performance in the task. Predicted embeddings from randomly selected sentences were visualized, showing similarities to GloVe embeddings. The format of the file is similar to Table 1, with the first line being the preprocessed input sentence."
}