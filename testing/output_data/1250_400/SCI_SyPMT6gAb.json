{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning is crucial for evaluating and improving policies using historical data from a logging policy. The challenge lies in deriving counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over conventional baseline algorithms. Off-policy learning is essential for evaluating deterministic policies using historical data, avoiding costly and risky on-policy evaluations in real-world scenarios like clinical trials and online advertising A/B testing. Off-policy evaluation is crucial for assessing policies using historical data to avoid costly and risky on-policy evaluations. Various methods like Q learning and doubly robust estimator have been studied in reinforcement learning and contextual bandits. Recently, off-policy learning has focused on using logged interaction data with limited feedback, such as scalar rewards. This approach allows for safe exploration of policy hypotheses before deployment. Off-policy learning with bandit feedback involves using logged interaction data with limited feedback, like scalar rewards. The challenge lies in handling the distribution mismatch between the logging policy and a new policy, leading to generalization errors. BID34 introduced a new framework for counterfactual risk minimization, incorporating sample variance as a regularization term in the objective to address this issue. Our contribution in this paper is three-fold: proposing a new learning principle for off-policy learning with bandit feedback by minimizing distribution divergence between the new policy and the logging policy, automatically balancing empirical risk and sample variance. The challenge remains in deriving accurate and efficient training algorithms under this framework. Our proposed learning principle for off-policy learning with bandit feedback involves regularizing the generalization error of the new policy by minimizing distribution divergence with the logging policy. This objective balances empirical risk and sample variance, utilizing neural networks for end-to-end training. Experimental results demonstrate significant performance improvements over conventional baselines, validating our theoretical proofs. In off-policy learning with logged bandit feedback, a policy maps input x to output y. Stochastic policies define a distribution over the output space parametrized by \u03b8. Actions are taken by sampling from this distribution. Feedback is observed by comparing the sampled action to an underlying 'best' action. In off-policy learning, feedback is observed by comparing actions sampled from a policy to an underlying 'best' action. The goal is to find a policy with minimum expected risk on test data. In the off-line logged learning setting, data is collected from a logging policy, and the aim is to find an improved policy with lower expected risks. Challenges arise when the distribution of the logging policy is skewed and doesn't have support everywhere. In off-policy learning, feedback is obtained by comparing actions from a policy to a 'best' action. Challenges arise when the logging policy's distribution is skewed, leading to missing feedback for certain actions. Empirical estimation is used due to the inability to compute expectations exactly, resulting in generalization error and the need for regularization. The vanilla approach involves propensity scoring using importance sampling to address distribution mismatch. Counterfactual risk minimization highlights flaws in the vanilla approach, such as lack of loss scaling invariance and high variance. Counterfactual risk minimization addresses flaws in the vanilla approach by proposing a regularization term for sample variance derived from empirical Bernstein bounds. The modified objective function includes this term to reduce variance and improve training efficiency. Despite the simplicity of the approach, approximating the regularization term via first-order Taylor expansion may introduce errors due to neglecting non-linear terms. This method allows for stochastic optimization but prohibits direct stochastic training due to the dependence of the variance term on the entire dataset. The text discusses the limitations of approximating the regularization term using a first-order Taylor expansion in a stochastic optimization algorithm. It suggests deriving a variance bound directly from the parametrized distribution to address the issue of neglecting non-linear terms and reducing sample variance. The text introduces probability density functions and a lemma on importance sampling weights. It then presents a theorem on the second moment of weighted loss and a bound on the expected risk using distribution divergence functions. The text discusses a generalization bound between expected risk and empirical risk using distribution divergence functions. It highlights bias-variance trade-offs in empirical risk minimization problems and suggests minimizing variance regularized objectives in bandit learning settings. The proof involves Bernstein inequality and second moment bounds. In bandit learning, minimizing variance regularized objectives can help avoid optimizing reweighed loss with high variance. A new constrained optimization formulation is explored as an alternative to traditional regularized ERM, with a pre-determined constant \u03c1 as the regularization hyper-parameter. This approach provides a good surrogate for the robust objective. The new constrained optimization formulation with a pre-determined constant \u03c1 as the regularization hyper-parameter provides a good surrogate for the robust objective. It eliminates the need to compute sample variance but can be challenging with a parametrized distribution and finite samples. Recent f-gan networks and Gumbel soft-max sampling can assist in estimating the divergence function. The stochasticity of the logging policy is crucial, as deterministic policies with peaked masses may lead to issues. The possibility of counterfactual learning is discussed in relation to the stochasticity of the logging policy. A deterministic policy with peaked masses may hinder learning as unexplored regions can lead to unbounded generalization bounds, making counterfactual learning impossible. The derived variance regularized objective requires minimizing the square root of a certain term, which can be simplified for analysis. The text discusses the challenges of counterfactual learning due to unbounded generalization bounds caused by a deterministic policy with peaked masses. It introduces a variance regularized objective that involves minimizing a specific term, leading to a connection with f-divergence measures. By applying the f-GAN method, a lower bound for the objective is obtained through Fenchel convex duality. The tightness of the bound is highlighted when a specific condition is met. The text discusses the application of Fenchel convex duality to obtain a lower bound for the objective in counterfactual learning. By using neural networks as the family of functions, the equality condition can be theoretically satisfied. The final objective is a saddle point of a function mapping input pairs to a scalar value, with the policy acting as a sampling distribution. This saddle point, trained with mini-batch estimation, is a consistent estimator of the true divergence. The final objective is a saddle point of a function T(x, y) mapping input pairs to a scalar value, with the policy acting as a sampling distribution. Trained with mini-batch estimation, this saddle point is a consistent estimator of the true divergence. The true divergence is denoted by Df = sup T T dhdx - f*(T)dh0dx, with an empirical estimator used. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. The error of empirical mean estimation to the true distribution is decomposed into terms related to neural networks and the difference between empirical and population distributions. The strong law of large numbers applies, leading to the conclusion that the terms approach zero. A generative-adversarial approach can be applied by representing the T function as a discriminator network and the policy distribution as a generator neural network. The T function is represented as a discriminator network, and the policy distribution is represented as a generator neural network for structured output problems. Gumbel soft-max sampling is used for differential sampling, and a training algorithm is provided for optimizing the generator distribution. The algorithm presented aims to minimize variance regularization term by deriving a training algorithm and backpropagating the gradient. It provides an end-to-end learning approach for counterfactual risk minimization from logged data, including a robust regularized formulation and training for the original ERM formulation. The algorithm involves sampling real and fake samples, updating the generator, and minimizing the reweighted loss through iterative steps. The algorithm aims to minimize variance regularization by updating the policy parameters to minimize reweighed loss and then updating the generator and discriminator to improve generalization performance. It addresses the problem of exploiting historic data in multi-armed bandit scenarios and proposes doubly robust estimators for this purpose. The text discusses the importance of regularizing variance to improve the generalization performance of policies in multi-armed bandit scenarios. Various approaches, such as doubly robust estimators, have been proposed to address this problem. Additionally, techniques from reinforcement learning, such as Q function learning and temporal difference learning, have been extended to off-policy learning. Recent works in deep reinforcement learning have also focused on off-policy updates using methods like multi-step bootstrapping and off-policy training of Q functions. Learning from log traces with propensity scores to evaluate candidate policies is also highlighted. Off-policy learning in reinforcement learning (RL) considers the Markov property of decision processes. Recent works in deep RL have addressed off-policy updates using methods like multi-step bootstrapping and off-policy training of Q functions. Learning from log traces with propensity scores to evaluate candidate policies is also discussed. In statistics, the problem is described as treatment effect estimation, focusing on estimating the effect of an intervention from observational studies. Various techniques have been developed to improve off-policy learning with bandit feedback, including variance regularization and generalization bounds in importance sampling problems. Variance regularization in off-policy learning with bandit feedback aims to address distribution mismatch between training and testing data. It has connections to distributionally robust optimization techniques and can potentially be applied to supervised learning and domain adaptation problems. The regularization technique minimizes the empirical risk over an ellipsoid uncertainty set, with Wasserstein distance being a well-studied constraint. Regularization for the objective function is closely connected to distributionally robust optimization techniques. The Wasserstein distance between empirical and test distributions is a key constraint for achieving robust generalization performance. Empirical evaluation involves converting supervised learning to bandit feedback methods, constructing logging policies, and using hamming loss as the loss function. Bandit feedback datasets are created by passing samples through logging policies and sampling actions. The text discusses using a conditional random field (CRF) policy trained on a small portion of data as the logging policy, using hamming loss as the loss function, and creating bandit feedback datasets. Evaluation metrics for the probabilistic policy include expected loss and average hamming loss of maximum a posteriori probability (MAP) predictions. MAP predictions are faster but may not capture the diversity of predictions, impacting generalization performance. The text discusses the importance of considering both MAP and EXP performance in evaluating learned policies. Different algorithms like IPS and POEM are compared using various optimization solvers. The effectiveness of variance regularization is verified by comparing neural network policies with and without divergence regularization. Four multi-label classification datasets from UCI are used for evaluation. Neural network policies with and without divergence regularization are compared to verify the effectiveness of variance regularization. Four multi-label classification datasets from UCI are used for evaluation, with detailed configurations provided in the Appendix. The networks are trained using Adam optimization with PyTorch implementation on Nvidia K80 GPU cards. The networks are trained with Adam optimization using PyTorch on Nvidia K80 GPU cards. Results show improved test performance with neural network policies compared to baseline CRF policies. Introducing additional variance regularization further enhances testing and MAP prediction loss. Two Gumbel-softmax sampling schemes, soft and straight-through, are evaluated. The introduction of additional variance regularization in neural network policies improves testing and MAP prediction loss compared to baseline CRF policies. Two Gumbel-softmax sampling schemes are evaluated, showing no significant difference. Varying the maximum number of iterations for divergence minimization leads to faster decrease in test loss and lower final test loss. By adding regularization, models show lower test loss and faster convergence rates. Increasing the number of iterations for divergence minimization leads to better test performance. The regularization helps generalize policies to test sets and improves training algorithm convergence. The number of training samples impacts test performance, with both regularized and non-regularized models showing increasing performance. When the number of training samples in the bandit dataset increases, models with regularization show better generalization performance compared to those without. However, after a certain point, the MAP prediction performance starts to decrease, indicating potential overfitting. Experimental results compare two training schemes and different sampling methods. After observing better generalization with regularization in bandit dataset training, a decrease in MAP prediction performance suggests potential overfitting. Experimental comparisons of two training schemes and sampling methods show slight performance improvement with weighted loss and distribution divergence. The difficulty lies in balancing gradients in the objective function. No significant difference is found between Gumbel-softmax sampling schemes. The impact of logging policy stochasticity on learning performance is discussed, with additional metrics visualized in Appendix 7. The effect of logging policies on learning performance is discussed, focusing on how stochasticity impacts algorithm improvement. By modifying the parameter h0 with a temperature multiplier \u03b1, the algorithm's ability to learn an improved policy is tested. Results show that NN policies outperform logging policies when h0's stochasticity is sufficient, but learning becomes harder as \u03b1 increases beyond 2/3. The study compares the performance of NN policies and logging policies in algorithm improvement. NN policies perform better when h0's stochasticity is sufficient, but learning becomes harder as the temperature parameter increases beyond 2/3. Stronger regularization in NN policies shows better performance, indicating robustness in learning principles. As h0 improves, models consistently outperform baselines, but the difficulty increases. The impact of logging policies on learned improved policies is also discussed. Our regularization helps the model be more robust and achieve better generalization performance. The difficulty increases as the quality of h0 improves. The impact of logging policies on learned improved policies is discussed, showing that better policies with lower hamming loss can produce bandit datasets with more correct predictions. By varying the proportion of training data points used to train the logging policy, it is found that NN and NN-Reg policies outperform the logging policy, addressing sampling biases. The increasing ratios of test expected loss to h0 performance indicate relative policy improvement. In this paper, a new training principle is proposed to improve off-policy learning for logged bandit datasets by regularizing variance. The training objective combines importance reweighted loss with a regularization term measuring distribution match. Neural network policies are trained end-to-end using variational divergence minimization and Gumbel soft-max sampling techniques. Evaluations on benchmark datasets demonstrate the effectiveness of the approach. The proposed training principle improves off-policy learning for logged bandit datasets by regularizing variance. Neural network policies are trained using variational divergence minimization and Gumbel soft-max sampling techniques. Evaluations on benchmark datasets show the effectiveness of the approach. Limitations include the need for propensity scores, but learning to estimate them can increase the applicability of the algorithms. The techniques and theorems may be extended to general supervised learning and reinforcement learning. The proposed algorithm improves off-policy learning for logged bandit datasets by regularizing variance. Importance weights are directly learned, with theoretical guarantees that can be extended to general supervised and reinforcement learning. The algorithm applies Lemma 1 to importance sampling weight function and loss, bounding the variance using Reni divergence and Bernstein's concentration bounds. The optimized generator approximates the minimizer of R(w) with regularization hyper-parameter \u03bb. The algorithm improves off-policy learning for bandit datasets by regularizing variance. It learns importance weights with theoretical guarantees, bounding the variance using Reni divergence and Bernstein's concentration bounds. The optimized generator approximates the minimizer of R(w) with a regularization hyper-parameter \u03bb. The algorithm improves off-policy learning for bandit datasets by regularizing variance. The statistics of the datasets are reported in a table. The effect of stochasticity of the logging policy on test loss with MAP predictions is analyzed. NN policies can find improvement over the baseline h0 policy in expected loss, but struggle to beat it in MAP predictions. Further investigation is needed to understand this phenomenon."
}