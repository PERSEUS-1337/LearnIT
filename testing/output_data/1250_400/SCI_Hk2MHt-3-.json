{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters and improved performance. This approach, known as \"coupled ensembles\", involves averaging log-probabilities of branches to enhance learning of better representations. The method is applicable to various neural network architectures and shows promising results on tasks like CIFAR-10, CIFAR-100, and SVHN. The approach of \"coupled ensembles\" is generic and can be applied to various neural network architectures. With DenseNet-BC ensembles and a parameter budget of 25M, error rates of 2.92%, 15.68%, and 1.50% are achieved on CIFAR-10, CIFAR-100, and SVHN tasks. The design of early convolutional architectures involved hyper-parameter choices, but recent models like ResNet and DenseNet follow a template with fixed filter size and feature maps, utilizing skip-connections. The proposed template in the current chunk extends the ResNet and DenseNet architectures by introducing \"coupled ensembling\" where the network is divided into branches. This approach achieves comparable performance to state-of-the-art models with fewer parameters. The contributions of the paper include showing the benefits of splitting parameters among branches, determining the best way to combine branch activations, and achieving significant improvements in performance. The paper introduces the concept of coupled ensembles in network architectures, splitting parameters among branches and combining branch activations to improve performance on CIFAR and SVHN datasets. The approach significantly reduces parameter count and achieves better results compared to current networks. The paper also discusses related work, evaluates the proposed approach, and suggests future research directions. The proposed network architecture shares similarities with Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network. However, the coupled ensemble networks differ in that they train a single model with branches, have a fixed parameter budget, combine branch activations over target categories, and use the same input for all branches. Multi-branch architectures have been successful in vision applications, with recent modifications proposed. Multi-branch architectures have been successful in vision applications with recent modifications proposed using grouped convolutions for spatial and depth-wise feature extraction. Shake-Shake regularization proposes a stochastic mixture of branches for improved performance. Our method proposes a generic rearrangement of architecture parameters for efficient parameter usage, unlike other techniques that require modifications at a local level. Additionally, we confirm empirically that our configuration leads to efficient parameter usage in neural network ensembles. Our proposed model architecture is a generic rearrangement of parameters that leads to efficient parameter usage in neural network ensembles. Unlike traditional ensembling techniques, our model consists of parallel branches trained jointly at a global network level, similar to ResNet and Inception modules. Our proposed model architecture involves a single model with parallel branches trained jointly at a global network level, similar to ResNet and Inception modules. \"Arranging\" parameters into parallel branches improves performance, and ensembling can further enhance performance. Snapshot ensembles use checkpoints during training for efficiency, but increase model size and prediction time. Our approach aims to maintain model size while improving performance or achieving the same performance with a smaller model. Our approach involves a single model with parallel branches, each taking input data and producing score vectors for target classes. The model architecture includes DenseNet-BC and ResNet with pre-activation as element blocks. The branches are combined using a fuse layer that averages their individual log probabilities. The classification task assigns each sample to one class from a finite set. In experiments, branches are combined by averaging individual log probabilities. Different fuse layer operations are explored for classification tasks like CIFAR, SVHN, and ILSVRC. Neural network models output score vectors for target classes, usually followed by a fully connected layer and SoftMax layer. Various network architectures differ in components before the last FC layer. During training, network architectures for image classification produce a probability distribution over target classes using a loss layer like negative log-likelihood. Differences among architectures lie in components before the last FC layer. Ensemble models combine predictions by averaging individual outputs, equivalent to a \"super-network\" with parallel branches. Averaging layer placement can be adjusted for memory efficiency. The model consists of parallel branches producing score vectors for target categories. These vectors are fused through a \"fuse layer\" during training to create a single prediction. Three options are explored for combining score vectors: Activation (FC) average, Probability (LSM) average, and Log Likelihood (LL) average. These methods are used during training and for inference. The model utilizes parallel branches to generate score vectors for target categories, which are combined through a \"fuse layer\" during training. Different methods such as Activation (FC) average, Probability (LSM) average, and Log Likelihood (LL) average are explored for combining the score vectors. This approach leads to improved performance with a lower parameter count in various experiments. The parameter vector of the composite branched model is a concatenation of parameter vectors from element blocks. The proposed architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets. The proposed architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets. The input images are normalized and standard data augmentation is used during training on CIFAR datasets. Dropout ratio of 0.2 is applied when training on SVHN with DenseNet. Testing is done after normalizing the input in the same way as during training. During training, standard data augmentation is used on CIFAR datasets, while no data augmentation is applied on SVHN. A dropout ratio of 0.2 is used for DenseNet when training on SVHN. Testing involves normalizing the input in the same manner as during training. Error rates are presented as an average of the last 10 epochs. Execution times were measured using a single NVIDIA 1080Ti GPU. Experiments on CIFAR-100 dataset involve DenseNet-BC with specific configurations. The proposed branched architecture is compared with an ensemble of independent models. The proposed branched architecture is compared with an ensemble of independent models in terms of error rates. Results show that the jointly trained branched configuration outperforms averaging predictions from identical models. Additionally, the error from the multi-branch model is considerably lower compared to a single branch model with a similar number of parameters. The proposed branched architecture outperforms averaging predictions from identical models. The error from the multi-branch model is considerably lower compared to a single branch model with a similar number of parameters. The arrangement of parameters into parallel branches is more efficient than a large single branch or multiple independent models. Experiments analyze the relation between the number of branches and model performance, evaluating different fusion combinations in a branched model with e = 4. The proposed branched model with e = 4 is evaluated with different \"fuse layer\" combinations to improve performance. Table 1 shows the top-1 error rate on the CIFAR-100 test set for various fusion choices during training and inference. The branched model outperforms a single branch model, with parameters distributed into parallel branches for efficiency. The branched model with e = 4 is evaluated with different \"fuse layer\" combinations to improve performance. Table 1 shows the performance of models under different \"fuse layer\" operations for inference. The average error rate of each \"element block\" trained jointly in coupled ensembles with LSM fusion is significantly lower than when trained separately, indicating better representations are learned. The branched model with 4 branches is evaluated with different \"fuse layer\" combinations to enhance performance. Coupled ensembles with LSM fusion result in lower error rates and better representations. Averaging log probabilities updates all branches consistently, leading to improved gradient signals. Ensemble combinations outperform single branch networks, with a notable decrease in error rate. Training with Avg. FC shows poor performance in individual branches. When training with Avg. FC, using 4 branches reduces error rates compared to a single branch model. The Avg. FC prediction performs better than Avg. SM prediction due to the spread of information at the FC layer. LSM fusion in branched models enhances performance, leading to lower error rates and improved representations. In this section, the optimal number of branches e for a given model parameter budget is investigated using DenseNet-BC as the \"element block\" on CIFAR-100. The results are shown in table 3, with different configurations of branches e, depth L, and growth rate k. The parameter counts in DenseNet-BC are quantified according to L and k values, which is critical in moderate size models like the 800K one targeted here. The study investigates the optimal number of branches for DenseNet-BC models on CIFAR-100 with different configurations of branches, depth, and growth rate. Model performance is robust, with e=3, L=70, k=9 yielding the best results. Using 2 to 4 branches improves performance significantly over the single branch case, while 6 or 8 branches perform worse due to thin element blocks. The study found that using 2 to 4 branches in DenseNet-BC models yields significant performance gains over the single branch case. However, using 6 or 8 branches performs worse possibly due to thin element blocks. Model performance is robust to slight variations in parameters, but increased performance comes with longer training and prediction times. The study also evaluated the coupled ensemble approach against existing models, using DenseNet-BC architecture as the \"element block.\" The study evaluated the performance of coupled ensembles with different architectures, including DenseNet-BC and ResNet BID8. Coupled ensembles with ResNet pre-act as element block and e = 2, 4 showed significantly better performance than single branch models. DenseNet-BC architecture was tested in 6 different network sizes, ranging from 0.8M to 25.6M parameters. The study compared the performance of coupled ensembles with different architectures, including DenseNet-BC and ResNet BID8. DenseNet-BC was tested in 6 network sizes with varying parameters. Results showed that coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperformed single branch models. The trade-off between depth L and growth rate k was found to be not critical for a given parameter budget. The study compared the performance of coupled ensembles with different architectures, including DenseNet-BC and ResNet BID8. DenseNet-BC was tested in 6 network sizes with varying parameters. Results showed that coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperformed single branch models. The trade-off between depth L and growth rate k was found to be not critical for a given parameter budget. Additionally, larger models of coupled DenseNet-BCs performed better than current state-of-the-art implementations, with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The coupled ensemble approach is limited compared to model architectures learned in a meta-learning scenario. The study compared the performance of coupled ensembles with different architectures, including DenseNet-BC and ResNet BID8. DenseNet-BC outperformed single branch models, with larger models performing better than current state-of-the-art implementations. The coupled ensemble approach is limited by the size of the network that can fit into GPU memory and the training time. The classical ensembling approach based on independent trainings was used to improve performance beyond 25M parameters. Multiple trainings were costly, so four large coupled ensemble models were ensembled instead. The study compared the performance of coupled ensembles with different architectures, including DenseNet-BC and ResNet BID8. DenseNet-BC outperformed single branch models, with larger models performing better than current state-of-the-art implementations. The coupled ensemble approach is limited by the size of the network that can fit into GPU memory and the training time. The classical ensembling approach based on independent trainings was used to improve performance beyond 25M parameters. Multiple trainings were costly, so four large coupled ensemble models were ensembled instead. The coupled ensemble approach already includes several models, with significant improvements seen from 1 to 3 models but not much from 3 to 16 models. Ensembling the four large coupled ensemble models resulted in a significant gain by fusing two models, with minimal improvement from further fusion. These ensembles of coupled ensemble networks outperform all state-of-the-art implementations, including other ensemble-based ones. The error rates between single and multi-branch models are highlighted, showing that a branched model with 13M parameters outperforms single branch models with double the parameters. The proposed approach involves replacing a single deep convolutional network with a number of \"element blocks\" resembling standalone CNN models. The proposed approach involves replacing a single deep convolutional network with a number of \"element blocks\" resembling standalone CNN models. These element blocks are coupled via a \"fuse layer\" to improve performance significantly over a single branch configuration. This improvement comes at the cost of slightly increased training and prediction times. The approach leads to the best performance for a given parameter budget, as shown in tables 3 and 4, and in figure 2. Additionally, the individual element block performance is better when trained together compared to independently. The increase in training and prediction times is mainly due to the sequential processing of branches. The proposed approach involves replacing a single deep convolutional network with \"element blocks\" connected via a \"fuse layer\" to improve performance. This leads to the best performance for a given parameter budget, with better individual block performance when trained together. The increase in training and prediction times is due to sequential processing of branches, impacting data parallelism on GPUs. Solutions include extending data parallelism to branches and spreading branches over multiple GPUs for improved performance. Preliminary experiments show lower error rates for coupled ensembles on ImageNet. The implementation involves using multiple 2D convolutions simultaneously and spreading branches over multiple GPUs for improved performance. Preliminary experiments on ImageNet show that coupled ensembles have lower error rates compared to single branch models. The structure of the networks used as element blocks is illustrated in Figure 3, with the possibility of placing the averaging layer in different locations for test and train versions. The implementation involves using multiple 2D convolutions simultaneously and spreading branches over multiple GPUs for improved performance. The structure of the networks used as element blocks is illustrated in Figure 3, with the possibility of placing the averaging layer in different locations for test and train versions. The averaging layer can be placed after the last FC layer, LSM layer, or LL layer. Parameter vectors define each branch, with a global network defined by a concatenation of all parameter vectors. Coupled networks use the same global parameter vector for training and testing, allowing for various training and prediction conditions. The overall network architecture is determined by global hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. Larger models are trained using \"micro-batches\" when a batch size of 64 is not feasible. Gradient is accumulated over micro-batches to approximate processing data directly. When training larger models with a batch size of 64 is not possible, data batches are split into micro-batches with b/m elements each. The gradient is accumulated over these micro-batches and averaged to approximate processing data directly. The BatchNorm layer uses micro-batch statistics, affecting the exact equivalence of the gradient. However, this difference is not significant in practice. Parameter updates are done using batch gradients, while forward passes are done with micro-batches for optimal throughput. The memory requirement in a single branch case depends on network depth and batch size. The micro-batch \"trick\" is used to adjust memory needs. When training larger models with a batch size of 64 is not possible, data batches are split into micro-batches with b/m elements each. The memory requirement in a single branch case depends on network depth and batch size. The micro-batch \"trick\" is used to adjust memory needs based on available resources. In practice, for experiments with 25M parameters, training was done within the 11GB memory of GTX 1080 Ti using micro-batch sizes of 16 for the single-branch. In experiments with 25M parameters, training was done within the 11GB memory of GTX 1080 Ti using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch ones. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but does not significantly increase speed or improve performance. Using two branches provides a significant gain over a single branch architecture of comparable size. An element of E i is referenced as E TAB7 shows the same results as in table 1 but for coupled ensembles with two branches only. Even using only two branches provide a significant gain over a single branch architecture of comparable size. TAB8 is an extended version of table 2 in which variation of the depth L and the growth rate k are also evaluated for an approximately fixed parameter count. The performance is quite stable against variation of the (L, k) compromise. The experiment on a validation set with a 40k/10k random split of the CIFAR-100 training set led to predict that the (L = 82, k = 8, e = 3) combination should be the best one on the test set. The (L = 70, k = 9, e = 3) combination appeared to be slightly better here but the difference is probably not statistically significant. TAB9 compares the parameter usage and performance of the branched coupled ensembles with model architectures recovered using meta learning techniques. In comparing parameter usage and performance, the study identified sources of variation including framework, random seed, CuDNN non-determinism, and fluctuations in batch normalization. The observed variation was found to be significant even with the same tool and seed. The study found significant variation in results even with the same tool and seed. Fluctuations in batch normalization, choice of model instance, and random initialization all contribute to dispersion in evaluation measures. The study observed dispersion in evaluation measures due to variations in random initialization. While local minima of properly designed neural networks should have similar performance, differences in measures below their dispersions can be nonsignificant. Statistical tests may not be helpful as differences can be observed between models with different seeds. Experiments quantified dispersion in a moderate scale model, like DenseNet-BC with L = 100, k = 12. Large trials for larger models may not be feasible. Experiments on a moderate scale model, DenseNet-BC with L = 100, k = 12, aimed to quantify dispersion. Different effects were tested using Torch7 and PyTorch with the same or different seeds. Results showed variations in error rates over multiple trials. The study compared error rates of models using Torch7 and PyTorch with the same or different seeds. Results showed no significant differences between implementations or seed usage. Standard deviation of measures was slightly smaller with the same seed. The study found no significant difference in error rates between models using Torch7 and PyTorch with the same or different seeds. The standard deviation of measures was slightly smaller with the same seed, indicating consistent results. The study found that error rates are significantly lower when measured at the best epoch compared to the single last epoch or last 10 epochs. To ensure reproducibility and fair comparisons, a method is proposed to select the best model without tuning on the test set. Using the error rate at the last iteration or the 10 last iterations shows no difference in the mean, but the standard deviation is smaller for the latter, making it preferable for single experiments. In this study, error rates are lower when measured at the best epoch compared to the single last epoch or last 10 epochs. Using the error rate at the last iteration or the 10 last iterations shows no difference in the mean, but the standard deviation is smaller for the latter, making it preferable for single experiments. For CIFAR experiments, the average error rate of the last 10 epochs is used for more robust and conservative results, while for SVHN experiments, the last 4 iterations are used due to a smaller number of bigger epochs. These observations suggest that using the average error rate from the last epochs leads to more robust and conservative results. In this study, comparisons between single-branch and multi-branch architectures were made at a constant parameter budget, showing an advantage for multi-branch networks. However, the training time for multi-branch networks is currently longer. Ways to reduce training time include reducing iterations, parameter count, or increasing width while reducing depth. Results for these options are shown in TAB12. The study compared single-branch and multi-branch architectures at a constant parameter budget, favoring multi-branch networks. Ways to reduce training time include reducing iterations, parameter count, or adjusting width and depth. Results for these options are shown in TAB12, with details on training times and configurations for CIFAR 10 and 100 datasets. In this section, the performance of single branch models and coupled ensembles is compared in a low training data scenario. Different options are explored, such as reducing the number of training epochs, depth, and parameter count. Results show that even though some options perform slightly worse than the full multi-branch baseline, they still outperform the single-branch baseline. DenseNet-BC L = 88, k = 20, e = 4 performs better than the single-branch baseline with a significantly reduced parameter count and training time. In this section, single branch models and coupled ensembles are compared in a low training data scenario. Results show that coupled ensembles significantly outperform single branch models for a fixed parameter budget. Experiments were conducted on two datasets, STL-10 and a subset of CIFAR-100, showing the superiority of coupled ensembles. Preliminary experiments on ILSVRC2012 also demonstrated the effectiveness of coupled ensembles over single-branch models. Experiments were conducted on images of size 256\u00d7256 with data augmentation involving random flips and crops of size 224\u00d7224. A DenseNet-169-k32-e1 single-branch model was compared to a coupled ensemble DenseNet-121-k30-e2, showing significant improvement with the ensemble approach. Future experiments with full-sized images and increased data augmentation are planned, with current results displayed in table 11."
}