{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, showing improved performance by incorporating ontological information. Ontologies provide a formal representation of domain knowledge through categories and relationships, aiding in structuring training data and neural network architecture design. Sound event classification typically does not utilize this additional information, despite neural networks being the state of the art for this task. The text discusses how sound event classification often overlooks the use of ontologies, which are formal representations of domain knowledge through categories and relationships. Neural networks, despite being state-of-the-art for this task, rarely consider ontologies in their design. Ontologies can provide structure to training data and network architecture, with examples including taxonomies based on abstraction hierarchies and interactions between objects and materials. Consideration of hierarchical relations in sound event classifiers can offer multiple benefits. Hierarchical relations in sound event classifiers offer benefits like back-off to general categories, disambiguation of acoustically similar classes, and penalizing misclassifications differently. Ontology-based network architectures have shown performance improvements in various domains, but are rarely used in sound event classification. Authors have proposed using ontologies in deep learning models for textual topic classification. Authors have proposed ontology-based network architectures for sound event classification, replicating a tree-like structure with intermediate layers to model the transformation from super class to sub classes. Using perceptrons for each node in the hierarchy, they showed improved performance by comparing predictions of classes and sub classes. Inspired by these approaches, a new ontology-based network framework for deep learning models is proposed in the following section, focusing on dealing with ontological information. The authors propose ontology-based networks for sound event classification, presenting a framework for deep learning models to handle ontological information. They describe assumptions, ontologies, and a Feed-forward model with an ontological layer. The framework extends to compute ontology-based embeddings using Siamese Neural Networks, considering ontologies with two levels but easily generalizable to more levels. Training data consists of audio representations associated with labels from the ontology. The framework presented in the study focuses on sound event classification using ontology-based networks. It involves training data with audio representations linked to labels from an ontology, with classes organized hierarchically. The framework can be extended to handle more levels in the ontology. The study focuses on sound event classification using ontology-based networks, where classes are organized hierarchically. The framework involves training data with audio representations linked to labels from an ontology. The proposed architecture includes a Feed-forward Network with an Ontological Layer, aiming to improve model performance by relating different classes during training. The proposed framework involves using an ontology-based neural network architecture for sound event classification. The architecture includes a Feed-forward Network with an Ontological Layer, where the base network learns weights for audio features to generate probability vectors for different classes. The ontological layer helps in relating classes during training to improve model performance. The proposed framework utilizes an ontology-based neural network architecture for sound event classification. The architecture involves a Feed-forward Network with an Ontological Layer that generates probability vectors for different classes based on audio features. The ontological layer helps in relating classes during training to enhance model performance by defining weights for standard layer connections. Training the model involves applying a gradient-based method to minimize the loss function, which is a combination of two categorical cross-entropy functions. The ontological layer defines weights for standard layer connections in the model. Training involves minimizing a loss function that combines two categorical cross-entropy functions. A hyperparameter \u03bb is tuned to adjust the combination. Ontology-based embeddings are learned using a Siamese neural network to preserve the ontological structure. The network enforces samples of the same class to be closer while separating samples of different classes based on subclasses and superclasses. The Siamese neural network (SNN) with the Feed-forward Network with Ontological Layer enforces samples of the same class to be closer, while separating samples of different classes based on subclasses and superclasses. The architecture of the SNN is shown in FIG1, with twin networks having the same base architecture and shared weights. Ontological embeddings are used to compute a Similarity metric (Euclidean Distance) to indicate the difference between samples with respect to the ontology. The Siamese neural network enforces samples of the same class to be closer, while separating samples of different classes based on subclasses and superclasses. Ontological embeddings are used to compute a similarity metric (Euclidean Distance) indicating differences between samples. The evaluation focuses on sound event classification performance using ontological-based neural network architectures for the Making Sense of Sounds Challenge 2 - MSoS dataset. The dataset for Making Sense of Sounds Challenge 2 (MSoS) includes audio files from various sources, with 1500 files in the development set and 500 files in the evaluation set. The files are divided into five categories, each containing a different number of sound types. The dataset is used for classifying abstract classes at different levels of the hierarchy. The Urban Sounds dataset (US8K) is also mentioned. The dataset for Making Sense of Sounds Challenge 2 (MSoS) includes audio files with a format of single-channel 44.1 kHz, 16-bit .wav files. The dataset contains 8,732 audio files divided into 10 subsets for training and testing. The Urban Sounds -US8K dataset is designed for classifying urban sounds with a taxonomy of 10 classes at the lowest level and 4 classes at the highest level. Audio files were taken from Freesound database and represented using state-of-the-art Walnet features BID1. The dataset consists of 8,732 audio files in a single-channel 44.1 kHz, 16-bit .wav format, divided into 10 subsets for training and testing. Audio recordings were represented using Walnet features BID1 and transformed via a convolutional neural network. The network architecture includes 4 layers with Batch Normalization, dropout rate of 0.5, and ReLU activation function. Parameters were tuned for both the network and the transformation of vectors. The dense layers in the network have a dimensionality of 128 for vector z, utilizing Batch Normalization, a dropout rate of 0.5, and ReLU activation function. Parameters were tuned for the network and transformation of vectors. Baseline models were considered for both level 1 and 2, without ontological information. Results of baseline models for MSoS and US8K datasets are shown in Table 1. The baseline performance for the MSoS challenge was reported as 0.81 for level 2. The baseline models for MSoS and US8K datasets were evaluated with different values of \u03bb to analyze the impact of the ontological layer. Results showed that using \u03bb values other than 0 or 1 improved performance. In the MSoS dataset, the best accuracy was achieved with \u03bb = 0.8, resulting in a 5.4% and 6% improvement over baseline models for level 1 and 2, respectively. For the US8K dataset, the best accuracy was obtained with \u03bb = 0.7, leading to a smaller improvement of 2.5% and 0.2% for level 1 and 2, respectively. Using the ontological structure, there was an absolute improvement of 5.4% and 6% in accuracy for level 1 and 2 in the MSoS dataset. In the US8K dataset, the improvement was smaller, with the best accuracy achieved using \u03bb = 0.7 resulting in a 2.5% and 0.2% improvement for level 1 and 2. The ontology-based embeddings showed tighter and better-defined clusters in t-SNE plots at different levels. The architecture described in Section 2.3 was tested to evaluate the performance of ontology-based embeddings for sound event classification. Using Walnet audio features, different super and sub class pairs were chosen to train the Siamese neural network for producing the embeddings. The SNN was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for optimal performance. 100,000 pairs yielded the best performance, and specific values were used for the loss function and classifiers at different levels. The experiment tested the ontology-based embeddings for sound event classification using a Siamese neural network. 100,000 pairs yielded the best performance, with specific values used for the loss function and classifiers at different levels. Results showed improved performance compared to the baseline, with better grouping of classes using ontology-based embeddings. t-SNE plots illustrated tighter and better-defined clusters with ontology-based embeddings. The experiment tested ontology-based embeddings for sound event classification, showing improved performance compared to the baseline. The t-SNE plots illustrated tighter and better-defined clusters with ontology-based embeddings, especially for level 2 classes. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89 accuracy in the Making Sense of Sounds Challenge. The framework proposed in the paper aims to design neural networks for sound event classification using hierarchical ontologies. In this paper, a framework was proposed to design neural networks for sound event classification using hierarchical ontologies. Two methods were shown to incorporate structure into deep learning models without adding more parameters. A Feed-forward Network with an ontological layer and a Siamese neural Network were used to improve performance in sound event classification. Results from datasets and challenges showed significant improvements over baselines, paving the way for further exploration of ontologies in sound event classification."
}