{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can create realistic images by mapping a latent space to observations. A non-statistical framework based on modular organization allows for targeted interventions on image datasets, enabling applications like style transfer and assessing robustness in pattern recognition systems. Deep generative models like Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) aim to create realistic images with disentangled latent representations for interpretability. However, current models lack a mechanistic or causal understanding of image properties. Access to a modular organization of generative models would enhance interpretability and enable new applications. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, supporting adaptability and robust decision making. Leveraging trained deep generative architectures for such extrapolations remains an open problem due to non-linearities and high dimensionality. This paper proposes a causal framework to explore modularity in deep generative models. In this paper, a causal framework is proposed to explore modularity in deep generative models, allowing for the assessment of how well they capture causal mechanisms and the role of specific internal variables. Counterfactuals are used to evaluate the effect of direct interventions in the network without influencing each other. The study explores how deep generative models capture causal mechanisms using counterfactuals to assess the role of internal variables. The models exhibit modularity in hidden units, allowing for editing of generated images. This work is related to interpretability in convolutional neural networks, with a focus on generative models. Generative models like InfoGANs and \u03b2-VAEs address disentanglement of latent variables for transformations on data points. The concept of intrinsic disentanglement uncovers internal network organization, different from interventions on GAN variables. Our approach, introduced independently, offers more flexibility compared to group representation theory for disentanglement. The framework proposed by Bau et al. (2018) focuses on interventions on internal variables of a GAN, requiring semantic information. In contrast, our approach, introduced independently, is more flexible and applies to arbitrary continuous transformations. Suter et al. (2018) also take an interventional approach to disentanglement, focusing on extrinsic disentanglement in a graphical model setting. Our general framework aims to precisely formulate the notion of disentanglement and connect it to causal concepts. The framework presented focuses on a generative model that maps a latent space to a manifold where data points live. A causal generative model is used to implement the mapping through a succession of operations. The framework aims to connect the notion of disentanglement to causal concepts. The framework connects the notion of disentanglement to causal concepts by using a Causal Generative Model (CGM) to map a latent space to a manifold where data points live. This involves a succession of operations and the use of endogenous variables represented by nodes in a causal graph. The internal representation is defined by mild conditions to ensure invertibility. The CGM framework connects disentanglement to causal concepts by using endogenous variables in a causal graph. It defines unit-level counterfactuals for latent inputs in the network. The CGM framework defines unit-level counterfactuals for latent inputs in the network, inducing a transformation of the generative model's output. Faithfulness of a counterfactual mapping is introduced to ensure interventions on internal variables align with the original model, preventing artifactual outputs or extrapolation to unseen data. Counterfactuals in machine learning can lead to artifactual outputs or extrapolation to unseen data. The concept of disentangled representation involves sparse encoding of real-world transformations. Supervised approaches manipulate relevant transformations explicitly, while unsupervised learning must learn these transformations from unlabeled data. State-of-the-art approaches enforce conditional independence between latent factors, leading to issues with prior constraints. State-of-the-art approaches aim to encode transformations by changes in latent factors and enforce conditional independence between them. However, this statistical approach faces challenges such as confounding factors in data distribution and the ill-posed nature of disentangled representation. Current unsupervised methods are mainly tested on synthetic datasets, with limited success on complex real-world data. The inductive bias for learning representations that benefit downstream tasks is still an open question. State-of-the-art unsupervised approaches have mainly been tested on synthetic datasets, with limited success on complex real-world data. Disentangled generative models on real-world datasets show lower visual sample quality compared to non-disentangled models. A non-statistical definition of disentanglement is proposed, focusing on transformations in the latent space that act on individual variables while leaving others available for encoding other properties. The notion of disentangled transformations in the latent space involves modifying different components of the representation, following the causal principle of independent mechanisms. This concept, known as extrinsic disentanglement, is agnostic to subjective property choices and statistical independence. It requires a different representation to uncover potentially related properties that are statistically independent but disentangled according to this definition. In the context of extrinsic disentanglement, disentangled transformations in the latent space require statistical independence between components. However, properties encoded by endogenous variables in a graphical model may not be statistically independent due to a common latent cause. To address this, a new definition of disentanglement allows for transformations of internal variables that are intrinsically disentangled with respect to a subset of endogenous variables. In the context of extrinsic disentanglement, disentangled transformations in the latent space require statistical independence between components. A new definition of disentanglement allows for transformations of internal variables that are intrinsically disentangled with respect to a subset of endogenous variables, enabling the implementation of arbitrary disentangled transformations. The proof of disentanglement in the latent space involves partitioning the intermediate representation into modules, where any transformation applied to a module leads to a valid transformation in the data space. This partitioning is essential for achieving a disentangled representation, which was not considered in classical approaches to disentanglement. Our framework suggests that a disentangled representation requires partitioning latent variables into modules, which was not previously considered. This modular structure allows for a broad class of disentangled transformations, with counterfactual interventions defining relevant transformations. The modular structure in the network allows for disentangled transformations, with counterfactual interventions defining relevant transformations. Assigning a constant value to endogenous variables helps in defining counterfactuals, aiming for faithful ones by sampling from the marginal distribution of the variables. The hybridization procedure involves taking two independent examples of latent variables to generate original outputs. The hybridization procedure involves taking two independent examples of latent variables to generate original outputs, allowing for assessing the causal effect of a given module on the output of the generator. This framework enables the generation of hybrid examples by mixing features from different generated images, based on the modular structure identified by the choice of subset E. The counterfactual hybridization framework assesses the causal effect of module E on the generator's output by generating pairs of latent vectors and estimating an influence map through mean absolute effect calculations. The approach considers unit-level causal effects and averages them over different interventions, resulting in an influence map that is then averaged across color channels. The approach involves computing the average treatment effect by taking the absolute value of unit-level causal effects and averaging them over different interventions. The influence map is then averaged across color channels to create a single grayscale heat-map pixel map. A scalar quantity is defined to quantify the magnitude of the causal effect by averaging the influence map across output pixels. The challenge lies in selecting subsets to intervene on, especially in networks with many units or channels per layer. A fine to coarse approach is used to extract groups, starting with estimating elementary influence maps for individual output channels and grouping them by similarity to define modules at a coarser scale. In the context of convolutional layers, elementary influence maps (EIM) are estimated for each output channel of the network. These maps are then grouped by similarity to define modules at a coarser scale. Representative EIMs for channels of a VAE trained on the CelebA face dataset suggest functional segregation, with some channels influencing finer face features while others affect the background or hair. Clustering of channels using their EIMs as feature vectors is performed in an unsupervised manner to achieve this grouping. In an unsupervised manner, clustering of channels is done using their EIMs as feature vectors. Influence maps are pre-processed by smoothing and thresholding, then fed into a Non-negative Matrix Factorization (NMF) algorithm to obtain cluster template patterns. Each influence map is assigned a cluster based on the template pattern with the highest weight. NMF is chosen for its ability to isolate meaningful image parts, and the approach will be compared to k-means clustering. The NMF algorithm is used to cluster channels based on their EIMs, assigning each map a cluster determined by the template pattern with the highest weight. A toy generative model is introduced to further justify the NMF approach, involving neural networks with hidden layers and random parameter choices. The model parameters involve sampling coefficients from a distribution, with specific conditions on sets of indices to encode influences in images. The identifiability result shows that the hidden layer partition corresponds to a disentangled representation, justifying the use of NMF for clustering channels based on their EIMs. The identifiability result for Model 1 shows that the hidden layer partition represents a disentangled structure, supporting the use of NMF for clustering channels based on their significant influences. The study explores the modularity of generative models trained on the CelebFaces Attributes Dataset, starting with a basic \u03b2-VAE architecture. The study investigated the modularity of generative models trained on the CelebFaces Attributes Dataset using a \u03b2-VAE architecture. The results showed that setting the number of clusters to 3 led to highly interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed these findings. The cluster stability analysis confirmed that setting the number of clusters to 3 led to highly interpretable cluster templates for background, face, and hair. The results showed that the NMF-based clustering outperformed the k-means algorithm, with a consistency of over 90% for 3 clusters. The cosine similarity between templates associated with matching clusters averaged at 0.9, further supporting the choice of 3 clusters for robust clustering. The clustering analysis showed that setting the number of clusters to 3 resulted in highly interpretable cluster templates for background, face, and hair. The average cosine similarity between matching cluster templates was 0.9, supporting the robustness of the clustering. Applying a hybridization procedure to the 3 modules obtained by clustering replaced features while maintaining the overall image structure. The \u03b2-VAE, designed for extrinsic disentanglement, may not be optimal compared to other approaches. The \u03b2-VAE, designed for extrinsic disentanglement, may not be optimal compared to other approaches. Further investigation is needed to explore whether better extrinsic disentanglement could also benefit intrinsic disentanglement in models not explicitly enforcing disentanglement. This approach can be applied to models not optimized for disentanglement, as demonstrated in experiments with basic models. Our approach was successfully applied to models not optimized for disentanglement, including a pretrained Boundary Equilibrium GAN (BEGAN) with minimal modifications. Selective transfer of features was observed by intervening on specific layers, resulting in noticeable effects on generated images. Selective transfer of features was observed by intervening on channels from the same cluster in two successive layers, resulting in noticeable effects on generated images. The hybridization procedure only mildly affected image quality, as confirmed by the Frechet Inception Distance evaluation. This approach was tested on high-resolution generative models and complex image datasets, showing promising scalability and generalization capabilities. The hybridization procedure had minimal impact on image quality, as confirmed by the Frechet Inception Distance evaluation. High-resolution generative models and complex image datasets were tested, demonstrating scalability and generalization capabilities. The BigGAN-deep architecture was used to generate hybrids by mixing features of different classes, showing the ability to create high-quality counterfactuals with modified backgrounds. The study demonstrated the ability to generate high-quality counterfactual images by mixing features of different classes using the BigGAN-deep architecture. Examples showed modified backgrounds while maintaining similar foreground objects. The generated counterfactual images were used to test the robustness of classifiers to contextual changes, comparing recognition rates of original classes across different layers. The study compared the recognition rates of pretrained classifiers on Tensorflow-hub to identify original classes. Results showed that classifiers performed differently at intermediate blocks, with Inception resnet outperforming others. Non-consensual classification results suggested that classifiers rely on different aspects of image content for decision-making. The study found evidence for interpretable modules in generative models trained on real-world datasets, leading to a better understanding of complex architectures and applications like style transfer and object recognition robustness assessment. This research aims to enhance the interpretability of deep neural networks and utilize them for various tasks efficiently. The research focuses on enhancing the interpretability of deep neural networks by utilizing trained generator architectures as mechanistic models. This approach involves using structural causal models to manipulate parts of the model independently, leading to a more sustainable research direction in Artificial Intelligence. The research utilizes structural causal models to enhance the interpretability of deep neural networks. A Causal Generative Model (CGM) captures the computational relations between input latent variables, generator's output, and endogenous variables, forming an intermediate representation. This model can manipulate parts of the model independently, leading to a more sustainable research direction in Artificial Intelligence. The Causal Generative Model (CGM) captures computational relations between input latent variables, generator's output, and endogenous variables to form an intermediate representation. It comprises a directed acyclic graph and a set of deterministic structural equations. The model aligns with a deterministic structural causal model by Pearl and has specificities reflecting practical model structures. The Causal Generative Model (CGM) consists of endogenous variables, latent inputs, and outputs aligned with a deterministic structural causal model by Pearl. It allows for modeling feed-forward networks with specific variable assignments and properties ensuring unambiguous assignments of variables and outputs. The Causal Generative Model (CGM) involves endogenous variables, latent inputs, and outputs aligned with a deterministic structural causal model by Pearl. It ensures unambiguous assignments of variables and outputs by introducing useful mappings, such as latent and endogenous mappings, constrained to subsets of their euclidean ambient space. This leads to the concept of an embedded CGM, where variables and outputs live on manifolds of smaller dimension than their ambient space. The embedded CGM involves mappings constrained to subsets of their ambient space, ensuring unambiguous assignments. The image sets are constrained by the parameters of the model and learning the generator parameters aims to match the support of the target data distribution. Learning the generator parameters in embedded CGMs aims to match the support of the target data distribution by manipulating output properties through transformations respecting the topology of the output space Y M. Injectivity of the generator function g M is a key requirement for embedded CGMs, with compact latent spaces leading to injective models. Generative models based on uniformly distributed latent variables are considered embedded CGMs if they satisfy the injectivity condition. The proof of embedding in CGMs relies on the injectivity of the generator function. Generative models with uniformly distributed latent variables are considered embedded CGMs if they are injective. VAEs' latent space can be made compact by restricting it to intervals, resulting in an embedded CGM. The CGM framework allows for defining counterfactuals in the network, following Pearl's concept of potential outcomes. The interventional CGM is defined by replacing structural assignments for variables in the network. Counterfactuals transform the output of the generative model and relate to disentanglement of internal variables. Intrinsic disentanglement involves a transformation of endogenous variables that only affects a subset of variables. Intrinsic disentanglement involves a transformation of endogenous variables that only affects a subset E, leading to a tuple of values v \u2208 V M. This concept relates to a causal interpretation of the generative model's structure, showing robustness to perturbations in subsystems. Counterfactuals represent examples of such perturbations, disentangled given their faithfulness. Intrinsic disentanglement involves a transformation of endogenous variables affecting subset E, leading to values v \u2208 V M. The proof of equivalence between faithful and disentangled transformations is discussed, showing that T is an endomorphism of V M when there is no common latent ancestor between subsets E and E. The absence of a common latent ancestor between subsets E and E ensures unambiguous assignment of values by non-overlapping latent variables A and B. This leads to a disentangled representation with modular subsets of endogenous variables, guaranteeing a rank K binary factorization. The model parameters ensure an injective mapping and counterfactual hybridization results in an influence map covering exactly I k. The conditions on I k and thresholding guarantee a rank K binary factorization of matrix B. The \u03b2-VAE architecture is similar to DCGAN with specified hyperparameters. The model consists of convolutional layers with skip connections for image sharpness. The pre-trained models used in the study include a model for the CelebA dataset with skip connections for image sharpness and a BigGan-deep architecture for 256x256 ImageNet. The architectures consist of convolutional layers with upsampling and skip connections, as well as ResBlocks for the generator. The models were not retrained, and details can be found in the respective papers by Berthelot et al. (2017) and Brock et al. (2018). The architecture used in the study includes layers with upsampling and skip connections, along with ResBlocks for the generator. The notion of influence maps generated by a VAE on the CelebA dataset is discussed, as well as FID analysis of BEGAN hybrids showing the closeness of hybrids to generated data. The entropy is computed for different intervened Gblocks and modules. The study discusses the architecture used, influence maps generated by a VAE on the CelebA dataset, and FID analysis of BEGAN hybrids. The entropy is computed for different intervened Gblocks and modules, showing that object texture is key for the classifier's decision. The hybrids based on interventions on Gblock number 4 have smaller entropy values, suggesting they are visually plausible images. The study explores the influence of interventions on Gblocks and modules in generating hybrids for the classifier's decision. The entropy analysis reveals that object texture plays a crucial role in classification outcomes. Additionally, discriminative models are tested on koala+teddy hybrids to assess classifier robustness. The resultant hybrids exhibit a mix of teddy bear and koala characteristics. The experiment investigates the use of interventions to assess classifier robustness using koala+teddy hybrids. The hybrids combine teddy bear and koala characteristics, with nasnet large showing more robustness to contextual changes compared to other classifiers."
}