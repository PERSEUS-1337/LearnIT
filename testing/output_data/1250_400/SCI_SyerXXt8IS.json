{
    "title": "SyerXXt8IS",
    "content": "Auto-generate input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, particularly in the insect olfactory network. The MothNet computational model, inspired by the moth olfactory network, generates new features for ML classifiers, outperforming traditional methods like PCA and NNs. This approach significantly improves performance on data sets like MNIST and Omniglot, reducing test set errors by 20% to 55%. These results demonstrate the value of BNN-inspired feature generators in the context of machine learning. The MothNet feature generator outperforms traditional methods like PCA and NNs, improving ML performance on datasets by 20% to 55%. This highlights the potential value of BNN-inspired feature generators in machine learning for rapid learning from limited data. Neural nets like MothNet can rapidly learn from few samples using competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. MothNet, a model of the insect olfactory network, demonstrated superior performance in learning vectorized MNIST digits with limited training samples. The MothNet model utilizes competitive inhibition in the antennal lobe (AL), sparsity in the mushroom body (MB), and Hebbian updates for weight adjustments. It aims to serve as a feature generator for machine learning classifiers, showing promise in learning from limited training samples. The MothNet model combines the AL-MB architecture with a downstream ML module to improve ML classifier performance on a non-spatial task using features generated by the AL-MB network. The trained MothNet Readouts significantly enhanced accuracies of NN, SVM, and Nearest Neighbors on the test set, indicating that the AL-MB network encoded class-relevant information inaccessible to ML methods alone. The MothNet model improved ML classifier performance by generating features that outperformed other methods like PCA, PLS, NNs, and transfer learning. The trained MothNet Readouts significantly enhanced accuracies of NN, SVM, and Nearest Neighbors on the test set, indicating that the AL-MB network encoded class-relevant information inaccessible to ML methods alone. The vMNIST dataset, created from downsampling and preprocessing MNIST data, showed improved accuracy with baseline ML methods. The full network architecture details of the AL-MB model (MothNet) are provided in [11]. The MothNet model improved ML classifier performance by generating features that outperformed other methods like PCA, PLS, NNs, and transfer learning. Trained MothNet Readouts enhanced accuracies of NN, SVM, and Nearest Neighbors on the test set, encoding class-relevant information inaccessible to ML methods alone. The vMNIST dataset, downsampling and preprocessing MNIST data, showed improved accuracy with baseline ML methods. Full network architecture details of the AL-MB model (MothNet) are provided in [11]. The data set includes samples with 85 pixels-as-features, and experiments were conducted comparing Cyborg vs baseline ML methods on vMNIST. MothNet improved ML classifier performance by generating features that outperformed PCA, PLS, and NNs. Trained MothNet Readouts enhanced accuracies of NN, SVM, and Nearest Neighbors on the test set, encoding class-relevant information inaccessible to ML methods alone. The effectiveness of MothNet features was compared to features generated by conventional ML methods using vMNIST experiments. The new features generated by MothNet significantly improved ML classifier performance compared to PCA, PLS, and NNs. These features were used as a front end for SVM and Nearest Neighbors. Additionally, transfer learning from an Omniglot dataset further enhanced the accuracy of the NN baseline. The MothNet architecture effectively captured new class-relevant features, demonstrating its superiority in feature generation. MothNet features significantly improved ML classifier performance compared to PCA, PLS, and NNs. The gains in accuracy ranged from 10% to 88% on the vMNIST ML baseline test set. MothNet features increased raw accuracy across all ML models, with a relative reduction in test set error of 20% to 55%. NN models saw the greatest benefits, with a 40% to 55% relative reduction in test error. MothNet features improved ML accuracy across all models, with a 20% to 55% reduction in test error. NN models benefited the most, with a 40% to 55% decrease in error. Even high baseline accuracies saw gains, and clustering information in MothNet readouts was leveraged effectively by ML methods. Gains were significant for N > 3, and various feature generators showed increases in mean accuracy for NN models. The MothNet architecture, utilizing feature generators like PLS and NN, significantly improved ML accuracy across all models. The competitive inhibition layer (AL) and high-dimensional sparse layer (MB) were key components, with the MB layer being most important for accuracy gains. Even with a pass-through AL, cyborgs still showed significant improvements in accuracy, with gains ranging from 60% to 100% compared to baseline ML methods. The AL layer added up to 40% of the total gain in generating strong features for NN models. The MothNet architecture, utilizing feature generators like PLS and NN, significantly improved ML accuracy. The competitive inhibition layer (AL) and high-dimensional sparse layer (MB) were key components, with the MB layer being most important for accuracy gains. A bio-mimetic feature generator with competitive inhibition, sparse projection, and Hebbian weight updates improved learning abilities on vMNIST and vOmniglot datasets. MothNet features outperformed standard methods like PCA, PLS, NNs, and pre-training. The competitive inhibition layer enhanced classification by creating attractors. The MothNet architecture improved ML accuracy by utilizing feature generators like PLS and NN. MothNet features were more useful than standard methods such as PCA, PLS, NNs, and pre-training. The competitive inhibition layer in MothNet enhanced classification by creating attractors for inputs, pushing similar samples away from each other towards their respective class attractors. The sparse connectivity from AL to MB in MothNet has computational and anti-noise benefits, resembling sparse autoencoders but with key differences. The MothNet architecture, unlike sparse autoencoders, utilizes feature generators like PLS and NN to improve ML accuracy. MothNet's competitive inhibition layer enhances classification by creating attractors for inputs, while the sparse connectivity from AL to MB provides computational and anti-noise benefits. The MothBrain (MB) differs from Reservoir Networks as MB neurons lack recurrent connections, and its Hebbian update mechanism is distinct from backprop, requiring very few samples to improve classification structure."
}