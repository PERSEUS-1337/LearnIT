{
    "title": "rJeeKTNKDB",
    "content": "The paper extends graph-to-graph translation methods for molecular optimization by incorporating multi-resolution representations and autoregressive graph decoding. The model significantly outperforms previous baselines on various molecular optimization tasks, aiming to improve biochemical properties of compounds through graph translation. The task involves translating molecular graphs to improve their biochemical properties, similar to machine translation. The model is trained on a corpus of molecular pairs where Y is a paraphrase of X with better chemical properties. The challenge lies in the vast space of potential candidates and the complexity of molecular properties. Prior work utilized valid chemical substructures to generate graphs but had limitations in tree and graph encoding. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation improves upon previous methods by interleaving the prediction of substructure components with their attachments in an auto-regressive manner. This approach allows for more consistent substructure attachments across different nodes in the junction tree, resulting in better molecular graph generation. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation introduces an auto-regressive decoder that predicts substructure components and their attachments in a sequential manner. This method efficiently models dependencies between successive attachments and substructure choices by representing molecules at different resolutions and decomposing generation steps into smaller hierarchical steps. Additionally, the approach can handle conditional translation by incorporating desired criteria as input. The proposed method introduces a more efficient decoding process for graph generation by decomposing generation steps into smaller hierarchical steps. It can handle conditional translation and addresses issues with inconsistent local substructure attachments during training. The model is evaluated on multiple molecular optimization tasks, outperforming previous state-of-the-art methods. The proposed autoregressive decoder improves molecular graph generation by addressing inconsistent local substructure attachments during training. It outperforms previous methods in discovering molecules with desired properties, showing significant improvements on optimization tasks. The model runs faster during decoding and incorporates hierarchical decoding and multi-resolution encoding. Additionally, conditional translation can succeed even with limited training data on molecular pairs. Conditional translation can succeed even with limited training data on molecular pairs, as shown by previous work on molecular graph generation. Various methods have been used to generate molecular graphs, including generative models that output adjacency matrices and node labels, as well as models that decode molecules sequentially node by node. Our work is closely related to methods that generate molecules based on substructures, adopting a two-stage procedure for realizing graphs. Graph neural networks have been extensively studied for graph encoding, with various methods used to generate molecular graphs. Our method differs from previous work by jointly predicting substructures and their attachments with an autoregressive decoder, avoiding local independence assumptions and stage-wise decoding. Our method differs from previous graph encoders by representing molecules as hierarchical graphs, spanning from atom-level graphs to substructure-level trees. It is closely related to approaches that learn to represent graphs hierarchically, utilizing graph coarsening algorithms to construct multiple layers of graph hierarchy. Our approach involves representing molecules as hierarchical graphs, from atom-level to substructure-level trees. Different from previous methods, we encode molecules into multiple sets of vectors for graph generation, dynamically aggregated by decoder attention modules. The goal is to learn a function that maps a molecule into another with improved chemical properties, using an encoder-decoder with neural attention. The graph translation task aims to learn a function that maps a molecule into another molecule with better chemical properties. This is achieved by representing molecules as hierarchical graphs with different components for substructure, attachment, and atom layers. The encoder-decoder with neural attention dynamically aggregates vectors for graph generation in each step. The decoder predicts substructure addition and attachment points in two steps to support hierarchical generation. The encoder-decoder model represents molecules as hierarchical graphs with substructure, attachment, and atom layers. The model encodes nodes into substructure, attachment, and atom vectors for decoding. The decoder predicts substructure addition and attachment points using attention mechanisms. The model encodes molecules as hierarchical graphs with substructure, attachment, and atom layers. Substructures are defined as subgraphs of the molecule induced by atoms and bonds. A substructure tree is constructed to show how substructures are connected in the molecule. The graph decoder generates a molecule by expanding its substructure tree in a depth-first order. The graph decoder generates a molecule by incrementally expanding its substructure tree in a depth-first order. It predicts new substructures and their attachments based on the input encoding. The model uses a MLP with attention to predict the probability of a new substructure being attached to the current node. If the probability is above 0.5, a new substructure is created and its type is predicted using another MLP. The model predicts new substructures and their attachments using a MLP with attention. If the probability of a new substructure being attached is above 0.5, a new substructure is created and its type is predicted. The attachment between substructures is defined by atom pairs, which are predicted in two steps. First, the atoms to be attached are predicted, and then the corresponding atoms to attach them to are found. The model predicts new substructures and their attachments using a MLP with attention. The prediction of configurations forms a vocabulary for each substructure, and the attachment points are found based on consecutive atoms. Candidate attachments are computed using atom representations. The predictions follow an autoregressive factorization, with each step depending on the previous one. Teacher forcing is applied during training, and attachment enumeration is manageable due to small substructure sizes. During training, teacher forcing is applied to the generation process of new substructures and their attachments. The encoder represents a molecule as a hierarchical graph with atom and attachment layers. The attachment enumeration is feasible due to small substructure sizes, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The atom layer is labeled with bond types. The attachment layer represents attachment configurations of substructures in molecule X. The substructure layer provides information for substructure prediction. Edges connect atoms and substructures between layers for information propagation. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. Each layer is encoded by a separate MPN, propagating information between atoms and substructures. The atom layer MPN encodes atom representations, while the attachment layer MPN encodes attachment configurations. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The atom layer MPN propagates message vectors between atoms for T iterations to output atom representations. The attachment layer MPN computes input features for nodes and edges based on embeddings and runs message passing to compute substructure representations. The substructure layer MPN computes input features for nodes and runs message passing to obtain substructure representations. In summary, the hierarchical encoder outputs a set of vectors cX. The hierarchical encoder uses a message passing network with three layers to compute substructure representations for molecules. The hierarchical decoder also employs a similar architecture to generate diverse outputs for molecular pairs in the training set. The model uses a variational translation approach to generate diverse outputs for molecular pairs in the training set. The latent vector z guides the mode of translation, sampled from a Gaussian prior during testing. The model is trained using variational inference, sampling z from the posterior distribution. The latent code z is used in the decoder to reconstruct the output Y, following a standard conditional VAE objective. The model uses a variational translation approach to generate diverse outputs for molecular pairs in the training set. During testing, the latent code z is passed to the decoder to reconstruct output Y. The model is extended to handle conditional translation by incorporating desired criteria as input. This allows users to control the outcome based on specified criteria, such as making Y drug-like and bioactive. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with an additional input g X,Y. The latent code is augmented as [z, g X,Y] and passed to the decoder. Users can specify criteria in g X,Y during testing to control the outcome. A novel conditional optimization task is constructed where desired criteria is fed as input. Molecular similarity between input X and output Y must be above a certain threshold at test time to prevent arbitrary translation. The model is evaluated on single-property optimization tasks with four different tasks in the dataset. The model is trained under an unconditional setting for single-property optimization tasks, including LogP Optimization with two similarity thresholds. Different criteria can be encoded as vector g for conditional translation setup. The desired criteria in g X,Y can control the outcome during testing. The text discusses the need to improve drug-likeness while avoiding DRD2 inactivity as an off-target to prevent side effects. Evaluation metrics include translation accuracy and diversity, with compounds successfully translated if they meet similarity and property constraints. The average property improvement is reported, along with the success rate of translation for other tasks. The text compares the method (HierG2G) with baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. It also introduces an atom-based translation model (AtomG2G) for direct comparison. Our model, HierG2G, outperforms baselines like JTNN and CG-VAE in translation accuracy and output diversity on four tasks. The AtomG2G model predicts atom and bond types in each generation step, achieving state-of-the-art results. Our model, HierG2G, outperforms JTNN and AtomG2G in translation accuracy and output diversity on four tasks, achieving state-of-the-art results. It runs 6.3 times faster than JTNN during decoding and shows over 10% improvement on the DRD2 task compared to AtomG2G. Our model also outperforms other translation methods like Seq2Seq in both accuracy and diversity. Our model, HierG2G, outperforms other models in translation accuracy and output diversity. Training on examples with strong constraints yields low success rates, but our conditional translation setup can transfer knowledge from other pairs. Ablation studies show that structure-based decoding benefits the DRD2 task more than the QED task. In the context of improving translation accuracy and output diversity, the HierG2G model experiments with different decoding strategies. By modifying the input of the decoder attention to include atom and substructure vectors, the model's performance decreases slightly on two tasks. Removing hierarchies in the encoder and decoder affects translation accuracy, with the removal of substructure layers leading to significant performance degradation. Replacing LSTM with GRU in the MPN architecture results in a decrease in translation performance, but HierG2G still outperforms JTNN. In this paper, a hierarchical graph-to-graph translation model is developed, generating molecular graphs using chemical substructures as building blocks. The model is fully autoregressive, learning coherent multi-resolution representations, outperforming previous models in various settings. The LSTM MPN architecture is used for both HierG2G and AtomG2G baseline, with experimental results showing superior performance. The message passing network MPN and attention layer details are provided, along with an illustration of the AtomG2G decoding process. The attention layer in the HierG2G and AtomG2G models uses a bilinear attention function with parameter \u03b8. AtomG2G is an atom-based translation method that represents molecules as molecular graphs. Training set sizes and substructure vocabulary sizes for each dataset are listed. The datasets used for single-property optimization are directly downloaded from a provided link. The training and substructure vocabulary sizes for each dataset are listed in Table 3. Multi-property optimization is achieved by combining the training sets of QED and DRD2 tasks. The test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters for HierG2G and AtomG2G models are specified, with both models trained using Adam optimizer. For AtomG2G and CG-VAE models, \u03bb KL = 0.3 and T = 20 iterations are used. CG-VAE is trained to generate molecules and predict properties. At test time, compounds are translated into latent representations and decoded to select the best property improvement. Keeping \u03bb KL low at 0.005 is crucial for meaningful results. The study focused on maximizing the predicted property score by using gradient ascent over z vectors. It was found that keeping the KL regularization weight low at \u03bb KL = 0.005 is crucial for meaningful results. Ablation studies were conducted, including experiments with different decoder models and reducing the number of hierarchies in the encoder and decoder MPN. In the experiments, the number of hierarchies in the encoder and decoder MPN is reduced. Two-layer model uses hidden vector h A k for predictions, while one-layer model uses atom vectors v\u2208S k h v. Hidden layer dimension is adjusted to match original model size."
}