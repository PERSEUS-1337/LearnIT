{
    "title": "HyM8V2A9Km",
    "content": "Sparse reward is a challenging issue in reinforcement learning. Hindsight Experience Replay (HER) addresses this by converting failure experiences into successful ones through relabeling goals. However, HER has limited applicability due to a lack of a compact goal representation. Augmenting experienCe via TeacheR's adviCE (ACTRCE) extends HER using natural language as the goal representation. ACTRCE efficiently solves difficult reinforcement learning problems in 3D navigation tasks, where non-language goal representations fail. The use of language goal representations allows the agent to generalize to unseen instructions and lexicons. Hindsight advice is crucial for solving challenging tasks, but only a small amount is needed for learning to progress effectively. The agent can generalize to unseen instructions and lexicons, with hindsight advice being crucial for solving challenging tasks. Deep reinforcement learning applications often require complex reward functions, but using sparse and binary rewards can simplify the learning process. Hindsight Experience Replay (HER) addresses the issue of sparse rewards by converting failure experiences into successful ones through relabeling goals. The sparse reward in deep reinforcement learning can make learning difficult. Hindsight Experience Replay (HER) addresses this by converting failed experiences into successful ones by relabeling goals. The assumption that there exists a goal for every state in the environment is crucial for HER to work efficiently. However, representing the goal using the state space can be inefficient and contain redundant information. In deep reinforcement learning, representing goals using the state space can be inefficient and redundant. Natural language goal representation offers a compact and informative solution, allowing for flexible descriptions of goals across tasks and environments. By combining the HER framework with language representation, we introduce an efficient technique called Augmenting experienCe via. In this paper, the HER framework is combined with natural language goal representation to introduce an efficient technique called Augmenting experienCe via TeacheR's adviCE (ACTRCE). This method involves a teacher giving advice to the agent in natural language after each episode, allowing the agent to form new experiences and alleviate the sparse reward problem. The language goal representation enhances the agent's ability to solve reinforcement learning problems in challenging environments, generalize to unseen instructions, and adapt to instructions with unseen lexicons. The agent can efficiently solve reinforcement learning problems in challenging environments by replacing the original goal with advice and a reward of 1. Language goal representation, combined with hindsight advice, allows the agent to generalize to unseen instructions and adapt to new lexicons. Using hindsight advice is crucial for solving challenging tasks, with even a small amount being sufficient for learning to progress. This work is significant for language learning and addresses the problem of grounding language in a simulated physical world using reinforcement learning techniques. Our work combines reinforcement learning and rich language advice to efficiently ground language. The traditional reinforcement learning setting involves an agent interacting with a Markov Decision Process to maximize cumulative return. The agent interacts with a Markov Decision Process to maximize cumulative return by choosing actions based on a policy. The goal is to maximize the expected discounted sum of rewards. Q-learning is an off-policy RL algorithm based on the Bellman equation, using semi-gradient descent to minimize the TD error. Deep Q-Network is a model that builds on Q-learning. Q-learning is an off-policy RL algorithm based on the Bellman equation, using semi-gradient descent to minimize the TD error. Deep Q-Network uses a neural network to approximate Q* and improves training stability with a replay buffer and target network. Goal-oriented reinforcement learning framework augments the Markov Decision Process with a goal space G, where a goal induces a reward function conditioned on the given goal. The agent's objective is to maximize the expected discounted cumulative return given the goal. The agent's objective is to maximize the expected discounted cumulative return given the goal. Reward function is sparse, making it hard for the agent to learn. HER proposes a solution by collecting experiences under the goal and relabeling them for learning. The agent collects experiences under the goal and learns from goal-transformed experience using an off-policy algorithm. A representation map from state space to goal space is assumed for flexible relabeling with a desirable goal, but constructing such a mapping is challenging. The goal representation suggested by the authors is redundant and limits the algorithm's applicability, especially in complex environments like a 3-D space with multiple possible states satisfying the goal. The algorithm's applicability is limited in complex environments like a 3-D space with multiple possible states satisfying the goal. The goal representation mapping from state to goal space is assumed for relabeling with a desirable goal, but constructing such a mapping is challenging. Language provides an abstract representation for a goal and reduces redundancy. The proposal suggests using natural language to represent the goal space, with a teacher providing a description of the goal for each state. This approach aims to reduce redundancy in representation and can help convert a failure trajectory into a successful one by relabeling the original goal with the teacher's advice. In a game, failure trajectories can be converted to successful ones by relabeling the original goal with teacher advice. Positive and negative reward signals are both necessary for training. Multiple goals can be satisfied in a state using off-policy RL algorithms and a language goal space. In a game, failure trajectories can be converted to successful ones by relabeling the original goal with teacher advice. Positive and negative reward signals are both necessary for training. Multiple goals can be satisfied in a state using off-policy RL algorithms and a language goal space. Given an off-policy RL algorithm and replay buffer, a group of teachers provide different advice corresponding to different goal descriptions, which are then used to augment the replay buffer for training. In a game, failure trajectories can be converted to successful ones by relabeling the original goal with teacher advice. Positive and negative reward signals are both necessary for training. Multiple goals can be satisfied in a state using off-policy RL algorithms and a language goal space. A group of teachers provide different advice corresponding to different goal descriptions, which are then used to augment the replay buffer for training. The approach involves converting a language goal into a continuous vector representation for neural network training using methods like one-hot vector representation and recurrent neural networks. The approach involves converting language goals into continuous vector representations for neural network training. This can be done by representing words as one-hot vectors and using a recurrent neural network, or by using a pre-trained language component. Integrating language representation into the model involves designing an architecture with three modules: a language component, a continuous vector converter, and an attention vector. The architecture consists of 3 modules: a language component converts instructions into a continuous vector, an observation processing component uses convolution neural networks to obtain image representation, and gated attention fuses goal information with observation. Experimental setup includes 2 environments: KrazyGrid World and ViZDoom. The proposed method's effectiveness is demonstrated through a comprehensive comparison of goal representations in hindsight advice, generalization, and semantic similarities. The experimental setup includes 2 environments: KrazyGrid World and ViZDoom. A comparison is made between different goal representations, such as one hot vectors, GRU embeddings, and pre-trained word embeddings. Results show that GRU and pre-trained embeddings scale better with increasing instructions. Hindsight language advice significantly improves sample efficiency, even with limited teacher advice. In challenging tasks, significant improvement in sample efficiency is seen with teachers' advice. Even limited advice can lead to progress, reducing the burden of the method. The experiments were conducted in KrazyGrid World, a 2D grid environment, and ViZDoom, a 3D environment based on the game Doom. The ViZDoom environment involves reaching goals of different colors with natural language instructions. Singleton tasks are individual language goals, while composed tasks combine multiple singleton tasks using \"and\" or \"or\" functions. The experiments show improved sample efficiency with teachers' advice in challenging tasks. In the experiments, singleton tasks A and B are combined using \"and\" and \"or\" functions to create new tasks \"A and B\" and \"A or B\". The completion criteria differ for each, with \"A and B\" requiring both tasks to be completed and \"A or B\" needing just one. The study compares language-based goal representations with non-language representations, showing that language goals are more effective as task difficulty increases. The DQN algorithm is used for reinforcement learning in both KGW and ViZDoom environments, with multi-environment training involving sampling 16 environments, collecting data, updating agents, and repeating the process. In this section, the effectiveness of language-based goal representations is compared to non-language representations. Language goal representations are shown to be more effective as task difficulty increases and can generalize to unseen goals in training. Different models for language sentence representation are described, including using GRUs and pre-trained sentence embeddings for robustness. In this study, language-based goal representations are found to be more effective as task complexity rises and can generalize to new goals. Different models for language sentence representation are explored, including using GRUs and pre-trained sentence embeddings for robustness. The one-hot vector representation is also discussed for non-language baseline instructions. The study compares the effectiveness of language-based goal representations with one-hot vector representations in learning tasks of varying complexity. While one-hot representations perform well in simpler tasks, they struggle in more challenging compositional tasks. Language representations enable better generalization to unseen instructions. The study compares the effectiveness of language-based goal representations with one-hot vector representations in learning tasks. Language representations enable better generalization to unseen instructions, achieving a 97% success rate. Visualization analysis shows significant differences in learned embeddings for different goal representations. The study compares language-based goal representations with one-hot vector representations in learning tasks. Significant differences in learned embeddings were found, with GRU and InferLite embeddings showing similar block-like structures. t-SNE embeddings showed meaningful clustering with language goal representations. Pre-trained embeddings were used to allow the model to generalize to unseen lexicons at test time through representation transfer. The study demonstrates the effectiveness of language goal representation using pre-trained embeddings for generalization to unseen lexicons at test time. By replacing words with synonyms, the agent achieves tasks above 66% of the time, showing the importance of understanding synonyms for robust learning in noisy settings. This method can be enhanced with advice from humans, who can describe the same meaning in various ways. In this section, the study compares the effectiveness of the ACTRCE method with DQN algorithm in learning challenging tasks with and without hindsight language advice. Results show that even a small amount of advice (1%) can significantly improve learning outcomes. The study also utilized recurrent neural networks for embedding language goals in both methods. Experiments on different grids with varying lava obstacles and colored goals were conducted, showing the success rate over 16 environments. The baseline DQN performance is also presented. Our method quickly learned and achieved good results on different grid environments with lava obstacles and colored goals, outperforming the baseline DQN which failed to learn. In ViZDoom experiments, our agent showed superior performance in more difficult tasks compared to DQN and A3C implementations. The study also highlights the agent's Multitask and Zero-Shot generalization performance. In challenging environments like the 7-objects grid, only the agent trained with ACTRCE was able to learn efficiently, outperforming the baseline DQN. The agent also excelled in tasks with 3 goals and 3 lavas in KGW, and in ViZDoom with 5 objects in easy mode, showcasing superior performance compared to the baseline DQN. The agent trained with ACTRCE showed superior performance in challenging environments with multiple objects, outperforming the baseline DQN. The agent excelled in tasks with different goals and environments, showcasing its ability to learn efficiently with hindsight advice. Several approaches have utilized natural language in reinforcement learning, such as translating language advice into a neural network to encourage desired actions. Human feedback in the form of natural language has been used to shape rewards for RL agents, and language has been proposed as a latent parameter space for few-shot learning problems. Additionally, there is research on using reinforcement learning to learn grounded language, known as task-oriented language grounding. BID0 proposed using language as a latent parameter space for few-shot learning problems in policy search. Misra et al. and Yu et al. mapped language instructions to actions in 2D environments. BID12 presented an agent learning to execute instructions in a 3D environment through reinforcement learning. Our work builds on Chaplot et al.'s gated-attention architecture for combining language and image features. Our work extends the use of natural language as a goal representation in a 3D environment, building on previous works that utilized language for few-shot learning and instruction execution. The proposed ACTRCE method demonstrates the benefits of using language as goal representations when combined with hindsight advice. The ACTRCE method uses natural language as a goal representation for hindsight advice in challenging 3D navigation tasks. It efficiently solves difficult reinforcement learning problems and can generalize to unseen instructions with a pre-trained language component. The algorithm relies on hindsight advice, but only a small amount is needed for learning to progress effectively. The ACTRCE algorithm efficiently handles noisy natural language advice, showing practicality with minimal advice. KrazyGrid World is a 2D grid environment with different tiles and colors, where the agent's goal is to reach goals of various colors. The grid state includes functionality and color attributes represented in one hot vectors, with a global view of the grid state provided to the agent. The environment consists of a 9x9 grid with 3 distinct colored goals and various lava obstacles. The episode ends when the agent reaches a goal or lava. In a 9x9 grid environment with 3 colored goals and lava obstacles, the episode ends when the agent reaches a goal or lava. The goal space was expanded to include compositions of goals, with the episode terminating when the agent reaches a lava or two different goals, or runs out of time. An extra action called \"flag\" was added for the agent to signal goal completion. The ViZDoom environment is a 3D learning space based on the game Doom. Agents navigate using actions like turning and moving forward to reach a specified object goal. The episode ends when the object is reached or time runs out, with rewards given accordingly. The ViZDoom environment involves agents navigating to reach a specified object goal within a limited time frame. The difficulty modes vary in terms of object and agent distribution, with easy mode having fixed spawn locations and objects placed in front of the agent, while hard mode involves random spawns and objects not necessarily in view. Compositional instructions consist of two single object instructions joined by \"and\", without superlatives like \"largest\". The ViZDoom environment involves agents navigating to reach a specified object goal within a limited time frame. Compositional instructions consist of two single object instructions joined by \"and\", without superlatives like \"largest\". The instructions must be unambiguous, with mutually exclusive sets of valid objects for each instruction. The environment includes a HUD displaying thumbnail images of reached objects, with the episode terminating once the agent reaches a second object. The ViZDoom environment involves agents navigating to reach a specified object goal within a limited time frame. A HUD displays thumbnail images of reached objects, with the episode ending once the agent reaches a second object. Synonym instructions are generated by replacing words with synonyms. Positive and negative feedback is given based on whether the agent reaches an object or not. In the ViZDoom environment, agents navigate to reach specific object goals within a time limit. Positive and negative feedback is provided based on whether the agent reaches objects. For unreached objects, instructions are randomly selected to guide the agent, with different strategies for singleton and compositional tasks. In the ViZDoom environment, agents receive feedback based on reaching objects. Instructions are generated for singleton and compositional tasks using convolution layers and LSTM for processing grid observations and language sentences. The text describes the use of LSTM and convolutional layers for processing grid observations and language sentences in the ViZDoom environment. The LSTM's hidden vector is used as an attention vector, which is multiplied with feature maps before passing through fully connected layers for action value prediction. The grid observations are preprocessed using convolution layers with ReLU activation functions, followed by bidirectional LSTM processing and additional convolution layers. The text describes processing observations in the ViZDoom environment using LSTM and convolutional layers. The history vector is used for attention in Bi-directional LSTMs, followed by convolution layers with ReLU activation functions. The final output predicts 5 action values. The architecture is similar to BID4, with a linear output layer and no dueling architecture. The state input is an RGB image, processed through convolution layers with ReLU activation functions. Language input is embedded using an embedding matrix. The text describes processing observations in the ViZDoom environment using LSTM and convolutional layers. The state input is an RGB image processed through convolution layers with ReLU activation functions. Language input is embedded using an embedding matrix. The network architecture does not use dueling architecture. The GRU BID5 with 256 hidden units processes word embeddings, followed by a fully connected layer with 64 output units for attention. The gated feature maps are flattened and passed through a fully connected layer with ReLU activation. The LSTM with 256 hidden units predicts 3 action values. Hyperparameters for KrazyGrid World experiments are tuned for learning rate, replay buffer size, and training frequency. For KrazyGrid World experiments, hyperparameters are tuned for learning rate, replay buffer size, and training frequency. Double DQN is used to reduce Q-value overestimation, and Huber loss is applied for stable gradients. ViZDoom environment utilizes training instructions from BID4 and A3C code from BID3 for training. DQN is implemented on top of Arnold's code, with a cyclic buffer replay buffer containing recent transitions. Episodes are generated with an -greedy policy. We implemented our version of DQN on top of Arnold's code, using A3C for training. The replay buffer contains recent transitions, with episodes generated using an -greedy policy. Double DQN is used to reduce Q-value overestimation, with Huber loss for stable gradients. The network is updated every 4 frames on easy mode and 16 frames on difficult mode, leading to better performance. The network is updated every 4 frames on easy and 16 frames on difficult mode for better performance. Sampling from the replay buffer involves selecting 32 consecutive frames from a random episode, leading to more accurate estimates of the hidden state. Running 16 parallel threads helps alleviate sample correlation in the mini-batch. The target network is synchronized with the current model every 500 time steps, and one additional thread evaluates the multi-task success rate during training. In this section, the details of teacher types are described. A subset Gd \u2286 G denotes all desired goals for the agent. Each episode, a goal g \u2208 Gd is sampled, and the agent explores the environment based on this goal. Advice is obtained from a teacher T at the end of the episode, depending on the type of teacher. Three types of teachers are considered: Optimistic, Knowledgeable, and Pessimistic. In this section, three types of teachers are described: Optimistic, Knowledgeable, and Discouraging. Optimistic teachers give advice only when the agent achieves a desirable goal, while Knowledgeable teachers describe all scenarios, including undesirable behaviors, as advice. Discouraging teachers describe a desired goal that the agent has not achieved as advice. The study compared different teaching methods to the DQN algorithm, evaluating their performance in KrazyGrid World with varying numbers of obstacles and goals. In the study, three teaching methods were evaluated in KrazyGrid World: Optimistic, Knowledgeable, and Discouraging. The results showed that the Discouraging method, denoted as ACTRCE, quickly learned and achieved good results on environments with 3 lavas and 3 goals. However, as the number of lavas increased, the task became harder and performance dropped. Knowledgeable teachers always helped speed up learning, providing language advice even in challenging scenarios. In KrazyGrid World, the Discouraging teaching method (ACTRCE) showed quick learning and good results with 3 lavas and 3 goals, but struggled as the number of lavas increased. Knowledgeable teachers helped speed up learning by providing language advice even in challenging scenarios. A transfer learning experiment was designed to test the hypothesis that learning easier tasks like \"reach lava\" could aid in learning more difficult tasks like \"reach goals\". In a transfer learning experiment, agents were pretrained with a pessimistic teacher in KrazyGrid World. Despite different goals, the pretrained agents learned faster than unpretrained ones, especially in environments with multiple lavas. This suggests that learning easier tasks can benefit learning more difficult tasks. In experiments on ViZDoom with single target for 5 objects on easy and hard mode, pretrained agents learned faster than unpretrained ones, especially in environments with multiple lavas. Learning easier goals can provide signals for harder goals, showing that tasks requiring similar modules can benefit from easier learning. DQN's learning was less consistent on hard mode compared to ACTRCE, which had low variance across seeds. In experiments on ViZDoom, DQN's learning was less consistent on hard mode compared to ACTRCE, which had low variance across seeds. A3C baseline from BID4 was used with less sample efficiency compared to DQN/ACTRCE implementation. Average episode length decreased with ACTRCE on harder tasks. The GRU hidden state representation of the sentence was used in the plots in FIG0, showing that the average episode length decreases with ACTRCE on harder tasks. A cumulative success rate (CSR) versus episode length curve was constructed to evaluate model performance, with the curve being monotonically increasing. FIG0 displays the Multi-task (MT) cumulative success rate for the 3 ViZDoom environment tasks. The Multi-task cumulative success rate for 3 ViZDoom tasks is shown in FIG0, with GRU hidden state language encoding. ACTRCE outperforms baseline DQN in longer trajectories for 5 objects hard mode, while DQN only succeeds in short episodes. In the 7 objects hard mode, ACTRCE maintains performance, while DQN struggles with longer trajectories. For the 5 objects composition task, ACTRCE shows two groups of trajectories based on proximity of target objects. In the 5 objects composition task, ACTRCE had two groups of trajectories based on the proximity of target objects. One group required less than 10 time-steps when the targets were adjacent, while the other group needed over 20 time-steps when the targets were not adjacent."
}