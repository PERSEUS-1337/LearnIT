{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the importance of providing online explanations during execution to reduce human mental workload is emphasized. The challenge lies in the interdependence of different parts of an explanation, requiring a careful approach to generating online explanations. Three different implementations with varying online properties are presented based on a model reconciliation setting. Evaluation was done with human subjects in a planning competition domain and simulation with different problems across two domains. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective, but a good explanation should consider the discrepancies between the human and robot models. Model reconciliation is essential for generating lucid explanations from the human's viewpoint. In prior work, discrepancies between human and robot models are encapsulated as model differences, leading to model reconciliation. The robot's behavior should make sense in the updated model to match the human's expectation. Model reconciliation addresses decision-making in the presence of such differences, but the mental workload for understanding explanations remains an issue in explanation generation. In prior work, discrepancies between human and robot models are reconciled as model differences, addressing decision-making. However, the mental workload for understanding explanations remains an issue. The concept of online explanations is introduced to reduce mental workload by intertwining communication with plan execution. This process spreads out information smoothly to avoid cognitive dissonance. The concept of online explanations reduces mental workload by spreading out information smoothly to avoid cognitive dissonance. This is illustrated through a scenario where Mark adjusts his study plan with Emma without revealing his initial intentions until after lunch. Mark adjusts his study plan with Emma without revealing his initial intentions until after lunch, demonstrating the importance of providing explanations in an online fashion. He gradually reveals his reasoning to maintain his plan as the execution unfolds, minimizing the information conveyed to reduce mental workload. In this paper, a new method for online explanation generation is developed, intertwining explanation with plan execution. The focus is on reducing mental workload by breaking explanations into multiple parts communicated at different times during plan execution. Three approaches for online explanation generation are implemented, each focusing on different properties such as matching the plan prefix, making the next action understandable, and matching the robot's plan with an optimal human plan. In this paper, a new method for online explanation generation is developed to reduce mental workload by breaking explanations into multiple parts communicated at different times during plan execution. Three approaches are implemented, focusing on matching the plan prefix, making the next action understandable, and aligning the robot's plan with an optimal human plan. The goal is to enable AI agents to be self-explanatory in their behaviors and operate effectively as teammates. Explainable AI is essential for human-AI collaboration as it helps improve human trust in AI agents and maintain shared situation awareness. The effectiveness of explainable agency is based on accurately modeling human perception of the AI agent. This allows the agent to generate legible motions, explicable plans, and assistive actions, prioritizing understanding over cost optimality. The model of other agents allows an AI agent to infer their expectations and generate legible motions, explicable plans, and assistive actions. The agent can signal its intention before execution to improve human understanding and explain its behavior by generating explanations based on the recipient's perception model. This approach prioritizes understanding over cost optimality. In this work, the focus is on generating online explanations that intertwine with plan execution, especially for complex explanations. The goal is to provide a minimal amount of information to explain part of the plan currently of interest, such as the next action. This approach is based on the model reconciliation setting and aims to improve understanding by generating explanations based on the recipient's perception model. Our problem is closely associated with planning problems, defined as a tuple (F, A, I, G) using PDDL, similar to STRIPS. Actions have preconditions, add and delete effects. The robot's plan to be explained must be optimal according to M R, assuming rational agents. Model reconciliation setting also considers the cost of the plan generated using M R. The robot's plan to be explained must be optimal according to M R, assuming rational agents. Model reconciliation setting considers the human's model M H, matching expected behavior for plan reconciliation. Explanation generation updates M H to make the robot's plan fully explainable in the human's model. A mapping function in BID6 converts planning problems into a set of features for problem specification. The explanation generation problem in BID6 involves reconciling the human's model (M H) with the robot's plan (\u03c0 * I,G) by generating a set of unit feature changes. A complete explanation is one that ensures the robot's plan is optimal in the human's model after the changes. The goal is to minimize the number of unit feature changes required for a complete explanation. Online explanation generation introduces a new approach to address the mental workload requirement of humans in understanding explanations. It provides a minimal amount of information during plan execution to explain the part of the plan that is of interest and not explainable. An online explanation consists of sub-explanations (e k , t k ) where e k represents the unit features to be changed at step t k in the plan. Online explanation generation involves providing explanations during plan execution to address the mental workload of humans. It consists of sub-explanations (e k , t k ) where e k represents unit features changed at step t k in the plan. The process includes three approaches: OEG with Plan Prefix matching, OEG with Next Action matching, and OEG with any prefix matching. The planning process must consider how model changes affect human expectations after each sub-explanation. The planning process for online explanation generation involves generating sub-explanations based on model changes to address human expectations. Model changes must be searched to ensure that plan prefixes remain consistent for further sub-explanations. This process is illustrated in FIG1. An OEG-PP consists of sub-explanations (e k , t k ) where e k represents unit features changed at step t k in the plan. The planning process for online explanation generation involves generating sub-explanations based on model changes to address human expectations. Model changes must be searched to ensure that plan prefixes remain consistent for further sub-explanations. This search process is illustrated in FIG1. An OEG-PP is a set of sub-explanations (e k , t k ) where e k represents unit features changed at step t k in the plan. The search for sub-explanations starts from the robot model and stops where the plan prefixes for the updated human model and the robot model match. Our approach for generating sub-explanations starts from the robot model and stops where the plan prefixes for the updated human model and the robot model match. This process, similar to MME, involves running multiple searches to match prefixes rather than the whole plan at once. Despite being more computationally expensive, our approach outperforms MCE and MME in terms of computation by considering only a small set of changes at a time. The maximum state space model modification in the robot model reconciles the two models up to the current plan execution step. The robot model reconciles the two models up to the current plan execution step by finding the largest set of model changes. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. The algorithm continues until the human's plan matches with that of the robot's plan, ensuring the robot and human plan have the same prefix at any step of plan execution. The approach relaxes the plan prefix condition to reconcile between M R and M H for the very next action in the plan, considering the human's limited cognitive memory span. The search is performed from M H \\M H for computational efficiency, focusing on explaining the immediate next action. The OEG-PP approach relaxes the plan prefix condition to reconcile between M R and M H by focusing on explaining the immediate next action for computational efficiency. The search process combines elements from M H and M R models to improve performance. The OEG-PP approach relaxes the plan prefix condition to reconcile between M R and M H by focusing on explaining the immediate next action for computational efficiency. In this setting, the robot aims to match its plan prefix with the human's optimal plan, using a compilation approach to reduce computational costs. The OEG-PP approach focuses on explaining the immediate next action to reconcile between M R and M H efficiently. A compilation approach is used to ensure that a plan prefix is always satisfied in the compiled model. This involves adding predicates to actions and using a recursive model reconciliation process to search for e k. The OEG-PP approach uses a compilation approach to reconcile between M R and M H efficiently by adding predicates to actions and using a recursive model reconciliation process to search for e k. The agent checks for a human optimal plan that matches the robot's plan up until the next action, stopping when no such plan exists, and continues this process until an optimal human plan is found. This approach was evaluated for online explanation generation with human subjects and in simulation, comparing results with Minimally Complete Explanation. The OEG-PP approach efficiently reconciles between human and robot plans by adding predicates to actions and using a recursive model reconciliation process. It was evaluated for online explanation generation with human subjects and in simulation, comparing results with Minimally Complete Explanation. The approach was tested on different problems in the rover and barman domains, aiming to reduce mental workload and improve task performance. The study aims to confirm the benefits of online explanation generation for reducing mental workload and improving task performance. Human subjects were evaluated in a modified rover domain where the robot explores Mars, takes samples, and communicates results. The robot must calibrate its camera, have space for samples, and drop samples to take new ones. In another domain, the robot acts as a barman serving drinks with specific constraints. The study compares minimally complete explanations (MCE) with OEG-PP, OEG-NA, and OEG-AP approaches in the rover and barman domains. Results show differences in the number of shared model features and total features in explanations. OEG-PP and OEG-NA sometimes have more features than MCE, as expected. In comparing MCE with OEG-PP, OEG-NA, and OEG-AP approaches, differences in the number of shared model features and total features in explanations were observed. OEG-PP and OEG-NA may have more features than MCE in some cases, due to focusing on generating minimal information at each time step. The remaining distance between the robot's plan and the human's plan is attributed to the approach taken by OEG-NA and OEG-AP in considering only the immediate next action or all optimal plans, respectively. In comparing MCE with OEG-PP, OEG-NA, and OEG-AP approaches, differences in shared model features and total explanations were noted. OEG-NA considers only the immediate next action, leading to a distance between robot and human plans. OEG-AP may not return the same plan as the robot's, as it considers all optimal human plans. The plan distance gradually decreases during execution, potentially reducing the human's mental workload. Model updates are sorted by feature size, with a focus on minimal changes from the robot's side. In a study comparing different approaches for online explanation generation, the researchers designed a human study using Amazon Mechanical Turk with 3D simulation. Subjects were introduced to the rover domain and given a task with a 30-minute time limit. The study compared three approaches for explanation generation and also tested an approach that randomly breaks explanations during plan execution. In an experiment using Amazon Mechanical Turk with 3D simulation, subjects acted as rover commanders on Mars, assessing the rover's actions. Explanations were provided in plain English with GIF images. To reduce influence between runs, each subject only performed one task setting. Additional spatial puzzles were included to increase cognitive demand. The rover's current action is being questioned in the experiment, with explanations provided by different approaches. Additional spatial puzzles were added to increase cognitive demand. Hidden information was deliberately removed to create scenarios where explanations must be provided. In the MCE setting, information is shared at the beginning, while in the MCE-R setting, information is communicated at different steps. In various settings, the robot provides explanations for its actions, with different approaches used for online explanation generation. Subjects are asked to assess the sense of the robot's actions at different steps. Efficiency of explanation approaches is evaluated using the NASA Task Load Index questionnaire. The study evaluated the efficiency of different explanation approaches using the NASA Task Load Index questionnaire, which assesses mental workload through various dimensions. The experiment involved creating an academic survey and recruiting 150 human subjects on MTurk. The study evaluated different explanation approaches using the NASA Task Load Index questionnaire. A total of 150 human subjects were recruited on MTurk for the academic survey. The subjects' understanding of the robot's plan was examined, and distances across different settings were compared. The distance metric calculated how well the human subjects understood the robot's plan by comparing the number of questionable actions to the total number of actions in a plan. A lower distance value indicated a closer alignment between the human's plan and the robot's plan. The study compared different explanation approaches using the NASA Task Load Index questionnaire with 150 human subjects. Results showed that OEG approaches reduced human mental workload better than MCE approaches, as evidenced by lower questionable actions and better performance in NASA TLX measures. The OEG approaches created more temporal demand due to intertwining explanation with plan execution. The experiment compared OEG and MCE approaches using objective and subjective measures. OEG approaches had fewer questionable actions, higher accuracy, and lower mental workload compared to MCEs. Time analysis showed OEG-NA had the shortest task completion time. In a comparison between OEG and MCE approaches, the overall p-value was 0.0068 across five categories. Time analysis showed varying completion times, with OEG-NA being the shortest. The accuracy of the secondary task did not show significant differences. A novel approach for explanation generation was introduced to reduce mental workload during human-robot interaction, focusing on breaking down complex explanations into easily understandable parts. Three different approaches were provided, each focusing on a different aspect of explanation generation intertwined with plan execution, aiming towards achieving explainable AI. In a novel approach, three different methods were introduced to generate easily understandable explanations intertwined with plan execution, aiming for explainable AI. Evaluation using simulation and human subjects showed improved task performance and reduced mental workload."
}