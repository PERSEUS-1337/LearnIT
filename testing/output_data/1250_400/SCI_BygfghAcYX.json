{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is proposed for two layer ReLU networks, offering a tighter generalization bound. The capacity bound correlates with test error behavior with increasing network sizes and partly explains the improvement in generalization with over-parametrization. Additionally, a matching lower bound for Rademacher complexity is presented, surpassing previous capacity lower bounds for neural networks. Deep neural networks have been successful in various tasks, with over-parametrized networks being common in practice. Neural networks have high capacity and can fit random labels easily, but when trained with real labels, they achieve smaller generalization error. Increasing model size helps improve generalization error, even without explicit regularization. Increasing model size improves generalization error in neural networks even without explicit regularization. Various studies have shown that training on models with more hidden units leads to decreased test error in image classification tasks. Different complexity measures have been proposed to explain this phenomenon, as traditional measures like VC bounds do not capture the behavior observed in practice. Complexity measures based on network parameters like VC bounds do not explain the generalization behavior in neural networks. Different norm, margin, and sharpness measures have been suggested to measure network capacity. Even when a network can perfectly fit training data, test error continues to decrease with larger networks. Unit capacity and unit impact are key factors in network capacity, with both shrinking faster than 1/ \u221a h where h is the number of hidden units. Unit capacity and unit impact are important factors in network capacity, with both shrinking faster than 1/ \u221a h where h is the number of hidden units. Existing complexity measures fail to explain why over-parametrization helps and increase with the size of the network. In this study, the authors focus on analyzing the phenomenon of increasing generalization bounds with network size. They simplify the architecture to two layer ReLU networks and prove a tighter generalization bound. Unlike existing bounds, their capacity bound correlates with test error and decreases with the number of hidden units. They characterize complexity at a unit level, showing that unit level measures shrink faster than 1/ \u221a h. The capacity bound for two layer ReLU networks correlates with test error and decreases with the number of hidden units. Complexity is characterized at a unit level, with measures shrinking faster than 1/ \u221a h. Generalization bound depends on layer norms and decreases with increasing network size. In the over-parametrized setting, training just the top layer minimizes training error due to randomly initialized hidden layer having all possible features. In the over-parametrized setting, training just the top layer minimizes training error by selecting the right features from a large number of hidden units representing all possible features. Initialization plays a significant role in optimization algorithms, with empirical observations and generalization bounds for linear networks. Our contributions in this paper include an empirical investigation into the role of over-parametrization in neural network generalization on various datasets. We show that existing complexity measures increase with the number of hidden units but do not fully explain generalization behavior. We also provide tighter generalization bounds for two-layer ReLU networks and propose a complexity measure that decreases with the increasing number of hidden units. Our proposed complexity measure for neural networks decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. We also provide a lower bound for the Rademacher complexity of two-layer ReLU networks with a scalar output, improving upon previous results. The text discusses two-layer fully connected ReLU networks for c-class classification tasks, defining the margin operator and ramp loss. The expected margin loss is bounded between 0 and 1, with empirical estimates denoted as L\u03b3(f). The text discusses the expected margin loss of a predictor for any distribution and margin \u03b3 > 0, with the loss bounded between 0 and 1. The Rademacher complexity is used as a capacity measure for function classes to fit random labels, with a generalization bound provided for any function f \u2208 F. Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels, with a focus on neural networks. The choice of function class is crucial to explain the decrease in generalization error with increasing width. Experimental observations on network layers with increasing hidden units are discussed, with results from the CIFAR-10 dataset. The experiments on network layers with increasing hidden units on the CIFAR-10 dataset show that while the Frobenius norm of weights increases with h, the distance to initialization per unit decreases. This shift in distribution of angles between learned and initial weights suggests a change in capacity, referred to as unit capacity. The distribution of angles between learned and initial weights shifts with increasing hidden units, indicating a change in capacity known as unit capacity. The Frobenius norm and distance to initialization decrease in the second layer of trained networks as size grows, suggesting a limited role of initialization. The impact of each classifier on the final decision diminishes as the norm of outgoing weights from hidden units decreases faster than 1/\u221ah. The impact of each classifier on the final decision diminishes as the norm of outgoing weights from hidden units decreases faster than 1/\u221ah. This unit impact is defined as the magnitude of the outgoing weights from a hidden unit, denoted as \u03b1 i = v i 2. Empirical observations suggest that neural networks from real data have bounded unit capacity and unit impact, leading to the study of generalization behavior in this context. The hypothesis class of neural networks is considered with bounded unit capacity and unit impact. Generalization properties of two layer ReLU networks are studied, with a focus on Rademacher complexity. The proof involves decomposing the network complexity into that of the hidden units. The Rademacher complexity of two layer neural networks is bounded by decomposing the complexity across hidden units, leading to a tighter bound. The generalization bound holds for any function in the class defined by specific \u03b1 and \u03b2 values. To extend the bound to all networks, a union bound over possible \u03b1 and \u03b2 values is taken. The generalization bound for any two layer ReLU network is provided in Theorem 2. The generalization bound for any two layer ReLU network is provided in Theorem 2, which holds for all networks by covering the space of possible \u03b1 and \u03b2 values. The bound empirically improves over existing bounds and decreases with increasing network width. Additionally, a lower bound for the Rademacher complexity is shown to match the first term in the generalization bound, demonstrating its tightness. The additive term resulting from the union bound over \u03b1 and \u03b2 values is small in practical networks, leading to a decrease in capacity with over-parametrization. The generalization bound in Theorem 2 for two-layer ReLU networks improves over existing bounds and decreases with network width. The additive term from the union bound is small in practical networks, leading to decreased capacity with over-parametrization. The bound is extended to p norms in Appendix Section B, showing a finer tradeoff between terms. Experimental comparison with Golowich et al. (2018) shows similarities in key complexity terms. The experimental comparison involves training two-layer ReLU networks of varying sizes on CIFAR-10 and SVHN datasets. Networks larger than size 128 show improved generalization even without regularization. Unit capacity and impact decrease with increasing network size. The number of epochs needed to reach a certain loss decreases for larger networks. The text discusses the unit-wise properties of networks, such as unit capacity and unit impact, which decrease with increasing network size. It also compares the behavior of different capacity bounds over networks of increasing sizes, showing that the effective capacity based on different measures decreases with increasing network size. The proposed bound is lower than other norm-based data-independent bounds and even improves over VC-dimension for networks larger than 1024. The numerical values are loose but provide insight into generalization behavior with respect to different complexity measures. Our capacity bound improves over VC-dimension for networks larger than 1024, showing decreasing behavior with network size. It outperforms other norm-based data-independent bounds and could potentially explain generalization in over-parametrized networks. Comparing complexity measures between networks trained on real and random labels reveals interesting insights. Our capacity bound improves over VC-dimension for networks larger than 1024, showing decreasing behavior with network size. It outperforms other norm-based data-independent bounds and could potentially explain generalization in over-parametrized networks. Comparing complexity measures between networks trained on real and random labels reveals interesting insights. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound of Theorem 1. The lower bound is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer, allowing for comparison with existing results and extending the lower bound to the bigger class F W. The lower bound for the Rademacher complexity of neural networks matches the dominant term in the upper bound of Theorem 1. It extends the lower bound to a bigger function class F W with an additional constraint on the spectral norm of the hidden layer. The complexity lower bound is tight, even with more information such as bounded spectral norm, indicating that the upper bound provided in Theorem 1 cannot be improved. The lower bound for the capacity of neural networks with bounded spectral norm and element-wise activation functions improves upon previous bounds by showing a gap between the Lipschitz constant and network capacity. This non-trivial bound excludes networks with rank-1 weight matrices, revealing a capacity gap between ReLU and linear networks. The construction can be extended to more layers by setting intermediate layer weight matrices to the Identity matrix. The capacity bound for neural networks with bounded spectral norm and element-wise activation functions reveals a gap between ReLU and linear networks. The construction can be extended to more layers by setting intermediate layer weight matrices to the Identity matrix. This new capacity bound improves upon previous bounds by showing a gap between the Lipschitz constant and network capacity. The paper presents a new capacity bound for neural networks that decreases with the number of hidden units, potentially explaining better generalization in larger networks. The focus is on the role of width in generalization behavior of two layer networks, with future study directions on depth and width interplay. Matching lower bounds are provided for capacity, but absolute values are still larger than training samples. Optimization convergence to low complexity networks is not addressed in this paper. In this paper, lower bounds for neural networks are discussed, with a focus on capacity decreasing with the number of hidden units. The study does not address optimization convergence to low complexity networks. An experiment is described where a pre-activation ResNet18 architecture is trained on the CIFAR-10 dataset. Trained a pre-activation ResNet18 architecture on CIFAR-10 dataset with specific configurations for convolution layers and residual blocks. Used SGD for training with mini-batch size 64, momentum 0.9, and initial learning rate 0.1. Data augmentation included random horizontal flip and random crop. Experimented with different architectures and stopped training based on loss criteria. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets with various architectures and training configurations. Data augmentation techniques were used, and training stopped based on specific loss criteria. The evaluation included calculating exact generalization bounds and adjusting them for binary classification. The study evaluated different architectures with a 0.01 loss, adjusting generalization bounds for binary classification. Random initialization was used as the reference matrix, and Gaussian kernel density estimation was applied for plotting distributions. Figures showed network behavior on SVHN and MNIST datasets, highlighting over-parametrization in MNIST and comparing generalization bounds. Theorem 2 was generalized to p norm, with Lemma 11 introducing a new proof element. The study evaluated different architectures with a 0.01 loss, adjusting generalization bounds for binary classification. Random initialization was used as the reference matrix, and Gaussian kernel density estimation was applied for plotting distributions. The left panel of FIG10 shows the over-parametrization phenomenon in the MNIST dataset, and the middle and right panels compare generalization bounds. Theorem 2 was generalized to p norm, with Lemma 11 introducing a new proof element. The main new ingredient in the proof is constructing a cover for the p ball with entry-wise dominance. Theorem 5 provides a bound for generalization error with probability 1 \u2212 \u03b4 over the choice of the training set. The generalization error is bounded for any function f(x) = V[Ux] +. The generalization error bound decreases with h for larger values, particularly for p = ln h. A vector-contraction inequality for Rademacher complexities is used in the proof, along with a technical result from Maurer (2016). The Rademacher complexity of the class of networks can be decomposed to that of hidden units. The Rademacher complexity of the class of networks can be decomposed to that of hidden units. The proof involves Jensen's inequality and a Rademacher Decomposition lemma. The induction proof shows the bounded inequality, and the Ledoux-Talagrand contraction is also discussed. The Rademacher complexity of the class of networks can be decomposed to that of hidden units. The proof involves Jensen's inequality and a Rademacher Decomposition lemma. The induction proof shows the bounded inequality, and the Ledoux-Talagrand contraction is also discussed. The ramp loss is shown to be Lipschitz with respect to each dimension, and a covering lemma is introduced to prove the generalization bound without assuming knowledge of network parameters. The covering lemma introduced allows for proving the generalization bound without prior knowledge of network parameters. Lemma 13 provides a bound on the generalization error with certain conditions. The proof involves constructing sets of vectors and applying union bound to establish the desired bounds. Lemma 14 provides specific results for the case p = 2 in generalization error bounds. It bounds the generalization error for a function f(x) = V[Ux] + with certain conditions on V and U matrices. The proof involves upper bounding the generalization bound from Lemma 13 for p = 2 and a specific choice of parameters. The generalization error for a function f(x) = V[Ux] + is bounded for any p \u2265 2, with specific conditions on V and U matrices. Lemma 15 provides a looser bound compared to Lemma 14 for p = 2, with additional constants and logarithmic factors. Lemma 15 provides a bound on the generalization error for a function f(x) = V[Ux] + with specific conditions on V and U matrices, for any p \u2265 2. The proof involves upper bounding the generalization bound from Lemma 13. Theorems 5 and 3 follow directly from Lemma 15, with the use of notation to hide constants and logarithmic factors. Theorem 3 is proven using specific conditions on matrices V and U, with notation to hide constants and logarithmic factors. The dataset is divided into groups with n copies of different elements. The function F is orthonormal, leading to an upper bound on U."
}