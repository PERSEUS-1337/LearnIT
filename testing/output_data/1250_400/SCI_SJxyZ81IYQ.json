{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively. This approach preserves semantic content better and follows a human language-like recursive structure. The current text chunk discusses the limitations of sequential models in image captioning, which fail to capture hierarchical structures in natural languages. These models rely too heavily on n-gram statistics and lack the ability to represent hierarchical dependencies among words in a caption. Sequential models in image captioning have drawbacks as they rely on n-gram statistics, favor frequent n-grams in training, and struggle to generalize due to entanglement of syntax and semantics. To address these issues, a new paradigm is proposed where semantics and syntax are separated into two stages, with explicit representation of semantic content derived from the image. The new paradigm in image captioning separates semantics and syntax into two stages. It derives a semantic representation from the image, using noun-phrases as the basis for constructing captions through recursive composition. The compositional procedure is implemented using parametric modular nets, not hand-crafted algorithms. The neural image captioner BID2 generates captions with n-gram building, but not always semantically correct. The paradigm separates semantics and syntax, using modular nets for phrase composition and completeness evaluation. This approach increases caption diversity, generalizes well to new data, and performs decently with limited training data. The proposed paradigm in image captioning increases caption diversity, generalizes to new data, and maintains good performance with limited training data. Recent works use convolutional neural networks for image representation and recurrent neural networks for caption generation. Recent works on image captioning utilize convolutional neural networks for image representation and recurrent neural networks for caption generation. Various approaches have been proposed, such as using a single feature vector for image representation, applying attention mechanisms to extract relevant image information, adjusting attention computation to consider generated text, and reformulating latent states as 2D maps to capture semantic information. Some methods also involve directly extracting phrases or semantic words from input images. The recent approaches in image captioning involve utilizing convolutional neural networks for image representation and recurrent neural networks for caption generation. Various methods have been proposed, including reformulating latent states as 2D maps to capture semantic information and directly extracting phrases or semantic words from input images. Additionally, some approaches focus on generating captions sequentially, favoring frequent n-grams and leading to issues like incorrect semantic coverage and lack of diversity. In contrast, a proposed paradigm works in a bottom-up manner by representing the input image with noun-phrases and constructing captions through a recursive composition procedure. The proposed paradigm in image captioning works bottom-up, representing input images with noun-phrases and using a recursive composition procedure to generate diverse captions. This approach contrasts with sequential methods that favor frequent n-grams, leading to issues like incorrect semantic coverage. The proposed compositional paradigm in image captioning allows for the generation of diverse captions by recursively composing noun-phrases extracted from input images. This approach contrasts with sequential methods and aims to improve semantic coverage. The proposed two-stage framework for image captioning involves deriving noun-phrases as an explicit semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. This approach aims to capture nonsequential dependencies among words and phrases in a sentence, unlike mainstream captioning models. The framework represents image semantics explicitly with noun-phrases like \"a black cat\" and \"two boys\", capturing object categories and associated attributes. The text discusses extracting noun-phrases as explicit image semantics, highlighting the importance of visual understanding tasks. It mentions the smaller number of distinct noun-phrases compared to images in datasets like MS-COCO, leading to treating noun-phrase extraction as a multi-label classification problem. Selected noun-phrases are treated as classes for classification. The text discusses formalizing noun-phrase extraction as a multi-label classification problem. It involves deriving distinct noun-phrases from training captions, extracting visual features from images, and performing binary classification for each noun-phrase. The process includes selecting top-scoring noun-phrases and pruning semantically similar concepts through Semantic Non-Maximum Suppression. The caption construction is done recursively using a compositional procedure called CompCap. The text discusses the process of constructing captions through a recursive compositional procedure called CompCap. It involves selecting top-scoring noun-phrases, pruning semantically similar concepts, and using modules like Connecting Module (C-Module) and Evaluation Module (E-Module) to generate and assess phrases for the caption. The CompCap process involves using the Connecting Module (C-Module) to select and evaluate connecting phrases for captions. The C-Module computes scores for phrases and selects the one with the maximum score as the new phrase. An Evaluation Module (E-Module) is then used to determine if the new phrase is a complete caption. If not, the process repeats until a complete caption is obtained. The C-Module aims to select a connecting phrase given left and right phrases and evaluate the connecting score. The C-Module selects connecting phrases based on left and right phrases, treating it as a classification problem due to the limited number of distinct connecting phrases. This approach differs from using an LSTM for decoding intermediate words in captions. The connecting module in MS-COCO BID4 uses distinct connecting phrases mined from training captions to classify left and right phrases, with a two-level LSTM model encoding the phrases and interacting with visual features for attention control and evolution of the encoded state. The model uses a two-level LSTM to encode phrases and interact with visual features for attention control. Encoders for left and right phrases have different parameters, with their encodings going through fully-connected layers and a softmax layer to determine connecting scores. A virtual negative class is added for unconnectable pairs, and scores are computed based on the C-Module. The Evaluation Module (E-Module) is used to determine if a phrase is a complete caption. It encodes the input phrase into a vector using a two-level LSTM model and evaluates the probability of it being a complete caption. Additionally, the E-Module can check other properties like caption quality using a caption evaluator. The framework can be extended for generating diverse captions using beam search or probabilistic sampling to form multiple beams. The framework for generating diverse captions can be extended using beam search or probabilistic sampling to avoid local minima. User preferences can also be incorporated by filtering noun phrases or modulating their scores. This control is easier to implement on an explicit representation than on an encoded feature vector. The experiments conducted on MS-COCO and Flickr30k datasets involve manipulating captions by filtering noun phrases or adjusting their scores. The vocabulary is standardized by converting words to lowercase and removing infrequent or non-alphabetic words. Training captions are limited to 18 words, and ground-truth captions are parsed into trees for data collection. The C-Module and E-Module are trained separately for classification tasks. In experiments on MS-COCO and Flickr30k datasets, training captions are truncated to 18 words and parsed into trees for data collection. The C-Module and E-Module are separately trained for classification tasks. The recursive compositional procedure is modularized for better generalization. Testing involves two forward passes for each module, with 2 or 3 steps typically needed to obtain a complete caption. CompCap is compared with other methods like NIC, AdapAtt, TopDown, and LSTM-A5 for image captioning. CompCap is compared with other state-of-the-art captioning models like AdapAtt and TopDown BID1, as well as LSTM-A5 BID19. All methods use ResNet-152 pretrained on ImageNet to extract image features and are trained with the same hyperparameters. Beam-search of size 3 is used for baselines, while CompCap empirically selects 7 noun-phrases with top scores for generating captions. During training, ResNet-152 is fixed without finetuning, and a learning rate of 0.0001 is set for all methods. Best performing parameters are selected for caption generation during testing. CompCap selects 7 top-scoring noun-phrases to represent the input image, balancing semantics and syntax. Comparisons on MS-COCO and Flickr30k datasets show CompCap excels in SPICE metric but lags in CIDEr, BLEU-4, ROUGE, and METEOR compared to baselines, reflecting the sequential and compositional nature of caption generation methods. CompCap with predicted noun-phrases performs best in SPICE metric but falls behind baselines in CIDEr, BLEU-4, ROUGE, and METEOR. The sequential generation procedure favors frequent training n-grams, while the compositional generation procedure preserves semantic content more effectively. An ablation study on components of the compositional paradigm shows a significant boost in metrics when using groundtruth noun-phrases from associated captions. This indicates that CompCap effectively preserves semantic content. The proposed compositional paradigm in CompCap effectively preserves semantic content by representing input images with groundtruth noun-phrases from associated captions. Using a composing order for integrating noun-phrases leads to improved metrics, showing CompCap's ability to generate better captions with semantic understanding. Additionally, CompCap excels at handling out-of-domain semantic content and requires less data to learn, as demonstrated in two studies. CompCap disentangles semantics and syntax, excels at handling out-of-domain content, and requires less data to learn. Two studies were conducted to verify this. The first experiment controlled data ratios for training, showing competitive results for CompCap. In the second study, CompCap trained on MS-COCO/Flickr30k and tested on Flickr30k/MS-COCO, outperforming baselines. This suggests the benefit of disentangling semantics and syntax, as semantics distribution varies across datasets, while syntax distribution remains stable. The diversity analysis of CompCap shows its ability to generate diverse captions by varying noun-phrases and composing order. Metrics like novel captions ratio, unique captions ratio, vocabulary usage, and pair-wise editing distances were computed to evaluate diversity. Results indicate high diversity in generated captions. CompCap obtained the best results in all diversity metrics, indicating that captions generated by CompCap are diverse and novel. The diversity analysis included metrics such as novel captions ratio, unique captions ratio, vocabulary usage, and pair-wise editing distances. In practice, 5 captions with top scores are used to compute diversity at the image level for each method. CompCap achieved the best results in all metrics, indicating diverse and novel captions. Error analysis revealed that misunderstandings of visual content led to errors in CompCap's captions, which could be addressed with more advanced noun-phrase extraction techniques. Sequential models tend to favor frequent n-grams, even with a perfect understanding of visual content. The proposed paradigm for image captioning aims to address these issues. The proposed paradigm for image captioning introduces a novel compositional approach to generating captions. It involves extracting explicit noun-phrases from input images and assembling them recursively to form captions with a hierarchical structure. This method aims to preserve semantics effectively, require less training data, generalize better across datasets, and produce more diverse captions. The key focus is on finding semantically similar noun-phrases to improve caption quality. The proposed compositional approach for image captioning involves extracting explicit noun-phrases from input images and assembling them hierarchically to generate captions. The method aims to preserve semantics effectively, require less training data, generalize better across datasets, and produce diverse captions. To improve caption quality, the key focus is on finding semantically similar noun-phrases through comparing central nouns and using encoders in the C-Module. The C-Module uses encoders to obtain two encodings for each noun-phrase, comparing their normalized euclidean distances to determine semantic similarity. Parameters for encoders in the C-Module are independent to account for the position of the phrase in the ordered pair, as shared parameters yielded less accurate results. The C-Module uses independent parameters for encoders to account for the position of phrases in ordered pairs, leading to better performance. Additional hyperparameters for CompCap can be adjusted, such as beam search sizes, but they have minor influence on performance."
}