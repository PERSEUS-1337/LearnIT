{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions (Q-values) are fundamental in reinforcement learning, with algorithms like SARSA and Q-learning. A new concept of action value is introduced, defined by a Gaussian smoothed version of the expected Q-value in SARSA. Smoothed Q-values still satisfy a Bellman equation, making them learnable from experience. Gradients of expected reward with respect to a parameterized Gaussian policy's mean and covariance can be derived from the smoothed Q-value function. New algorithms are developed for training a Gaussian policy directly from a learned Q-value approximator, showing strong results on continuous control benchmarks. The ability to learn both mean and covariance during training allows for strong results in continuous control benchmarks. Different notions of Q-value lead to distinct families of reinforcement learning methods, such as SARSA, Q-learning, Soft Q-learning, and PCL. The choice of Q-value function significantly impacts the resulting algorithm. In this work, a new notion of action value called the smoothed action value function Q \u03c0 is introduced. Unlike previous notions, the smoothed Q-value associates a value with a distribution over actions rather than a specific action at each state. It is defined as the expected return of first taking an action sampled from a normal distribution centered at a, then following actions sampled from the current policy thereafter. Smoothed Q-values have properties that make them attractive for use in reinforcement learning. The smoothed Q-value is introduced as the expected return of first taking an action sampled from a normal distribution centered at a, then following actions sampled from the current policy. It satisfies single-step Bellman consistency and allows for bootstrapping in training a function approximator. Smoothie algorithm is proposed to train a policy using the derivatives of a trained smoothed Q-value function. Smoothie algorithm proposes training a policy using derivatives of a smoothed Q-value function, allowing for exploratory behavior with a non-deterministic Gaussian policy. It can incorporate proximal policy optimization techniques by penalizing KL-divergence, improving stability and performance. Smoothie algorithm can easily integrate proximal policy optimization by adding a KL-divergence penalty, enhancing stability and performance. Results on continuous control benchmarks are competitive, especially for challenging tasks with limited data. The goal is to find an agent that maximizes cumulative discounted reward in a Markov decision process framework. The problem is framed as a Markov decision process (MDP) with state and action spaces. The agent's behavior is modeled using a stochastic policy, and the optimization objective is the expected discounted return. The policy gradient theorem expresses the gradient of the policy's tunable parameters. Many reinforcement learning algorithms, like policy gradient and actor-critic, utilize these concepts. The policy gradient theorem expresses the gradient of a policy's tunable parameters for reinforcement learning algorithms. This paper focuses on multivariate Gaussian policies over continuous action spaces, using a mean and covariance function to map the observed state to a Gaussian distribution. New RL training methods are developed for this family of parametric policies. The observed state of the MDP is represented as a feature vector \u03a6(s) in R ds, and the Gaussian policy is parametrized by mean and covariance functions \u00b5(s). New RL training methods are developed for this family of parametric policies, building on prior work on learning Gaussian policies. The key observation is that under a deterministic policy, one can estimate the expected future return from a state s and express the gradient of the optimization objective for a parameterized policy. The BID21 algorithm focuses on optimizing the value function approximator Q \u03c0 w by minimizing the Bellman error for transitions sampled from a dataset. It also discusses the use of off-policy distribution \u03c1 \u03b2 (s) to improve sample efficiency in practice. In practice, BID5 and BID21 use an off-policy distribution \u03c1 \u03b2 (s) from a replay buffer to improve sample efficiency. This substitution may not hold exactly, but prior work shows it works well. Smoothed action value functions are introduced in this paper, providing effective signals for optimizing Gaussian policy parameters. Smoothed Q-values differ from ordinary Q-values by assuming only the mean of the first action is known, requiring an expectation calculation for nearby actions. Smoothed action values are defined to compute Q \u03c0 (s, a) by performing an expectation of Q \u03c0 (s, \u00e3) for actions \u00e3 drawn near a. Instead of learning a function approximator for Q \u03c0 (s, a) and drawing samples, a direct function approximator for Q \u03c0 (s, a) is learned. The Bellman equation enables direct optimization of Q \u03c0 by sampling r and s from R(s, \u00e3) and P(s, \u00e3). The Bellman equation allows for direct optimization of smoothed Q-values by sampling r and s from R(s, \u00e3) and P(s, \u00e3). Parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 in terms of mean and covariance parameters enables optimization of Q \u03c0 through gradient descent. The derivative of the objective w.r.t. covariance parameters can be estimated using the second derivative of Q \u03c0 w.r.t. actions. The derivative of Q \u03c0 with respect to actions can be computed exactly by expressing both sides of the equation using standard matrix calculus. Two ways to optimize Q \u03c0 are discussed, with the second approach using a single function approximator for Q \u03c0 resulting in a simpler implementation. Sampling from a replay buffer with knowledge of the sampling probability allows for optimization of Q \u03c0 by minimizing a weighted Bellman error. The optimization of Q \u03c0 can be achieved by sampling from a replay buffer with knowledge of the sampling probability and minimizing a weighted Bellman error. It is unnecessary to track the probabilities of sampled actions, as a near-uniform distribution of actions conditioned on states is typically provided by the replay buffer. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods to constrain gradient steps. These methods have not been applicable to algorithms like DDPG due to deterministic policies. A proposed formulation allows for trust region optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. The optimization is straightforward as the KL-divergence of two Gaussians can be expressed analytically. The paper proposes a formulation for trust region optimization in policy learning, using a penalty on KL-divergence from a previous policy. This method is a generalization of deterministic policy gradient algorithms like DDPG, with updates for training the policy mean and Q-value approximator being identical. The proposed Q-value function in the context of trust region optimization in policy learning is similar to the deterministic value function of DDPG. Stochastic Value Gradient (SVG) also trains stochastic policies like DDPG but lacks updates for the covariance. Expected policy gradients (EPG) generalize DDPG by providing updates for the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. The proposed Q-value function in trust region optimization for policy learning is similar to DDPG's deterministic value function. Expected policy gradients (EPG) generalize DDPG by updating the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. This approach avoids approximating integrals and relies on neural network function approximators to estimate the smoothed Q-value function. The training scheme for learning the covariance of a Gaussian policy is based on Gaussian integrals, similar to techniques used in variational auto-encoders and Gaussian back-propagation. The novel training scheme proposed in this paper relies on neural network function approximators to estimate the smoothed Q-value function. It introduces a new perspective where Q-values represent the averaged return of a distribution of actions, different from recent advances in distributional RL. The paper focuses on Gaussian policies but suggests further investigation into applying this perspective to a wider class of policy distributions. A new RL algorithm, Smoothie, is introduced, which maintains a parameterized Q function and uses it to train a Gaussian policy. The paper introduces a new RL algorithm called Smoothie, which maintains a parameterized Q function to train a Gaussian policy. The algorithm is evaluated against DDPG and shows promising results. Further investigation is suggested to apply this perspective to a wider class of policy distributions. Smoothie is evaluated against DDPG on a simple synthetic task with a reward function of two Gaussians. Smoothie learns both the mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima due to fixed exploratory noise. Smoothie successfully solves the task by adjusting the policy mean and standard deviation during training, while DDPG struggles to escape local optima due to fixed exploratory noise. Smoothie's smoothed reward function guides the policy towards the better Gaussian, allowing for suitable adjustment of the covariance during training. Smoothie adjusts the covariance \u03a3 \u03c6 during training by increasing and decreasing it as the reward function changes. It successfully escapes lower-reward local optima and adapts its policy variance accordingly. The implementation utilizes feed forward neural networks for policy and Q-values, with the covariance \u03a3 \u03c6 parameterized as a diagonal given by e \u03c6. DDPG uses an Ornstein-Uhlenbeck process for exploration. Smoothie, a reinforcement learning algorithm, competes with DDPG in standard continuous control benchmarks. It adjusts the covariance during training, outperforming DDPG in final reward performance, especially in challenging tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient. Results are compared in FIG2 after hyperparameter search. Smoothie outperforms DDPG in challenging tasks like Hopper, Walker2d, and Humanoid. Results are compared in FIG2 after hyperparameter search, showing Smoothie performing competitively or better across all tasks. Smoothie exhibits a slight advantage in Swimmer and Ant, with more dramatic improvements in Hopper, Walker2d, and Humanoid, doubling the average reward in Hopper. These results for Humanoid are the best published for a method training on the order of millions of samples. Smoothie shows significant improvements in tasks like Hopper, Walker2d, and Humanoid, with the average reward doubling in Hopper. The results for Humanoid are the best published for a method training on millions of samples. The introduction of a KL-penalty improves Smoothie's performance, especially on harder tasks, providing a solution to the instability in DDPG training. The algorithm Smoothie improves performance on harder tasks like Hopper and Humanoid by incorporating a KL-penalty to address instability in DDPG training. A new Q-value function, Q \u03c0, is introduced, which allows Smoothie to successfully learn both mean and covariance during training, leading to performance that can match or surpass DDPG. The smoothed Q-values provide a more sensible approach compared to standard Q-values. The success of Q \u03c0 in learning both mean and covariance during training can match or surpass DDPG, especially with a penalty on policy divergence. Smoothed Q-values make the reward surface smoother and easier to learn, with a direct relationship to the expected return objective. Future work should explore these claims and apply them to other policies."
}