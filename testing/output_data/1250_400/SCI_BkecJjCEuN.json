{
    "title": "BkecJjCEuN",
    "content": "The aim of this work is to enhance the label efficiency of large neural networks for audio data by combining multitask and self-supervised learning on unlabeled data. Through various self-supervised tasks, significant performance improvements of up to 6% in supervised classification tasks are achieved with limited labeled data. Data augmentation in the multitask setting further enhances performance. Deep neural networks are crucial for modeling and classifying auditory data. Incorporating self-supervised tasks and data augmentation in multitask learning improves performance by up to 6% in audio classification tasks. Deep neural networks are essential for modeling auditory data, but labeled datasets are scarce, making unsupervised learning a promising direction. The study focuses on developing techniques for better generalization by combining self-supervised audio tasks with supervised tasks during model training. Incorporating self-supervised audio tasks during model training improves performance in audio classification tasks. The study demonstrates the successful identification of relevant self-supervised tasks and their joint training with supervised tasks to enhance performance. WaveNet is utilized as a feature extractor for rich audio representations from raw waveform data. The framework is applied to three supervised classification tasks, showing the potential of leveraging unlabeled data and data augmentation techniques for performance improvement. Additionally, self-supervised tasks can serve as a pre-training stage for transfer learning. The study explores self-supervised audio tasks to enhance performance in audio classification. It suggests that models trained for multiple tasks can synergize to uncover underlying structures, improving single-task performance with less data. This approach aims to learn a general-purpose representation through multitask learning. The study investigates self-supervised audio tasks to improve audio classification performance by leveraging multitask learning. It explores using self-supervised learning in the audio domain, implementing an end-to-end audio processing network based on the WaveNet architecture to find a common embedding of acoustic waveforms. The study implemented an end-to-end audio processing network based on the WaveNet architecture to find a common embedding of acoustic waveforms. The network consists of a trunk and head networks trained jointly for each experiment, with a WaveNet trunk comprising 3 blocks of 6 dilation stacks. The trunk has an effective receptive field length of approximately 12 ms and was tested on three supervised tasks: audio tagging, speaker identification, and speech. The WaveNet trunk consists of 64 convolutional units per module, with outputs from filter and gate modules multiplied and summed with input. It has an effective receptive field length of 190 samples or 12 ms. Tested on audio tagging, speaker identification, and speech command recognition tasks using labeled and unlabeled data. Audio segments are cropped to 2 seconds before being fed to the network. The task averages output across time to produce a single output vector for the entire audio sequence. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers sourced from interviews with celebrities. Each audio segment is cropped to 2 seconds before being fed to the network. Training involves minimizing cross entropy between softmax outputs and one-hot encoded labels. Clips are normalized due to variations in audio quality. The speech command recognition task involves training on the Speech Commands dataset with 65,000 utterances of 30 short words. The dataset includes 12 categories, with 10 words classified and the rest as unknown or silence. The task utilizes a head architecture with a global average pooling layer, 2-layer perceptron, batch normalization, ReLU nonlinearity, and a softmax layer with cross-entropy loss evaluation. The head consists of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The speech command recognition task involves training on the Speech Commands dataset with 65,000 utterances of 30 short words. The dataset includes 12 categories, with 10 words classified and the rest as unknown or silence. The speech command recognition head consists of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The convolution layers have widths of 100, 50, and 25 and strides of 16, 8, and 4, respectively. Self-supervised tasks such as next-step prediction, noise reduction, and upsampling were implemented and trained on both the main task's data and unlabeled data sampled from the Librispeech dataset. The auxiliary tasks in the study used the Librispeech dataset for training, focusing on a generic multitask framework for audio with waveform inputs. While spectral/cepstral representations can improve classification performance, they limit the range of audio processing tasks. Different tasks may require varying network architectures, restricting the potential benefits of self-supervised tasks for understanding learning dynamics. The study focused on using Librispeech dataset for auxiliary tasks in a generic multitask framework for audio with waveform inputs. Multitask learning improved performance without increasing training data, showing benefits for supervised tasks. Future work aims to close the performance gap between models trained on spectral representations and waveforms. Multitask training with Librispeech dataset improved audio tagging performance by incorporating additional unsupervised tasks. Increasing unlabeled data size led to further improvements in performance metrics, with up to a .056 increase in MAP@3 with 500 hours of additional data. Similar trends were observed for speech command classification and speaker identification tasks, showcasing the benefits of multitask learning in improving performance. Multitask learning improved performance in audio tagging, speech command classification, and speaker identification tasks. Additional unlabeled data enhanced classification accuracy, with top-5 performance increasing from 73.81% to 75.22%. Data augmentation techniques like pitch shifting and additive noise also showed performance gains in the tasks. Training with pitch-shift augmentation and additional self-supervised tasks led to the highest performance increase of .089 in MAP@3, showing that these methods complement each other in improving label efficiency. Transfer learning was explored by pre-training three self-supervised tasks on unlabeled data before fine-tuning with a smaller amount of labeled data, resulting in improved performance in audio tasks. Transfer learning involves pre-training self-supervised tasks on unlabeled data before fine-tuning with a smaller amount of labeled data for improved performance in audio tasks. The approach shows performance gains and scalability with the quantity of unlabeled data, enhancing existing data augmentation schemes. The approach of pre-training self-supervised tasks on unlabeled data shows performance gains and scalability in audio tasks. The methodology suggests potential for generalization to a broad range of supervised audio tasks, raising questions about the limits of auxiliary tasks for model improvement and the extraction of audio representations. The chosen auxiliary tasks require higher temporal resolutions to handle a broader range of auditory tasks. Our model, based on the WaveNet architecture, utilizes causal dilated convolutions to process high temporal resolution raw audio signals efficiently. The trunk architecture follows the WaveNet structure, with task-specific heads for different tasks. The WaveNet trunk architecture consists of stacked dilated causal convolutions with residual connections and nonlinearities. Each block contains S dilated causal convolution layers with increasing dilation factors. Each layer involves \"Filter\" and \"Gate\" computations to produce hidden state vectors and layer outputs. The WaveNet trunk architecture consists of stacked dilated causal convolutions with residual connections and nonlinearities. Each block has an effective receptive field of 1 + b(2S-1), resulting in a total receptive field of \u03c4 = 1+N(2S-1). After a hyperparameter search, N = 3 blocks with S = 6 layers each were chosen, giving a total receptive field of \u03c4 = 190, equivalent to about 12 milliseconds of audio sampled at 16kHz. Each task-specific head processes input data after passing through the shared trunk. The WaveNet trunk architecture consists of stacked dilated causal convolutions with residual connections and nonlinearities. Each task-specific head is a simple neural network that processes input data after passing through the shared trunk. Task-specific heads are designed with as few layers as necessary to solve the task, allowing the shared trunk to learn effectively. Primary supervised tasks include \"audio tagging\", while auxiliary tasks like \"next-step prediction\", \"noise reduction\", and \"upsampling\" are trained on varying amounts of unlabeled data. Each task specifies its own objective function, optimizer, learning rates, and annealing schedules. The next-step prediction task involves predicting the next value in a sequence of audio waveform frames. This task allows for the creation of large training datasets from unlabeled audio data. The prediction head consists of a 2-layer stack of convolutional layers with ReLU nonlinearities, with the first layer having 128 units and the second layer having a single output unit. The head takes in \u03c4 frames of data from the trunk and produces an output representing the model's prediction for the next audio frame. The prediction head consists of a 2-layer stack of convolutional layers with ReLU nonlinearities. The first layer has 128 units, and the second layer has a single output unit. The head takes in \u03c4 frames of data from the trunk and produces an output representing the model's prediction for the next audio frame, treating it as a regression problem with mean squared error as the loss function. The noise reduction task treats noise as an additive random process on top of the clean audio waveform. The noise reduction task involves predicting the clean audio sample from noisy samples using a model trained with a smoothed L1 loss function. The task is similar to next-step prediction, with models adapted to solve either task due to their similar structures. The noise reduction head is structured similarly to the next-step head and is trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. The smooth L1 loss is preferred for denoising tasks over mean squared error. An unsupervised upsampling task can be created by downsampling the audio source and using the downsampled signal as input data. The network's job is to infer high frequency information lost during the transform. The network's task is to infer high frequency information lost during the transform by repeating every time-point of the resampled signal 4 times. The model is trained using raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets, with code written in PyTorch framework. Samples are cropped to two seconds, downsampled to 16 kHz, and normalized to lie in the interval [-1, 1]. The audio samples were preprocessed in the PyTorch framework BID16 by cropping to two seconds, downsampling to 16 kHz, and normalizing to [-1, 1]. Noise for the noise-reduction task was added from ChiME3 datasets at SNR levels of 10dB to 15dB. Hyperparameter search was conducted for the number of blocks, layers, units, and learning rate. Architecture specifications had minimal impact on network performance. The network's architecture specifications had minimal impact on performance. Hyperparameter search was conducted for the number of blocks, layers, units, and learning rate. The final choice of hyperparameters was made by selecting values that optimized performance on both the main and auxiliary tasks. The model was trained on all tasks simultaneously using a uniform weighting strategy for the loss function. In training the model, a uniform weighting strategy was used for the loss function across all tasks. The \"Adam\" optimizer with specific parameters was employed, and the learning rate was decayed every 5 epochs. A batch size of 48 was utilized due to computational constraints. Additional tasks like noise reduction and upsampling required separate forward propagations. Important model parameters can be found in TAB3."
}