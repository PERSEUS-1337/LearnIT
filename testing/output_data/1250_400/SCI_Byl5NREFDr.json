{
    "title": "Byl5NREFDr",
    "content": "Model extraction in natural language processing involves an adversary reconstructing a victim model using only query access. The attacker can successfully extract the model without real training data by using random word sequences and task-specific heuristics. This exploit is possible due to the prevalence of transfer learning methods in NLP. Defense strategies like membership classification and API watermarking can be effective but may be circumvented by clever adversaries. Machine learning models are valuable intellectual property, often accessed through web APIs. Attackers can extract models using queries to train local copies, known as \"model stealing\" or \"model extraction.\" Defense strategies like membership classification and API watermarking can be effective but may be circumvented by clever adversaries. The adversary can extract NLP models by issuing queries to train local copies, even without access to the API provider's training data. This is made easier with contextualized pretrained representations like ELMo and BERT, which boost performance and require only shallow task-specific networks. The paper demonstrates that NLP models fine-tuned from BERT can be extracted without access to training data. Extraction attacks are possible with randomly sampled word sequences and simple heuristics, contrasting prior work. Using Wikipedia sentences and paragraphs as queries improves extraction performance. These attacks are cost-effective, with the most expensive attack estimated at $500. The study shows that NLP models fine-tuned from BERT can be extracted without training data. Extraction attacks using randomly sampled word sequences and Wikipedia queries are cost-effective. The process involves querying a victim BERT model and fine-tuning one's own model based on the predicted answers. Despite the effectiveness of random queries in model extraction, they are mostly nonsensical and uninterpretable. The study explores the effectiveness of randomly-generated queries in extracting NLP models fine-tuned from BERT. It is found that queries closer to the original data distribution work better for extraction. Pretraining on the attacker's side facilitates model extraction. Simple defenses like membership classification and API watermarking are effective against naive adversaries but fail against clever ones. The research aims to inspire stronger defenses against model extraction and a better understanding of vulnerabilities in models and datasets. The study focuses on model extraction attacks on NLP systems, particularly on BERT-large models. Prior efforts have mainly targeted computer vision applications, but this work explores the vulnerability of text-based systems. Existing methods for model extraction do not transfer well to text-based systems due to the discrete nature of the input space. Previous attempts at extraction on NLP systems have used pool-based active learning, but this study examines a more realistic extraction setting with nonsensical inputs. The study explores model extraction attacks on NLP systems, specifically on BERT-large models. Previous methods do not transfer well to text-based systems due to the discrete input space. Unlike prior work, this study focuses on nonsensical inputs for tasks like question answering. Rubbish inputs, randomly-generated examples that yield high-confidence predictions, have been studied in model extraction literature. Prior work successfully extracted SVMs and 1-layer networks using i.i.d noise, but scaling to deeper neural networks like BERT has not been explored. Unnatural text inputs have shown to produce overly confident model predictions, break translation systems, and trigger disturbing outputs from text generators. In contrast, this study demonstrates the effectiveness of such inputs in training models for real NLP tasks without real examples. BERT, Bidirectional Encoder Representations from Transformers, is a 24-layer transformer that converts word sequences into high-quality vector representations. BERT, Bidirectional Encoder Representations from Transformers, is a 24-layer transformer that converts word sequences into high-quality vector representations. BERT's parameters are learned through masked language modeling on unlabeled text data, revolutionizing NLP with state-of-the-art performance on various tasks. Fine-tuning methodology is commonly used in NLP systems, where a task-specific network is combined with BERT to achieve optimal performance. A 1-layer feedforward network with parameters is used to construct a composite function for a task T. The final parameters are learned using training data with a small learning rate. Description of extraction attacks involves reconstructing a local copy of a victim model using a black-box API. The attacker uses a task-specific query generator to construct queries and fine-tunes a public release of a model to obtain the extracted model. The attacker fine-tunes a public release of a model to obtain extracted models on various NLP tasks using different query generators. The attacker fine-tunes a public release of a model to obtain extracted models on various NLP tasks using different query generators, such as RANDOM and WIKI. These generators alone are insufficient for tasks with complex interactions, so task-specific heuristics are applied, like replacing words in the premise for MNLI and sampling words from the passage for SQuAD/BoolQ questions. The attacker fine-tunes a public release of a model to obtain extracted models on various NLP tasks using different query generators. Task-specific heuristics are applied, such as replacing words in the premise for MNLI and sampling words from the passage for SQuAD/BoolQ questions. The extraction procedure is evaluated in a controlled setting with different query budgets for each task, and commercial cost estimates are provided using the Google Cloud Platform's Natural Language API calculator. The text discusses the evaluation of extracted models on various NLP tasks with different query budgets using the Google Cloud Platform's Natural Language API calculator. It highlights the high accuracies of some tasks even at low query budgets and the diminishing accuracy gains at higher budgets. The extracted models show surprising accuracy on the original development sets of all tasks, even when trained with nonsensical inputs. Despite this, the agreement between the outputs of the extracted model and the victim model is only slightly better than accuracy in most cases. The evaluation of extracted models on NLP tasks using the Google Cloud Platform's Natural Language API calculator shows high accuracies even with low query budgets. Despite being trained on nonsensical inputs, the extracted models perform well on original development sets. However, the agreement between the extracted and victim models is only slightly better than accuracy in most cases. An ablation study with alternative query generation heuristics is conducted for SQuAD and MNLI datasets. The evaluation of extracted models on NLP tasks using the Google Cloud Platform's Natural Language API shows high accuracies even with low query budgets. The results indicate that access to the full probability distribution is not crucial for model extraction. Query efficiency is measured with varying query budgets, showing that extraction is often successful even with small budgets. The analysis raises questions about the properties of nonsensical input queries and the effectiveness of extraction without large pretrained language models. Approximate costs for attacks can be estimated from Table 2. This section analyzes why nonsensical input queries are effective for model extraction, exploring properties and effectiveness without large pretrained language models. The study focuses on RANDOM and WIKI extraction configurations for SQuAD models, investigating if different victim models produce the same answers to nonsensical queries. In this section, the study examines the RANDOM and WIKI extraction configurations for SQuAD models to determine if different victim models agree on answers to nonsensical queries. Five victim SQuAD models were trained with varying random seeds, showing high agreement on SQuAD training and development set queries but significantly lower agreement on WIKI and RANDOM queries. The results suggest that victim models are brittle on nonsensical inputs, but high-agreement queries may be more useful for model extraction. The study found that victim models are brittle on nonsensical inputs, but high-agreement queries are more useful for model extraction. Extracting models using high-agreement subsets showed large F1 improvements compared to random and low-agreement subsets. This suggests that agreement between victim models is a good indicator of input-output pair quality for extraction. The potential interpretability of high-agreement nonsensical queries to humans remains an open question for future research. The study found that victim models are brittle on nonsensical inputs, but high-agreement queries are more useful for model extraction. An investigation was conducted to determine if high-agreement nonsensical textual inputs have a human interpretation. Annotators matched victim models' answers 23% of the time on the WIKI subset and 22% on the RANDOM subset, while scoring significantly higher on original SQuAD questions. Annotators used a word overlap heuristic to select answer spans. An investigation revealed that annotators scored higher on original SQuAD questions compared to RANDOM ones. Annotators commonly used a word overlap heuristic to select answer spans. The study also explored the impact of different pretraining setups on extraction accuracy, considering scenarios where the attacker may not have information about the victim's architecture. BERT models, specifically BERT-large and BERT-base, are compared in terms of extraction accuracy for attackers starting from scratch. Starting from BERT-large yields higher accuracy, even when the victim uses BERT-base. Fine-tuning BERT gives attackers an advantage due to the good language representation it provides. Training a QANet model without pretraining shows the importance of starting from a good point. Training a QANet model without pretraining shows the importance of starting from a good point. BERT-based models are vulnerable to model extraction, prompting the investigation of defense strategies that preserve API utility while remaining undetectable to attackers. BERT-based models are vulnerable to model extraction, leading to the exploration of defense strategies that preserve API utility while being undetectable to attackers. Two defenses are discussed, focusing on membership inference to identify nonsensical inputs or adversarial examples and issuing random outputs to eliminate extraction signals. The API issues random outputs to prevent model extraction signals. Membership inference is treated as a binary classification problem using MNLI and SQuAD datasets. Classifiers transfer well to balanced development sets and remain robust to query generation processes. Watermarking is another defense against extraction. Watermarking is a defense against model extraction, where a fraction of queries are modified to return incorrect outputs. This defense aims to detect extracted models by memorizing watermarked queries. Watermarking is evaluated on MNLI and SQuAD datasets, showing results on watermarked models. Table 8 shows results on watermarked models for MNLI and SQuAD datasets. Watermarking is effective in detecting extracted models by modifying a fraction of queries. Watermarked models perform differently from non-watermarked models, especially on the watermarked subset of the training data. Training with more epochs amplifies these differences. Limitations include the fact that watermarking can only be used after an attack has occurred. Watermarking is effective in detecting extracted models by modifying queries. Watermarked models perform differently from non-watermarked models, especially on the training data subset. Limitations include the need to use watermarking after an attack and the possibility of attackers taking steps to prevent detection. Model extraction attacks against NLP APIs are effective at extracting good models with low query budgets, even with nonsensical input queries. Model extraction attacks against NLP APIs are effective at extracting good models with low query budgets, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Future directions include leveraging nonsensical inputs for model distillation, using query efficiency to diagnose dataset complexity, and exploring victim model agreement for active learning in model extraction. In this paper, the authors explore improving model distillation, diagnosing dataset complexity using query efficiency, and investigating victim model agreement for active learning in model extraction. Cost estimates from Google Cloud Platform's Calculator were used for Natural Language APIs, with inputs limited to 1000 characters per query. Costs for different datasets were calculated by counting input instances with more than 1000 characters multiple times. Extrapolated costs of entity analysis and sentiment analysis APIs were used for tasks not covered by Google Cloud APIs. The paper discusses the costs associated with using APIs for various tasks such as entity analysis and sentiment analysis. It mentions the challenges in estimating the price of issuing queries and the possibility of extracting data through web scraping. The variability in API costs depending on factors like computing infrastructure and revenue models is also highlighted. In discussing API costs for tasks like entity analysis and sentiment analysis, it is noted that web scraping can be used to extract data at a large scale for free. API costs can vary based on factors like computing infrastructure and revenue models. The focus is on the relatively low costs needed to extract datasets rather than actual cost estimates. For tasks like machine translation and speech recognition, costs are relatively inexpensive. For example, it costs -$430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translation queries. Input generation algorithms for datasets like SST2 and RANDOM involve building a vocabulary from wikitext103 and randomly sampling tokens. Input generation algorithms for datasets like SST2, RANDOM, and MNLI involve building a vocabulary from wikitext103 and randomly sampling tokens. The top 10000 tokens are preserved while others are discarded, with sentences chosen randomly and words replaced as needed. The final hypothesis is constructed by randomly sampling words from the top-10000 wikitext103 vocabulary. Paragraphs and questions are generated by sampling tokens from the wikitext103 vocabulary and choosing random lengths and question starters. The question generation process involves randomly sampling tokens to build questions, appending them with a question mark, and prepending with a randomly chosen question starter word. Different methods are used for sampling questions from various datasets. Additional query generation heuristics are studied, showing that random sampling works better when paragraphs reflect unigram frequency in wikitext103. In this section, additional query generation heuristics are explored through comparisons of extraction datasets for SQuAD 1.1 and an ablation study on MNLI. Findings show that starting questions with common question starter words like \"what\" improves performance, and datasets with a few different words tend to be balanced with strong extraction signals. Using frequent words from the top 10000 wikitext103 words aids extraction. The study found that lexical overlap affects dataset balance, with shuffled hypotheses leading to unbalanced datasets. Human annotators were used to evaluate question sets, showing varying inter-annotator agreement levels. The study conducted an ablation study on input features for the membership classifier, comparing the effectiveness of logits from the BERT classifier and the last layer representations. Results showed that the last layer representations were more effective in distinguishing between real and fake inputs. The study compared the effectiveness of the BERT classifier logits and the last layer representations for the membership classifier. Results in Table 9 show that the last layer representations are more effective in distinguishing between real and fake inputs. Using both feature sets yielded the best results in most cases."
}