{
    "title": "SJg9z6VFDr",
    "content": "Recently, a new model called Graph Ordinary Differential Equation (GODE) has been proposed for graph data, inspired by neural ordinary differential equations (NODE) for Euclidean data. GODE uses continuous-depth models and two efficient training methods. It outperforms existing graph networks and can be easily adapted to improve accuracy in various tasks. The GODE model introduces bijective blocks for $\\mathcal{O}(1)$ memory consumption and can be adapted to different graph neural networks, improving accuracy in node and graph classification tasks. Unlike CNNs limited to grid data like images and text, graphs represent objects as nodes and relations as edges, making them suitable for irregularly structured datasets like social networks and protein interactions. Graph data structures represent objects as nodes and relations as edges, making them suitable for irregularly structured datasets like social networks and protein interaction networks. Traditional methods like random walk and graph embedding have limitations in modeling graphs due to low expressive capacity. Graph neural networks (GNN) have been proposed as a new class of models inspired by the success of CNNs, using convolution operations to capture local information on graphs. There are two main types of methods for performing convolution on a graph: spectral methods and non-spectral methods. Spectral methods involve computing the graph Laplacian and performing filtering in the spectral domain, while non-spectral methods aim to approximate filters without computing the graph Laplacian for faster processing. GraphSAGE is a recently proposed method that learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it difficult to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) views a neural network as an ordinary differential equation, extending the concept to model continuous processes in graphs. The NODE framework extends neural networks to model continuous processes in graphs, but struggles with gradient estimation during training, leading to inferior performance compared to discrete-layer models in image classification tasks. A memory-efficient framework is proposed to improve gradient estimation accuracy. The NODE framework, while extending neural networks to model continuous processes in graphs, faces challenges with gradient estimation during training, resulting in lower performance compared to discrete-layer models in image classification tasks. A memory-efficient framework is introduced to enhance gradient estimation accuracy, leading to significant improvements in benchmark classification tasks. The model achieves constant memory usage when applied to restricted-form invertible blocks. GODE models are proposed for graph data, showing improved performance on various datasets. Previous studies have viewed neural networks as differential equations, with NODE treating the network as a continuous ODE. Various methods have been proposed to improve the expressive capacity of NODEs, but none have addressed the inaccurate gradient estimation issue. Spectral and non-spectral graph neural networks (GNNs) are discussed, with a focus on their filtering methods in the Fourier domain of a graph. Spectral GNNs require information of the whole graph to determine the graph Laplacian, while non-spectral GNNs only consider message aggregation around neighbor nodes. Various spectral methods are briefly introduced, highlighting the heavy computation burden of non-localized filters. Several spectral methods for graph convolution have been introduced, with approaches such as using Chebyshev expansion to approximate filters and localized first-order approximation for superior performance in node classification tasks. Non-spectral methods focus on convolution operations considering only neighboring nodes, with techniques like MoNet using a mixture of CNNs and GraphSAGE sampling a fixed size of neighbors for fast inference. Invertible blocks in neural networks, proposed by et al. (2016), allow for accurate reconstruction of input from outputs. They have been utilized in normalizing flow models for calculating data distribution log-density. Jacobsen et al. (2018) further applied bijective blocks to construct invertible networks, while Gomez et al. (2017) suggested their use in backpropagation. In normalizing flow models, invertible blocks are used to calculate data distribution log-density. Jacobsen et al. (2018) applied bijective blocks to create invertible networks, while Gomez et al. (2017) proposed using invertible blocks for memory-efficient backpropagation. The discrete-layer models with residual connections can be represented as neural ordinary differential equations (NODE) when adding more layers with shared weights. When adding more layers with shared weights, discrete-layer models with residual connections can be represented as neural ordinary differential equations (NODE). The forward pass of a NODE involves integrating the hidden states over time, with the output layer applied at the final time step. Integration can be done using various ODE solvers such as the Euler Method or Runge-Kutta Method. The adjoint method is commonly used in this context. The transformation of states z is modeled as the solution to the NODE, with an output layer applied on z(T). Integration in the forward pass can be done with various ODE solvers. The adjoint method, widely used in optimal process control, is followed. Model parameters are denoted as \u03b8, independent of time. The adjoint is defined, and different methods for back-propagation on NODE are compared in Figure 1. The adjoint method is used for back-propagation in NODE. In reverse-time, the hidden state may cause errors in gradient calculation. Direct back-propagation involves saving evaluation time points during forward pass and re-building the computation graph during backward pass. This ensures accurate reconstruction of the hidden state for precise gradient evaluation. The loss function L is optimized using gradient descent on \u03b8. Eq. 6 is a reverse-time integration solved with any ODE solver. Storing z(t) during forward pass requires large memory consumption. In summary, forward pass solves Eq. 2 forward in time, while backward pass reconstructs the hidden state accurately for precise gradient evaluation. In reverse-time integration, Eq. 6 is solved with any ODE solver to evaluate z(t) by solving Eq. 2 reverse-time. The backward pass requires determining f(z(t), t, \u03b8) and z(t) by solving Eq. 2 reverse-time, leading to potential inaccuracies in gradient calculation. The instability of reverse-time ODE can cause a mismatch between hidden states solved forward-time and reverse-time, resulting in errors in gradient calculation. If the ODE is stable in both forward-time and reverse-time, then Re(\u03bbi(Jf)) = 0 for all i. The instability of reverse-time ODE can lead to errors in gradient calculation due to a mismatch between hidden states solved forward-time and reverse-time. If the ODE is stable in both forward-time and reverse-time, then Re(\u03bbi(Jf)) = 0 for all i, indicating stability. The accuracy of the computed gradient can be affected by numerical errors, especially when |Re(\u03bb)| is large. To address this, back-propagation through the ODE solver is proposed as a solution. The proposed solution to address numerical errors in gradient calculation involves directly back-propagating through the ODE solver. This method ensures accurate hidden states by reconstructing z(t i ) at evaluated time points {t i }. The adjoint method with discrete time is defined to ensure accuracy in the forward-time ODE solution. The proposed solution involves direct back-propagation through the ODE solver for accurate gradient estimation. The adjoint method with discrete time ensures accuracy in the forward-time ODE solution, with detailed derivations provided in appendices E and F. Algorithm 1 outlines the process for accurate gradient estimation in ODE solvers for free-form functions. The proposed solution involves direct back-propagation through the ODE solver for accurate gradient estimation. Algorithm 1 summarizes the method, which includes adaptive stepsize variation during forward pass and reverse-time integration during backward pass. The algorithm supports free-form continuous dynamics and has no constraints on the form of the function f. Memory consumption analysis is also discussed, considering the number of forward evaluation steps and layers in f. The algorithm proposed involves direct back-propagation through the ODE solver for accurate gradient estimation. It supports free-form continuous dynamics and has no constraints on the form of the function f. Memory consumption analysis shows that the method is more memory-efficient compared to naive solvers, especially when using step-wise checkpoint methods or invertible blocks. The memory consumption can be reduced to O(Nf) with invertible blocks. The solver can handle free-form functions and invertible blocks, reducing memory consumption to O(Nf). Invertible blocks split input x into two parts, with bijective mappings denoted by F and G neural networks. Differentiable bijective functions can be applied for various tasks. Theorem 1 states that if \u03c8(\u03b1, \u03b2) is a bijective function w.r.t \u03b1 when \u03b2 is given, then the block defined by Eq. 8 is a bijective mapping. This allows for different \u03c8 functions to be applied for different tasks, making the process memory-efficient. Graph neural networks are introduced with discrete layers, extending to graph ordinary differential equations (GODE) for continuous cases. GNNs are generally represented in a message passing scheme, where nodes and edges are used to represent graphs. Graph neural networks (GNNs) are typically represented in a message passing scheme, where nodes exchange information with their neighbors through message passing, aggregation, and updating stages. This process involves differentiable functions and neural networks to handle information flow within the graph structure. Graph neural networks (GNNs) utilize aggregation functions like mean and sum for permutation invariant operations. The states of nodes are updated based on original states and message aggregation. A continuous-time GNN, known as graph ordinary differential equation (GODE), replaces discrete-time functions with a message passing process. GODE, being an ODE, can capture non-linear functions and potentially outperform discrete-layer models. The asymptotic stability of GODE is linked to over-smoothing phenomena. Graph convolution is a form of Laplacian smoothing, where the input and output of a graph-conv layer are related through matrices and a scaling constant. Transitioning from a discrete to a continuous model enhances the capabilities of GNNs. The graph convolution is a special case of Laplacian smoothing, represented as Y = (I \u2212 \u03b3D \u22121/2LD\u22121/2)X. The continuous smoothing process involves eigenvalues of the Laplacian and ODE stability. Integration time affects node features in experiments with a CNN-NODE on image classification tasks. Integration time T affects node features in experiments with a CNN-NODE on image classification tasks. The method was evaluated on various benchmark datasets including bioinformatic, social network, and citation networks. For graph classification tasks, raw datasets were used without pre-processing, while for node classification tasks, transductive inference was performed following a specific train-validation-test split. Details of datasets are provided in the appendix. Additionally, a ResNet18 was directly modified into its corresponding NODE model for image classification tasks. For image classification tasks, a ResNet18 was modified into a NODE model with a specific function sequence. GODE can be applied to various graph neural networks by replacing functions or structures. Different GNN architectures were used for a fair comparison, including GCN, GAT, ChebNet, and GIN with varying depths of layers. The study compared different graph neural network (GNN) models, including GCN, GAT, ChebNet, and GIN, with varying depths of layers. The models were trained with different hyper-parameters and channel numbers for graph and node classification tasks. The performance of each model was evaluated based on accuracy across multiple runs. Additionally, the study compared the adjoint method and direct back-propagation on the same network. The study compared different graph neural network (GNN) models with varying depths of layers and hyper-parameters for graph and node classification tasks. They experimented with different channel numbers and number of hops for GCN, ChebNet, and GAT. Direct back-propagation outperformed the adjoint method, validating the analysis on the instability of the adjoint method. The training method reduced error rates on image classification tasks and NODE18 outperformed deeper networks like ResNet101 on CIFAR10 and CIFAR100 datasets. Our training method reduces error rates on image classification tasks and NODE18 outperforms deeper networks like ResNet101 on CIFAR10 and CIFAR100 datasets. The method is robust to different orders of ODE solvers and supports NODE and GODE models with free-form functions. Our method supports NODE and GODE models with free-form functions, demonstrating the effectiveness of GODE models over their discrete-layer counterparts. Different \u03c8 functions behaved similarly on node classification tasks, indicating the importance of the continuous-time model. Lower memory cost is also validated. Most GODE models outperformed discrete-layer models significantly, validating their effectiveness; different \u03c8 functions behaved similarly on node classification tasks, indicating the importance of the continuous-time model. Lower memory cost is also validated. Results for different models on graph classification tasks are summarized in Table 4. Integration time influences model performance, with short times leading to insufficient information gathering and long times causing oversmoothing issues. Inference in NODE and GODE models tests integration time influence on graph models. Short integration times lead to lack of information gathering, while long times cause oversmoothing issues. GODE enables continuous diffusion modeling on graphs with a memory-efficient back-propagation method. The paper addresses gradient estimation for NODE, improving accuracy on benchmark tasks. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics. The paper focuses on improving accuracy on benchmark tasks by applying NODE models to various datasets including citation networks, social networks, and bioinformatics. It introduces a parameter state checkpoint method for bijective blocks and provides pseudo code for forward and backward functions in PyTorch. The structure of invertible blocks is explained with modifications to enable accurate inversion. The paper introduces a parameter state checkpoint method for bijective blocks to improve accuracy in NODE models. Pseudo code for forward and backward functions in PyTorch is provided, emphasizing memory efficiency. Results show reduced memory consumption compared to conventional backpropagation. The study compared memory consumption between a memory-efficient method and conventional backpropagation on bijective blocks. Results on the MUTAG dataset showed that the memory-efficient approach had lower memory usage as the depth of the network increased. The memory-efficient method only slightly increased memory consumption due to caching states of F and G, while the conventional method showed a significant increase in memory usage. The memory-efficient approach for bijective blocks involves caching states of F and G, resulting in minimal memory consumption compared to input data. The stability of the ODE in both forward and reverse time is crucial for the bijective mapping to be successful. The stability of the ODE in both forward and reverse time is crucial for the bijective mapping to be successful. Theorem 1 states that a bijective block with defined forward and reverse mappings is bijective. The proof involves showing that the mapping is both injective and surjective. The text discusses the construction of a bijective mapping using forward and reverse functions. It demonstrates the computation graph and gradient derivation in a neural-ODE model. The continuous model follows an ODE with differentiable function f. The goal is to optimize parameters \u03b8 from an optimization perspective, extending from continuous to discrete cases. The text discusses the optimization of parameters in a continuous model defined by an ODE. It introduces the forward pass, loss function, and training process formulation. The Lagrangian Multiplier Method is used to solve the optimization problem, with a focus on the Karush-Kuhn-Tucker (KKT) conditions for optimality. The following sections derive results from the KKT condition. The text discusses using the Lagrangian Multiplier Method to solve the optimization problem defined by an ODE. It focuses on the Karush-Kuhn-Tucker (KKT) conditions for optimality and derives results from these conditions. The analysis extends from continuous to discrete cases, replacing integration with finite sums to obtain corresponding results."
}