{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. This operator can be integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM is applied to mobile service traffic and air quality forecasting using point-cloud streams. Results show accurate long-term predictions, outperforming other neural network models. Point-cloud stream forecasting involves predicting data streams from geospatial point-cloud sources, such as mobile network antennas and air quality sensors, which operate on irregular and unordered sets of points with complex spatial correlations. Point-cloud stream forecasting operates on irregular and unordered sets of points with complex spatial correlations. Vanilla LSTMs have limited abilities in exploiting spatial features, while ConvLSTM and PredRNN++ are restricted to grid-structural data. Different approaches to geospatial data stream forecasting include predicting over grid-structured data, mapping point-cloud input to a grid, and forecasting directly over point-cloud data streams. The proposed PointCNN leverages spatial-local correlations of point clouds, irrespective of input order. CloudLSTM architecture introduces the DConv operator for forecasting over point cloud-streams. Variants of CloudLSTM can be combined with Seq2seq learning and attention mechanisms for precise forecasting. The CloudLSTM architecture introduces the DConv operator for forecasting over point cloud-streams, combining it with Seq2seq learning and attention mechanisms for precise forecasting. Point clouds are defined as containing N points with value features and coordinates, with U different channels obtained at each time step. Ideal point-cloud stream forecasting models should exhibit order invariance and embrace key properties for spatiotemporal forecasting. An ideal point-cloud stream forecasting model should have order invariance, information intactness, interaction among points, robustness to transformations, and location variance. The model should capture local dependencies among neighboring points and be able to adapt to dynamic spatial correlations over time. This is achieved through the Dynamic Point Cloud Convolution (DConv). The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM model, ensuring order invariance, information intactness, interaction among points, robustness to transformations, and location variance. DConv generalizes convolution on grids by operating on point-clouds, maintaining the input's information intactness property. The DConv operator takes U in channels of a point-cloud S and outputs U out channels of a point-cloud with the same number of elements as the input. It involves summing element-wise products over features and points in a subset to obtain values and coordinates for each point in the output set. The DConv operator involves summing element-wise products over features and points in a subset to obtain values and coordinates for each point in the output set. Learnable weights are defined as 5D tensors shared across different anchor points in the input map, with scalar weights for input and output channels. Bias terms are also defined for each output map, and the sigmoid function limits the range of predicted coordinates. The DConv operator involves summing element-wise products over features and points in a subset to obtain values and coordinates for each point in the output set. Learnable weights are defined as 5D tensors shared across different anchor points in the input map, with scalar weights for input and output channels. Bias terms are also defined for each output map, and the sigmoid function limits the range of predicted coordinates. Coordinates of raw point-clouds are normalized to (0, 1) before feeding them to the model for improved transformation robustness. The K nearest points can vary for each channel at each location, reflecting different types of measurements in the pointcloud dataset. Spatial correlations between measurements may vary due to human mobility. The DConv operator weights its K nearest neighbors across all features to produce values and coordinates in the next layer, allowing for learnable spatial correlations between different measurements. CloudLSTM contributes to improving forecasting performance by not fixing the K nearest neighbors across channels, encouraging each channel to find the best neighbor set. The DConv operator weights its K nearest neighbors across features to produce values and coordinates in the next layer, capturing local dependencies and improving robustness to global transformations. It is a symmetric function that learns the layout and topology of the cloud-point, enabling \"location-variance\" in the output. DConv improves robustness to transformations, meets desired properties, and enables dynamic positioning tailored to each channel and time step. It can be efficiently implemented using 2D convolution and builds upon PointCNN and Deformable Convolution for pointcloud structural data. The DConv operator builds upon PointCNN and Deformable Convolution for pointcloud structural data, introducing variations tailored to maintain order invariance without extra complexity or information loss. It deforms input maps and aligns weights based on distance rankings, ensuring robustness to transformations and dynamic positioning for each channel and time step. The DConv operator, a variation of DefCNN over point-clouds, ensures order invariance without added complexity. It can be integrated into CloudLSTM for spatial and temporal correlation learning. The CloudLSTM cell includes input, forget, and output gates, memory cell, and hidden states represented as point clouds. It is combined with Seq2seq learning and soft attention mechanism for forecasting. The overall Seq2seq CloudLSTM architecture consists of encoder and decoder stacks for encoding historical information and decoding into tensors. Neural models, such as Seq2seq CloudLSTM, utilize encoder and decoder stacks with soft attention mechanism for spatiotemporal modeling on grid-structural data. Point Cloud Convolutional layers process data before forecasting, similar to word embedding in NLP tasks. A two-stack encoder-decoder architecture with 36 channels per CloudLSTM cell is found to be optimal. Additionally, DConv is explored in vanilla RNN and Convolutional GRU models. In this study, a two-stack encoder-decoder architecture with 36 channels per CloudLSTM cell is employed. The performance of CloudRNN and CloudGRU, which do not use the attention mechanism, is also explored. The models are evaluated using measurement datasets of traffic and air quality indicators. The proposed CloudLSTM is used to forecast future demands and indicators, with a comparison to 12 baseline deep learning models. The study employs a CloudLSTM model to forecast mobile service demands and air quality indicators in target regions. It compares the CloudLSTM with 12 baseline deep learning models using TensorFlow and TensorLayer libraries. The models are trained on a computing cluster with NVIDIA GPUs and optimized using the Adam optimizer. Experimental results are reported for spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments. Coordinate features are omitted in the final output due to fixed data source locations. The experimental results focus on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments. Experiments were conducted on mobile traffic forecasting using large-scale multi-service datasets collected in European metropolitan areas. Traffic volume data from non-uniformly distributed antennas was aggregated over 5-minute intervals, resulting in 24,482 traffic snapshots for 38 different mobile services. The antennas in urban regions are non-uniformly distributed, forming 2D point clouds. Traffic volume data is collected in Megabytes at each antenna, aggregated over 5-minute intervals for 38 mobile services. Air quality forecasting is done using a dataset with six indicators collected by 437 monitoring stations in China. The stations are divided into two city clusters, with data measured hourly and missing values filled using linear interpolation. The dataset includes 8,760 snapshots for each city cluster, with data measured hourly and missing values filled using linear interpolation. Measurements for mobile services and air quality indicators are transformed into input channels of the point-cloud S. The point-clouds are normalized and transformed into grids for baseline models. The training plus validation to test set ratio is 8:2. CloudLSTM is compared with PointCNN and CloudCNN for performance evaluation. The dataset consists of 8,760 hourly snapshots for each city cluster, with missing values filled using linear interpolation. Mobile service and air quality indicators are transformed into input channels of the point-cloud S. The training plus validation to test set ratio is 8:2. CloudLSTM is compared with various baseline models for performance evaluation, including PointCNN, CloudCNN, PointLSTM, CloudRNN, and CloudGRU. The accuracy of CloudLSTM is measured in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The study discusses various neural network models for mobile traffic prediction, including 3D-CNN, LSTM, ConvLSTM, PredRNN++, and CloudLSTM. Performance is evaluated using metrics like MAE, RMSE, PSNR, and SSIM. Models are tested for short-term and long-term forecasting, with different time horizons. In the air quality forecasting case, models receive half-day measurements for evaluation. The study evaluates various RNN-based models for mobile traffic prediction with different time horizons. Models are tested for short-term and long-term forecasting, with a focus on air quality forecasting. CloudLSTM, CloudRNN, and CloudGRU variants outperform other benchmark architectures in terms of MAE, RMSE, PSNR, and SSIM metrics. The study compares RNN-based architectures for mobile traffic prediction, highlighting the superiority of CloudLSTM, CloudRNN, and CloudGRU variants over other models. CloudLSTM performs better than CloudGRU and CloudRNN, with the DConv operator proving more effective than vanilla convolution and PointCNN. The attention mechanism improves forecasting performance by capturing better dependencies between input sequences and vectors in decoders. The study evaluates the forecasting performance of RNN-based architectures, extending the prediction horizon to 36 time steps. Results show reliable long-term forecasting for most models in city 1, while low K values may impact CloudLSTM performance in city 2. CloudLSTMs outperform ConvLSTM in 12-step air quality forecasting, showing up to 12.2% and 8.8% improvement across metrics. The study evaluates the forecasting performance of RNN-based architectures for long-term forecasting up to 72 time steps. CloudLSTMs outperform ConvLSTM in 12-step air quality forecasting, showing significant improvement in MAE and RMSE metrics. CloudCNN also proves superior to PointCNN in feature extraction from point-cloud data. The results demonstrate the effectiveness of CloudLSTM models for spatiotemporal data modeling. Performance evaluations of long-term forecasting up to 72 time steps are conducted on RNN-based models using strict variable-controlling methodology. Comparing different models like LSTM, ConvLSTM, PredRNN++, PointLSTM, and CloudLSTM, it is found that the D-Conv operator significantly improves performance. CloudRNN and CloudGRU are inferior to CloudLSTM, while the attention mechanism in CloudLSTM has minimal impact. The core operator, RNN structure, and attention mechanism are ranked by their contribution, with CloudLSTM being introduced as a dedicated model for spatiotemporal forecasting on pointcloud data streams. The CloudLSTM model is introduced as a dedicated neural model for spatiotemporal forecasting on pointcloud data streams. It builds upon the DConv operator, which performs convolution over point-clouds to learn spatial features while maintaining permutation invariance. DConv can be combined with various RNN models, Seq2seq learning, and attention mechanisms efficiently. The input and output of DConv are 3D tensors, and for each point in the input, the set of top K nearest neighbors is found. This input is transformed into a 4D tensor for processing. The DConv operator transforms input tensors into 4D tensors for convolution. The process involves finding nearest neighbors and weighting computations. The output is reshaped and a sigmoid function is applied for optimization in deep learning frameworks. The DConv operator transforms input tensors into 4D tensors for convolution by finding nearest neighbors and performing weighting computations. The complexity of DConv is analyzed by separating the operation into two steps: finding neighboring sets and performing weighting computations. The overall complexity is equivalent to a vanilla convolution operator, introducing extra complexity by searching for the K nearest neighbors. The DConv operator introduces extra complexity by searching for the K nearest neighbors for each point, but this complexity does not increase significantly with higher dimensional point clouds. Normalizing the coordinates features enables transformation invariance with shifting and scaling, making the model invariant to these transformations. The proposed CloudLSTM is combined with an attention mechanism to enhance the encoder and decoder states. In this study, the proposed CloudLSTM model, combined with an attention mechanism, enhances encoder and decoder states. The context tensor for the encoder state is represented with a score function. The proposal is compared against baseline models like MLP, CNN, 3D-CNN, DefCNN, LSTM, ConvLSTM, and PredRNN++, showing its effectiveness in mobile traffic forecasting and spatiotemporal predictive learning. The LSTM is a widely used RNN for time series forecasting. ConvLSTM is a baseline model for spatiotemporal predictive learning, while PredRNN++ is the state-of-the-art for spatiotemporal forecasting on grid-structural data. CloudRNN and CloudGRU share a Seq2seq architecture with CloudLSTM but do not use the attention mechanism. Different models considered in the study have detailed configurations and parameter numbers listed in Table 3. Increasing the number of layers did not improve the performance of ConvLSTM, PredRNN++, and PointLSTM. The PredRNN++ has a slightly different structure compared to other Seq2seq models. In the study, different Seq2seq models with CloudLSTM architecture were evaluated using various configurations and performance metrics like MAE, RMSE, PSNR, and SSIM. The models included 2-stack Seq2seq CloudLSTM with different K values and an attention mechanism. The use of 3x3 filters in image applications was found effective, supporting fair comparisons with CloudLSTMs. The study evaluates different Seq2seq models with CloudLSTM architecture using various configurations and performance metrics like MAE, RMSE, PSNR, and SSIM. Anonymized locations of antenna sets in cities are shown, with data collected via traditional flow-level deep packet inspection at the packet gateway. Data protection constraints prevent disclosure of operator names, target regions, and classifier operation details. The measurement data is collected via traditional flow-level deep packet inspection at the packet gateway (P-GW) using proprietary traffic classifiers to associate flows with specific services. Anonymized locations of antenna sets in cities are shown, and all measurements were conducted under the supervision of the national privacy agency. The dataset used for the study only contains mobile service traffic information at the antenna level, ensuring full anonymization and compliance with privacy regulations. The raw data cannot be made public due to a confidentiality agreement with the data owner, and the analysis includes 38 different services. The dataset used for analysis is fully anonymized and does not contain personal information. It includes mobile service traffic information from 38 different services. Streaming is the dominant type of traffic, accounting for almost half of the total consumption. Other services like web, cloud, social media, and chat also consume significant fractions of mobile traffic. Gaming only contributes 0.5% to the demand. Additionally, an air quality dataset from 43 cities in China is included, collected by the Urban Computing Team at Microsoft Research. The air quality dataset from 43 cities in China, collected by the Urban Computing Team at Microsoft Research, contains 2,891,393 air quality records from 437 monitoring stations over a year. The stations are divided into two clusters based on geographic locations. Missing data has been filled through linear interpolation. The dataset is available at https://www.microsoft.com/en-us/research/project/urban-air/. The forecasting accuracy of Attention CloudLSTMs for individual mobile services is evaluated, showing similar performance across cities at both the service and category levels. The MAE evaluation of the proposed Attention CloudLSTMs shows similar performance across cities for individual mobile services and categories. Services with higher traffic volume tend to have higher prediction errors due to more frequent fluctuations. RNN-based models for air quality forecasting also show increasing errors over time. Larger K values in CloudLSTM improve robustness and slow down error growth. The MAE for long-term air quality forecasting on city clusters shows error growth over time for all models. Larger K values in CloudLSTM improve robustness and slow down error growth, consistent with findings in mobile traffic forecasting. Visualization of hidden features provides insights into model knowledge. The input data snapshots are samples selected from City 2 with 260 antennas/points. Each scatter subplot in Fig. 10 shows the H t for encoders and decoders. Features are extracted at a higher level in stack 2, exhibiting more direct spatial correlations with the output. NO 2 forecasting examples in City Cluster A and B are shown in Fig. 11 and 12, generated by RNN-based models for air quality prediction. Point-clouds are converted into heat maps using 2D linear interpolation. CloudLSTMs offer better predictions by capturing trends in point-cloud streams and delivering high long-term visual fidelity. Other architectures degrade rapidly in performance over time. The proposed architectures, particularly Attention CloudLSTMs, show better prediction capabilities by capturing trends in point-cloud streams and maintaining high visual fidelity over time. By utilizing DConv with Sigmoid functions to regulate coordinate features and stacking multiple DConv via LSTM, CloudLSTM enhances representability and refines the positions of input points for accurate forecasting, even with outlier points. Density-based spatial clustering of applications with noise (DBSCAN) is used to identify outliers. The proposed CloudLSTM model refines input point positions for accurate forecasting, even with outlier points. Using DBSCAN, outliers are identified in the air quality dataset, with CloudLSTM showing the lowest prediction error compared to other models. CloudCNN with DConv operator performs best among CNN-based models. Further experiments confirm CloudLSTM's robustness to outliers in controlled scenarios. The CloudCNN model, utilizing the DConv operator, demonstrates superior forecasting performance compared to other CNN-based models. In a controlled experiment with outlier weather stations, CloudLSTM shows consistent performance regardless of the distance to outliers. The CloudLSTM model performs well in forecasting weather data, showing consistent performance regardless of the distance to outliers. It outperforms its counterpart PointLSTM and simple baselines like MLPs and LSTMs that rely on k-nearest neighbors for forecasting. The CloudLSTM model outperforms MLPs and LSTMs that rely on k-nearest neighbors for forecasting. The number of neighbors K affects the model's receptive field, with small K focusing on local spatial dependencies and large K looking at larger location spaces. CloudLSTM extracts local spatial dependencies through DConv kernels and merges global spatial dependency, showing superior performance compared to simple baselines. Seasonal information in mobile traffic series can further improve forecasting performance. To efficiently capture seasonal information in mobile traffic series for forecasting, a method is proposed that concatenates 30-minute sequences with a sub-sampled 7-day window. This approach reduces the input length to 90 (6 + 84) and improves forecasting performance. Experiments conducted on a subset of the dataset show the effectiveness of incorporating seasonal information in the forecasting model. By incorporating a sub-sampled 7-day window with 30-minute sequences, the forecasting model shows improved performance by capturing seasonal information. This method reduces input length to 90 and enhances model accuracy, although it increases complexity. Future work aims to fuse seasonal information more efficiently with minimal complexity increase."
}