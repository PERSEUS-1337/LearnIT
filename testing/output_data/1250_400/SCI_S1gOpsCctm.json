{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning. A new technique, Quantized Bottleneck Insertion, helps to create finite representations of RNN memory vectors and observation features. This allows for better analysis and understanding of RNN behavior. Results on synthetic environments and Atari games show small finite representations can lead to improved interpretability in policies learned through deep reinforcement and imitation learning. In this paper, the focus is on improving the interpretability of policies learned through deep reinforcement learning and imitation learning, particularly those represented as recurrent neural networks (RNNs). The challenge lies in understanding and explaining the complex memory representations used by RNN policies, which are updated through gating networks like LSTMs and GRUs. The goal is to learn more compact memory representations that capture discrete concepts, which could enhance the interpretability of RNN policies. The paper focuses on improving the interpretability of RNN policies by transforming continuous memory into a finite-state representation using Quantized Bottleneck Networks (QBNs). This approach aims to capture discrete concepts in memory usage for better explainability. The approach introduces Quantized Bottleneck Networks (QBNs) to transform an RNN policy with continuous memory and observations into a Moore Machine representation. QBNs encode memory states and observation vectors encountered during RNN operation, replacing the \"wires\" in the policy. The resulting Moore Machine Network (MMN) with quantized memory and observations is nearly equivalent to the original RNN, and can be fine-tuned for accuracy. Training QBNs is shown to be effective using \"straight through\" gradient estimators. Experiments in synthetic domains test different memory types and benchmark grammar. Our approach introduces Quantized Bottleneck Networks (QBNs) to transform RNN policies into Moore Machine representations. Training QBNs using \"straight through\" gradient estimators is effective, as demonstrated in experiments on synthetic domains and benchmark grammar learning problems. Additionally, experiments on 6 Atari games show that near-equivalent MMNs can be extracted, providing insights into RNN memory usage and control strategies. Our work introduces Quantized Bottleneck Networks (QBNs) to convert RNN policies into Moore Machine representations. Previous efforts have focused on extracting Finite State Machines (FSMs) from recurrent networks, but our approach is unique in learning finite-memory representations of continuous RNN policies. This involves training QBNs using gradient estimators and extracting near-equivalent MMNs from Atari games, shedding light on RNN memory usage and control strategies. Our approach introduces Quantized Bottleneck Networks (QBNs) to convert RNN policies into Moore Machine representations. Unlike previous efforts that focused on extracting FSMs from recurrent networks, our method learns finite-memory representations of continuous RNN policies. This involves training QBNs using gradient estimators and extracting near-equivalent MMNs from Atari games, providing insights into RNN memory usage and control strategies. Our work extends previous efforts by introducing Quantized Bottleneck Networks (QBNs) to convert RNN policies into Moore Machine representations. Unlike prior work focused on learning fully binary networks, we aim to learn discrete representations of memory and observations for interpretability. Our work introduces Quantized Bottleneck Networks (QBNs) to convert RNN policies into Moore Machine representations, focusing on learning discrete memory and observation representations for interpretability rather than efficiency. RNNs are commonly used in reinforcement learning to maintain hidden states and output actions based on observations. Our work introduces Moore Machines and their deep network counterparts as a way to extract compact quantized representations of hidden states and observations in reinforcement learning. Moore Machines are described by a finite set of hidden states, observations, actions, a transition function, and a policy mapping hidden states to actions. A Moore Machine is a finite state machine where states are labeled by output values corresponding to actions. It is described by a set of hidden states, observations, actions, a transition function, and a policy mapping hidden states to actions. A Moore Machine Network (MMN) represents the transition function and policy using deep networks, providing a mapping from continuous observations to a finite discrete observation space. The state and observation representations are quantized as discrete vectors. In this work, a Moore Machine Network (MMN) is discussed as a traditional RNN with memory composed of k-level activation units and environmental observations transformed into a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for non-trivial problems, even though RNNs can be learned with relative ease. Training high-performing MMNs from scratch for tasks like Atari has proven difficult. The text discusses the challenges of learning Moore Machine Networks (MMNs) from scratch, especially for complex tasks like Atari games. A new approach is introduced, leveraging the ability to learn RNNs by incorporating quantized bottleneck networks (QBNs) into the original recurrent net. This results in a network that consumes quantized features and maintains quantized state, effectively creating an MMN. The text introduces Quantized Bottleneck Networks (QBNs) as a new approach to learning Moore Machine Networks (MMNs). QBNs are autoencoders with a constrained k-level activation bottleneck, aiming to discretize a continuous space by quantizing the encoding layer. The QBN output is quantized using a 3-level quantization scheme, and tanh activation nodes are considered for encoding, although producing level 0 quantization can be challenging during learning. The QBN output is quantized using a 3-level scheme with an activation function \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x) to support 3-valued quantization. The quantize function makes b(x) non-differentiable, but the straight-through estimator treats it as the identity function during back-propagation, allowing effective use of the quantize function in the QBN. The straight-through estimator is effective in dealing with the issue of quantization in QBNs. By treating the quantize function as the identity function during back-propagation, it allows for effective training of QBNs as autoencoders. Training two QBNs on observed features and states can provide high-quality k-level encodings, viewed as \"wires\" that propagate input to output. The approach involves training two QBNs on observed features and states to obtain high-quality encodings. These encodings act as \"wires\" that are inserted into the original RNN, transforming it into an MMN. Despite imperfect reconstruction, the MMN may not behave identically to the original RNN. The RNN can be transformed into an MMN by inserting bottlenecks that provide a quantized representation of features and states. Fine-tuning the MMN by training on the original RNN data can help match the softmax distribution over actions. Visualization tools can be used to understand the memory and feature bits, but solving the full interpretation problem is not within the scope of this work. The MMN, when trained to output different actions than the RNN, was found to be more stable. Visualization tools can be used to analyze memory and feature bits for semantic understanding. Creating a Moore Machine from the MMN can help understand the role of different machine states and their relationships. The Moore Machine is constructed using consecutive pairs of quantized states, features, and actions, with the state-space corresponding to distinct quantized states and the observation-space to unique feature vectors. The Moore Machine is constructed from quantized features and actions, with a transition function derived from data. Minimization techniques are applied to reduce the number of states in the resulting machine. Experiments aim to extract Moore Machine Networks (MMNs) from Recurrent Neural Networks (RNNs) without performance loss and improve interpretability of recurrent policies. In this section, the experiments focus on extracting Moore Machine Networks (MMNs) from Recurrent Neural Networks (RNNs) to improve interpretability of recurrent policies. Two domains are considered: a synthetic environment called Mode Counter and benchmark grammar learning problems. The Mode Counter Environments (MCEs) allow for varying amounts and types of memory usage in policies. The Mode Counter Environments (MCEs) involve transitioning between modes using memory and observations in different ways to determine optimal performance. Three MCE instances are tested, including an Amnesia scenario where optimal actions can be selected based solely on current observations without the need for memory. In three MCE instances, memory and observations are used differently to determine optimal performance. The instances include Amnesia, where memory is not needed for optimal actions, Blind, where memory is essential for tracking mode sequences, and Tracker, where both memory and observations are required. Each instance uses a recurrent architecture with specific layers and nodes. The recurrent architecture used in MCE instances involves a feed-forward layer, a GRU layer, and a softmax layer for action distribution. Imitation learning is used for training, achieving 100% accuracy on the imitation dataset. MMN training involves quantized bottleneck units with varying sizes. Training in MCE environments with QBNs is faster compared to RNN training. The decoder for both b f and b h has a symmetric architecture to the encoder. Training of b f and b h in the MCE environments was fast compared to RNN training. QBNs with bottleneck sizes of B f \u2208 {4, 8} and B h \u2208 {4, 8} were trained and embedded into the RNN to create a discrete MMN. Fine-tuning was done for some cases, resulting in improved MMN performance. In one case, inserting one bottleneck at a time led to perfect performance. After fine-tuning, the agent achieved perfect MMN performance immediately after bottleneck insertion due to low reconstruction error. Inserting one bottleneck at a time resulted in perfect performance in one case, with 98% accuracy. The number of states and observations in the MMs extracted from the MMNs decreased after minimization, indicating that MMN learning does not always result in minimal discrete representations. However, after minimization, exact minimal machines were obtained for each MCE domain in all but one case. After fine-tuning, the agent achieved perfect MMN performance immediately after bottleneck insertion. The number of states and observations in the MMs extracted from the MMNs decreased after minimization, indicating that MMN learning does not always result in minimal discrete representations. However, after minimization, exact minimal machines were obtained for each MCE domain in all but one case. The MMNs learned via QBN insertions were equivalent to the true minimal machines, showing optimal performance in most cases. The exception occurred when the MMN did not achieve perfect accuracy. The machines for Blind and Amnesia illustrate different memory use strategies, with Blind having a single observation symbol and Amnesia having distinct observation symbols leading to the same state. The machine for Amnesia shows that each observation symbol leads to the same state for action choice, policies are determined by current observation. Evaluation conducted over 7 Tomita Grammars treating them as environments with 'accept' and 'reject' actions. RNN training involved one-layer GRU with 10 hidden units and fully connected softmax layer with 2 nodes. Training dataset comprised of equal number of accept/reject strings with lengths in range [1, 50]. The training involved a one-layer GRU with 10 hidden units and a fully connected softmax layer with 2 nodes for accept/reject classification. Imitation learning was used with Adam optimizer and learning rate of 0.001. The test results showed high accuracy, with MMNs maintaining RNN performance without fine-tuning in most cases. Fine-tuning only provided minor improvements. The MMNs, created by inserting bottlenecks in RNNs, maintained RNN performance without fine-tuning in most cases. Fine-tuning only provided minor improvements. MM extraction and minimization resulted in reduced state-space while maintaining performance. Applying the technique to RNNs for Atari games showed promising results. In this section, the technique of extracting finite state representations for Atari policies from RNNs learned for six Atari games using OpenAI gym is discussed. The input observations for Atari are more complex than previous experiments, making it unclear if similar results can be expected. The Atari agents have the same recurrent architecture with specific preprocessing steps for input observations. The Atari agents have a recurrent architecture with specific preprocessing steps for input observations, including gray-scaling, down-sampling, cropping, and normalization. The network consists of convolutional layers, a GRU layer, and a fully connected layer for predicting the value function. The A3C RL algorithm is used for training, with loss computed using Generalized Advantage Estimation. The RNN performance on six games is reported in the second column of a table. The RNN performance on six games is reported in the second column of a table, using Generalized Advantage Estimation. The encoder and decoder sizes were adjusted to match the dimension of continuous observation features. Training data for bottlenecks was generated using noisy rollouts to increase diversity and robust learning. Bottlenecks were trained for different sizes, significantly larger than earlier experiments, due to the complexity of Atari. Training bottlenecks for larger sizes was necessary due to the complexity of Atari games. The bottlenecks were inserted into the RNN to create an MMN for each game. The MMNs performed well on games like Pong, Freeway, Bowling, and Boxing, either matching or coming close to RNN scores without impacting performance. Fine-tuning was required for Boxing and Pong, while Freeway and Bowling did not need it. Breakout and Space Invaders showed different results with the MMNs. The MMNs learned after fine-tuning achieved lower scores than the original RNNs in Breakout and Space Invaders due to poor reconstruction in rare parts of the game, impacting performance. This highlights the need for more intelligent approaches in training QBNs to capture critical information in such states. Before minimization, the MMs had large numbers of discrete states and observations, reflecting the complexity of using large values of B h and B f. After fine-tuning, MMNs had lower scores in Breakout and Space Invaders due to poor reconstruction in rare game states. Minimization reduced the number of states and observations significantly, making analysis feasible by hand. In Atari, memory use in Pong was observed with three states and 10 observation symbols, each transitioning to the same state. In Atari, memory use was observed in Pong with three states and 10 observation symbols. The Pong policy maps individual observations to actions without the need for memory. In Bowling and Freeway, the minimal MM only has one observation symbol, ignoring input images for action selection. Freeway's policy always takes the Up action at each time step, similar to a Blind MCE policy. The MM extraction approach discovered trivial and interesting policies in Atari games like Freeway and Bowling. Freeway always takes the Up action, while Bowling has an open-loop policy structure. Breakout, Space Invaders, and Boxing use memory and observations in their policies. Further semantic analysis of observations and states is needed for a complete understanding of Atari policies. The approach involves extracting finite state Moore Machines from RNN policies by using Quantized Bottleneck Networks to encode memory and input features. This allows for the extraction of discrete Moore machines for analysis and usage. Results show accurate extraction of ground truth in known environments and maintained performance in Atari games with unknown ground truth machines. The approach involves extracting finite state Moore Machines from RNN policies using Quantized Bottleneck Networks. Results show accurate extraction of ground truth in known environments and maintained performance in Atari games with unknown ground truth machines. The extracted machines provide insight into memory usage of policies, revealing small memory requirements and cases where memory was not utilized effectively. Future work includes developing tools for analyzing discrete observations and states to gain further insight into policies. The curr_chunk discusses the parameterization of a Markov Chain Environment (MCE) and how the hidden state changes based on mode transitions. It also mentions the agent's observation process and the distribution over initial modes. The focus is on developing tools for analyzing discrete observations and states to gain deeper insights into policies. The agent in the Markov Chain Environment (MCE) does not directly observe the state but receives continuous-valued observations at each step. The agent must remember the current mode and use memory to keep track of how long the mode has been active to determine when to pay attention to the observations. Experiments are conducted with different MCE instances, including one with uniform sampling of initial modes and transition distributions. In the Markov Chain Environment (MCE), the agent uses memory to track the mode's duration and determine when to pay attention to observations. Three MCE instances are tested: Amnesia, Blind, and Tracker, each with different settings to evaluate the use of memory in decision-making. The MCE instances tested are Amnesia, Blind, and Tracker, with Tracker being the most general and challenging due to the growing number of modes and their life-spans. The environment requires an optimal policy to pay attention to observations and use memory to track the current mode and mode count."
}