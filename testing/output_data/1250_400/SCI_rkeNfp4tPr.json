{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is popular for training deep neural networks. Momentum adjustment reduces convergence time in non-stochastic convex optimization. Empirical evidence shows that stochastic momentum improves convergence time in deep network training. Theoretical justification for its use has been a significant open question. The proposed answer is that stochastic momentum helps escape saddle points faster and find a second order stationary point more quickly. In this paper, the authors propose that stochastic momentum improves deep network training by helping SGD escape saddle points faster and find second order stationary points more quickly. Theoretical analysis suggests that a large momentum parameter is ideal, which aligns with empirical findings. Experimental results further support these conclusions, highlighting the widespread adoption of SGD with stochastic momentum in various machine learning applications. Modern techniques in computer vision, speech recognition, natural language processing, and reinforcement learning utilize SGD with stochastic momentum for training models. The success of momentum in achieving faster convergence has made it a crucial tool in designing optimization algorithms. Despite its widespread use, the empirical improvements and guidelines for setting the momentum parameter remain unclear. In this paper, a theoretical analysis is provided for SGD with stochastic momentum, focusing on the benefits of using momentum in optimization algorithms. The study identifies conditions that guarantee faster escape from saddle points compared to standard SGD, supporting the use of stochastic momentum in practice. In this paper, the analysis focuses on SGD with stochastic heavy ball momentum for finding second-order stationary points in non-convex optimization. The method involves updating iterates with momentum and maintaining a weighted average of stochastic gradients. The goal is to escape saddle points faster than standard SGD, with conditions ensuring the effectiveness of stochastic momentum. The approach aims to achieve a (\u03b5, \u03b4)-second-order stationary point where the gradient and Hessian are bounded. The paper focuses on using stochastic heavy ball momentum in SGD to find second-order stationary points in non-convex optimization. The goal is to reach an approximate second-order stationary point with conditions ensuring the effectiveness of stochastic momentum. The method aims to escape saddle points faster than standard SGD by maintaining a weighted average of stochastic gradients. The paper introduces a required condition for using momentum in stochastic optimization to reach second-order stationary points faster. It discusses the Correlated Negative Curvature (CNC) assumption and properties like Almost Positively Aligned with Gradient (APAG) that help in escaping saddle points efficiently. The paper discusses the benefits of stochastic momentum in reaching second-order stationary points faster by introducing the CNC assumption and properties like APAG. It shows that a larger momentum parameter can help escape saddle points more efficiently in optimization and deep learning. The paper explains the advantages of stochastic momentum in optimization and deep learning, particularly in escaping saddle points faster by moving along the direction of the smallest eigenvector of the Hessian matrix. The paper discusses the benefits of stochastic momentum in optimization, emphasizing the importance of moving along the direction of the smallest eigenvector of the Hessian matrix to escape saddle points quickly. The study shows that updates can escape the saddle point region by ensuring that the update direction is strongly non-orthogonal to the direction of large negative curvature. The study demonstrates the effectiveness of stochastic momentum in optimization by ensuring updates escape saddle points through non-orthogonal directions. Momentum accelerates the escape process by a factor of 1 - \u03b2, but \u03b2 cannot be arbitrarily close to 1. The study shows that stochastic momentum accelerates saddle-point escape by a factor of 1 - \u03b2. However, \u03b2 cannot be chosen very close to 1. Empirical evidence supports the benefits of stochastic momentum in optimization tasks with significant saddle points. In optimization tasks with significant saddle points, stochastic momentum accelerates escape by a factor of 1 - \u03b2, but \u03b2 should not be chosen very close to 1. The objective in phase retrieval involves finding an unknown w * \u2208 R d using a few samples y i = (a i w * ) 2, with applications in physical sciences. The heavy ball method accelerates escape from saddle points in optimization tasks. Larger choices of \u03b2 significantly speed up convergence in phase retrieval problems. Trajectories with large momentum escape saddle points more quickly than those with smaller momentum, leading to a dramatic speedup in finding optimal solutions. The heavy ball method accelerates escape from saddle points in optimization tasks, providing a dramatic speedup in finding optimal solutions. Specialized algorithms exploit negative curvature to reach second order stationary points faster. Specialized algorithms exploit negative curvature to escape saddle points faster and reach second order stationary points. Previous works have shown that adding isotropic noise in each iteration guarantees escape from saddle points. The assumption of Correlated Negative Curvature for stochastic gradient allows the algorithm to avoid perturbing updates with isotropic noise. Our work assumes Correlated Negative Curvature for stochastic momentum instead of gradient, comparing results with related works in Appendix A. We assume L-Lipschitz gradient and \u03c1-Lipschitz Hessian, ensuring convergence properties. Stochastic momentum is bounded with spectral norm denoted by \u03a0 i M i. Analysis relies on three dynamic properties, aiming to demonstrate empirically in standard problems. The analysis of stochastic momentum relies on three key properties, including Almost Positively Aligned with Gradient (APAG) and Almost Positively Correlated with Gradient (APCG). These properties aim to ensure that the momentum term aligns well with the gradient in natural settings. The momentum term in stochastic momentum algorithms exhibits Gradient Alignment or Curvature Exploitation (GrACE) to ensure it aligns well with the gradient. This property is crucial for making progress in the algorithm, especially in regions with large gradients. Another related property, Almost Positively Correlated with Gradient (APCG), measures the correlation between the momentum term and the gradient in terms of local curvature. Empirical evidence supports these properties on natural problems. The PSD matrix Mt measures the local curvature of the function with respect to the trajectory of SGD with momentum. Empirical evidence supports the APCG property on natural problems, especially in saddle regions with significant iterations. The value of Mt is almost always nonnegative, indicating APAG and APCG properties in experiments. Additionally, for the phase retrieval problem, expected values may be nonnegative. The figures show that SGD with momentum exhibits APAG and APCG properties in experiments. The analysis indicates that nonnegative values are expected for the phase retrieval problem, even in the presence of negative curvature. The alignment between stochastic momentum and gradient, as well as curvature exploitation, play crucial roles in the optimization process. The analysis follows a similar template to previous works and is structured into three cases based on the gradient and Hessian eigenvalues. Algorithm 2 is analyzed with a boosted step size, showing that the iteration complexity is only slightly worse with a larger constant on the right-hand side of the equation. The analysis of Algorithm 2 involves boosting the step size and analyzing the iteration complexity. The algorithm is shown to make progress in different cases, ultimately reaching a second-order stationary point with high probability. The proof utilizes tools from previous works but introduces novel momentum analysis. The analysis of Algorithm 2 involves boosting the step size and analyzing the iteration complexity to reach a second-order stationary point with high probability. The proof utilizes tools from previous works but introduces novel momentum analysis, showing the advantage of using stochastic momentum for SGD. Higher \u03b2 enables faster escape from saddle points. Constraints on \u03b2 are necessary to prevent it from being too close to 1. Higher \u03b2 enables faster escape from saddle points. Constraints on \u03b2 are necessary to prevent it from being too close to 1. In the high momentum regime, Algorithm 2 is shown to be better than CNC-SGD, helping find a second-order stationary point faster. Empirical findings support the effectiveness of higher momentum in escaping saddle points. In the high momentum regime, Algorithm 2 is proven to be superior to CNC-SGD, aiding in faster escape from saddle points. Empirical evidence shows that a higher momentum can lead to quicker discovery of second-order stationary points. The analysis focuses on the process of escaping saddle points by SGD with momentum, demonstrating that it takes at most T thred iterations to escape the region with a decrease in function value by at least F thred. The analysis focuses on proving that the function value must decrease by at least F thred in T thred iterations using a contradiction technique. By leveraging negative curvature, it is shown that the expected distance decreases. Larger momentum helps in escaping saddle points faster. Lemmas provide upper and lower bounds for the expected distance, showing the effectiveness of SGD with momentum in escaping saddle points. The proof in Appendix C shows that C upper,t in Lemma 1 increases with time t. Lemma 2 provides a lower bound for the expected distance using the recursive dynamics of SGD with momentum. Lemma 3 establishes the dominant term in the lower bound, crucial for ensuring it surpasses the upper bound. The analysis relies on conditions and notations from Lemmas 1 and 2, with a focus on the eigenvector v corresponding to the minimum eigenvalue of the Hessian H. The proof in Appendix C shows that the upper bound in Lemma 1 increases with time. Lemma 2 provides a lower bound for the expected distance in SGD with momentum dynamics. By following Lemmas 1 and 2, we establish conditions for the lower bound to exceed the upper bound, focusing on the eigenvector corresponding to the minimum eigenvalue of the Hessian H. Additionally, three properties are identified to ensure faster convergence to a second-order stationary point in SGD with momentum. In this paper, three properties are identified that guarantee faster convergence to a second-order stationary point in SGD with momentum. A higher momentum parameter leads to escaping strict saddle points faster, as SGD with momentum enlarges the projection to an escape direction. The conditions for these properties to hold are not clear, but understanding them could shed light on the success of SGD with momentum in non-convex optimization and deep learning. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard algorithms. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard algorithms, except in specific cases like convex quadratic objectives. Recent analysis shows that stochastic heavy ball momentum and Nesterov's momentum converge at a rate of O(1/ \u221a t), similar to standard SGD. Variants of stochastic accelerated algorithms have been proposed, but do not capture the stochastic heavy ball method. The expected gradient norm converges at rate O(1/ \u221a t), similar to standard SGD. Some works propose variants of stochastic accelerated algorithms with first order stationary point guarantees, but do not include the stochastic heavy ball momentum used in practice. Kidambi et al. (2018) show that SGD with heavy ball momentum may not achieve the best convergence rate for specific problems. Specialized algorithms aim at reaching a second order stationary point by explicitly exploiting negative curvature. Specialized algorithms exploit negative curvature to escape saddle points faster than standard SGD. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates. The iteration complexity results of different SGD variants are compared. The focus is on explaining why SGD with heavy ball momentum works well in practice without certain schemes used in other algorithms. Our analysis focuses on the effectiveness of stochastic heavy ball momentum in optimization algorithms, contrasting it with other approaches. We build on previous work to show the advantages of using this momentum. Lemmas 6, 7, and 8 highlight the properties of SGD with momentum, emphasizing its ability to decrease function value and make progress under certain conditions. Lemma 7 states that under the APAG property, SGD with momentum decreases function value by a constant when the gradient norm is large. Lemma 8 bounds the increase of function value of the next iterate using the GrACE property. If SGD with momentum has the APAG property, the update step w t+1 = w t \u2212 \u03b7m t satisfies certain conditions. The update step w t+1 = w t \u2212 \u03b7m t satisfies certain conditions under the GrACE property. By considering the \u03c1-Lipschitzness of Hessian, the stochastic momentum m t, and the step size \u03b7, the update rule can be analyzed. The proof involves bounding the conditional expectation and utilizing the triangle inequality. Additionally, the upper bound of E t0 [4\u03b7 is determined by incorporating the zero-mean noise \u03be t and the Lipschitz property of the Hessian. The update step w t+1 = w t \u2212 \u03b7m t satisfies conditions under the GrACE property, considering the \u03c1-Lipschitzness of Hessian, stochastic momentum m t, and step size \u03b7. The proof involves bounding conditional expectation and utilizing the triangle inequality, with an upper bound of E t0 [4\u03b7 incorporating zero-mean noise \u03be t and Lipschitz property of the Hessian. Lemma 2 defines a quadratic approximation at w t0, with various components qv,t\u22121, qm,t\u22121, qq,t\u22121, qw,t\u22121, q\u03be,t\u22121. Lemma 3 and Lemma 5 provide proofs based on quadratic approximations and conditions from previous theorems. The APCG property of SGD with momentum is discussed, leading to constraints on various parameters like c, L, and \u03c3. The proof involves bounding conditional expectations and utilizing the triangle inequality. Lemma 3 and Lemma 5 provide proofs based on quadratic approximations and conditions from previous theorems. The APCG property of SGD with momentum is discussed, leading to constraints on various parameters like c, L, and \u03c3. The proof involves bounding conditional expectations and utilizing the triangle inequality. Additionally, constraints on the parameter \u03b2 are discussed to ensure its value is not too close to 1. The proof in Lemma 5 involves bounding conditional expectations and utilizing the triangle inequality. Constraints on the parameter \u03b2 are discussed to ensure its value is not too close to 1. The analysis includes upper bounding E t0 [ q q,t\u22121 ] and deriving an upper bound of E t0 [ \u2207f (w t0+s ) \u2212 \u2207Q(w t0+s ) ]. The notation \u03a0 t\u22121 j=s+1 G j 2 is analyzed, with constraints on \u03bb and \u03b7 to satisfy certain conditions. The proof in Lemma 5 involves bounding conditional expectations and utilizing the triangle inequality. Constraints on the parameter \u03b2 are discussed to ensure its value is not too close to 1. The analysis includes upper bounding E t0 [ q q,t\u22121 ] and deriving an upper bound of E t0 [ \u2207f (w t0+s ) \u2212 \u2207Q(w t0+s ) ]. The notation \u03a0 t\u22121 j=s+1 G j 2 is analyzed, with constraints on \u03bb and \u03b7 to satisfy certain conditions. If f (\u00b7) has Lipschitz Hessian, then using the definitions and notations, various bounds are derived for different terms in the equations. The current chunk discusses the lower bounding of conditional expectations using Lemmas 11, 12, and 13. It involves defining matrices and proving their properties, such as symmetry and positive semidefiniteness. The proof utilizes various conditions and properties to establish the lower bounds. The matrix B is symmetric positive semidefinite, proven by expressing each G j in the form of G j = U D j U. The lower bounding of conditional expectations is discussed, utilizing Lemmas 11, 12, and 13. The proof involves showing that the lower bound is larger than the upper bound, leading to a contradiction. The proof involves showing that the lower bound is larger than the upper bound, leading to a contradiction. By leveraging negative curvature, it is demonstrated that the function value must decrease at least F thred in T thred iterations on expectation. This is achieved by utilizing Lemmas 11, 12, and 13, and by choosing T thred large enough to guarantee the inequality holds. By leveraging negative curvature, the function value must decrease at least F thred in T thred iterations on expectation. This is achieved by choosing T thred large enough to guarantee the inequality holds. With momentum, SGD reaches a second order stationary point in T iterations with high probability. When the gradient is large and the GrACE property is maintained, the algorithm reaches a second order stationary point in a certain number of iterations with high probability. The proof relies on a lemma guaranteeing that sampling points from a specific set leads to a second order stationary point. The conditions for this to occur are discussed, ensuring a decrease in function value over iterations. The proof relies on Lemma 15, ensuring a decrease in function value over iterations by satisfying specific conditions. The events in Lemma 15 are considered, leading to a second order stationary point with high probability. The conditions for this to occur are discussed, allowing the application of the lemma and the completion of the proof of the theorem. The proof is based on Lemma 15, ensuring a decrease in function value over iterations by satisfying specific conditions. By setting parameters according to Table 3, Algorithm 2 can find a second order stationary point faster than previous methods."
}