{
    "title": "H1cT3NTBM",
    "content": "In this paper, the use of neural networks for music information retrieval tasks is explored, focusing on improving convolutional neural networks (CNNs) on spectral audio features. Three aspects of CNN design are investigated: network depth, residual blocks with grouped convolution, and global time aggregation. The study, centered on singer classification and singing performance embedding, concludes that global time aggregation significantly enhances CNN performance. Additionally, a singing recording dataset is released for training and evaluation. The research aims to leverage recent advancements in deep learning to enhance the learning capability of neural networks for music analysis. In this paper, experiments explore the use of ResNet and ResNeXt variants to deepen convolutional layers in neural networks for music information retrieval. Existing research typically uses vanilla convolutional layers with limited depth, but these variants offer enhanced learning capabilities for time-frequency representations in audio analysis. The paper proposes using grouped convolution in deep neural networks for music information retrieval, utilizing ResNet and ResNeXt variants to deepen convolutional layers. These variants offer improved learning capabilities for time-frequency representations in audio analysis, addressing temporal relations through attention mechanisms from natural language processing. The paper explores using attention mechanisms from natural language processing to model temporal relations in time-frequency representations. It investigates the effects of global aggregation along the time axis using average, max, or attention mechanisms in singer classification and performance embedding tasks. The goal of singer classification is to predict the singer's identity from an audio recording. Singer performance embedding aims to create a space where singers with similar styles are closer together. This can help identify singing characteristics. The challenge is separating the \"singer effect\" from the \"song effect\" in audio analysis tasks. The application context involves isolating the \"singer effect\" from the \"song effect\" in audio analysis tasks. To effectively model singing, a representation emphasizing singer similarity while reducing the song similarity effect is needed. This is similar to face identification in computer vision, where the goal is to minimize the impact of environment and pose on face recognition. Singer performance embedding aims to create a space where singers with similar styles are closer together, aiding in identifying singing characteristics. The singer performance embedding approach aims to learn an embedded space for singing voice audio recordings, similar to face verification in computer vision. By using a siamese neural network, the goal is to place recordings of the same identity close together and those of different identities far apart in the learned embedding space. This allows for the identification of \"singing style\" or \"singing characteristics\" by examining the clusters formed from the embeddings of audio recordings. The architecture employs CNNs to extract features and a global aggregation layer for classification. The architecture for singer identity classification and singing performance embedding uses CNNs to extract features. The output layer differs between the tasks, with softmax for classification and a linear layer for embedding. This enables fast similarity comparison of spectrogram sequences by calculating Euclidean distance between fixed length embedding vectors. The siamese architecture is used to learn embedding space for singing recordings, allowing for efficient similarity comparison. A new set of \"balanced\" singing recordings is released for unbiased evaluation. The neural network constructing blocks, dataset, and experiment details are described in the paper. The neural network constructing blocks used in the experiments include a vanilla convolution layer and a ResNet design with a bottleneck block. The ResNet variant is extended with a grouped convolutional block from ResNeXt. The general design pattern involves feeding input time-frequency features as 2-D images into convolutional layers, followed by a global time-wise aggregation layer and dense layers. The ResNet variant used in the experiments includes a bottleneck block and is extended with a grouped convolutional block from ResNeXt. The configuration involves a max pooling layer placed between convolutional blocks, with the first layer followed immediately by a max pooling layer and subsequent layers having max pooling layers inserted between every two consecutive convolutional layers/blocks. The neural network architecture used in this paper includes a max pooling layer with a pool size of (2, 2) and stride of (2, 2) placed between convolutional blocks. Batch normalizations are applied after each non-linearity activation. The 3-D feature map shape is (# of channels, # of time frames, # of frequency bins). The neural network architecture in this paper includes convolutional layers, global time-wise aggregation, and dense layers. The attention mechanism used is the feed-forward version for non-sequence prediction tasks. The feed-forward attention mechanism proposed in BID15 is utilized in this neural network architecture. It calculates a weight vector over time-steps for input feature vectors, resulting in a weighted average output. This attention operation acts as an aggregation over the time-axis, similar to max or average pooling, but with learnable parameters. It differs from standard pooling as it globally reduces the dimension of the aggregation axis to 1. The neural network architecture utilizes a feed-forward attention mechanism for aggregation, different from standard pooling methods. The tasks explored include singer identity classification and singing performance embedding, with clear evaluation criteria for model performance. The embedding task focuses on spatial relationships between samples. Evaluation metrics and plots are used for both tasks. The DAMP dataset is used for singer identity classification, but its unbalanced nature makes it challenging for learning algorithms. An additional dataset with balanced collections of songs by each singer is collected for training and evaluation. The unbalanced DAMP dataset makes it difficult for learning algorithms to avoid bias towards singer-specific songs. To address this, the DAMP-balanced dataset was created with 24874 singing recordings by 5429 singers, featuring 14 songs. The dataset includes a test set of the last 4 songs and allows for different train/validation splits of the first 10 songs. The DAMP-balanced dataset is suitable for singing performance embedding tasks, while the original DAMP dataset is used for singer identity classification algorithms. The DAMP-balanced dataset, with 24874 recordings by 5429 singers, is suitable for singing performance embedding tasks. Time-frequency representations extracted from raw audio signals, specifically Mel-spectrograms, are used as input features for neural networks. The Mel-scaled magnitude spectrogram (Mel-spectrogram) is used as input feature for neural networks in this study. While the constant-Q transformed spectrogram (CQT) is also commonly used in music tasks, Mel-spectrogram outperforms CQT due to its ability to preserve octave relationships between frequency bins. The linear relationships in CQT do not apply well to different harmonics of one pitch, making Mel-spectrogram a better choice for analyzing single singing voices. The Mel-scaled magnitude spectrogram is used as input for neural networks in this study. Audio recordings are resampled to 22050Hz and transformed into Mel-spectrograms using FFT. The spectrogram is squared, transformed into decibels, and values below -60dB are clipped to zero. Each singing performance audio recording is chopped into overlapping matrices with a duration of 6 seconds and 20% hop size. The Mel-spectrogram of each singing performance audio recording is chopped into overlapping matrices with a duration of 6 seconds and 20% hop size. Gradient descent is optimized with ADAM, a learning rate of 0.0001, and a batch size of 32. L2 weight regularization is applied, and hyperparameters are chosen using Bayesian optimization. Early stopping tests are conducted every 50 epochs for singer identity classification and singing performance embedding tasks. The rectified linear unit activation function is used in all layers. The singer identity classification model uses a patience of 300 epochs with 99.5% improvement, while the singing performance embedding task has a patience of 1000 epochs. The neural network architecture includes convolutional layers with rectified linear unit activation function, filter sizes of (10, 10) and (5, 5), and fully connected layers with 3 layers of 1024 hidden units each. A subset of 46 singers from the DAMP dataset is used for classification, with a 10-fold cross validation approach. Different neural network configurations, including vanilla CNN and ResNeXt building blocks, are explored for the classification task. The singer identity classification model uses different neural network configurations, including vanilla CNN and ResNeXt building blocks. Various aggregation methods are explored, and experimental results show that neural network models outperform the baseline SVM classifier by at least 35%. The singer identity classification model utilizes different neural network configurations, with global aggregation methods improving performance by 5% to 10%. A subset of DAMP-balanced data is used for the singing performance embedding experiment, aiming to create an embedding space that clusters recordings by the same singer together. A siamese neural network architecture is employed for this purpose. The siamese neural network architecture is used to create an embedding space that clusters recordings by the same singer together. Pairs of samples are labeled based on their identity, and the contrastive loss function is optimized to train the network. The siamese neural network architecture is utilized to cluster recordings by the same singer together. The contrastive loss function is optimized to train the network, with observations on training/validation plots indicating different aggregation methods and network configurations. The siamese neural network architecture is used to cluster recordings by the same singer. Different aggregation methods and network configurations are compared, showing that shallow architectures perform slightly better than deeper ones with similar parameters. Results are visualized in Figure 4, comparing embeddings from the network with handcrafted features for singer classification. The embeddings group performances by the same singer, while handcrafted features capture the \"song\" effect. The baseline handcrafted audio features and learned embeddings from a singing performance experiment were compared for singer classification. The embeddings grouped performances by the same singer, while handcrafted features captured the \"song\" effect. Classification accuracies using k-nearest neighbor method were obtained for different network configurations. The results are visualized in Figure 5. The k-nearest neighbor classification results show that singing performance embedding learning can dilute the \"song\" effect and enhance the \"singer style\". The feed-forward global aggregation helps in reducing the \"song effect\" while improving singer characterization. The balanced nature of the dataset allows for accurate classification of performed songs. In this paper, empirical investigations were conducted on how recent developments in deep learning could aid in solving singer identification and embedding problems. Global aggregation over time was found to significantly improve performance, with feed-forward attention accelerating the learning process compared to other global aggregations. The feed-forward attention layer learns a \"frequency template\" for each convolutional channel, enabling focus on different parts along the frequency axis. The paper introduces the concept of \"frequency templates\" learned by the feed-forward attention layer in deep neural networks with over 15 convolutional layers for music information retrieval. A dataset of 20000 single singing voice recordings is released, and future work includes experimenting with striding in convolutional layers and improving global aggregation by considering temporal order. The released DAMP-balanced dataset for singer classification has the same songs for each singer. Future work includes experimenting with striding in convolutional layers and improving global aggregation by considering temporal order. The dataset is collected from the Sing! Karaoke app, similar to the original DAMP dataset. The DAMP-balanced dataset differs from the original DAMP dataset in how audio recordings and metadata are collected. The original DAMP dataset randomly selected 10 singing performances from 3462 Sing! Karaoke app users, with no constraints on song collections. In contrast, the DAMP-balanced dataset specifically queries for users who sang a specific set of popular songs, with only one performance per song per user. This dataset includes 14 popular songs, and queries were created to cover all combinations of splitting the songs into different collections. The dataset includes 14 popular songs, with queries created to cover all combinations of splitting the songs into different collections. For example, one split resulted in 276 performances for training and 88 performances for validation, involving 46 and 22 singers respectively. Another split had 459 users and 1836 performances for the first 4 songs, and 3 users with 18 performances for the following 6 songs. The DAMP-balanced dataset includes different numbers of singers in each set, leading to varying total performances of songs. This structure allows for train/validation rotation within the first 10 songs and provides balanced test sets for models training on other datasets."
}