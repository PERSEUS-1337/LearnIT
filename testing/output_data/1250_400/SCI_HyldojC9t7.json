{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It utilizes random feature maps to build a kernel specified by the distance measure, improving generalizability over Nearest-Neighbor estimates. D2KE is a versatile approach that can handle complex structured inputs of variable sizes. Our proposed framework excels in classification experiments across various domains such as time series, strings, and histograms for texts and images. It outperforms existing distance-based learning methods in terms of accuracy and computational efficiency. Constructing a dissimilarity function between instances is often easier than creating a feature representation, especially for structured inputs like time series, strings, histograms, and graphs. There are established dissimilarity measures for complex structured inputs, such as Dynamic Time Warping for time series and Edit Distance for strings. Standard machine learning methods are primarily designed for vector representations, with limited work on structured inputs. Even for complex structured inputs, various dissimilarity measures exist, such as Dynamic Time Warping for time series, Edit Distance for strings, Hausdorff distance for sets, and Wasserstein distance for distributions. Standard machine learning methods are typically designed for vector representations, with less focus on distance-based methods for structured inputs. Nearest-Neighbor Estimation (NNE) is a common distance-based method, but it can be unreliable due to high variance when neighbors are far apart. Research has been done on global distance-based machine learning methods to address this issue. Global distance-based machine learning methods have been developed to address issues with high variance in Nearest-Neighbor Estimation for complex structured inputs. These methods involve treating the data similarity matrix as a kernel Gram matrix and using standard kernel-based methods like Support Vector Machines. However, most similarity measures do not provide a positive-definite kernel, leading to non-convex optimization problems. Research has focused on estimating a positive-definite Gram matrix that approximates the similarity matrix through methods like clipping, flipping, or shifting eigenvalues. The empirical risk minimization problem is often not well-defined due to non-convexity issues. Various methods have been proposed to estimate a positive-definite Gram matrix that approximates the similarity matrix, such as clipping, flipping, or shifting eigenvalues. However, these modifications can lead to a loss of information and inconsistency between training and testing samples. Another approach involves selecting a subset of training samples as a representative set, but a more general framework offers a richer family of kernels that outperform this method in various applications. In this paper, a novel general framework called D2KE is proposed to construct a family of PD kernels from a dissimilarity measure on structured inputs. This approach draws from the literature of Random Features but builds novel kernels specifically designed for a given distance measure. The kernels produced by this framework are Lipschitz-continuous w.r.t. the given distance measure and provide better generalization properties than nearest-neighbor estimation. The D2KE framework constructs PD kernels from a given distance measure for structured inputs. These kernels are Lipschitz-continuous and provide better generalization properties than nearest-neighbor estimation. The framework also produces a feature embedding for each instance, improving classification and regression models' performance in various domains. From a distance kernel learning perspective, a methodology is proposed to construct PD kernels via Random Features for structured inputs. This framework accelerates kernel machines across various domains such as time-series and strings. Existing approaches for distance-based kernel learning have limitations, but this new method provides theoretical and empirical justifications for its effectiveness. The methodology proposes constructing positive-definite kernels through strict conditions on the distance function or empirical PD Gram matrices. Various dissimilarity measures do not satisfy the conditions for obtaining a PD kernel. Different approaches include finding a Euclidean embedding or using an SVM solver in Krein spaces. Specific methods focus on building PD kernels for structured inputs like text and time-series. The text discusses different approaches to constructing positive-definite kernels, including using SVM solver in Krein spaces and building PD kernels for structured inputs like text and time-series. Interest in approximating non-linear kernel machines using randomized feature maps has increased due to reduced training and testing times. Various explicit nonlinear random feature maps have been developed for different types of kernels. The use of randomized feature maps has become popular for reducing training and testing times in kernel-based learning algorithms. Various explicit nonlinear random feature maps have been created for different types of kernels. The Random Fourier Features (RFF) method, which approximates a Gaussian Kernel function, has been extensively studied. Methods to accelerate RFF on high-dimensional input data matrices have been proposed. However, existing RF methods only consider inputs with vector representations. The proposed method leverages structured matrices for faster matrix computation and less memory consumption. Unlike existing methods that only consider vector inputs, D2KE handles structured inputs of different sizes and computes the random features with a structured distance metric. D2KE constructs a new PD kernel through a random feature map, making it computationally feasible. A recent work has developed a kernel and algorithm for computing embeddings of single-variable real-valued time-series, but it cannot be applied to discrete structured inputs like strings, histograms, and graphs. Our unified framework extends beyond existing methods like BID49, providing a general theoretical analysis for various structured inputs. We aim to estimate a target function from samples of structured input objects, considering dissimilarity measures instead of feature representations. The size of structured inputs may vary widely. The text discusses the use of dissimilarity measures instead of feature representations for structured inputs in learning tasks. It emphasizes the importance of having a compact and simple feature representation for the target function. The dissimilarity measure should satisfy certain properties to ensure small differences in the function and a compact space of small intrinsic dimension. The text discusses the importance of dissimilarity measures in learning tasks for structured inputs. It emphasizes the need for a compact feature representation for the target function, with properties like Lipschitz Continuity and covering number N(\u03b4; X, d) to ensure small differences in the function and a compact space of small intrinsic dimension. The text discusses the importance of dissimilarity measures in learning tasks for structured inputs, emphasizing the need for a compact feature representation with properties like Lipschitz Continuity and covering number N(\u03b4; X, d) to ensure small differences in the function and a compact space of small intrinsic dimension. It extends the analysis to any structured input space X with an associated distance measure d and finite covering number N(\u03b4; X, d), defining effective dimension and providing an example in the case of measuring the space of Multiset. The text discusses the importance of dissimilarity measures in learning tasks for structured inputs, emphasizing the need for a compact feature representation with properties like Lipschitz Continuity and covering number N(\u03b4; X, d) to ensure small differences in the function and a compact space of small intrinsic dimension. It extends the analysis to any structured input space X with an associated distance measure d and finite covering number N(\u03b4; X, d), defining effective dimension and providing an example in the case of measuring the space of Multiset. The (modified) Hausdorff Distance is used to calculate the covering number of V under the ground distance \u2206, with a focus on the estimation error of the k-Nearest-Neighbor estimate of f (x) and the scaling of the number of samples required for bounded error. The text introduces an estimator based on a RKHS derived from a distance measure, aiming to improve sample complexity for higher effective dimension problems. It addresses the challenge of converting a distance measure into a positive definite kernel by introducing the D2KE approach, which constructs positive definite kernels from a given distance measure using a family of kernels parameterized by random structured objects. The text introduces a family of kernels constructed from a structured input domain X and a distance measure d(., .). The kernels are parameterized by random structured objects and a distribution over them, aiming to improve sample complexity for higher effective dimension problems. The kernel can be interpreted as a soft version of the distance substitution kernel, providing insights into its relationship with the distance measure. The text discusses a kernel that is a soft version of the distance substitution kernel, providing insights into its relationship with the distance measure. It also introduces a method for approximating the kernel using Random Features, making it suitable for large-scale settings with a large number of samples. The kernel discussed is defined via a random feature map, making it suitable for large-scale settings with a large number of samples. The RF approximation allows for learning a target function as a linear function of the RF feature map by minimizing domain-specific empirical risk. This approach is different from a recent work that selects random features in a supervised setting. The RF based empirical risk minimization for D2KE kernels is outlined in Algorithm 1, involving structured distance measures and exponent functions. The D2KE kernels in Algorithm 1 involve computing random feature embeddings using structured distance measures and exponent functions, contrasting with traditional RF methods. The estimator's statistical performance is analyzed in Section 5, comparing it to K-nearest-neighbor. The approach is related to the representative-set method by setting \u2126 = X and p(\u03c9) = p(x), resulting in a kernel Equation (4) dependent on data distribution. A Random-Feature approximation is obtained by creating an R-dimensional feature embedding from a part of the training data, as shown in Algorithm 1. The kernel Equation (4) in Algorithm 1 depends on the data distribution, with a Random-Feature approximation obtained by creating an R-dimensional feature embedding. This approach provides a generalization error bound even as R approaches infinity, unlike the representative-set method. The choice of p(\u03c9) is crucial in the kernel, with \"close to uniform\" choices often outperforming p(\u03c9) = p(x). The choice of p(\u03c9) is crucial in the kernel, with \"close to uniform\" choices often outperforming p(\u03c9) = p(x). Examples from experiments show better performance with random distributions in various domains compared to the Representative-Set Method. The choice of p(\u03c9) is crucial in the kernel, with random distributions yielding better performance than the Representative-Set Method. The selected distributions generate objects capturing relevant semantic information for estimation of f(x) under the dissimilarity measure d(x, \u03c9). The proposed framework analyzes error decomposition in the context of RKHS and random feature approximation. The RKHS corresponding to the kernel is crucial for function approximation, with a smaller function space compared to Lipschitz-continuous functions. The RKHS implied by the kernel in Equation FORMULA12 is a smaller function space than Lipschitz-continuous functions. Any function in the RKHS is Lipschitz-continuous w.r.t. the given distance. Imposing additional smoothness via the RKHS norm constraint and kernel parameter aims to approximate the true function well. The estimation error is defined by eigenvalues of the kernel and a tuning parameter, with a probability bound on the difference between functions. The RKHS provides a better estimation error compared to Lipschitz-continuous functions. The estimation error is defined by eigenvalues of the kernel and a tuning parameter, with a probability bound on the difference between functions. The estimation error has a better dependency on n (i.e. n^-1/2) compared to the k-nearest-neighbor method, especially for higher effective dimension. The analysis of the kernel in Equation FORMULA12 for random feature approximation is challenging due to the lack of an analytic form. The analysis of the kernel in Equation FORMULA12 for random feature approximation is challenging due to the lack of an analytic form. The error from RF approximation can be bounded, and the approximation error of the kernel is analyzed for uniform convergence. Proposition 2 provides an approximation error in terms of kernel evaluation, and the empirical risk minimization solution is considered using the Representer theorem. The text discusses bounding the error in random feature approximation and analyzing the approximation error of the kernel for uniform convergence. Proposition 2 provides an approximation error based on kernel evaluation, and the Representer theorem is used to consider the optimal solution for empirical risk minimization. Corollary 1 states conditions for guaranteeing a certain error bound, with details on Lipschitz-continuous constants and bounds on parameters. The proposed framework aims to achieve near-optimal performance with a certain number of Random Features. The proposed framework aims to achieve near-optimal performance with a certain number of Random Features. It involves bounding the error in random feature approximation and analyzing the approximation error of the kernel for uniform convergence. The framework can achieve -suboptimal performance by estimating the function and comparing it to the desired target function. The method is evaluated in various domains such as time-series, strings, texts, and images using different dissimilarity measures. The study compares different distance-based methods using well-known dissimilarity measures such as Dynamic Time Warping, Edit Distance, Earth Mover's distance, and (Modified) Hausdorff distance. These measures are computationally demanding and C-MEX programs were adapted or implemented for efficiency. Four datasets were selected for experiments in each domain, including multivariate time-series data and IQ samples from a wireless line-of-sight. The study compared distance-based methods using various dissimilarity measures like Dynamic Time Warping, Edit Distance, Earth Mover's distance, and (Modified) Hausdorff distance. Datasets were chosen for time-series, string, text, and image data, with specific characteristics outlined. Baselines included KNN and 4 other state-of-the-art methods. The study used SIFT descriptors to represent images with varying feature vector sizes. Datasets were split into train and test subsets, and compared D2KE against 5 baselines including KNN, DSK_RBF, DSK_ND, KSVM, and RSM. Baselines had different computational complexities. The study compared D2KE against 5 baselines including KNN, DSK_RBF, DSK_ND, KSVM, and RSM, with different computational complexities. D2KE has linear complexity in both the number of data samples and the length of the sequence, outperforming the baselines in terms of computational efficiency. In experiments comparing D2KE to baseline methods, D2KE consistently outperforms or matches them in classification accuracy while requiring less computation time. D2KE performs better than KNN and achieves superior performance compared to DSK_RBF, DSK_ND, and KSVM methods, indicating that a representation induced from a truly PD kernel utilizes data more effectively. Random objects sampled by D2KE outperform RSM in practical construction of feature matrices. Our method outperforms others by utilizing a representation induced from a truly PD kernel effectively. Random objects sampled by D2KE perform significantly better than RSM in constructing feature matrices. The proposed framework derives a positive-definite kernel and feature embedding function from dissimilarity measures for structured input domains. This approach opens up a new direction for creating embeddings based on distance to random objects, with potential extensions to deep architectures for structured inputs. Our framework subsumes existing approaches and introduces a new direction for creating embeddings based on distance to random objects. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous, leading to a bound on the magnitude of Hoefding's inequality. By choosing appropriate parameters, we can achieve the desired result. The text discusses the optimization process for finding the best parameters in different methods using cross-validation. It also mentions the use of specific kernels and the generation of random samples for a new method called D2KE. The text discusses the implementation of different methods for optimization, including the use of specific kernels and random sample generation for the new method D2KE. Various datasets from different domains were used for the experiments, and computations were performed on a DELL system with Intel Xeon processors. The experiments utilized various datasets from different domains on a DELL system with Intel Xeon processors. Multithreading was employed for distance computations, and a Gaussian distribution with parameters \u03c3 was used. D2KE consistently outperformed other methods in classification accuracy for multivariate time-series data, requiring less computation time. Random time series were searched in the ranges [1e-3 1e3] and [2 50]. D2KE outperforms other baselines in classification accuracy and computation time for multivariate time-series. It performs substantially better than KNN, DSK_RBF, DSK_ND, and KSVM, suggesting that a representation induced from a truly p.d. kernel makes better use of the data. RSM is closest in construction of the feature matrix but D2KE performs significantly better. The RSM method constructs feature matrices by choosing a subset of data points, which may suffer from noise or redundant information. In contrast, our method samples short random sequences to denoise and identify patterns in the data. The number of possible random sequences is unlimited, making the feature space more abundant. Additionally, RSM may have high computational costs for long time-series due to its quadratic complexity. Levenshtein distance is chosen as the distance measure for string data to capture global alignments. The Levenshtein distance is used as the distance measure for string data to capture global alignments. D2KE consistently outperforms other distance-based baselines, showing a clear advantage. DSK_RBF performs similarly, possibly due to producing a c.p.d. kernel that can be converted into a p.d. kernel. Our method, D2KE, outperforms baselines with better performance on large datasets and lower computational complexity. For text data, we use the earth mover's distance as the distance measure, which has shown strong performance when combined with KNN for document classifications. In text data analysis, the earth mover's distance is used as a distance measure for document classification, showing superior performance when combined with KNN. D2KE outperforms other baselines on various datasets, showcasing the effectiveness of distance-based kernel methods over KNN in text data analysis. D2KE outperforms other baselines on text data, showing superior performance in document classification. It achieves a significant speedup compared to other methods, thanks to the use of random features. For image data, the modified Hausdorff distance is used as a distance measure between images, with SIFT-descriptors and random images generated for analysis. In image data analysis, D2KE outperforms other baselines by using random features and the modified Hausdorff distance with SIFT-descriptors. It shows superior performance and scalability compared to other methods, making it a strong alternative to KNN and RSM."
}