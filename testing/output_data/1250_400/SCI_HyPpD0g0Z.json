{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features that do not change across domains and \"orthogonal\" features that can vary across domains. The goal is to use the invariant features for classification to guard against future domain shifts. The distributional change of features across different domains is not directly observable due to the latent nature of the domain. In data augmentation, we can generate multiple images from an original image using an ID variable to refer to the relevant original image. This method only requires a small fraction of images to have an ID variable. In data augmentation, images can be generated using an ID variable to refer to the original image. The causal framework introduced adds the ID variable to the model, treating the domain as a latent variable. By grouping samples with the same class and identifier, counterfactuals are created under different style interventions. Regularizing the network with a graph Laplacian improves performance in settings with changing domains. Deep neural networks have excelled in prediction tasks, but issues can arise from dependencies that vanish in test distributions due to domain shifts caused by changing conditions. Deep neural networks have achieved outstanding performance on prediction tasks like visual object and speech recognition. Issues can arise when the learned representations rely on dependencies that vanish in test distributions, leading to domain shifts caused by changing conditions. An example is the \"Russian tank legend\" where a machine learning system was trained to distinguish between Russian and American tanks based on image quality, leading to high accuracy in training but likely to fail in practice. Hidden confounding factors can give rise to indirect associations. The system learned to discriminate between images of different qualities, but hidden confounding factors like image quality and tank origin can lead to indirect associations. Deep learning requires large sample sizes to average out confounding effects and achieve invariance to known factors. Adversarial examples, misclassified by ML models but not humans, highlight differences between human and artificial cognition. Adversarial examples are imperceptibly perturbed inputs that fool ML models but not humans. Humans can achieve invariance to rotations with just one example, unlike DNNs. There is interest in mimicking human ability to learn invariances from few instances and aligning DNN features with human cognition. Biases in training datasets can lead to discriminatory outcomes in ML models. Existing biases in training datasets can lead to discriminatory outcomes in ML models. For example, Google's photo app mistakenly tagged two non-white people as \"gorillas\" due to biased training data. To address this issue, counterfactual regularization (CORE) is proposed to control latent features extracted by estimators. CORE focuses on 'conditionally invariant' core features related to the target of interest, making the estimator robust to adversarial domain shifts. Counterfactual regularization (CORE) aims to ensure that a classifier uses only core features relevant to the target of interest in a stable manner. It makes the estimator robust to adversarial domain shifts by focusing on invariant core features. CORE exploits knowledge about grouping instances related to the same object, reducing the need for data augmentation and improving predictive performance in small sample size settings. The manuscript discusses motivating examples, related work, introduces counterfactual regularization, and evaluates CORE's performance in various experiments, including using the CelebA dataset with face images of celebrities. In \u00a74, counterfactual regularization (CORE) is formally introduced for logistic regression. The CelebA dataset contains face images of celebrities, used for classifying whether a person wears glasses. Grouping information is utilized to ensure the same prediction for all images of the same person. Additional instances of the same person are considered as counterfactual observations. The text discusses the utilization of grouping information in training sets to improve test error rates in machine learning models. By exploiting the group structure, test errors can be significantly reduced compared to pooling all samples. This method, known as counterfactual regularization (CORE), is particularly effective in scenarios like data augmentation. Counterfactual regularization (CORE) is utilized to reduce test errors by exploiting group structure in training sets. This method improves efficiency in data augmentation by creating additional samples through interventions on style features, resulting in invariance of the estimator with respect to transformations. By using grouping information, CORE enforces stronger invariance compared to normal data augmentation, leading to a significant reduction in test error rates. The use of Counterfactual regularization (CORE) in data augmentation enforces invariance with respect to style features, resulting in a significant reduction in test error rates. This approach, which includes augmented samples from the same object, shows improved performance on rotated examples compared to traditional data augmentation methods. Additionally, CORE differs from other approaches like Domain-Adversarial Neural Networks (DANN) by requiring grouped observations instead of relying on unlabeled data from the target task. The BID13 and BID14 models focus on learning representations without discriminative information about input origin. BID14 identifies conditionally independent features by minimizing MMD distance between distributions in different domains. A key difference is that BID14's domain identifier is observable, while it is latent in our approach. Our approach differs from BID14 by using a latent domain identifier and penalizing the classifier for using any latent features. Causal modeling aims to guard against adversarial domain shifts and interventions on predictor variables, providing valid predictions. However, transferring these results to adversarial domain changes in image classification faces challenges due to the anti-causal nature of the classification task and the need to protect against such shifts. The challenges in transferring causal modeling results to adversarial domain changes in image classification include the anti-causal nature of the task and the need to guard against style feature shifts. Various approaches leveraging causal motivations for deep learning have been proposed, but they do not address the specific setting of anti-causal prediction and non-ancestral interventions on style variables. Various approaches focus on cause-effect inference, with the Neural Causation Coefficient (NCC) used to estimate the probability of X causing Y in image features. Generative neural networks are utilized for cause-effect inference, identifying v-structures and orienting graph edges. Regularizers combining penalties with estimated causal probabilities are devised, aiding in causality detection networks. Inference methods involve regularizers combining penalties with estimated causal probabilities for causality detection networks. Causal generative models connect GANs with group theoretic frameworks, while causal implicit generative models sample from conditional and interventional distributions using a Conditional GAN architecture. Deep latent variable models and proxy variables are used to estimate individual treatment effects, and causal reasoning is applied to address fairness considerations in machine learning algorithms. BID21 uses causal reasoning to address fairness in machine learning by deriving nondiscrimination criteria based on protected attributes and proxies. Algorithms avoiding proxy discrimination require classifiers to be constant with proxy variables in the causal graph, similar to distinguishing between core and style features for disentangling factors of variation. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce dependence of latent representation on specified transforms, such as location, in the original image. The goal is to reduce the dependence of the latent representation on a specified transform of the object in the original image. Matsuo et al. (2017) aim to learn a latent representation that excludes certain style features, which could include location, image quality, posture, brightness, background, and contextual information. They address a confounding situation where the distribution of style features differs based on the class, utilizing grouped observations in a variational autoencoder framework to separate style and content. In a classification task, the focus is on directly solving it without explicitly estimating latent factors as in a generative framework. The standard notation for classification is described, along with a causal graph comparing adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. The predictor X is defined as the p pixels of an image, with the prediction y given X = x represented by a function f \u03b8 with parameters \u03b8 \u2208 R d. The prediction in regression or classification tasks is made using a function f \u03b8 with parameters \u03b8 \u2208 R d, where \u03b8 corresponds to the weights in a DNN. The goal is to minimize the expected loss by choosing the weights that minimize the empirical loss. The penalty term could be a ridge penalty or other penalties that exploit underlying geometries. The full structural model for all variables is shown in a diagram. The structural model includes a latent domain variable D and an ID variable that groups observations. The prediction is anti-causal, with the class label Y causing changes in the image X through core and style latent variables. External interventions are possible on style features but not on core features. The class label Y on image X is influenced by core features X ci and style features X \u22a5. Interventions are possible on style features but not on core features. The distribution of X ci |Y is constant across domains, while the distribution of X \u22a5 |Y can change. The style features X \u22a5 and Y are confounded by latent domain D. Core features X ci are conditionally independent of D|Y. Style interventions affect both X \u22a5 and the image X. In this work, interventions can be made on style features, influencing the image prediction. The causal graph is used to explain domain adaptation, transfer learning, and guarding against adversarial examples. The intervention magnitude is typically assumed to be within a certain norm around the origin, causing imperceptible changes in the image. The text discusses interventions on style features in images, aiming to minimize adversarial loss by devising a classification that considers imperceptible changes in the image. It also explores adversarial domain shifts with strong interventions on style features, assuming certain aspects of the image can be changed while others cannot. The text discusses protecting against shifts in test data distributions by distinguishing between core and style features in images. It addresses the challenge of causal inference and the inability to observe counterfactuals in certain scenarios. The text discusses the challenge of causal inference and the impossibility of observing counterfactuals in certain scenarios, such as in medical treatments. Counterfactuals are situations where treatment changes while other variables are constant, making it impossible to observe both outcomes simultaneously. In image analysis, counterfactuals are conceivable as style interventions can be changed while keeping the object constant. In image analysis, style interventions can be used to observe the same object under different conditions, similar to treatments in medical examples. These interventions change variables like background, posture, and image quality, allowing for counterfactual scenarios. The focus is not on the treatment effect but on ruling out parts of the feature space for classification. The goal is to penalize any changes in classification under different style interventions. The text discusses the use of style interventions in image analysis to rule out parts of the feature space for classification. It focuses on penalizing changes in classification under different style interventions, rather than the treatment effect. The pooled estimator treats all examples identically by summing over the loss with a penalty parameter, such as a ridge penalty. The pooled estimator treats all examples identically by summing over the loss with a penalty parameter like a ridge penalty. The adversarial loss of the pooled estimator may be infinite, but it can work well in terms of adversarial loss. Conditions (i) and (ii) ensure that the estimator will perform well in terms of adversarial loss if certain edges are absent. To minimize the adversarial loss, it is important to keep the function as constant as possible. The pooled estimator works well in terms of adversarial loss if certain edges are absent. To minimize the adversarial loss, the function must be kept as constant as possible in the invariant parameter space. The challenge lies in inferring the invariant space from data to approximate the optimal invariant parameter vector using empirical risk minimization. The challenge is inferring the invariant space from data to approximate the optimal invariant parameter vector using empirical risk minimization. The unknown invariant parameters space is approximated by an empirically invariant space, where a regularization constant allows for variations in class labels predictions. The true invariant space is a subset of the empirically invariant subspace, and under certain assumptions, they converge as the sample size increases. The true invariant space is a subset of the empirically invariant subspace. The matrix L ID is a graph Laplacian BID4 with n connectivity components. The graph Laplacian regularization penalizes the sum of variances \u03c3 2 i (\u03b8) and is formed based on the identifier variable ID. The experiments show the importance of defining the graph using the identifier variable ID. The experiments demonstrate the significance of forming a graph based on the identifier variable ID. Regularizations other than graph Laplacian do not effectively guard against adversarial domain shifts. The CORE estimator outperforms the pooled estimator in handling confounded training data sets and changing style. The experiments show that the CORE estimator outperforms the pooled estimator in handling confounded training data sets and changing style features in test distributions. Various experiments were conducted to assess the performance, including classifying elephants and horses based on color, gender, wearing glasses, and brightness. Additional experimental results and implementation details are provided in the sections mentioned. An open question remains on how to set the tuning parameter or penalty in Lagrangian form. The implementation of CORE will be available along with code for reproducing experiments. The tuning parameter \u03bb in Lagrangian form is discussed, showing that performance is not highly sensitive to its choice. Stickmen images are used as an example, with height as a core feature for differentiating between adults and children. There is a dependence between age and movement due to a hidden common cause. The data generating process is illustrated in a figure. The data generating process involves a hidden common cause D, with images of children showing large movements and adults showing small movements. Test sets 2 and 3 intervene on X \u22a5, leading to changes in movement associations. CORE outperforms the pooled estimator with as few as 50 counterfactual observations. The study compares the performance of CORE and the pooled estimator on test sets with different counterfactual observations. CORE shows good predictive performance with as few as 50 counterfactual examples, while the pooled estimator fails. Including more counterfactual examples does not improve the pooled estimator's performance due to bias. The study uses the CelebA dataset to classify images based on whether the person is wearing eyeglasses, considering image quality as a confounding factor. The study uses the CelebA dataset to classify images based on whether the person is wearing eyeglasses, considering image quality as a confounding factor. Counterfactual observations are generated by sampling new image quality values. Different test sets are used to compare the performance of CORE and the pooled estimator, with CORE showing better predictive performance with fewer counterfactual examples. In the study using the CelebA dataset, counterfactual observations are created by altering image quality values. Test sets reveal that the pooled estimator outperforms CORE on test set 1 due to utilizing image quality information. However, the pooled estimator struggles on test sets 2-4, while CORE's predictive performance remains stable despite changing image quality distributions. In contrast to the pooled estimator, CORE's predictive performance is not affected by changing image quality distributions. The study aims to assess if CORE can exclude \"color\" from its learned representation by including counterfactual examples of different colors using the AwA2 dataset. CORE's ability to exclude \"color\" from its learned representation is tested using counterfactual examples of different colors in the AwA2 dataset. The study includes grayscale images for elephants to observe the impact on classification performance. Test sets with modified colorspaces show that the pooled estimator struggles when color information is altered, unlike CORE. The study tests CORE's ability to exclude color information from its representation using counterfactual examples in the AwA2 dataset. The pooled estimator struggles with modified colorspaces in test sets, while CORE shows color invariance. Adding grayscale images of elephants improves recognition in CORE, highlighting its demand for prediction invariance. This analysis hints at potential fairness considerations if color were a protected attribute. The CORE estimator aims for prediction invariance by learning color invariance with added grayscale images. It satisfies fairness by not including color in its representation, unlike the pooled estimator. The CORE estimator distinguishes core and style features in images and uses counterfactual regularization to achieve robustness against interventions on style features. The CORE estimator aims to achieve invariance of classification performance by exploiting instances of the same object in the training data. It can handle sampling biases and unknown style features by penalizing variations between instances of the same object. Larger models like Inception or ResNet can be used with CORE to achieve invariance to style features. The CORE estimator aims to achieve invariance of classification performance by penalizing variations between instances of the same object. Using larger models like Inception or ResNet with CORE can help achieve invariance to style features. Future directions include exploring the benefits of CORE for training Inception-style models and using video data for grouping and counterfactual regularization. Temporal information can be used for grouping and regularization, potentially helping to debias word embeddings. The structural equation for an image X is linear in style features X\u22a5, with logistic regression predicting a class label Y. Interventions act additively on style features X\u22a5, which act linearly on the image X via a matrix W. Core features Xci are conditionally invariant, with logistic regression used for prediction.\u03b8 is estimated from training data with a logistic loss function. The logistic regression model predicts a class label Y from image data X, with interventions acting on style features X\u22a5. The logistic loss function is used for training and testing, with expected losses under standard and adversarial interventions. The assumptions for the formulation rely on conditions such as positive density on training data and full rank matrix W. The formulation of Theorem 1 relies on assumptions including sampling from a distribution for training data, full rank matrix W, and having a sufficient number of counterfactual examples. The sampling process involves collecting independent samples and redrawing values for counterfactual examples. Theorem 1 states that the pooled estimator has infinite adversarial loss under certain assumptions. It also discusses the CORE estimator and its results for misclassification loss. The proof involves showing that W t\u03b8pool = 0 with probability 1 by considering the oracle estimator and the constraint W t \u03b8 = 0. The proof shows that W t\u03b8pool = 0 with probability 1 by considering the oracle estimator and the constraint W t \u03b8 = 0. This implies that the directional derivative of the training loss with respect to any \u03b4 \u2208 R p in the column space of W should vanish at the solution \u03b8 * . The proof demonstrates that the oracle estimator remains the same regardless of the training data used, implying that the directional derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. This is shown by considering the interventions \u2206i,j and their effect on the column space of W in X. The left hand side of the equation has a continuous distribution, leading to the conclusion that the probability of it not being identically 0 is 1. The proof shows that the oracle estimator remains unchanged regardless of the training data used, indicating that the training loss derivative with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. This is demonstrated by considering interventions \u2206i,j and their impact on the column space of W in X, leading to the conclusion that the probability of the left-hand side of the equation not being identically 0 is 1. The proof demonstrates that the oracle estimator remains unchanged regardless of the training data used, showing that the training loss derivative with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. This is proven by considering interventions \u2206i,j and their impact on the column space of W in X, leading to the conclusion that the probability of the equation not being identically 0 is 1. The CelebA dataset is used to classify gender based on images, creating a confounding by including mostly images of men wearing glasses while women's images do not include glasses. The proof shows that the oracle estimator remains unchanged regardless of the training data used, indicating that the training loss derivative with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. The CelebA dataset is utilized to classify gender based on images, introducing a confounding factor by featuring mostly men wearing glasses and women without glasses. Counterfactuals are created by using images of the same person without glasses for males and with glasses for females. Test sets with different gender-glasses associations are used to assess the performance of training a four-layer CNN versus using Inception V3 features and retraining the softmax layer. The results show similar trends as the complexity parameter c increases. In assessing the performance of training a four-layer CNN versus using Inception V3 features and retraining the softmax layer, similar trends are observed as the complexity parameter c increases. The results show that the performance difference between CORE and the pooled estimator becomes smaller as c increases, due to the binary nature of X \u22a5 in this example. The pooled estimator performs worse on test set 2 as m becomes larger, indicating a larger exploitation of X \u22a5 as m grows. The study focuses on classifying whether a person in an image is wearing eyeglasses in a confounded setting involving a hidden common cause of Y and X \u22a5, where D indicates whether the image was taken outdoors or indoors. The study analyzes a confounded setting where the brightness of an image is related to whether the person wears glasses. Different test sets are used to evaluate the performance of the pooled estimator compared to CORE. The pooled estimator performs better on test set 1 by utilizing brightness information, while CORE is restricted from doing so. Test set 2 shows that the pooled estimator performs worse as the exploitation of brightness increases. The study compares the performance of the pooled estimator and CORE on different test sets with varying brightness levels. The pooled estimator outperforms CORE on test set 1 by utilizing brightness information, but struggles on test sets 2 and 4 when brightness distributions differ significantly. Results for different parameters can be found in FIG0.5. Different counterfactual settings are explored, including using a different image of the same person or a different person. The study evaluates different counterfactual settings for improving predictive performance. Counterfactual setting 1 works best as it allows for explicit control over the brightness factor. Setting 2, using different images of the same person, presents challenges in isolating brightness. Grouping images of different persons also shows some improvement in predictive performance. The optimal tuning parameter \u03c4 or penalty \u03bb remains an open question, with misclassification rates shown in FIG0.6. The study explores different counterfactual settings to enhance predictive performance, with a focus on setting the tuning parameter \u03c4 or penalty \u03bb. Results show that even grouping images of different persons can improve performance. Varying the number of identities in the training dataset affects misclassification rates, with CORE showing better performance for smaller sample sizes. As sample sizes increase, the performance of CORE and the pooled estimator become comparable. The study explores how CORE improves predictive performance compared to pooling all images, especially with small sample sizes. As sample sizes increase, the performance of CORE and the pooled estimator become comparable. Results show that CORE has lower misclassification rates on rotated digits, making data augmentation more efficient. The study demonstrates that CORE improves predictive performance compared to pooling all images, especially with small sample sizes. Results show lower misclassification rates on rotated digits, making data augmentation more efficient. The CORE estimator's performance is not sensitive to the number of counterfactual examples, once there are enough in the training set. Counterfactual setting 1 works best, with small differences between settings 2 and 3 in predictive performance. In experiments for counterfactual settings 1-3 and c = 5000, counterfactual setting 1 works best with small differences between settings 2 and 3 in predictive performance. There is a notable performance difference between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator. The models were implemented in TensorFlow, using the same network architecture and training procedure for CORE and the pooled estimator. The Adam optimizer was used for all experiments, with results based on training each model five times to assess variance. The pooled estimator and CORE models use the same network architecture and training procedure, with only the loss function differing due to the counterfactual regularization term. Experimental results are based on training each model five times to assess variance, with mini batches shuffled to ensure inclusion of counterfactual observations. Mini batch size is set to 120, making optimization more challenging for small c values."
}