{
    "title": "HJcjQTJ0W",
    "content": "Massive data on user local platforms pose challenges for deep neural network (DNN) training. Cloud-based training offers benefits but raises privacy concerns. To address this, a method using intermediate representations of data is proposed, splitting DNNs between local platforms and the cloud. Local NN generates feature representations without training to protect data privacy. Cloud NN is trained based on these representations. The idea is validated by studying privacy loss and accuracy dependency on local NN topology for image classification tasks. PrivyNet is proposed to optimize accuracy while maintaining privacy constraints. The idea of splitting DNNs between local platforms and the cloud is validated by studying privacy loss and accuracy dependency on local NN topology for image classification tasks. PrivyNet is proposed to optimize accuracy while maintaining privacy constraints. The efficiency and effectiveness of PrivyNet are demonstrated with the CIFAR-10 dataset. To protect user data privacy during cloud-based deep model training, different data pre-processing schemes are proposed. Transformed representations are generated locally and uploaded for learning tasks, balancing utility and privacy requirements. The transformation scheme should be flexible for different platforms and data types. Privacy and utility trade-off is a key focus in privacy research. Different measures of privacy and utility are proposed based on rate-distortion theory, statistical estimation, and learnability. Various transformations have been suggested to explore the trade-off between privacy and utility. Syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness are used to protect sensitive attributes in static databases. However, applying syntactic anonymization to high-dimensional continuous data is challenging. Differential privacy, achieved by adding noise, offers a more formal privacy guarantee. Differential privacy is proposed as a formal privacy guarantee for high-dimensional continuous data, but it has limitations in preventing total information leakage. Existing methods for achieving differential privacy often require local platforms for deployment, making it challenging for lightweight platforms. Non-invertible linear and nonlinear transformations are also suggested for data anonymization, with nonlinear transformations offering better privacy protection. PrivyNet is a DNN training framework that offers fine-grained control over the trade-off between privacy and utility. It divides the DNN model into two parts, deploying them separately on local platforms and the cloud. The local NN generates intermediate representations, while the cloud NN is trained for the target learning task. This framework allows for better privacy protection compared to linear transformations, with control over privacy and utility trade-off. PrivyNet is a DNN training framework that divides the model into local and cloud parts for better privacy protection. The local NN generates intermediate representations, while the cloud NN is trained for the learning task. Privacy is achieved through non-linear transformations like convolution and pooling. The local NN is derived from pre-trained NNs to balance utility and privacy by controlling specific features to release. PrivyNet is a framework that splits DNN models for cloud-based training with privacy control. It uses pre-trained NNs to extract useful features while protecting privacy. The framework characterizes privacy loss and utility, identifies key factors for trade-off, and proposes a hierarchical strategy to optimize utility. PrivyNet is validated using CNN-based image classification, demonstrating efficiency and effectiveness in leveraging pre-trained NNs for intermediate representation generation. PrivyNet utilizes pre-trained NNs for feature extraction in CNN-based image classification, ensuring efficiency and privacy. The framework measures utility through task accuracy and privacy by comparing reconstructed images to originals. The image classification network is trained on feature representations, while the image reconstruction network reconstructs original images. The approach aligns with an adversarial model for privacy control. The utility of PrivyNet is measured by task accuracy and privacy is assessed by comparing reconstructed images to originals. The IRN is trained on feature representations with unknown transformations, aligned with an adversarial model for privacy control. The utility of PrivyNet is evaluated based on task accuracy, achieved by a classifier, while privacy is assessed by comparing reconstructed images to originals using a reconstruction model. The IRN is trained on feature representations with unknown transformations for privacy control. The privacy of transformed representations is evaluated using a reconstruction model that minimizes distance between reconstructed and original images. Privacy loss is measured by peak signal-to-noise ratio (PSNR), with larger PSNR indicating higher privacy loss. The impact of FEN topology on privacy and utility is characterized, with settings described in detail. The FEN is derived from VGG16 pre-trained on Imagenet, using CNN for image classification and reconstruction tasks. Architecture details are provided in the appendix. The FEN topology, derived from VGG16 pre-trained on Imagenet, is evaluated for its impact on privacy and utility. Changes in the number of FEN layers and output depth affect privacy loss and accuracy differently. Privacy loss is measured using peak signal-to-noise ratio (PSNR), with smaller PSNR indicating less privacy loss. The behavior of utility and privacy is compared and plotted in FIG2. The impact of changes in the number of FEN layers and output depth on privacy and utility is analyzed. Privacy loss is indicated by smaller PSNR, showing less privacy loss with either reduced output depth or increased FEN layers. The trade-off between accuracy and PSNR is shown in FIG2 (c), highlighting that FEN with different topologies have similar utility when privacy loss is high, and FEN with more layers provide better utility when privacy loss is low. The impact of FEN layers and output depth on privacy and utility is analyzed. FEN with different topologies show similar utility when privacy loss is high, while more layers provide better utility when privacy loss is low. Channel selection also affects privacy and utility, with significant differences in performance observed between channels. The impact of FEN layers and output depth on privacy and utility is analyzed, showing that utility and privacy are more dependent on the number of FEN layers and output channel depth compared to output channel selection. Large discrepancies in utility and privacy are observed between different FEN layer configurations and output depths. The impact of FEN layers and output depth on privacy and utility is analyzed, showing that utility and privacy are more dependent on the number of FEN layers and output channel depth. The privacy and utility of representations generated from pre-trained CNN-based transformation are characterized, revealing the trade-off between utility and privacy controlled by FEN topology. The framework PrivyNet is proposed to optimize utility under privacy constraints, considering factors like FEN layers, output depth, and channel selection. In the next section, the framework PrivyNet is proposed to optimize utility under privacy constraints by determining the FEN topology. This involves considering factors like FEN layers, output depth, and channel selection. The framework involves privacy characterization using cloud-based services and performance profiling of different NNs on local platforms to achieve this optimization. The PrivyNet framework optimizes utility under privacy constraints by determining the FEN topology through privacy characterization and performance profiling. Channel pruning based on private data is conducted to select effective channels, and the FEN topology is determined by randomly selecting output channels. The assumption of availability of original images is crucial for worst-case evaluation of privacy loss. The assumption of availability of original images is necessary for worst-case evaluation of privacy loss in the PrivyNet framework. It enables a worst-case evaluation on privacy loss induced by releasing feature representations, while also assuming the transformation induced by the FEN is unknown to attackers to limit privacy loss. Anonymity protection for the FEN is crucial as attackers may have access to the architecture and weights of pre-trained NNs. The pre-characterization stage involves performance/storage profiling on local platforms. The pre-characterization stage involves performance and storage profiling on local platforms and cloud-based privacy characterization for the pre-trained NNs to protect the anonymity of the FEN. Performance and storage profiling is important due to different computation capabilities and storage configurations of platforms, which directly impact the FEN's topology. Privacy characterization for the pre-trained NNs is done by leveraging cloud-based services and training the reconstruction network on publicly available data. Verification of this characterization is done on different datasets like CIFAR-10. The reconstruction network in NNs is trained on publicly available data of the same dimension and distribution. Privacy characterization is done for different datasets like CIFAR-10 and CIFAR-100, with similar PSNR results for FEN with different topologies. Less than 1000 samples are needed for an accurate characterization with data augmentation. The number of FEN layers and output channel depth have significant impacts on privacy and accuracy. In PrivyNet, the topology for the FEN is determined based on privacy and accuracy considerations. The number of FEN layers and output channel depth are crucial factors. When privacy requirements are high, a FEN with more layers is selected, while output depth is determined by privacy constraints. Conversely, when privacy requirements are low, a larger PSNR is aimed for. The strategy is based on constraints on computation, storage, and privacy loss. When privacy requirements are high, a deep FEN with more layers is chosen, and output depth is determined by privacy constraints. Conversely, for low privacy requirements, a shallow FEN is selected to achieve the desired privacy level, with output channel depth adjusted accordingly. This approach minimizes local computation and storage consumption. When privacy requirements are high, a deep FEN with more layers is chosen, and output depth is determined by privacy constraints. Conversely, for low privacy requirements, a shallow FEN is selected to achieve the desired privacy level, with output channel depth adjusted accordingly. This approach minimizes local computation and storage consumption. Output channel selection is crucial to balance utility and privacy, as shown in Figures 4 and 9. Channel pruning is necessary to avoid poor utility with high privacy leakage. Utility and privacy loss for a single channel are not correlated, as demonstrated in Figure 10. In optimizing utility and privacy in channel pruning, it is important to consider the correlation between them. Figures 4 and 9 show the necessity of channel pruning to avoid poor utility with high privacy leakage. The correlation between utility and privacy loss for a single channel is negligible, as demonstrated in Figure 10. This observation allows for the optimization of utility while suppressing privacy loss simultaneously. Fisher's linear discriminability analysis is used to identify channels with the worst utility, while privacy loss for each channel is determined from offline pre-characterization. In optimizing utility and privacy in channel pruning, Fisher's linear discriminability analysis is used to identify channels with the worst utility. Privacy loss for each channel can be acquired from offline pre-characterization, helping to prune channels with the largest privacy loss. Fisher's LDA measures the distance of representations generated by channels for images within the same class, leveraging the covariance matrix. This analysis is effective in identifying ineffective channels. In optimizing utility and privacy in channel pruning, Fisher's linear discriminability analysis is used to identify channels with the worst utility. The analysis measures the distance of representations generated by channels for images within the same class, leveraging the covariance matrix. This helps in pruning channels with the largest privacy loss, improving accuracy for the learning task. The effectiveness of the LDA-based supervised channel pruning algorithm is verified by determining the channels with the worst utility and pruning them to improve accuracy. Using Fisher's discriminability, 69.7% of the worst 32 channels can be pruned on average, compared to 50.3% with random pruning. This results in a 33.5% reduction in the probability of selecting a bad channel randomly. The number of samples required for LDA-based pruning is also explored. The LDA-based supervised channel pruning algorithm effectively prunes channels with the worst utility, resulting in a 33.5% reduction in selecting bad channels randomly. The number of samples needed for pruning is explored, showing minimal computational complexity. The effectiveness of supervised channel pruning is demonstrated by comparing it to random selection within the output channels. The study demonstrates the effectiveness of supervised channel pruning by comparing it to random selection within output channels. Results show that pruning channels with the worst utility and largest privacy loss leads to better utility and less privacy leakage. The study compares supervised channel pruning to random selection within output channels, showing that pruning channels with the worst utility and largest privacy loss results in better utility and less privacy leakage. LDA-based pruning strategy achieves 1.1% better accuracy and 1.25 dB smaller PSNR compared to random selection without pruning. The method also achieves similar accuracy with slightly less privacy loss compared to characterization-based pruning. The effectiveness of the supervised pruning strategy is verified through detailed statistics and comparisons in Table 13. In this section, the paper discusses the adversarial model adopted, focusing on protecting the anonymity of the Feature Extraction Network (FEN). Strategies include building a pool of pre-trained NNs for FEN derivation, making it harder for attackers to guess the FEN's structure. In our framework, we consider methods for protecting the anonymity of the Feature Extraction Network (FEN). This includes building a pool of pre-trained NNs like VGG16, VGG19, ResNet, and Inception. By enlarging the pool, it becomes harder for attackers to guess the FEN's derivation. Additionally, applying channel selection to output and intermediate channels makes it difficult for attackers to determine the channels forming the FEN. The privacy and utility trade-off is empirically verified using VGG16 layers and CIFAR-10 dataset. In our framework, we focus on protecting the anonymity of the Feature Extraction Network (FEN) by enlarging the pool of pre-trained NNs like VGG16, VGG19, ResNet, and Inception. Channel selection for output and intermediate layers makes it challenging for attackers to determine the FEN's channels. Empirical verification using VGG16 layers and CIFAR-10 dataset shows minimal impact on privacy and utility when reducing channel depth. The anonymity of the FEN is well protected, making it difficult for attackers to determine the number of layers and channels. PrivyNet is a flexible framework designed to protect the anonymity of the Feature Extraction Network (FEN) by using pre-trained NNs like VGG16, VGG19, ResNet, and Inception. It enables cloud-based training with fine-grained privacy protection, making it difficult for attackers to determine the number of layers and channels in the FEN. This framework can be beneficial for modern hospitals to train models for disease diagnosis while protecting patient privacy. It also simplifies DNN training for pervasive mobile platforms. PrivyNet provides a framework for hospitals to release informative features instead of original data, simplifying DNN training. It can also enable mobile platforms to upload data to the cloud while protecting privacy. The platform is simple, platform-aware, and flexible for privacy/utility control. The datasets used include CIFAR-10 and CIFAR-100. The CIFAR-10 dataset contains 60000 32 \u00d7 32 color images in 10 classes, split into 50000 training images and 10000 test images. The CIFAR-100 dataset has images of 100 classes, with 600 images per class. VGG16, pre-trained on ImageNet, is used for privacy and accuracy characterization. CNN is used for image classification (h) and image reconstruction (g), with a state-of-the-art generative NN architecture based on ResNet blocks for g. In the experiments, a state-of-the-art generative NN architecture based on ResNet blocks is used for image reconstruction. The learning rates and mini-batch sizes for image reconstruction and classification tasks are specified, along with data augmentation techniques. The IRN's topology is determined before characterization to ensure its capability. In the experiments, a state-of-the-art generative NN architecture based on ResNet blocks is used for image reconstruction. The learning rate is adjusted during training, and data augmentation techniques are applied. The topology of the IRN is determined to ensure image recovery capability. Performance and storage characterization are conducted by profiling pre-trained NNs on local platforms. In experiments, a ResNet-based generative NN architecture is used for image reconstruction. Performance and storage characterization are conducted by profiling pre-trained NNs on local platforms, showing the implications of PSNR values and the impact of VGG16 layers on computation and storage requirements. The bottleneck may vary for platforms with different configurations, highlighting the need for a flexible framework. The complexity of the second part of the computation is determined by the number of samples N LDA and the dimension of the output representations W \u00d7 H. The complexity for S b is O(KW 2 H 2) and for S w is O(N LDA W 2 H 2). Computing W \u22121 and the largest eigenvalue of W \u22121 B, both W \u00d7 H in size, has a complexity of O(W 3 H 3). Overall, the complexity is O((K + N LDA)W 2 H 2 + W 3 H 3), with N LDA being a key factor in determining extra computation. The complexity of the second part of the computation is O((K + N LDA)W 2 H 2 + W 3 H 3). N LDA is a key factor determining extra computation, with small N LDA leading to minimal overhead."
}