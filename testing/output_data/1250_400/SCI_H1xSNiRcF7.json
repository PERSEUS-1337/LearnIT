{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. A novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes, improving optimization robustness. In this work, a novel hierarchical embedding model is introduced, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions. The approach improves optimization robustness in the disjoint case and demonstrates enhanced performance on various tasks. Embedding methods have been crucial in machine learning, converting semantic problems into geometric ones, with recent advancements like Word2Vec leading to a renaissance in the field. Embeddings in machine learning have evolved from simple vector space models to more complex geometric structures like Gaussian embeddings, order embeddings, and box embeddings. These geometric objects better capture asymmetry, entailment, ordering, and transitive relations, providing a strong inductive bias for various tasks. The focus is on the probabilistic Box Lattice model for its empirical performance in modeling transitive relations. The Box Lattice model of BID22 focuses on probabilistic interpretation and modeling complex joint probability distributions, using box embeddings to represent overlapping boxes instead of vector lattice ordering. However, the \"hard edges\" of boxes pose challenges for gradient-based optimization, especially with sparse data where correcting overlap discrepancies is crucial. The Box Lattice model of BID22 uses box embeddings to represent overlapping boxes for complex joint probability distributions. However, the \"hard edges\" of boxes create challenges for gradient-based optimization, especially with sparse data where correcting overlap discrepancies is crucial. To address this issue, a new model is proposed that relaxes the hard edges of boxes into smoothed density functions using Gaussian convolution, showing superior results in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. The new model relaxes hard edges of boxes into smoothed density functions using Gaussian convolution, showing superior results in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. It extends the Box Lattice model of BID22 and improves upon existing state-of-the-art results. The generalization of order embeddings, also known as box embeddings, was proposed by BID18. This model differs from previous models in its interpretation and approach to learning hierarchical embeddings. Methods based on embedding points in hyperbolic space have also been introduced, optimizing an energy function. The negative curvature of hyperbolic space is beneficial for learning tree structures but less suitable for non-treelike DAGs. Gaussian convolution is used to smooth the energy landscape of the model, a common technique in optimization methods. Our approach involves smoothing the energy landscape of the model using Gaussian convolution, a technique common in optimization methods. We focus on embedding orderings and transitive relations, seeking to learn an embedding model that maps concepts to subsets of event space, with an inductive bias suited for transitive relations and fuzzy concepts of inclusion and entailment. This probabilistic approach differs from traditional methods in knowledge graph embedding. The text discusses representing ontologies as geometric objects using order theory, vector and box lattices. It explains posets as non-strict partially ordered sets and lattices as posets with unique least upper and greatest lower bounds. Bounded lattices have additional elements for these bounds and binary operations for join and meet. A bounded lattice in order theory contains additional elements for least upper bound and greatest lower bound, with binary operations for join and meet. Special cases include extended real numbers and sets partially ordered by inclusion. The dual lattice can be formed by swapping operations and reversing the poset relation. A semilattice has either a meet or join. In the context of the paper, \u2227 and \u2228 may represent min and max of real numbers for clarity. In the context of lattice theory, a vector lattice, also known as a Riesz space, is a vector space with a lattice structure. The vector lattice R^n uses a product order from real numbers for its partial order, with meet and join operations being pointwise min and max. Order Embeddings of BID20 represent partial orders as vectors using the dual lattice, with objects becoming more specific as they move away from the origin. Order Embeddings of BID20 embed partial orders as vectors using the reverse product order, with objects becoming more specific as they move away from the origin. Vilnis et al. introduced a box lattice for knowledge graphs, where each concept is associated with two vectors representing the minimum and maximum coordinates of an axis-aligned hyperrectangle. The box lattice structure is defined by the set inclusion between boxes, creating a natural partial order and lattice structure. The box lattice structure involves representing a box x with maximum and minimum intervals at each coordinate. The lattice structure uses max and min operations on scalar coordinates, with meet as the largest common box and join as the smallest containing both x and y. Marginal probabilities are associated with the volume of boxes under a probability measure, where the probability of an event x is calculated based on the interval boundaries of the associated box. The use of a uniform measure constrains boxes to the unit hypercube, ensuring probabilities are \u2264 1. When using gradient-based optimization to learn box embeddings, an issue arises when two concepts are incorrectly labeled as disjoint, resulting in zero gradient signal flow. This problem is exacerbated in sparse lattices where most boxes have little to no intersection, making recovery difficult with the naive measure. The authors propose a surrogate function to optimize in cases of disjoint intervals in sparse lattices, aiming to improve optimization and model quality by avoiding gradient sparsity. They suggest a more principled framework to develop alternate measures that address this issue, demonstrating the concept with a one-dimensional example. The authors propose a relaxation of the standard box embeddings to address gradient sparsity issues. They suggest using kernel smoothing with Gaussian kernels to replace indicator functions, improving optimization and preserving geometric intuition. The authors propose using kernel smoothing with Gaussian kernels to replace indicator functions in the embeddings, improving optimization and preserving geometric intuition. The solution involves convolution with a normalized Gaussian kernel, leading to a closed form solution for evaluating lattice elements. The authors suggest using kernel smoothing with Gaussian kernels to replace indicator functions in embeddings, improving optimization and preserving geometric intuition. The formula for the solution involves convolution with a normalized Gaussian kernel, leading to a closed form solution for evaluating lattice elements. The formula for the solution involves convolution with a normalized Gaussian kernel, leading to a closed form solution for evaluating lattice elements. The antiderivative of the logistic sigmoid is used in the formula, and the consequences of treating the outputs as probabilities are discussed. The curr_chunk discusses the limitations of using a meet operation on a function lattice, particularly in relation to probabilities and optimization properties. It also explores equations involving the hinge function and intervals, highlighting differences between the hinge function and the softplus function. In the zero-temperature limit, equations 3 and 7 are equivalent, but equation 7 is idempotent for overlapping intervals. This leads to defining probabilities using equation 7 for normalization. Softplus function can output values greater than 1 and needs normalization in experiments with a small number of entities. Softplus upper-bounds the hinge function, capable of outputting values greater than 1, requiring normalization in experiments with a small number of entities. Two approaches are used for normalization: unconstrained learning for small entity experiments and projection onto the unit hypercube for data where computing values repeatedly is infeasible. The final probability is calculated by the product over dimensions. This approach retains the inductive bias of the original box model, is equivalent in the limit, and satisfies the necessary conditions. The approach retains the inductive bias of the original box model, equivalent in the limit, and satisfies necessary conditions. A comparison of different functions is shown in FIG2, with the softplus overlap performing better for disjoint boxes. Experiments on the WordNet hypernym prediction task were conducted, with details in Appendix D.1. The hypernym hierarchy in WordNet contains 837,888 edges after transitive closure. Positive examples are randomly chosen from these edges, while negative examples involve swapping terms with random words. Experimental details are provided in Appendix D.1. The smoothed box model performs almost as well as the original box lattice in terms of test accuracy. Further experiments are conducted on the WordNet mammal subset to compare different models. In further experiments on the WordNet mammal subset, different numbers of positive and negative examples were used to compare the box lattice, a smoothed approach, and order embeddings (OE) as a baseline. The training data consists of the transitive reduction of the mammal WordNet subset, while the dev/test data is the transitive closure of the training data. The performance of the models varied based on the level of label imbalance, with the smoothed box model consistently outperforming OE and the original box model. This superior performance on imbalanced data is crucial for real-world entailment graph learning scenarios. In experiments on the WordNet mammal subset, different numbers of positive and negative examples were used to compare the box lattice, a smoothed approach, and order embeddings. The smoothed model consistently outperformed the baseline on imbalanced data, which is crucial for real-world entailment graph learning. The experiments were conducted on the Flickr entailment dataset, showing slight performance gains, especially on unseen captions. In experiments on the WordNet mammal subset, different numbers of positive and negative examples were used to compare the box lattice, a smoothed approach, and order embeddings. The smoothed model consistently outperformed the baseline on imbalanced data. The experiments were conducted on the Flickr entailment dataset, showing slight performance gains, especially on unseen captions. Applying the method to a market-basket task using the MovieLens dataset, the task is to predict users' preference for movie A given that they liked movie B. The conditional probability P (A|B) was calculated and compared with several baselines. Separate embeddings for target and conditioned movies were used due to the asymmetric training matrix. The experimental results show that the smoothed box embedding method outperforms the original box lattice and other baselines, especially in Spearman correlation, which is crucial for recommendation tasks. The model is easier to train due to fewer hyperparameters, and a study on its robustness to initialization conditions is presented in the appendix. The smoothed box embedding method outperforms the original box lattice and other baselines in recommendation tasks, especially in Spearman correlation. It is easier to train with fewer hyperparameters and shows robustness to poor initialization. The model is effective with sparse data and tackles learning problems in geometrically-inspired embedding models. Further research is needed as embedding structures become more complex. The text discusses exploring function lattices and constraint-based approaches in learning, specifically evaluating a Gaussian overlap formula for lattice elements. The MovieLens dataset is highlighted for optimization by a smoothed model due to its distribution of probabilities. The text explores the robustness of a smoothed box model on the MovieLens dataset by adjusting the initialization to control the proportion of disjoint boxes before learning. The study investigates the impact of adjusting the initialization of boxes on the MovieLens dataset. Results show that the smoothed box model performs well even with disjoint initialization, unlike the original box model. Detailed methodology and hyperparameter selection methods are provided for each experiment. Further information and code to replicate experiments can be found at https://github.com/Lorraine333/smoothed_box_embedding. The experimental setup for the WordNet experiments involves evaluating the model every epoch on the development set for a large fixed number of epochs. Baseline models are trained using BID22 parameters, while the smoothed model uses hyperparameters determined on the development set. Negative examples are randomly generated based on the ratio for each batch of positive examples. The model architecture includes a single-layer LSTM that reads captions and produces a box embedding parameterized by min and delta, with embeddings produced by feedforward networks. Hyperparameters are determined on the development set, and the best development model is used to report the test set score. The LSTM model reads captions to generate box embeddings using feedforward networks. It is trained for a fixed number of epochs and evaluated on the development set. Hyperparameters are determined on the development set, and the best model is used to score the test set. Evaluation occurs every 50 steps on the development set, with optimization stopping if no improvement is seen after 200 steps."
}