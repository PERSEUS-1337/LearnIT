{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity and mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators can lead to overfitting, while weak discriminators may not detect mode collapse. In this paper, it is shown that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by designing discriminators with strong distinguishing power against specific generator classes. The Integral Probability Metric induced by these discriminators can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution is close to the true distribution and does not drop modes. Preliminary experiments suggest that the lack of diversity in GANs may be due to optimization sub-optimality rather than statistical inefficiency. Recent work has highlighted concerns that distributions learned by GANs may suffer from mode collapse or lack of diversity, missing significant modes. Various strategies have been proposed to improve the quality and stability of GAN training, but understanding of GANs is still evolving. Preliminary experiments suggest that the lack of diversity in GANs may be attributed to optimization sub-optimality rather than statistical inefficiency. Recent work has highlighted concerns about mode collapse and lack of diversity in GANs. The paper suggests that designing discriminators with strong distinguishing power can alleviate mode collapse. The focus is on the Wasserstein GAN formulation, specifically the F-Integral Probability Metric between distributions. The goal is to learn the data distribution by solving a specific optimization problem. The F-Integral Probability Metric (F-IPM) is used in Wasserstein GANs to learn data distribution by optimizing an objective function. Mode collapse, where the generated distribution lacks diversity, is a key concern in GANs due to the weaker nature of IPM compared to the Wasserstein-1 distance. The main concern with GANs is \"mode collapse,\" where the learned distribution generates high-quality but low-diversity examples. The issue arises from the weaker nature of IPM compared to W 1 distance, leading to a mode-dropped distribution that can fool the former. Increasing the strength of the discriminator to larger families like all 1-Lipschitz functions is a proposed solution, but the Wasserstein-1 distance lacks good generalization properties. The Wasserstein-1 distance lacks generalization properties, making it difficult to establish GAN theories. Powerful discriminators lead to overfitting, while weak discriminators result in diversity issues. This paper proposes a solution by designing a strong discriminator class against a specific generator class. The paper proposes a solution by designing a strong discriminator class F that can approximate the Wasserstein distance for the data distribution p and any q \u2208 G. The focus is on discriminators with restricted approximability w.r.t. a generator class G and the data distribution p. The paper introduces a discriminator class F with restricted approximability to address mode collapse and transition from population-level to empirical-level guarantees in approximating the Wasserstein distance for data distribution p and any q \u2208 G. The paper introduces a theoretical framework for Wasserstein GANs that addresses mode collapse and transitions from population-level to empirical-level guarantees. It develops techniques for designing a discriminator class F with restricted approximability to ensure statistical properties with polynomial samples. The paper presents a theoretical framework for Wasserstein GANs, focusing on designing a discriminator class F with restricted approximability for various generator classes. Techniques are developed for ensuring statistical properties with polynomial samples, including diversity guarantees for distributions like Gaussians and exponential families. Special neural network discriminators are studied for distributions generated by invertible neural networks. In Section 3, one-layer neural networks with ReLU activations are sufficient for Gaussians, while linear combinations of sufficient statistics work for exponential families. Section 4 explores distributions from invertible neural networks, showing that a specific discriminator class guarantees restricted approximability. The invertibility assumption limits distributions to the entire space, which may not align with the low-dimensional manifold of natural images. The distribution of natural images is often believed to reside on a low-dimensional manifold, which contradicts the invertibility assumption that limits distributions to the entire space. The KL-divergence is not suitable for measuring statistical distance when both distributions have low-dimensional supports. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning such distributions. The main proof technique involves developing tools to approximate the log-density of a smoothed neural network generator. The advantage of GANs over MLE in learning distributions with low-dimensional supports is demonstrated by approximating the log-density of a neural network generator. The IPM correlates with Wasserstein distance for low-dimensional distributions and with KL-divergence for invertible generator families. The test IPM could be an alternative for measuring diversity and quality in more complex settings where KL-divergence or Wasserstein distance are not measurable. The lack of diversity in GANs may be due to sub-optimal optimization rather than statistical inefficiency. Various tests have been developed to measure diversity, memorization, and generalization in GANs. These tests indicate that while memorization is not an issue, lack of diversity is common. The lack of diversity in GANs is a common issue, with various proposed solutions such as interpolation between images, semantic combination of images, and classification tests. Mode collapse is a theoretical problem formalized by Arora et al., with different architectures and algorithms proposed to address it. Feizi et al. showed guarantees of training GANs with quadratic discriminators, but there are no provable solutions in more general cases. Zhang et al.'s work demonstrates the importance of using the IPM as a proper metric in GAN training. The curr_chunk discusses statistical guarantees in Wasserstein distance for distributions like injective neural network generators, emphasizing the importance of proper density. Liang (2017) explores GANs in a non-parametric setup, highlighting the relationship between sample complexity and generator smoothness. The rate of learning GANs is non-parametric-exponential in dimension unless the Fourier spectrum decays rapidly, which may not be practical. The sample complexity for learning GANs improves with the smoothness of the generator family, but the rate of learning is non-parametric unless the Fourier spectrum decays rapidly. Invertible generator structures like in Flow-GAN show that GAN training can lead to high KL divergence on real datasets. Successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. If the data can be generated by an injective neural network, the closeness between the learned distribution and the true distribution can be bounded in Wasserstein distance. The theory suggests that closeness between learned and true distributions can be bounded in Wasserstein distance if data can be generated by an injective neural network. IPM includes statistical distances like TV and Wasserstein-1 distance. KL divergence and Wasserstein-2 distance are also important measures between distributions. The Wasserstein-2 distance is defined as \u03a0, the set of couplings of distributions with finite second moments. The Rademacher complexity of a function class is defined, and the training IPM loss for Wasserstein GAN is discussed. Generalization of the IPM is governed by the Rademacher complexity, as stated in Theorem 2.1. Notations for Gaussian distributions and restricted approximability of discriminators for simple parameterized distributions are also mentioned. The curr_chunk discusses the design of discriminators with restricted approximability for simple parameterized distributions like Gaussian distributions. It proves that one-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees. The set of Gaussian distributions with bounded mean and well-conditioned covariance is considered, and the IPM induced by certain discriminators has restricted approximability. The lower and upper bounds differ by a factor of 1/ \u221a d. The curr_chunk discusses the restricted approximability of discriminators for Gaussian distributions and extends the concept to mixture of Gaussians and exponential families. It shows that one-layer neural networks can distinguish Gaussian distributions with restricted approximability guarantees, with bounds differing by a factor of 1/ \u221a d. The proof for the results is deferred to specific sections. The curr_chunk discusses exponential families and their discriminators, showing that linear combinations of sufficient statistics form a family of discriminators with restricted approximability. Theorem 3.2 states conditions for the log partition function, diameter of X, and Lipschitz continuity of T(x). The Rademacher complexity bound is also discussed, with geometric assumptions on sufficient statistics for the Wasserstein distance. In Section 4, the text discusses designing discriminators with restricted approximability for neural net generators in GANs. It covers invertible neural networks generators with proper densities in Section 4.1 and extends the results to injective neural networks generators in Section 4.2, where latent variables can have lower dimensions than observable dimensions. In Section 4.2, the text extends the results to injective neural networks generators, allowing latent variables with lower dimensions than observable dimensions. The generators are parameterized by invertible neural networks, with the ability to model data around a \"k-dimensional manifold\" with some noise. The text discusses the use of invertible neural networks to model data around a \"k-dimensional manifold\" with some noise. The neural networks are parameterized by parameters belonging to a specific set, and the activation function is assumed to be twice-differentiable. The standard deviation of the hidden factors is constrained within a certain range. It is noted that the neural net is invertible, and its inverse is also a feedforward neural net with a specific activation function. The text discusses the use of invertible neural networks to model data on a k-dimensional manifold with noise. A smoothed version of Leaky ReLU is mentioned as satisfying all conditions on activation functions. Assumptions are necessary on generator networks to prevent implementation of pseudo-random functions. The function log p \u03b8 can be computed by a neural network with specific parameters and activation functions. The family of neural networks with certain activation functions contains all functions log p \u2212 log q. The exact form of the parameterized family is not crucial in practice. The text discusses the use of invertible neural networks to model data on a k-dimensional manifold with noise. The exact form of the parameterized family is not crucial in practice. The proof involves the change-of-variable formula and the observation that the log-det of the Jacobian involves computing the determinant of the weight matrices. Theorem 4.2 states that the discriminator class F has restrictions on invertible-generator distributions. The text discusses the use of invertible neural networks to model data on a k-dimensional manifold with noise. The proof involves the change-of-variable formula and the observation that the log-det of the Jacobian involves computing the determinant of the weight matrices. Theorem 4.2 states that the discriminator class F has restrictions on invertible-generator distributions, with a focus on the KL divergence and the IPM. The proof sketch of Theorem 4.2 involves choosing the discriminator class as in Lemma 4.1, bounding the quantity by the Wasserstein distance, and addressing Lipschitz constraints in the generator functions. The upper bound can be achieved through truncation arguments or by requiring the Lipschitz constant to grow. The upper bound in the proof involves addressing Lipschitz constraints in the generator functions. Two workarounds are presented - truncation argument for a W1 bound or a W2 bound with linear growth in Lipschitz constant. The training success with small expected IPM implies closeness of estimated distribution q to true distribution p in Wasserstein distance. In this section, injective neural network generators are considered for generating distributions on a low dimensional manifold. A novel divergence between distributions is designed, sandwiched by Wasserstein distance, and optimized as IPM. The neural nets generate distributions on a k-dimensional manifold in R^d, with G being the family of distributions. The key idea is to design a variant of the IPM for efficient algorithms to achieve a small training error. In this section, a novel divergence between distributions is designed, sandwiched by Wasserstein distance, and optimized as IPM. The key idea is to design a variant of the IPM for efficient algorithms to achieve a small training error. The smoothed F-IPM between distributions p and q is defined, with a generalization bound provided for certain discriminator classes. Theorem 4.5 introduces a discriminator class F for distributions p and q in G, ensuring small discrepancies lead to small Wasserstein distances, preventing mode collapse in neural network generators. Theoretical results suggest mode collapse is avoided when the discriminator family F restricts approximability to the generator family G. Specific discriminator classes are designed to guarantee this, with synthetic experiments confirming the theory's consistency in practice. The Wasserstein distance W1 (p, q) is discussed in relation to restricted approximability in GAN training. Specific discriminator classes are designed to ensure small discrepancies lead to small Wasserstein distances, with synthetic experiments confirming this theory in practice. The experiments show that IPM is correlated with the Wasserstein / KL divergence, suggesting that optimization difficulty rather than statistical inefficiency may be the main challenge in GAN training. The experiments demonstrate good statistical behaviors on \"typical\" discriminator classes, showing correlation between IPM and Wasserstein distance. Synthetic experiments with WGANs successfully learn various curves in two dimensions, showcasing the ability to learn distributions supported on one-dimensional manifolds using the Wasserstein distance. The experiments show that WGANs can effectively learn distributions supported on one-dimensional manifolds in two dimensions using the Wasserstein distance. Standard neural network architectures are used for the generator and discriminator, with the RMSProp optimizer and specific learning rates. Comparisons are made between different metrics in the study. The generator and discriminator classes have specific architectures and optimizer settings. Two metrics, neural net IPM and Wasserstein distance, are compared between the ground truth and learned distributions. Results show close alignment between the generator and ground truth distribution at iteration 10000. The study presents sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence. The analysis technique involves designing discriminators tailored to the generator class for better generalization and mode collapse avoidance. The hope is to extend these techniques to other distribution families with tighter sample complexity bounds in the future. The analysis technique involves designing discriminators tailored to the generator class for better generalization and mode collapse avoidance. The hope is to extend these techniques to other distribution families with tighter sample complexity bounds in the future. The upper bound W F (p 1 , p 2 ) \u2264 W 1 (p 1 , p 2 ) follows directly from the fact that functions in F are 1-Lipschitz. The discriminator family is designed for better generalization and mode collapse avoidance. The upper bound W F (p 1 , p 2 ) \u2264 W 1 (p 1 , p 2 ) is derived from the fact that functions in F are 1-Lipschitz. The mean distance is computed using a linear discriminator as the sum of two ReLU discriminators. The neuron distance between two Gaussians is calculated, showing that at least one term is greater than \u00b5 1 \u2212 \u00b5 2 2 /2. The neuron distance between two Gaussians is calculated, showing that at least one term is greater than \u00b51 \u2212 \u00b52 2 /2. The W2 distance is used to bridge the KL and the F-distance, with a lower bound established using a perturbation bound. The lower bound for the KL divergence between two Gaussian distributions is established using the W2 distance and perturbation bounds. The exponential family properties are utilized to further bound the KL divergence. The curr_chunk discusses bounding the KL divergence for Gaussian distributions using exponential family properties and Wasserstein bounds. It also touches on Rademacher complexity and learning mixture of Gaussians with neural networks. The curr_chunk discusses the implementation of a neural network for learning mixtures of Gaussians, focusing on bounding the KL divergence and utilizing Gaussian concentration results. It also establishes upper and lower bounds for the discriminator function. The curr_chunk discusses regularity properties of distributions in the Bobkov-Gotze sense, bounding the Rademacher complexity of a neural network, and utilizing Gaussian concentration results. It also covers the expected supremum over a covering set and the covering number calculation. The curr_chunk discusses bounding the expected supremum over a covering set and the covering number calculation for distributions in the Bobkov-Gotze sense. It also shows how to upper bound the f-contrast by Wasserstein distance and provides a truncated W1 bound for distributions on Rd. The curr_chunk discusses upper bounding the f-contrast by Wasserstein distance for distributions on Rd, providing a truncated W1 bound, and extending the results with a straightforward proof. The curr_chunk discusses the computation of the inverse of x in a neural network and representing log p \u03b8 (x) using a feedforward net with activation \u03c3 \u22121. The chunk also introduces the density function \u03c6 \u03b3 and the log density formula. The chunk discusses computing the inverse of x in a neural network and representing log p \u03b8 (x) using a feedforward net with activation \u03c3 \u22121. It introduces the density function \u03c6 \u03b3 and the log density formula, showing how to compute log p \u03b8 (x) with no more than + 1 layers and O( d 2 ) parameters. The chunk discusses the computation of log p \u03b8 (x) using a neural network with specific activations and parameters. It also introduces the Gozlan condition and proves a restricted approximability bound in terms of the W 2 distance. The chunk discusses the Lipschitzness of the mapping in a neural network and the satisfaction of the Gozlan condition. It also provides bounds on the Wasserstein distances and the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The chunk discusses the Lipschitzness of the mapping in a neural network and the satisfaction of the Gozlan condition. It also provides bounds on the Wasserstein distances and the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The reparametrized \u03b8 = (W i , b i ) i\u2208[ ] belongs to the (overloading \u0398). The chunk discusses reparametrization of the neural network and the Rademacher complexity of F. It introduces additional re-parametrization and defines a metric for the Rademacher process. The chunk discusses discretization error and expected max over a finite set in the context of reparametrization of the neural network and Rademacher complexity of F. It presents bounds and constants related to the generalization error. The chunk discusses the discretization error and expected max over a finite set in the context of reparametrization of the neural network and Rademacher complexity of F. It presents bounds and constants related to the generalization error, focusing on the Lipschitzness of hidden layers in the network. The chunk discusses the Lipschitzness of hidden layers in a neural network, focusing on bounds and constants related to generalization error. It verifies results for different layers and analyzes the sub-exponential and sub-Gaussian properties of random variables within the network. The chunk discusses the sub-exponential and sub-Gaussian properties of random variables in a neural network, providing bounds and constants related to generalization error. It analyzes the Lipschitzness of hidden layers and verifies results for different layers. The text discusses bounding the expected maximum using a covering argument and Jensen's inequality. It introduces a truncated version of the convolution of a Gaussian distribution and analyzes the distribution obtained by adding Gaussian noise to a sample from G\u03b8. The text introduces regularity conditions for the family of generators G, including bounds on partial derivatives and the Lipschitz property of the inverse activation function. It also mentions asymptotic notation for notational convenience. The main theorem in the text states that a certain family of functions F approximates the Wasserstein distance. Theorem E.1 and Theorem E.2 provide conditions under which this approximation holds, with specific bounds and properties required for the generator class G. The proof relies on the existence of a parameterized family F that can approximate the log density of distributions in G. Theorem E.2 states that for \u03b2 = O(poly(1/d)), there exists a family of neural networks F that can approximate the log density of distributions in G. The approach involves approximating p \u03b2 (x) using Laplace's method of integration, providing both lower and upper bounds for the distribution. Theorem E.1 is proven by utilizing Theorem E.2, which shows the existence of neural networks that approximate log densities of distributions. The proof involves deriving lower and upper bounds for the distribution using a specific procedure. The upper bound is obtained by setting a certain value for \u03b2, while the optimal coupling of p and q is considered for the claim. The proof of Theorem E.1 involves deriving lower and upper bounds for the distribution using a specific procedure. The upper bound is obtained by setting a certain value for \u03b2, while the optimal coupling of p and q is considered for the claim. The rest of the section is dedicated to proving Theorem E.2, which will be discussed in Section E.3. In Section E.3, helper lemmas are proven through reverse induction on l. The proof involves producing estimates iteratively and analyzing the size/Lipschitz constant of the neural network. The integral on the right is the cdf of a Gaussian with specific properties. The proof concludes by bounding the smallest eigenvalue and demonstrating the claim. The algorithm presented approximates the integral needed and can be implemented by a small, Lipschitz network. It involves calculating gradients and finding the nearest matrix in a set with specific properties. The algorithm approximates the integral using a small, Lipschitz network by calculating gradients and finding the nearest matrix in a set with specific properties. It involves selecting matrices to ensure separation of eigenvalues. The algorithm selects matrices to ensure eigenvalue separation, approximates integrals using a Lipschitz network, and conducts synthetic WGAN experiments with invertible neural net generators and discriminators designed for restricted approximability. In synthetic WGAN experiments, invertible neural net generators and discriminators with restricted approximability are used to compute KL divergence and demonstrate correlation with empirical IPM WF(p, q) on synthetic data. Data is generated from a ground-truth invertible neural net generator, with Leaky ReLU activation function and well-conditioned weight matrices. The discriminator architecture is chosen based on restricted approximability guarantee. The Leaky ReLU activation function with negative slope 0.5 is used in the Gaussian model. Weight matrices are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen based on restricted approximability guarantee. Training involves generating stochastic batches, solving the min-max problem in Wasserstein GAN formulation, and using RMSProp optimizer. Evaluation metrics include KL divergence between true and learned generators. In the Wasserstein GAN formulation, 10 updates of the discriminator are performed between each generator step using various regularization methods. The RMSProp optimizer is used as the update rule. Evaluation metrics include KL divergence, training loss, and neural net IPM. In the training of GANs, the balance of steps for discriminator and generator is crucial. The WGAN loss is optimized separately with the generator fixed and discriminator trained without regularization. The goal is to find an approximate maximizer for the contrast function. The theory suggests that WGAN can learn the true generator in KL divergence. Experiments with a two-layer neural net generator in 10 dimensions validate this hypothesis. In experiments with a two-layer neural net generator in 10 dimensions, WGAN training with a discriminator design of restricted approximability is able to learn the true distribution in KL divergence. The KL divergence starts at around 10^-30 and the best run achieves a KL lower than 1, indicating that GANs are finding the true distribution without mode collapse. The W F (eval) and KL divergence are highly correlated, showing strong evidence of the model's performance. The W F (eval) and KL divergence are highly correlated, indicating GANs are finding the true distribution without mode collapse. Adding gradient penalty improves optimization, reflected in the W F curve. W F can serve as a good metric for monitoring convergence. IPM with vanilla discriminators also correlates well with KL-divergence. The left-most figure shows KL-divergence between true and learned distribution, middle shows estimated IPM, and right shows training loss. The estimated IPM in evaluation correlates well with the KL-divergence, indicating that the inferior performance of the WGAN-Vanilla algorithm is due to training performance rather than statistical properties. This phenomenon may also occur in training GANs with real-life data. In this section, the correlation between perturbations and the KL divergence and neural net IPM is tested. Pairs of perturbed generators are compared, showing a positive correlation between the KL divergence and neural net IPM. The majority of points align with the theory that neural net distance scales linearly with KL divergence. The experiments show a positive correlation between KL divergence and neural net IPM. Outliers with large KL are due to perturbations causing poorly conditioned weight matrices. Using vanilla fully-connected discriminator nets, generators still converge well in KL divergence, but correlation is slightly weaker compared to restricted approximability. Vanilla discriminator structures may be satisfactory for good generators, with room for improvement in distance quality. The experiments show a positive correlation between KL divergence and neural net IPM. Using vanilla fully-connected discriminator nets, generators still converge well in KL divergence, but correlation is slightly weaker compared to restricted approximability. Specific designs may help improve the quality of the distance W F."
}