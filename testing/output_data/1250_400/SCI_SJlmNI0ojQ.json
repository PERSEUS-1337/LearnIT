{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models have gained popularity for their ease of training, scalability to large datasets, and lack of a lexicon requirement. Contextual acoustic word embeddings are constructed directly from a supervised sequence-to-sequence model using attention distribution. These embeddings perform competitively on standard sentence evaluation tasks and match text-based embeddings in spoken language understanding tasks. Learning fixed-size representations for variable length data is a key focus in both text and speech-based applications. In the natural language processing and speech recognition communities, research focuses on learning fixed-size representations for variable length data like words or sentences. Popular methods include word2vec, GLoVE, CoVe, and ELMo for text-based embeddings, while speech recognition involves inputting short-term audio features. Prior work has addressed the challenge of learning word representations from variable length acoustic frames by aligning speech and text or chunking input speech into fixed-length segments. Our work focuses on obtaining acoustic word embeddings from utterance-level acoustics, unlike previous techniques that ignore specific audio context. We present methods for generating these embeddings using an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word speech recognition. This approach eliminates the need for chunking or pre-defined word boundaries in input speech segmentation. Our work demonstrates the usability of attention in aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE) from a speech recognition model. These embeddings are competitive with text-based word2vec embeddings on standard sentence evaluation benchmarks. Our work showcases the effectiveness of Contextual Acoustic Word Embeddings (CAWE) generated directly from a speech recognition model. These embeddings are competitive with text-based word2vec embeddings on standard sentence evaluation benchmarks and demonstrate utility in speech-based downstream tasks like Spoken Language Understanding. The text discusses training models with large vocabularies using smaller amounts of data and solutions for generating out-of-vocabulary words. It also mentions S2S models for pure-word large vocabulary recognition and improvements in training these models. Additionally, it explores ways to learn acoustic word embeddings. In this work, the BID12 model is used to learn acoustic embeddings, expanding on previous works that explored ways to learn acoustic word embeddings. Different methods are discussed, including unsupervised learning and supervised Convolutional Neural Network models for speech recognition. BID4 proposes an unsupervised method for learning speech embeddings with fixed context, while the drawbacks include the need for forced alignment between speech and words for training. Our work ties A2W speech recognition model with learning contextual word embeddings from speech. The S2S model is similar to the Listen, Attend and Spell model, consisting of an encoder network, a decoder network, and an attention model. The encoder maps input acoustic features into higher-level features. The model described is similar to the Listen, Attend and Spell model, with an encoder network, a decoder network, and an attention model. The encoder uses a pyramidal multi-layer BLSTM network, while the decoder is an LSTM network that generates targets using an attention mechanism. The attention mechanism enforces monotonicity in alignments by applying a convolution across time. The model follows the same experimental setup and hyper-parameters as word-based models in previous work. The model enforces monotonicity in alignments by applying a convolution across time to the attention of previous time steps, leading to a peaky distribution. It uses a location-aware attention mechanism to assign higher probability to certain frames. The model constructs \"contextual\" acoustic word embeddings using hidden representations from the encoder and attention weights from the decoder, similar to CoVe for text embeddings. The method for acoustic word embeddings involves using a location-aware attention mechanism to segment continuous speech into words and obtain word embeddings. Attention weights on acoustic frames reflect their importance in classifying a word, allowing for the construction of word representations based on the importance of each frame. The method involves using attention weights on acoustic frames to construct word representations based on their importance in classifying a word. Acoustic word embeddings are obtained by mapping words to acoustic frames with the highest attention weights, and three different methods are described for obtaining these embeddings. The text describes three methods for obtaining acoustic word embeddings using attention weights on acoustic frames. The methods include unweighted Average, attention weighted Average, and maximum attention. These techniques are referred to as Contextual Acoustic Word Embeddings (CAWE) due to the use of attention scores. The study uses the Switchboard corpus for speech recognition experiments. The study evaluates Acoustic Word Embeddings (CAWE) using attention scores on acoustic frames. Two datasets are used: Switchboard corpus with telephonic conversations and How2 dataset with instructional videos. The A2W model achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome set. The embeddings are tested on 16 benchmark tasks including Semantic Textual Similarity, classification, sentiment analysis, and opinion polarity. The study evaluates Acoustic Word Embeddings (CAWE) using attention scores on acoustic frames from the Switchboard corpus and How2 dataset. The embeddings are tested on 16 benchmark tasks covering Semantic Textual Similarity, classification, sentiment analysis, opinion polarity, entailment, and semantic relatedness. Logistic regression is used for classification tasks to abstract away model complexities. In downstream evaluations, logistic regression is used for classification tasks with CAWE and CBOW features without tunable embedding parameters. CAWE-M outperforms U-AVG and CAWE-W on STS tasks, with CAWE-W usually performing worse due to noisy estimation of word embeddings. U-AVG performs worse than CAWE-W on STS and SICK-R tasks. The embeddings in CAWE-M are constructed using the most confident attention score, while U-AVG performs worse due to weighting all encoder hidden representations equally. The training set for the speech recognition model is used for comparisons, with a smaller vocabulary for CAWE due to the A2W model's limited word recognition capabilities. The CAWE embeddings outperform word2vec on 10 out of 16 tasks when concatenated with text embeddings from Switchboard training data. How2 dataset is closer to text, leading to stronger CBOW embeddings compared to Switchboard. The CAWE embeddings show improvement when concatenated with text embeddings from Switchboard data, especially in Switchboard compared to How2 dataset. Additionally, CAWE is evaluated on the ATIS dataset for Spoken Language Understanding, which is similar in domain to Switchboard. The model architecture includes an embedding layer, RNN variant, dense layer, and softmax, trained for 10 epochs with RMSProp. Our model architecture, similar to a simple RNN-based model, includes an embedding layer, RNN variant (Simple RNN, GRU), dense layer, and softmax. We train the model for 10 epochs with RMSProp and compare performance using different word embeddings. We demonstrate that speech-based word embeddings can match text-based embeddings in downstream tasks, highlighting the utility of our approach. Additionally, we propose a method to learn contextual acoustic word embeddings from a speech recognition model. The study compares test scores using different word embeddings (CAWE-M, CAWE-W, CBOW) and fine-tuning them for tasks. They introduce a method to learn contextual acoustic word embeddings from a speech recognition model. The role of attention in constructing these embeddings is analyzed, showing competitiveness with word2vec. Two variants of contextual acoustic word embeddings outperform the unweighted average method by up to 34% in tasks. These embeddings match text-based embeddings in spoken language understanding, suggesting their use as pre-trained models for speech tasks. The study anticipates that contextual audio embeddings will enhance downstream tasks similar to text embeddings, despite the complexity of noisy audio input. Future work will focus on scaling the model to larger datasets and vocabularies, and comparing with non-contextual acoustic word embeddings. Contextual audio embeddings are expected to improve speech-based downstream tasks similar to text embeddings, despite the challenges of noisy audio input. Future work will focus on scaling the model to larger datasets and vocabularies, and comparing with non-contextual acoustic word embeddings."
}