{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, such as Choco-SGD, enables data privacy and on-device learning over networks. This approach achieves linear speedup in the number of workers for high compression ratios on non-convex functions and non-IID training data. It has practical applications in training deep learning models on decentralized user devices and in datacenters, offering computational scalability and data-locality. Decentralized training methods in deep learning have been successful in research and industry, offering computational scalability and data-locality. Recent theoretical results show that decentralized schemes can be as efficient as centralized approaches. Gradient compression techniques have been proposed to reduce data sent over communication links. Tang et al. (2018) introduced DCD and ECD algorithms for decentralized training with communication compression. CHOCO-SGD is a new algorithm introduced for decentralized training of deep neural networks, overcoming restrictions on compression operators. It focuses on generalization performance on test sets, departing from previous works that mainly considered training performance on train sets. The algorithm is evaluated in a peer-to-peer setting similar to federated learning, showing speed-ups. In a peer-to-peer setting similar to federated learning, CHOCO-SGD shows speed-ups over decentralized baseline with less communication overhead. In a datacenter setting, it improves time-to-accuracy on large tasks like ImageNet training. However, decentralized algorithms struggle to match centralized performance as the number of nodes increases. These findings highlight deficiencies in current decentralized training schemes. Decentralized training schemes struggle to match centralized performance as the number of nodes increases. CHOCO-SGD converges at a rate that matches centralized baselines with exact communication, showing a linear speedup in the number of workers. The practical side presents a version of CHOCO-SGD with momentum and analyzes its performance on on-device training over a peer-to-peer social network. The decentralized training schemes struggle to match centralized performance as the number of nodes increases. CHOCO-SGD shows a linear speedup in the number of workers. A version with momentum is analyzed for on-device training over a peer-to-peer social network and in a datacenter setting for computational scalability of training deep learning models. Performance of decentralized schemes when scaling to larger nodes is investigated, highlighting shared difficulties. Various methods for training in communication restricted settings have been proposed, including decentralized schemes, gradient compression, and asynchronous methods. In this paper, various methods for decentralized training have been proposed, including decentralized schemes, gradient compression, and asynchronous methods. The focus is on combining decentralized SGD schemes with gradient compression, particularly using gossip averaging approaches. The convergence rate of these methods typically depends on the spectral gap of the mixing matrix. Studies have shown convergence rates consistent with centralized mini-batch SGD. Quantization and communication compression with quantization have gained popularity in deep learning. Theoretical guarantees have been established for schemes with unbiased compression, and extended to biased compression as well. Schemes with error correction show the best performance in practice and offer the strongest theoretical guarantees. Proximal updates and variance reduction have also been recently studied. Decentralized Optimization with Quantization has been a topic of interest in the field. Gossip averaging can face challenges with quantization noise, leading to divergence. Adaptive schemes have been proposed to balance compression accuracy and communication cost. For deep learning applications, DCD and ECD algorithms have been introduced to converge at a similar rate as the centralized baseline. The CHOCO-SGD algorithm, introduced in Koloskova et al. (2019), can handle high compression rates and has been analyzed for convex functions. For non-convex functions, it shows a convergence rate of \u03b4 > 0. Another method, DeepSqueeze by Tang et al. (2019a), also converges with arbitrary compression ratio but in experiments, CHOCO-SGD achieves higher test accuracy. CHOCO-SGD algorithm achieves higher test accuracy compared to DeepSqueeze in experiments. It is a decentralized optimization algorithm that utilizes a mixing matrix and communication among nodes in a distributed setup. The CHOCO-SGD algorithm utilizes a weighted graph for communication links and compression operators to transmit compressed messages. The weights are based on local node degrees, and the algorithm aims to achieve higher test accuracy in a decentralized optimization setup. Compression operators in CHOCO-SGD do not need to be unbiased, allowing for a wider range of compression schemes. Each worker stores its private variable and updates it using stochastic gradient and gossip averaging steps. The nodes communicate with neighbors using compressed updates, maintaining averages of iterates despite quantization noise. The CHOCO-SGD algorithm updates variables for neighbors using compressed updates, allowing for parallel execution of communication and gradient computation. Each node only needs to store 3 vectors regardless of the number of neighbors. A momentum-version of CHOCO-SGD is proposed for non-convex problems, with technical assumptions on bounded variance of stochastic gradients. The convergence rate is denoted by c := \u03c1 2 \u03b4 82. The CHOCO-SGD algorithm extends its analysis to non-convex problems by introducing technical assumptions on bounded variance of stochastic gradients. The convergence rate is denoted by c := \u03c1 2 \u03b4 82, showing linear speed-up compared to SGD on a single node. The newly developed momentum version of CHOCO-SGD is presented in Algorithm 2, which includes weight decay factor \u03bb and momentum factor \u03b2. The experiments involve testing various compression operators with momentum in algorithms like CHOCO-SGD. The setup includes a ring topology with 8 nodes training ResNet20 on the Cifar10 dataset. Different algorithms like DCD, ECD, DeepSqueeze, and standard mini-batch SGD are implemented with momentum. Compression schemes are applied to every layer of ResNet20 separately, with two unbiased schemes implemented: quantization that rounds weights to b-bit representations and random sparsification. The experiments involve testing various compression operators with momentum in algorithms like CHOCO-SGD. Compression schemes are applied to every layer of ResNet20 separately, with unbiased quantization and random sparsification implemented. Two biased compression schemes are also used: top fraction selection and sign compression. DCD and ECD are analyzed for unbiased quantization only, while CHOCO-SGD and DeepSqueeze are studied for biased schemes. Results show that unbiased compression schemes ECD and DCD perform well at low compression ratios but struggle at high ratios, consistent with previous findings. DCD paired with biased top sparsification outperforms unbiased random sparsification, despite lacking theoretical support. CHOCO-SGD demonstrates good generalization across scenarios with minimal accuracy drop. Sign compression achieves high accuracy with significantly fewer bits per weight compared to full precision. CHOCO-SGD can generalize well with minimal accuracy drop in all scenarios, while sign compression achieves high accuracy with fewer bits per weight. The focus now shifts to challenging decentralized real-world scenarios where each device has access to local data, limited communication bandwidth, unknown network topology, and a large number of connected devices. This decentralized setting is motivated by privacy concerns. In a fully decentralized setting, data is stored or acquired locally on each device, communication bandwidth is limited, network topology is unknown, and a large number of devices are connected. Training data is split between nodes without shuffling, and privacy is a key motivation. Centralized approaches like all-reduce are not efficient, so comparisons are made with a central coordinator for aggregation. The scenario of decentralized deep learning with sign compression on the Cifar10 dataset is explored. In a decentralized setting, data is stored locally on each device with limited communication bandwidth. Centralized approaches like all-reduce are not efficient, so comparisons are made with a central coordinator for aggregation. CHOCO-SGD with sign compression is compared to decentralized SGD without compression and centralized SGD without compression. Training is done on 4, 16, 36, and 64 nodes using different network topologies. Results show testing accuracy after 300 epochs, with CentralizedSGD performing well. In a decentralized setting, data is stored locally on each device with limited communication bandwidth. Centralized approaches like all-reduce are not efficient, so comparisons are made with a central coordinator for aggregation. CHOCO-SGD with sign compression is compared to decentralized SGD without compression and centralized SGD without compression. Training is done on 4, 16, 36, and 64 nodes using different network topologies. Results show testing accuracy after 300 epochs, with CentralizedSGD performing well. The parameters for each method are listed in Table 2. The learning rate is kept constant and separately tuned for all methods, with consensus learning rate tuned for CHOCO-SGD. Results are summarized in Figure 1, showing that CentralizedSGD outperforms CHOCO-SGD due to graph topology influence and communication compression. The performance degradation is attributed to slower convergence, not a generalization issue. Increasing epochs improves decentralized scheme performance, but the gap between centralized and decentralized algorithms remains. In the real decentralized scenario, the focus is on reducing communication costs for users' mobile data. CHOCO-SGD performs the best with slight degradation as the number of nodes increases. Torus topology is beneficial for large networks due to good mixing properties, while small networks show little difference between torus and other topologies. Both decentralized and centralized SGD require a significantly larger number of bits to achieve reasonable accuracy. In a real decentralized scenario, CHOCO-SGD reduces communication costs for users' mobile data. Torus topology benefits large networks but cancels out the spectral gap advantage due to increased communication. Both Decentralized and Centralized SGD require more bits for accuracy. Experiments on a Real Social Network Graph involve training models on user devices connected by a social network, using ResNet20 for image classification and LSTM for language modeling tasks. The results of the experiments show that the decentralized algorithm performs best for image classification training accuracy, followed by the centralized and quantized decentralized. However, the centralized scheme has the highest test accuracy. CHOCO-SGD outperforms the exact decentralized scheme in terms of test accuracy for the same transmitted data. In language modeling tasks, CHOCO-SGD outperforms centralized SGD in test perplexity. For language modeling tasks, CHOCO-SGD outperforms centralized SGD in test perplexity and shows improved performance with scaling to more nodes in decentralized optimization methods. Decentralized schemes offer a solution for scaling issues in well-connected environments like datacenters with fast network connections. The decentralized optimization method CHOCO-SGD outperforms centralized SGD in test perplexity for language modeling tasks. It shows improved performance with scaling to more nodes in well-connected environments like datacenters with fast network connections. Lian et al. (2017) and Assran et al. (2019) have also demonstrated the benefits of decentralized schemes in certain scenarios. Our experiments on 8 machines with 4 Tesla P100 GPUs each utilized decentralized communication with compressed communication in a ring topology. We used CHOCO-SGD with a mini-batch size of 128 per GPU and achieved a slight 1.5% accuracy loss compared to all-reduce. CHOCO-SGD took less time to perform the same number of epochs and showed promising results. Our study demonstrates a 20% time gain over the common all-reduce baseline using CHOCO-SGD for decentralized deep learning training in bandwidth-constrained environments. The algorithm shows linear speedup in the number of nodes and performs well on image classification tasks. The study shows a 20% time gain using CHOCO-SGD for decentralized deep learning training in bandwidth-constrained environments, demonstrating linear speedup in the number of nodes and good performance on image classification tasks. The main contribution is enabling training in communication-restricted environments while respecting data locality constraints. The proof of Theorem 4.1 is presented, showing Algorithm 1 as a special case of a more general class of algorithms. The study demonstrates a 20% time gain using CHOCO-SGD for decentralized deep learning training in bandwidth-constrained environments, with linear speedup in the number of nodes and good performance on image classification tasks. The proof of Theorem 4.1 shows Algorithm 1 as a special case of a more general class of algorithms, following a structure similar to Koloskova et al. (2019). The convergence of algorithms involving stochastic gradient updates and averaging steps is analyzed, with linear convergence shown for the specific averaging scheme used in CHOCO-SGD. The curr_chunk discusses decentralized SGD with arbitrary averaging schemes, including Exact Averaging and CHOCO-SGD. It mentions the convergence rate, linear rate, and specific algorithms like D-PSGD and CHOCO-GOSSIP. The text also refers to a matrix notation and assumptions for the averaging scheme. The curr_chunk discusses recovering algorithms like D-PSGD and CHOCO-SGD with specific consensus averaging schemes. It highlights the independence of communication and gradient computation parts, with a proof of convergence under certain assumptions and constant step size. The text also mentions a linear speed up in convergence rate. The proof of Lemma 21 by Koloskova et al. (2019) shows that under certain assumptions and with a constant step size, Algorithm 3 converges at a rate that exhibits a linear speed up compared to SGD on one node. The convergence rate is affected by the underlying averaging scheme, with CHOCO-SGD converging at a rate dependent on the eigengap of the mixing matrix. The proof technique suggests a potential artifact or consequence of supporting high compression. Theorem A.2 provides guarantees for the averaged vector of parameters in a decentralized setting, where averaging all parameters across machines is expensive. Corollary A.3 shows convergence of local weights under the same setting, with a proof that utilizes L-smoothness of the function. The results hold for T larger than 64nL^2, but this requirement can be relaxed. Theorem A.4 discusses the convergence of Algorithm 3 in a decentralized setting with constant step sizes. It converges at a speed determined by the convergence rate of the underlying averaging scheme. Corollary A.5 further explores the convergence of local weights in Algorithm 1, with a proof that utilizes L-smoothness of the function. Theorem A.4 and Corollary A.5 discuss the convergence of decentralized algorithms with constant step sizes and local weights. Algorithm 4 CHOCO-SGD combines weight decay and momentum for improved performance in a decentralized setting. Algorithm 4 CHOCO-SGD combines weight decay and momentum for improved performance in a decentralized setting. It can be interpreted as an error feedback algorithm where quantization errors are saved into internal memory and corrected for in each iteration. The comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Two models, ResNet20 and another, are trained using this approach. In this section, the model training procedure and hyper-parameter tuning are discussed. The comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Two models, ResNet20 for image classification on Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2, are trained with specific configurations. The experimental setup includes using a three-layer LSTM with hidden dimension of size 650, fine-tuning gradient clipping and dropout values, training ResNet20 and LSTM for 300 epochs, setting the mini-batch size to 32, and adjusting the learning rate based on node degree. Learning rate warmup and decay strategies are implemented during training. During the training procedure, the initial learning rate is decayed by a factor of 10 at 50% and 75% of the total epochs. The learning rate is tuned per sample\u03b7 using a linear scaling rule based on node degree. Optimal\u03b7 is searched in a grid to ensure best performance. Tables demonstrate fine-tuned hyperparameters for training ResNet-20/LSTM on different topologies with fixed data partitioning and no shuffling. Table 5 shows the tuned hyperparameters of CHOCO-SGD for training ResNet-20/LSTM on a social network topology with 32 nodes. The training data is split between nodes without shuffling, with a mini-batch size of 32 per node and a maximum node degree of 14. The learning curve and accuracy plots for the topology are depicted in Figures 1, 6, 8, and 9, showing the convergence of local models towards a consensus. In Figure 8, plots are provided for training top-1, top-5 accuracy, and test top-5 accuracy for the datacenter experiment. Figure 9 shows the test accuracy of the averaged model and the averaged distance of local models from the averaged model. Towards the end of optimization, local models reach a consensus, with their test performances matching that of the averaged model. Interestingly, before decreasing the stepsize at epoch 225, local models diverge from the averaged model, converging only when the stepsize decreases. This behavior was also observed in Assran et al. (2019)."
}