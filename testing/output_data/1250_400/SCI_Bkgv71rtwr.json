{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where the source and target domains may not share the same categories. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), a novel approach that incorporates category-agnostic clusters in the target domain to improve domain adaptation. Clustering is used to identify underlying data patterns in unlabeled target samples, aiding in generalization for both closed-set and open-set scenarios. SE-CC utilizes category-agnostic clusters in the target domain to enhance domain adaptation by revealing underlying data patterns. Clustering is employed to ensure the learned representation preserves the data structure, with mutual information maximization further improving the representation. Experimental results on Office and VisDA datasets show superior performance compared to existing approaches, especially in scenarios with limited annotated data. Closed-set domain adaptation is crucial for achieving superior results in comparison to state-of-the-art approaches, especially in scenarios with limited annotated data. Convolutional Neural Networks have driven vision technologies to new heights, but the impracticality of large quantities of annotated data necessitates alternative methods. Unsupervised domain adaptation can help generalize a target model by leveraging labeled source samples and unlabeled target samples. However, existing models often struggle with domain shift, limiting their applicability in open-set scenarios where domains do not share the same set of categories. Existing models in domain adaptation struggle with open-set scenarios where domains do not share the same categories. One approach to address this issue is by using an additional binary classifier to distinguish known and unknown target samples, discarding the unknown samples during adaptation. This helps in generalizing the target model and improving classification accuracy. To address open-set scenarios in domain adaptation, a binary classifier is used to distinguish known and unknown target samples. Unknown samples are discarded during adaptation, but this approach may not fully exploit the data structure. To improve performance, clustering is performed on all unlabeled target samples to model diverse semantics of known and unknown classes in the target domain. This clustering helps in creating domain-invariant representations for known classes and discriminative representations for both unknown and known classes. In order to improve domain adaptation in open-set scenarios, clustering is utilized to decompose target samples into category-agnostic clusters. This approach aims to create domain-invariant representations for known classes and discriminative representations for both unknown and known classes in the target domain. Additionally, a new Self-Ensembling with Category-agnostic Clusters (SE-CC) model is introduced, which includes an additional clustering branch to refine the learnt representations and preserve the inherent structure of the target domain. The SE-CC framework integrates a clustering branch to predict cluster assignments for target samples, using KL-divergence to minimize mismatch and preserve data structure. Mutual information is maximized to enhance feature representation. The framework is jointly optimized for unsupervised domain adaptation. The SE-CC framework integrates a clustering branch to predict cluster assignments for target samples and enhance feature representation. Unsupervised domain adaptation involves learning transferrable features in CNNs by minimizing domain discrepancy through methods like Maximum Mean Discrepancy (MMD) and domain confusion loss. The domain discriminator predicts the domain of input samples and enforces domain invariance through domain confusion loss. Open-set domain adaptation addresses scenarios with new classes in the target domain. Various methods like adversarial training and exploiting known/unknown classes are used to improve feature representations for unknown target samples. Various methods like adversarial training and exploiting known/unknown classes are used to improve feature representations for unknown target samples in domain adaptation. Baktashmotlagh et al. (2019) factorize the source and target data into shared and private subspaces to model known and unknown target samples, while clustering is used to decompose unlabeled target samples into category-agnostic clusters for Self-Ensembling in both closed-set and open-set scenarios. SE-CC utilizes clustering to decompose unlabeled target samples into category-agnostic clusters, which are integrated into Self-Ensembling for closed-set and open-set scenarios. A clustering branch in the student infers cluster assignment distribution for each target sample, aligning it with the original cluster distribution to preserve data structure. The student's feature representation is enhanced by maximizing mutual information among feature map, classification, and cluster assignment distributions. SE-CC utilizes category-agnostic clusters for representation learning in open-set domain adaptation. It maximizes mutual information among input features, cluster, and class probability distributions to align sample distributions and discriminate between known and unknown classes. This approach enhances representation learning by preserving target data structure and has not been fully explored in previous studies. In open-set domain adaptation, SE-CC integrates category-agnostic clusters to enhance representation learning by maximizing mutual information among input features, clusters, and class probability distributions. This approach aims to align sample distributions and distinguish between known and unknown classes, improving domain-invariant representations and classifiers for target domain recognition. Open-set domain adaptation involves learning domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown samples. Self-Ensembling, based on Mean Teacher, encourages consistent classification predictions between teacher and student models under small perturbations of input images. Self-Ensembling, inspired by Mean Teacher, aims to ensure consistent classification predictions between teacher and student models by penalizing differences in classification probabilities. The training process involves updating the student model through gradient descent and the teacher model through exponential moving average. Additionally, an unsupervised conditional entropy loss is used to drive decision boundaries away from high-density regions in the target domain. The overall training loss includes supervised cross entropy loss on the source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. The Self-Ensembling approach aims to drive decision boundaries away from high-density regions in the target domain. The training loss includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data. Open-set domain adaptation is challenging as it requires classifying both inliers and outliers into known and unknown classes. To address this, clustering is used to model diverse semantics in the target domain, guiding the domain adaptation process. Clustering is utilized to model diverse semantics in the target domain for guiding domain adaptation in the Self-Ensembling approach. K-means is used to decompose unlabeled target samples into clusters, enforcing domain-invariant feature representations for known classes and discriminative features for unknown and known classes. In the Self-Ensembling approach, clustering with k-means is used to decompose unlabeled target samples into clusters, revealing underlying structure tailored to the target domain. Target samples are represented as output features of pre-trained CNNs for clustering, with periodic cluster refreshing not significantly impacting results. The inherent cluster distribution of each target sample is encoded through joint relations with category-agnostic clusters. The inherent cluster distribution of each target sample is encoded through joint relations with category-agnostic clusters. A clustering branch in the student model predicts the distribution over all clusters for cluster assignment of each target sample. The clustering branch in the student model predicts the cluster assignment distribution for each target sample x S t using a modified softmax layer. The KL-divergence loss is used to measure the mismatch between the estimated and inherent cluster distributions, enforcing the learnt representation to preserve the data structure of the target domain. The KL-divergence loss is defined to measure the mismatch between estimated and inherent cluster distributions, enforcing the learnt representation to preserve the data structure of the target domain. Inter-cluster relationships are incorporated as a constraint to maintain relations among cluster assignment parameter matrices, ensuring similarity between semantically similar clusters. The KL-divergence loss is further relaxed, and Mutual Information Maximization is used to strengthen the learnt target feature in an unsupervised manner. The student in our SE-CC model produces classification and cluster assignment distributions for a target sample. Mutual Information Maximization is used to enhance the learned target feature in an unsupervised way by maximizing mutual information among input features and output distributions. A MIM module is designed to estimate and maximize local and global mutual information among input feature map, output classification distribution, and cluster assignment distribution. The global Mutual Information is estimated by encoding the output feature map into a global feature vector, concatenating it with classification and cluster assignment distributions, and feeding it into a discriminator to determine alignment. The discriminator consists of three fully-connected networks with nonlinear activation, outputting a probability score representing the alignment of the input feature with the distributions. The global Mutual Information discriminator is implemented with three stacked fully-connected networks plus nonlinear activation to estimate the Mutual Information. It uses a softplus function and global features of different target images. Additionally, local Mutual Information is exploited among local input features and output distributions by spatially replicating and concatenating them for discrimination. The SE-CC model utilizes a local Mutual Information discriminator with stacked convolutional layers to match input features with classification and cluster assignments. The final objective combines local and global Mutual Information estimations with a tradeoff parameter. The training objective integrates cross entropy loss on source data and unsupervised self-ensembling loss. The SE-CC model combines local and global Mutual Information estimations with a tradeoff parameter. The training objective integrates cross entropy loss on source data, unsupervised self-ensembling loss, conditional entropy loss, and KL-divergence loss of clustering branch. Empirical verification is done on the Office Saenko et al. VisDA dataset for synthetic-real image transfer. The training domain consists of 3D CAD models, while the validation domain includes real images from COCO and the testing domain comprises video frames from YTBB. Synthetic images are used as the source, and COCO images as the target for evaluation. Different classes are assigned as known and unknown for open-set adaptation, with specific ratios of samples in the target domain. Three metrics are used for evaluation: Knwn for known classes, Mean for known and unknown classes, and Overall for all target samples. ResNet152 is utilized as the backbone for CNNs in clustering. For closed-set adaptation, ResNet152 is used as the backbone for CNNs. Open-Set Domain Adaptation on Office is evaluated with different models, including AODA and SE-CC \u2666. SE-CC \u2666 learns classifiers without unknown source samples and performs better than other models. The SE-CC classifier in open-set domain adaptation outperforms other models like RTN and RevGrad, as well as AODA, ATI-\u03bb, and FRODA. It improves classification accuracy on challenging transfers and leverages category-agnostic clusters for domain-invariant feature representation. In open-set domain adaptation, SE-CC outperforms RTN, RevGrad, AODA, ATI-\u03bb, and FRODA by injecting category-agnostic clusters as a constraint for feature learning and alignment. This approach improves classification accuracy and leverages domain-invariant feature representation. In closed-set domain adaptation, SE-CC achieves better performance than other state-of-the-art techniques by exploiting category-agnostic clusters in the target domain. Ablation study shows how different designs in SE-CC influence overall performance, including Conditional Entropy and KL-divergence Loss. In closed-set domain adaptation, SE-CC outperforms other techniques by utilizing category-agnostic clusters in the target domain. Ablation study examines the impact of different designs in SE-CC on overall performance, including Conditional Entropy, KL-divergence Loss, and Mutual Information Maximization. Table 5 shows performance improvements on VisDA for open-set domain adaptation with various designs. CE enhances classifier performance regardless of domain adaptation architectures, improving Mean accuracy from 65.2% to 66.3%. KL and MIM are specific designs in SE-CC that refine features and maximize mutual information for downstream tasks. In our study, we introduce Self-Ensembling with Category-agnostic Clusters (SE-CC) for open-set domain adaptation on VisDA. CE enhances classifier performance, increasing Mean accuracy from 65.2% to 66.3%. KL and MIM designs in SE-CC contribute to a total performance gain of 4.2% in Mean metric, showcasing the effectiveness of exploiting target data structure and mutual information maximization. SE-CC separates unknown target samples from known ones and integrates category-agnostic clusters for improved adaptation in both open-set and closed-set scenarios. In our study, we introduce Self-Ensembling with Category-agnostic Clusters (SE-CC) for open-set domain adaptation on VisDA. The approach separates unknown target samples from known ones and integrates category-agnostic clusters for improved adaptation in both open-set and closed-set scenarios. Clustering is used to decompose target samples into category-agnostic clusters, and a clustering branch is integrated into the student model to align cluster assignment distribution. Mutual information among input features, classification outputs, and clustering branches is utilized to enhance learned features. Experimental results on Office and VisDA show performance improvements compared to state-of-the-art techniques. The detailed frameworks for global and local mutual information estimation are illustrated in Figure 3. The implementation is mainly developed with PyTorch. The implementation of SE-CC for open-set domain adaptation on VisDA involves global and local mutual information estimation, with network weights optimized using PyTorch and SGD. Experiment settings include learning rate, mini-batch size, training iterations, and cluster number parameters. Performance improvements are observed compared to state-of-the-art techniques on Office and VisDA datasets. The study evaluates the performance of SE-CC for open-set domain adaptation on VisDA datasets by determining the number of clusters using Gap statistics method and tuning hyper-parameters. KL-divergence is found to be a better measure than L1 and L2 distance for mismatch evaluation. Different variants of the MIM module are also assessed for mutual information estimation between input features and outputs. The study evaluates the performance of SE-CC for open-set domain adaptation on VisDA datasets by determining the number of clusters using Gap statistics method and tuning hyper-parameters. KL-divergence is found to be a better measure than L1 and L2 distance for mismatch evaluation. Evaluation of Mutual Information Maximization in SE-CC shows that CLS, CLU, and CLS+CLU improve performance by exploiting mutual information between input features and outputs of classification and clustering branches. CLS+CLU achieves the largest performance boost by combining outputs from both branches for mutual information estimation. The results highlight the importance of exploiting mutual information among input features and outputs of downstream tasks in the MIM module. SE brings source and target distributions closer for domain-invariant representation. The study evaluates the performance of SE-CC for open-set domain adaptation on VisDA datasets by determining the number of clusters and tuning hyper-parameters. SE-CC improves performance by exploiting mutual information between input features and outputs of classification and clustering branches. SE-CC separates unknown target samples from known samples, making them indistinguishable in two domains."
}