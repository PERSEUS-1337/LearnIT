{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified by showing that the expected coarse gradient correlates positively with the population gradient, making its negation a descent direction for minimizing the population loss. The use of a straight-through estimator (STE) in training activation quantized neural networks is crucial for minimizing a piecewise constant training loss. Proper selection of STE ensures that the expected coarse gradient aligns with the population gradient, leading to a descent direction for minimizing the population loss. However, a poor choice of STE can result in instability near local minima, as demonstrated in CIFAR-10 experiments. This is important for achieving memory savings and energy efficiency in deep neural networks (DNN) during inference. Recent efforts have focused on training coarsely quantized DNN to achieve memory savings and energy efficiency at inference time. This involves minimizing a nonconvex empirical risk function subject to weight quantization constraints. Weight quantization of DNN has been extensively studied in the literature. Weight quantization of DNN has been extensively studied in the literature, with the gradient in training activation quantized DNN being almost everywhere zero. To address this issue, a non-trivial search direction is constructed by modifying the chain rule, using the straight-through estimator (STE) in the backward pass. Alternative approaches include stochastic neurons and the feasible target propagation algorithm for learning hard-threshold networks. The straight-through estimator (STE) was proposed as an alternative approach for training activation quantized DNNs. It originates from the perceptron algorithm and uses a modified chain rule with the derivative of the sigmoid function. This method allows for training multi-layer networks with binary activations by backpropagating as if the activation had been the identity function. The straight-through estimator (STE) was introduced as a method to train DNNs with quantized activations. Various studies have explored different variations of STE, such as using the derivative of the sigmoid function or the signum activation function. Despite empirical success, there is limited theoretical understanding of STE in training DNNs with stair-case activations. Goel et al. (2018) demonstrated the convergence of the Convertron algorithm using leaky ReLU activation in a one-hidden-layer network. The Convertron algorithm, utilizing leaky ReLU activation in a one-hidden-layer network, demonstrated convergence. Other approaches, like implicit weighted nonlocal Laplacian layers and backward pass differentiable approximation, have been proposed to improve DNN generalization accuracy and break adversarial defenses. The \"coarse gradient\" concept is introduced in this paper through the STE-modified chain rule. In the context of improving DNN generalization accuracy and breaking adversarial defenses, the backward pass differentiable approximation was introduced as a defense mechanism. The concept of \"coarse gradient\" is discussed in relation to the STE-modified chain rule, exploring the optimization perspective and theoretical considerations of different STEs in training quantized ReLU nets. In the context of improving DNN generalization accuracy and breaking adversarial defenses, the backward pass differentiable approximation was introduced as a defense mechanism. The concept of \"coarse gradient\" is discussed in relation to the STE-modified chain rule, exploring the optimization perspective and theoretical considerations of different STEs in training quantized ReLU nets. Specifically, the study focuses on the theoretical analysis and empirical performances of three representative STEs for learning a two-linear-layer network with binary activation and Gaussian data: derivatives of the identity function, vanilla ReLU, and clipped ReLUs. The research proves that proper choices of STEs lead to training algorithms that are descent, with negative expected coarse gradients based on STEs of vanilla and clipped ReLUs being descent directions for minimizing population loss. The study also highlights the instability of the training algorithm near certain local minima when using the identity STE. Empirical performances of the three STEs on MNIST and CIFAR-10 classifications with general quantized ReLU are examined, showing that both vanilla and clipped ReLUs perform well. The study explores the theoretical analysis and empirical performances of three representative Straight-Through Estimators (STEs) for training quantized ReLU nets. It shows that the training algorithm can be unstable near certain local minima when using the identity STE. Empirical results on MNIST and CIFAR-10 show that clipped ReLU STE performs best for deeper networks like VGG-11 and ResNet-20. The research also discusses convergence guarantees for the identity STE in perceptron and Convertron algorithms. The study discusses the convergence guarantees of perceptron and Convertron algorithms for the identity STE, highlighting its limitations in networks with two trainable layers. It also compares the performance of different STEs, emphasizing the importance of the monotonicity of quantized activation functions. The research presents results on the energy landscape of a two-linear-layer network with binary activation and Gaussian data, and evaluates the empirical performances of various STEs in 2-bit and 4-bit scenarios. In section 2, the study explores the energy landscape of a two-linear-layer network with binary activation and Gaussian data. Section 3 presents the main results and mathematical analysis for STE. Section 4 compares the empirical performances of different STEs in 2-bit and 4-bit activation quantization, highlighting instability issues observed in CIFAR experiments. Technical proofs and figures are deferred to the appendix. The model considered in this section involves trainable weights w and v in linear layers, with an activation function acting on the vector Zw. The first layer functions as a convolutional layer, while the second layer serves as the classifier. The label generation is based on specific parameters, and a squared sample loss function is utilized. The activation function used here is a binary function, different from ReLU, and the input data is assumed to be sampled from a Gaussian distribution. The model involves trainable weights w and v in linear layers with a binary activation function on the vector Zw. The input data is assumed to be sampled from a Gaussian distribution. The gradient of the objective function is not available for network training, so the expected sample gradient is used instead. The idea of STE is to replace the zero component in the training with a non-trivial function. The idea of STE is to replace the zero component in the training with a non-trivial function \u00b5, which is the derivative of a (sub)differentiable function. Using STE \u00b5 to train a two-linear-layer CNN with binary activation results in coarse gradient descent. The landscape of the population loss function f (v, w) is discussed, including the angle between w and w*. The population loss function f (v, w) landscape is explored, defining the angle between w and w* as \u03b8(w, w*). Analytic expressions of f (v, w) and \u2207f (v, w) are elaborated. The possible local minimizers of the model are discussed, including stationary points and non-differentiable points. Saddle points are shown to be the only potential spurious local minimizers. The text discusses the landscape of the population loss function f(v, w), focusing on the angle between w and w* as \u03b8(w, w*). It explores the possible local minimizers, showing that non-differentiable points where \u03b8(w, w*) = 0 and v = v* are global minimizers. Saddle points are identified as the only potential spurious local minimizers. The text also mentions the Lipschitz continuity of the population gradient \u2207f(v, w) and the behavior of the coarse gradient descent algorithm in the presence of saddle points and spurious local minimizers. The text discusses the behavior of the coarse gradient descent algorithm in the presence of saddle points and spurious local minimizers. It proves that using the derivative of vanilla or clipped ReLU converges to a critical point, while the identity function does not. The convergence guarantee is established under certain conditions. The convergence guarantee for the coarse gradient descent algorithm is established under the assumption of infinite training samples. With a small number of data points, the empirical loss descends roughly along the negative coarse gradient direction. As the sample size increases, the loss gains monotonicity and smoothness, explaining the effectiveness of stochastic gradient descent with large datasets in deep learning. The results hold even if the Gaussian assumption on input data is weakened to a rotation-invariant distribution. The mathematical analysis for the main results involves the empirical loss moving in the direction of negative coarse gradient. The key observation is that the coarse partial gradient has non-negative correlation with the population partial gradient, forming a descent direction for minimizing the population loss. If certain conditions are met, there exists a constant for the expected coarse and population gradients. The analysis shows that the coarse gradient moves towards the negative direction, correlating with the population gradient for minimizing loss. Conditions determine a constant for the expected gradients, ensuring descent behavior. When Algorithm 1 using ReLU STE converges, the gradients vanish simultaneously, indicating saddle points. When Algorithm 1 converges, the coarse gradient using clipped ReLU STE correlates positively with the true gradient of the population loss function. The coarse gradient vanishes only at critical points, indicating descent behavior. Additionally, the expected gradients vanish simultaneously at saddle points during convergence. Lemma 8 states that when Algorithm 1 converges, the expected coarse and true gradients vanish simultaneously at saddle points. However, Lemma 9 suggests that the coarse gradient descent may not converge near local minimizers under certain conditions. The text discusses the behavior of gradient descent near local minimizers and spurious minimizers. It compares the performances of different activation functions on deep neural networks using quantized activations on MNIST and CIFAR-10 benchmarks. The theory suggests that vanilla and clipped ReLUs have different empirical performances on deeper networks. In this section, the performances of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations are compared. The resolution \u03b1 for quantized ReLU must be carefully chosen to maintain accuracy. A modified batch normalization layer is used to pre-compute the best fitting \u03b1 for the activation layer. In this section, a modified batch normalization layer is used to pre-compute the best fitting \u03b1 for the activation layer. The optimizer used is stochastic gradient descent with momentum = 0.9. Training is done for 50 epochs on LeNet-5 and 200 epochs on VGG-11 and ResNet-20. Parameters are initialized with those from their pre-trained full-precision counterparts. The experiments involved training LeNet-5 on MNIST for 50 epochs and VGG-11 and ResNet-20 on CIFAR-10 for 200 epochs with momentum = 0.9. The learning rate schedule is specified in TAB2 in the appendix. Results in Table 1 show that clipped ReLU performs best, followed by vanilla ReLU and then the identity function. Clipped ReLU is particularly effective for deeper networks, while vanilla ReLU shows comparable performance on the shallow LeNet-5 network. The use of the identity function in ResNet-20 with 4-bit activations leads to instability, as predicted in Theorem 1. The experiments involved training LeNet-5 on MNIST for 50 epochs and VGG-11 and ResNet-20 on CIFAR-10 for 200 epochs with momentum = 0.9. Results show that clipped ReLU performs best, followed by vanilla ReLU and then the identity function. The identity function in ResNet-20 with 4-bit activations leads to instability, as predicted. The training using the identity STE ends up with a much worse minimum. The training using the identity STE leads to a worse minimum due to coarse gradient not vanishing at good minima. Similarly, ReLU STE on 2-bit activated ResNet-20 performs poorly due to instability at good minima. When initialized with weights from vanilla and clipped ReLUs on ResNet-20 with 4-bit activations, the coarse gradient descent using identity STE is repelled. The coarse gradient descent using the identity STE is repelled, with a learning rate set to 10^-5 until epoch 20. Theoretical justification for STE as a descent training algorithm was provided, considering derivatives of identity function, vanilla ReLU, and clipped ReLU. Negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing population loss, while identity STE generates incompatible coarse gradients. CIFAR experiments confirmed instability with improper STE choices. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. The coarse gradient descent using the ReLU STE with a 10^-5 learning rate is unstable, leading to increasing classification and training errors. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. Lemma 12 further explores Gaussian random vectors and their properties. Lemma 13 and Lemma 14 provide further insights into Gaussian random vectors and their properties. Lemma 13 discusses inequalities involving angles and random vectors, while Lemma 14 explores projections and the Cauchy-Schwarz inequality. Lemma 13 and Lemma 14 provide insights into Gaussian random vectors. Lemma 13 discusses inequalities with angles and random vectors, while Lemma 14 explores projections and the Cauchy-Schwarz inequality. Since cos(\u03c6)\u03be is even, we have p(\u03c0/2, w) \u2264 q(\u03c0/2, w). The angle between I n \u2212 ww w 2 w * and I n \u2212ww w 2 w * is equal to the angle between w and w. If w = 0 n, the population loss f(v, w) is given by. If w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0), the partial gradients of f(v, w) w.r.t. v and w are. The saddle points obey FORMULA11. Lemma 13 and Lemma 14 provide insights into Gaussian random vectors, discussing inequalities with angles and projections. If w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0), the partial gradients of f(v, w) w.r.t. v and w are considered. The stationary points are shown to be saddle points, with the Hessian matrix being indefinite. The text discusses saddle points in the context of Gaussian random vectors, with a focus on partial gradients and Hessian matrix properties. Lemmas are used to establish inequalities and properties related to the objective function. The text discusses Lemmas 4 and 5, which establish inequalities and properties related to partial gradients and inner products in the context of saddle points for Gaussian random vectors. Lemma 5 establishes properties of partial gradients and inner products for saddle points with Gaussian random vectors. Lemma 6 and Lemma 7 further elaborate on these properties. Lemma 7 discusses the inner product between expected coarse and true gradients with certain conditions. It also provides a proof for these properties. Lemma 8 is similar to Lemma 6, with q(\u03b8, w) non-negative and only equal to 0 at \u03b8 = 0, \u03c0. Lemma 9 shows the expected coarse partial gradient w.r.t. w. Lemma 10 discusses the inner product between expected coarse and true gradients when w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0)."
}