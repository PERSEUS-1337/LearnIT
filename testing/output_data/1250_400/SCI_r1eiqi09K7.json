{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization is shown to yield faster convergence and lower train loss values compared to standard algorithms in experiments involving embedding the WordNet taxonomy in the Poincare ball. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when dealing with a large number of parameters. Recent advancements have led to successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD. These algorithms are designed for optimizing parameters in a Euclidean space, but there is a growing interest in optimizing parameters on Riemannian manifolds. Experimental results show that Riemannian adaptive methods offer faster convergence and lower train loss values compared to standard algorithms, as demonstrated in tasks such as embedding the WordNet taxonomy in the Poincare ball. Recent advancements in optimization algorithms have led to successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD designed for optimizing parameters in a Euclidean space. However, there is a growing interest in optimizing parameters on Riemannian manifolds, with Riemannian adaptive methods showing faster convergence and lower train loss values in various applications. These adaptive algorithms are yet to find their respective Riemannian counterparts, despite their empirical success. Recent advancements in optimization algorithms have introduced successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD for optimizing parameters in a Euclidean space. However, there is a growing interest in optimizing parameters on Riemannian manifolds, with Riemannian adaptive methods showing faster convergence and lower train loss values in various applications. Despite their empirical success, these adaptive algorithms are yet to find their respective Riemannian counterparts. The text discusses the development of Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces, specifically in hyperbolic taxonomy embedding. The absence of Riemannian adaptive algorithms is seen as a hindrance to competitive optimization-based Riemannian embedding methods, particularly in the context of embedding methods in hyperbolic spaces. The text also touches on elementary notions of differential geometry. The recent rise of embedding methods in hyperbolic spaces could benefit from developments in competitive optimization-based Riemannian embedding methods. A manifold M of dimension n can be locally approximated by a Euclidean space R^n and is a generalization of the notion of surface. A Riemannian metric defines the geometry locally on M, with inner-products varying smoothly with x. A Riemannian metric \u03c1 defines the geometry locally on a manifold M, with inner-products varying smoothly with x. It induces a global distance function on M and allows for Riemannian SGD updates for smooth functions on the manifold. Riemannian SGD updates on a manifold involve using the gradient of the objective function and a step-size \u03b1. The update is performed using the exponential map or a retraction map when the former is not known. Algorithms like ADAGRAD and ADAM offer different update rules for optimizing learning. The ADAM update rule, proposed by BID9, includes momentum and adaptivity terms. When \u03b21 = 0, it is similar to RMSPROP but with an exponential moving average. AMSGRAD was introduced to fix a convergence issue in ADAM. The momentum term in ADAM for \u03b21 = 0 has shown significant empirical improvements. BID18 identified a mistake in ADAM's convergence proof and proposed AMSGRAD or ADAMNC as solutions. Coordinate-wise updates require a coordinate system, but on a Riemannian manifold, different charts can be defined around each point for working with local coordinate systems. The formalism allows working with local coordinate systems on a Riemannian manifold, with quantities defined intrinsically. The RSGD update is intrinsic, involving exp and grad. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free manner. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free manner, as parallel transport in a Riemannian manifold depends on the chosen path and curvature, which can break the sparsity of gradients and hinder adaptivity. The interpretation of adaptivity as optimizing different features at different speeds is lost when the coordinate system used for gradients depends on the optimization path. The text discusses the challenges of designing adaptive schemes in a general Riemannian manifold due to the absence of intrinsic coordinates. It proposes treating each component as a \"coordinate\" to simplify adaptation. In a general Riemannian manifold, designing adaptive schemes is challenging due to the absence of intrinsic coordinates. The proposal is to treat each component as a \"coordinate\" to simplify adaptation. Additionally, ADAM, ADAGRAD, and AMSGRAD were briefly discussed, with ADAM being a combination of ADAGRAD with momentum and AMSGRAD implementing a modification for convergence proof. ADAMNC is similar to ADAM but with a non-constant schedule for parameters. ADAMNC is a variant of ADAM with a non-constant schedule for parameters. It incorporates a modification proposed by BID18 for \u03b2 2, allowing it to recover the sum of squared-gradients of ADAGRAD. The assumptions and notations involve geodesically complete Riemannian manifolds with lower bounded sectional curvature. The projection operator \u03a0 Xi, parallel transport P i, exponential exp i, and log log i maps are defined in the context of feasible parameters on the product manifold. The text discusses the Riemannian AMSGRAD algorithm and its convergence guarantees, comparing it to standard AMSGRAD. It also mentions the derivation of RADAM and ADAM from these algorithms. The convergence guarantee for RAMSGRAD is presented in Theorem 1, defining the quantity \u03b6. The convergence guarantee for RAMSGRAD is presented in Theorem 1, defining the quantity \u03b6. Comparisons are made with AMSGRAD, showing that when certain conditions are met, the convergence guarantees coincide. Additionally, the convergence guarantee for RADAMNC is shown in Theorem 2, with specific conditions leading to a convergence proof for RADAGRAD. The convergence guarantees for RAMSGRAD and RADAMNC are presented in Theorems 1 and 2 respectively. The role of convexity is discussed, comparing convex and geodesically convex functions in the proofs. Regret bounds for convex objectives are derived using specific equations. The differentiable functions f : R n \u2192 R and g : M \u2192 R are convex and geodesically convex respectively. Regret bounds for convex objectives are obtained by bounding specific terms, with a focus on bounding g t , x t \u2212 x * using the cosine law. In Riemannian manifolds, this is generalized using lemma 6 for geodesically convex subsets with lower bounded sectional curvature. The curvature dependent quantity \u03b6 allows for bounding \u03c1. In Riemannian manifolds, lemma 6 is used to generalize regret bounds for convex objectives, with a focus on bounding specific terms like g t , x t \u2212 x * using the cosine law. The curvature dependent quantity \u03b6 from this lemma helps in bounding \u03c1. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed against the non-adaptive RSGD method. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed against the non-adaptive RSGD method using the Poincar\u00e9 model in hyperbolic geometry. The choice of this model is justified by the closed form expressions available for all quantities used in the algorithms. Riemannian gradients are rescaled Euclidean gradients, and various mathematical tools like distance functions, geodesics, exponential and logarithmic maps are utilized in the optimization process. The transitive closure of the WordNet taxonomy graph contains 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in Dn to minimize distances between connected words. The loss function used is similar to log-likelihood, with negative word pairs sampled for approximation. Metrics include loss value and mean average precision (MAP) for ranking distances between directed edges. The loss function used in the study is similar to log-likelihood, approximating the partition function with negative word pairs sampling. Metrics include loss value and mean average precision (MAP) for ranking distances between directed edges. Training details involve a \"burn-in phase\" for 20 epochs with fixed learning rate and sampling negative words based on their graph degree. During the \"burn-in phase\" for 20 epochs, negative words are sampled based on their graph degree raised at power 0.75, improving metrics. RADAM showed slightly better results than RAMS-GRAD. Replacing the true exponential map with its first-order approximation led to lower loss values. \"Retraction\"-based methods are reported separately from fully Riemannian analogues. Results for \"exponential\" and \"retraction\" based methods are shown in FIG2. Results for different optimization methods were compared in a study, with RADAM consistently achieving the lowest training loss. The study also showed that RADAM outperformed other methods on the MAP metric for both reconstruction and link prediction settings in the full Riemannian setting. Interestingly, in the \"retraction\" setting, RADAM reached the lowest training loss value and performed on par with other methods. After the introduction of Riemannian SGD by BID1, various first-order Riemannian methods emerged, including Riemannian SVRG, Riemannian Stein variational gradient descent, and Riemannian accelerated gradient descent. Stochastic gradient Langevin dynamics was also generalized to optimize on the probability simplex. Additionally, Riemannian counterparts of SGD with momentum and RMSprop were proposed by BID20. The text discusses new methods for convergence analysis in geodesically convex cases, including Riemannian counterparts of SGD with momentum and RMSprop. However, previous algorithms compromise convergence guarantees by performing coordinate-wise adaptive operations in the tangent space. Another version of Riemannian ADAM for the Grassmann manifold is introduced, but lacks adaptivity across manifolds and convergence analysis. The text proposes generalizing adaptive optimization tools for Riemannian manifolds, focusing on adaptivity across manifolds and providing convergence analysis. Experimental results show superiority over non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The text discusses the use of inequalities and lemmas to prove convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. It introduces Lemma 6, the Cosine inequality in Alexandrov spaces, and Lemma 7, an analogue of Cauchy-Schwarz. Lemma 8 (BID0) states the inequality for non-negative real numbers y1, ..., yt. It is used in the convergence proof for ADAMNC by BID18."
}