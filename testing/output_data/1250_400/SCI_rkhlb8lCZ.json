{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks have advanced 2D and 3D image classification. Wavelet Pooling is introduced as an alternative to traditional pooling methods, reducing feature dimensions and addressing overfitting. Experimental results show it outperforms other pooling methods. The proposed method outperforms traditional pooling methods like max, mean, mixed, and stochastic pooling in Convolutional Neural Networks (CNNs). CNNs are the standard in image and object classification, consistently achieving higher accuracy rates than vector-based deep learning techniques. Researchers constantly upgrade CNN components like the convolutional and pooling layers to improve accuracy and efficiency. Pooling, rooted in predecessors like Neocognitron and Cresceptron, reduces spatial dimensions, parameters, and overfitting in the network. Pooling in deep learning involves subsampling the results of convolutional layers to reduce spatial dimensions, parameters, and overfitting in the network. Popular methods include max pooling and average pooling, but they have weaknesses hindering optimal learning. Other approaches like mixed pooling and stochastic pooling use probabilistic methods to address these issues. However, all pooling operations employ a neighborhood approach similar to nearest neighbor interpolation in image processing, which can introduce artifacts like edge halos and blurring. Minimizing discontinuities in the data is crucial for network regularization. The text discusses different pooling operations in deep learning, focusing on a proposed wavelet pooling algorithm that aims to minimize artifacts and improve feature representation. The algorithm is compared to other pooling methods like max, mean, mixed, and stochastic pooling using various image classification datasets. The goal is to enhance network regularization and classification accuracy. The text discusses pooling operations in deep learning, comparing a proposed wavelet pooling algorithm to max and average pooling methods. Pooling involves condensing the output of convolutional layers by summarizing regions into one neuron value. Max pooling selects the maximum value of a region, while average pooling calculates the average value. The text explains max pooling and average pooling in deep learning. Max pooling selects the maximum value of a region, while average pooling calculates the average value. Both methods have their advantages and disadvantages, with max pooling potentially erasing details from an image and overfitting training data, while average pooling can dilute pertinent details. Researchers have developed probabilistic pooling methods like mixed pooling, which combines max and average pooling by randomly selecting one method during training. This method can be applied in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. Mixed pooling is represented by an equation where \u03bb is a random value indicating max or average pooling for a specific region/feature/layer. Another method, stochastic pooling, improves upon max pooling by randomly sampling from neighborhood regions based on probability values. Probabilistic pooling methods like mixed pooling combine max and average pooling by randomly selecting one method during training. Stochastic pooling improves upon max pooling by sampling from neighborhood regions based on probability values, avoiding the shortcomings of max and average pooling while enjoying some of the advantages of max pooling. The proposed wavelet pooling method reduces feature map dimensions using wavelets to minimize artifacts and improve image classification. It discards first-order subbands to capture data compression organically, reducing jagged edges and other artifacts. The method performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT) for efficient implementation. The proposed wavelet pooling scheme reduces feature map dimensions by performing a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT). This method captures data compression, minimizing artifacts and improving image classification. The proposed wavelet pooling scheme reduces feature map dimensions by performing a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT). This method captures data compression, minimizing artifacts and improving image classification. The algorithm involves obtaining detail subbands (LH, HL, HH) and an approximation subband (LL) at each decomposition level, reconstructing image features using 2nd order wavelet subbands, and performing backpropagation by reversing the forward propagation process. The proposed wavelet pooling scheme utilizes the Haar wavelet for 2nd order decomposition, reconstructing image features for backpropagation. Experiments are conducted using MatConvNet with stochastic gradient descent, a 64-bit operating system, Intel Core i7-6800k CPU, and GeForce Titan X Pascal GPUs. Different regularization techniques are tested on CIFAR-10 and SHVN datasets, with pooling methods applied exclusively for each network layer. The experiments involve testing different regularization techniques on CIFAR-10 and SHVN datasets by replacing Local Response Normalization with Batch Normalization and using Dropout BID22. The pooling methods are exclusively applied for each network layer, with the proposed method outperforming all others according to TAB0. Max pooling shows signs of overfitting, while mixed and stochastic pooling have a more stable trajectory. Average and wavelet pooling exhibit smoother learning curves. Our proposed method outperforms all others in the experiments on CIFAR-10 and SHVN datasets. Max pooling tends to overfit quickly, while mixed and stochastic pooling show a more stable trajectory. Average and wavelet pooling demonstrate smoother learning curves. The experiments include two sets, one without dropout layers and the other with dropout and batch normalization, with the latter showing improved accuracy. In experiments on CIFAR-10 and SHVN datasets, our proposed method outperforms others. Max pooling overfits quickly, while wavelet pooling resists overfitting. Different pooling methods show varying learning progressions. Two sets of experiments are run, one without dropout and the other with dropout and batch normalization. The SHVN dataset is used for experiments with and without dropout. The proposed method shows the second lowest accuracy. Different pooling methods exhibit varying learning progressions. The KDEF dataset is used for experiments on facial expressions and emotions. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions. Missing or corrupted images are fixed by mirroring counterparts in MATLAB. The dataset is split into 3,900 training images and 1,000 test images, resized to 128x128. Dropout layers help regulate the network and prevent overfitting. The proposed method achieves the second highest accuracy. The data is shuffled and split into 3,900 training images and 1,000 test images resized to 128x128. Dropout layers prevent overfitting, with wavelet pooling showing resistance to overfitting. Computational complexity is a concern for wavelet pooling, presented as a proof-of-concept with room for improvement in efficiency. Wavelet pooling implementation is not efficient, presented as a proof-of-concept with potential for improvement in computational efficiency. The code is not optimized, and accuracy results serve as a starting point for enhancements. Efficiency is measured in terms of mathematical operations for different pooling methods. Wavelet pooling implementation is inefficient, with potential for improvement in computational efficiency. Efficiency is measured in terms of mathematical operations for different pooling methods, with average pooling being the most computationally efficient. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. Despite this, with improvements in coding practices, GPUs, and the FTW algorithm, wavelet pooling shows potential to outperform traditional methods in CNNs. Our proposed method performs best in the MNIST dataset, competes well in CIFAR-10 and KDEF datasets, and shows respectable performance in the SHVN dataset. Our proposed method outperforms traditional methods in CNNs, excelling in the MNIST dataset and competing well in CIFAR-10 and KDEF datasets. It shows respectable performance in the SHVN dataset and responds well to network regularization with the addition of dropout and batch normalization. Future work could explore varying wavelet basis for pooling and adjusting upsampling and downsampling factors for better image feature reduction. Future work could involve varying wavelet basis for pooling, adjusting upsampling and downsampling factors for better image feature reduction, retaining discarded subbands for backpropagation, improving computational efficiency of the FTW method, and comparing the structural similarity of wavelet pooling with other methods to validate its effectiveness."
}