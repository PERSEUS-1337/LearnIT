{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires costly per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing models across different data domains without annotations, a domain adaptation method is proposed. This method focuses on learning discriminative feature representations of patches based on label histograms in the source domain, followed by an adversarial learning scheme to refine the features. The domain adaptation method aims to adapt source data to an unlabeled target domain by learning discriminative feature representations of patches based on label histograms. An adversarial learning scheme is used to align feature distributions between target and source patches, leading to improved performance in semantic segmentation tasks. The framework also incorporates a global alignment process and achieves state-of-the-art results on benchmark datasets. The framework utilizes a learning scheme to align feature distributions between target and source patches, improving semantic segmentation performance. It integrates a global alignment process and achieves state-of-the-art results on benchmark datasets, including synthetic-to-real and cross-city scenarios. Recent deep learning methods have shown progress in vision tasks like object recognition and semantic segmentation, relying on large-scale annotations for training. However, models often struggle to generalize to new domains. Domain adaptation methods aim to bridge this gap between annotated source domains and unlabeled target domains, with various techniques developed for image classification. Domain adaptation methods aim to bridge the gap between annotated training data and learned models that struggle to generalize well. These methods are crucial for pixel-level predictions, such as semantic segmentation, where the cost of annotating ground truth is expensive. Despite advancements in image classification, there is still significant room for improvement in domain adaptation for pixel-level prediction tasks. Domain adaptation is essential for pixel-level predictions like semantic segmentation due to the high cost of annotating ground truth. Existing methods use feature-level or output space adaptation to align distributions between source and target domains using adversarial learning. There is still significant room for improvement in this area. Cities may have different appearance distributions, and conditions can vary within the same city over time or weather. State-of-the-art methods use feature-level or output space adaptation to align distributions between source and target domains using adversarial learning. Global distribution alignment may not be sufficient due to differences in camera pose or field of view, leading to misalignment and potential bias during adaptation. Instead of relying on global distribution alignment, which may differ significantly between domains, the focus is on matching patches that are more likely to be shared across domains. Adversarial learning is used to align patch-level distributions, despite the high variation among patches. Instead of global alignment, focus on matching shared patches across domains. Use adversarial learning to align patch-level distributions, considering label histograms as a factor. The approach focuses on aligning patch-level distributions using label histograms as a factor, inspired by learning disentangled representations. The goal is to learn discriminative representations for patches to address the high-variation problem. The approach aims to align patch-level distributions by learning discriminative representations for source patches and pushing target representations close to the source distribution. Adversarial modules are used to align global and patch-level distributions between two domains. The approach involves using adversarial modules to align global and patch-level distributions between source and target domains. It utilizes learned representations to better align patches and addresses the high-variation problem. Pixel-level annotations are used to extract patch-level representations, which are then grouped into clusters using K-means clustering. The proposed alignment method involves using pixel-level annotations to extract patch-level representations, which are then grouped into clusters using K-means clustering. This clustering is used as ground truth to train a classifier for transferring discriminative patch representations from the source to the target domain. The proposed method involves clustering patch-level representations into K clusters, which are used as ground truth to train a classifier for transferring learned discriminative representations from the source to the target domain. An adversarial loss is used to align the feature representations of target patches with the distribution of source patches in the clustered space. This representation learning is guided by the label histogram and differs from methods using pre-defined factors like object pose. In experiments, the method uses an adversarial loss to align feature representations of target patches with the distribution of source patches in a clustered space. The representation learning is guided by the label histogram and differs from methods using pre-defined factors like object pose. The experiments include synthetic-to-real and cross-city scenarios for pixel-level road-scene image segmentation. The experiments involve domain adaptation for pixel-level road-scene image segmentation using pre-defined factors like object pose. Various settings are explored, including synthetic-to-real and cross-city scenarios. Extensive ablation studies validate the proposed framework, showing favorable performance compared to state-of-the-art methods in accuracy and visual quality. The framework is versatile and can be applied to other structured forms. The proposed framework for domain adaptation in structured output prediction combines global and patch-level alignments, outperforming state-of-the-art methods in accuracy and visual quality. The framework is versatile and can be applied to other structured outputs such as depth. The contributions of this work include proposing a domain adaptation framework for structured output prediction using global and patch-level adversarial learning modules. Additionally, a method to learn discriminative representations guided by label histograms of patches is developed, showing improved patch-level alignment. The adaptation method outperforms various baselines and state-of-the-art methods on semantic segmentation. The proposed domain adaptation framework for structured output prediction utilizes discriminative representations guided by label histograms of patches, improving patch-level alignment. The method outperforms various baselines and state-of-the-art methods on semantic segmentation tasks. Domain adaptation approaches align feature distributions between source and target domains for image classification. Domain adaptation methods for image classification and pixel-level prediction tasks are discussed, focusing on learning disentangled representations. These approaches align feature distributions between source and target domains, utilizing hand-crafted features and deep architectures to minimize domain discrepancies. Adversarial learning schemes and Maximum Mean Discrepancy are commonly used to achieve domain-invariant features. Domain adaptation methods aim to align feature distributions between source and target domains. Various techniques such as hand-crafted features, deep architectures, adversarial learning schemes, and Maximum Mean Discrepancy are utilized to learn domain-invariant features. Recent work also focuses on enhancing feature representations through pixel-level transfer and domain separation. Unlike image classification, domain adaptation for structured pixel-level predictions is less explored. Domain adaptation for structured pixel-level predictions involves developing variants with different classifiers and loss functions. Recent work focuses on enhancing feature representations through pixel-level transfer and domain separation. This approach has not been widely studied compared to image classification tasks. The domain adaptation problem on semantic segmentation for road-scene images has been tackled by using adversarial networks to align global feature representations across domains. Additionally, a category-specific prior is extracted from the source domain. The CDA method BID36 applies SVM classifier to capture label distributions on superpixels for training the adapted model on semantic segmentation for road-scene images. The CDA method BID36 uses SVM classifier to capture label distributions on superpixels for training the adapted model on semantic segmentation for road-scene images. In addition, class-wise domain adversarial alignment is performed by assigning pseudo labels to the target data, and an object prior from Google Street View is used to help alignment for static objects in domain adaptation methods on structured output. In contrast to previous domain adaptation methods, our framework focuses on learning discriminative representations for patches to aid in patch-level alignment, preserving structured information. Our framework focuses on learning discriminative representations for patches to aid in patch-level alignment, preserving structured information without the need for additional priors/annotations. Our framework focuses on learning patch-level representations to aid in alignment without the need for additional priors/annotations. It emphasizes learning a latent disentangled space for better understanding in tasks like facial recognition, image generation, and view synthesis. Learning a latent disentangled space has led to a better understanding for tasks such as facial recognition, image generation, and view synthesis. Approaches use pre-defined factors to learn interpretable representations of the image, such as graphic codes disentangled with respect to image transformations like pose and lighting. Generative adversarial networks (GANs) have also been developed for synthesizing 3D objects from a single image. BID18 and BID35 focus on learning graphic codes and synthesizing 3D objects from images by disentangling factors like pose and rotation. AC-GAN BID24 uses a GAN with an auxiliary classifier to condition on image labels and attributes. These methods aim to learn a disentangled space for the target task but are limited to a single domain. In response to previous research on disentangling factors like pose and rotation, a new domain adaptation framework is proposed. This framework utilizes label distributions as a disentangled factor without the need for pre-defined factors. The proposed domain adaptation framework aims to learn discriminative representations for patches by utilizing label distributions as a disentangled factor. This framework does not require pre-defined factors and includes an adversarial learning scheme to align distributions across domains. The goal is to align the predicted output distribution of target data using source and target images and labels. Our proposed domain adaptation framework aims to align the predicted output distribution of target data with the source distribution by utilizing discriminative representations for patches and an adversarial learning scheme. This involves a loss function for supervised learning on the source data and an adversarial loss to align global distributions. Additionally, a classification loss in a clustered space is incorporated to learn patch-level information. The proposed domain adaptation framework aligns the predicted output distribution of target data with the source distribution using discriminative representations for patches and an adversarial learning scheme. It involves a loss function for supervised learning on the source data, an adversarial loss to align global distributions, and a classification loss in a clustered space to learn patch-level information. Additionally, an adversarial loss is employed to align patch-level distributions between the source and target data. The adaptation framework aligns target data output distribution with the source using discriminative representations for patches and adversarial learning. It includes supervised learning loss on the source data, an adversarial loss for global distribution alignment, and a classification loss for patch-level information. An additional adversarial loss aligns patch-level distributions between the source and target data. The adaptation framework aligns target data output distribution with the source using discriminative representations for patches and adversarial learning. It includes supervised learning loss on the source data, an adversarial loss for global distribution alignment, and a classification loss for patch-level information. Additionally, there is an adversarial loss to align patch-level distributions between the source and target data. The adaptation task involves various loss functions, including supervised loss for structured prediction and discriminative representation on source data, clustering process on ground truth label distribution, global and patch-level adversarial loss functions, and weights for different loss functions. The baseline model and proposed framework details are described in the following sections. Figure 3 illustrates the main components and loss functions of the method. The target distribution is aligned using global and patch-level adversarial loss functions, denoted as L g adv and L l adv, with weights \u03bb for different loss functions. The baseline model includes a supervised cross-entropy loss L s and an output space adaptation module using L g adv for global alignment. The loss L s is optimized by a fully-convolutional network G for structured output prediction. The method involves a baseline model with a supervised cross-entropy loss and an output space adaptation module using global alignment. The loss is optimized by a fully-convolutional network for structured output prediction. An adversarial loss is also used following GAN training principles. The method includes an adversarial loss Lg adv optimized through GAN training principles, involving a generator G and discriminator Dg for binary classification. The min-max problem is then optimized for G and Dg. The method involves optimizing a min-max problem for a generator G and discriminator Dg through GAN training principles. It proposes patch-level domain alignment to find transferable structured output representations shared across source and target images from smaller patches. The method involves optimizing a min-max problem for a generator G and discriminator Dg through GAN training principles. It proposes patch-level domain alignment to find transferable structured output representations shared across source and target images from smaller patches. The proposed network architecture consists of a generator G and a categorization module H for learning discriminative patch representations. The proposed method involves clustering patches from source-domain examples to create prototypical patch patterns, which are then used to guide patches from the target domain to adapt to the source patch representations. This process is achieved through an adversarial objective, aiming to align structured output representations shared across source and target images at the patch level. The proposed method involves clustering patches from source-domain examples to create prototypical patch patterns. Patches from the target domain adapt to this disentangled space of source patch representations by selecting the closest cluster via adversarial objective. Learning discriminative representations in a disentangled space is challenging without class labels, but the method aims to guide patches for alignment. The proposed method involves creating prototypical patch patterns by clustering source-domain patches. Target domain patches align with the disentangled space of source patches via adversarial objective, utilizing per-pixel annotations for semantically meaningful patch representations. In this work, per-pixel annotations in the source domain are used to construct a semantically disentangled space of patch representations by creating label histograms for patches. Randomly sampled patches are used to extract spatial label histograms, which are then concatenated into a vector for K-means clustering. The text discusses creating a disentangled space of patch representations using label histograms. Patches are randomly sampled from source images, spatial label histograms are extracted, and K-means clustering is applied. A classification module is added to the network to incorporate the clustered space during training. We apply K-means clustering on histograms of patches to assign labels based on closest distance. A classification module is added to the network to simulate label histogram construction and learn a discriminative representation. The learned representation F_s corresponds to patches on the input image spatial map, with group labels assigned accordingly. The learning process involves constructing a clustered space using a cross-entropy loss. Patch-level Adversarial Alignment is then used to align target patch representations with the clustered space from the source domain, utilizing an adversarial loss between F_s and F_t. The learning process involves aligning target patch representations with a clustered space using Patch-level Adversarial Alignment and an adversarial loss between F_s and F_t. The goal is to align patches regardless of their location in the image, reshaping F by concatenating K-dimensional vectors along the spatial map. The goal is to align patches in the image regardless of their location, reshaping F by concatenating K-dimensional vectors along the spatial map. This reshaped data is denoted as F and an adversarial objective is formulated using a discriminator to classify if the feature representation is from the source or target domain. The min-max problem is integrated into the formulation. By using a convolution layer with proper stride and kernel size, reshaped data F is formulated for the adversarial objective. The discriminator Dl classifies if F is from the source or target domain. The optimization process involves updating discriminators Dg and Dl, and the network G and H alternately in a min-max problem. In network optimization, the training of a GAN involves alternating between updating the discriminators Dg and Dl, and the network G and H. The discriminator Dg is trained to distinguish between source and target distributions, while the discriminator Dl classifies feature representations from the source or target domain. The goal of this step is to push the target distribution closer to the source distribution using the optimized discriminators Dg and Dl, while maintaining good performance on the main tasks using the network G and H. This involves minimizing the binary cross-entropy loss and combining two supervised loss functions. The goal is to update the Network G and H to align the target distribution with the source distribution using optimized discriminators Dg and Dl. This involves combining supervised loss functions and adversarial loss functions to enhance feature representations in G. The text discusses the combination of supervised and adversarial loss functions to update networks G and H for aligning target distribution with the source distribution. The discriminator Dg uses fully-convolutional layers with 5 convolution layers. The feature representations are enhanced in G during the testing phase, with no impact on runtime compared to the baseline approach. The discriminator Dg uses fully-convolutional layers with 5 convolution layers, while the discriminator Dl utilizes 3 fully-connected layers. The generator is also mentioned in the text. The generator G includes a categorization module H based on DeepLab-v2 with ResNet-101 architecture pre-trained on ImageNet. Leaky ReLU activation is used in the discriminator Dl with 3 fully-connected layers. The generator G includes a categorization module H based on DeepLab-v2 with ResNet-101 architecture pre-trained on ImageNet. The module H adds spatial mapping and convolution layers to produce a feature map F with channel number K. The proposed architecture is illustrated in Figure 3. The proposed architecture includes an adaptive average pooling layer to generate a spatial map with a desired receptive field. This map is then fed into two convolution layers to produce a feature map F with channel number K. Implementation details involve using the PyTorch toolbox on a single Titan X GPU for training with specific optimizer settings. The proposed framework is implemented using PyTorch on a single Titan X GPU. The discriminators are trained with the Adam optimizer, while the generator uses Stochastic Gradient Descent. Learning rates are decreased using polynomial decay during training. The momentum is 0.9, weight decay is 5 \u00d7 10 \u22124, and initial learning rate is 2.5 \u00d7 10 \u22124. Ablation study on GTA5-to-Cityscapes using ResNet-101 network with corresponding loss functions. Learning rates are decreased using polynomial decay with a power of 0.9. Hyper-parameters such as image and patch sizes are provided. The proposed framework for domain adaptation on semantic segmentation is evaluated using GTA5-toCityscapes scenario. The model is trained with specific hyper-parameters and loss functions for 100K iterations. Ablation study validates each component, showing the method outperforms state-of-the-art approaches. The proposed framework for domain adaptation on semantic segmentation is evaluated through an extensive ablation study on the GTA5-toCityscapes scenario. The method demonstrates superior performance compared to state-of-the-art approaches on various benchmark datasets and settings, including synthetic-to-real and cross-city scenarios. The domain adaptation method is evaluated on semantic segmentation with synthetic-to-real and cross-city scenarios, adapting datasets like GTA5 BID27 to Cityscapes BID5 and Cityscapes sunny images to Oxford RobotCar BID23. The SYNTHIA BID28 dataset is used with a larger domain gap compared to Cityscapes images. Cityscapes sunny images are adapted to the Oxford RobotCar BID23 dataset containing rainy scenes. 10 sequences in the Oxford RobotCar dataset are manually selected, with 7 for training and 3 for testing. 895 images are sampled for training, and 271 images are annotated for per-pixel semantic segmentation ground truth evaluation. In the Oxford RobotCar dataset, 10 sequences with rainy scenes are selected for training and testing. 895 images are used for training, and 271 images are annotated for per-pixel semantic segmentation evaluation. Intersection-over-union (IoU) ratio is used as the metric for evaluation in experiments. Ablation study on GTA5-to-Cityscapes scenario is conducted to analyze the impact of different loss functions and design choices. In Table 1, an ablation study on the GTA5-to-Cityscapes scenario evaluates different loss functions and design choices using intersection-over-union (IoU) ratio as the metric. Adding disentanglement without alignments improves performance, demonstrating enhanced feature representation. The proposed framework in Table 1 shows different steps including disentanglement, global alignment, and patch-level alignment. Adding disentanglement alone improves performance, demonstrating enhanced feature representation. Combining global and patch-level alignments achieves the highest IoU at 43.2%. Our method combines global and patch-level alignments to achieve the highest IoU at 43.2%. Experiments show that both losses, Ld and Lladv, are necessary for effective patch-level alignment, with a performance loss of 1.9% and 1.5% if either is removed. Our method combines global and patch-level alignments to achieve the highest IoU at 43.2%. Both losses, Ld and Lladv, are crucial for effective patch-level alignment, with a performance loss of 1.9% and 1.5% if either is removed. The reason behind this is that Ld constructs a clustered space for Lladv to perform patch-level alignment effectively. Without reshaping the features as independent data points, the performance is impacted. Without ReshapedF, features are reshaped as independent data points in the clustered space for patch-level alignment. Performance drops 2.4% in IoU without reshaping. Patches with similar representations should be aligned regardless of their locations. Visualization in FIG1 shows t-SNE visualization of patch-level features in the clustered space. Without reshaping, performance drops 2.4% in IoU. Patches with similar representations should be aligned regardless of their locations. Visualization in FIG1 shows t-SNE visualization of patch-level features in the clustered space, highlighting the effectiveness of adaptation. The clustered space adaptation aligns features into groups, improving source/target overlap. Comparison with state-of-the-art algorithms in various scenarios shows favorable performance. Example patch visualizations are provided in the appendix. In adapting GTA5 to Cityscapes, experimental results show that the proposed method outperforms state-of-the-art adaptations in feature, pixel-level, and output space alignments. Using the ResNet-101 base network, the method improves IoU by 1.8% and achieves the best IoU in 14 out of 19 categories. The proposed method improves IoU by 1.8% and achieves the best IoU in 14 out of 19 categories through feature and output space adaptations. Results for adapting SYNTHIA to Cityscapes show similar improvements compared to state-of-the-art methods. Visual comparisons are shown in Figure 5. In TAB2, results for adapting SYNTHIA to Cityscapes show improvements compared to state-of-the-art methods. Visual comparisons are presented in Figure 5. Adapting between real images across different cities and conditions is demonstrated by adapting Cityscapes to Oxford RobotCar, showcasing the proposed method's effectiveness. The proposed method demonstrates domain adaptation for structured output, showing improved segmentation details compared to existing methods. Results show a mean IoU of 63.6%, 1.4% higher than the baseline. Further comparisons and results are available in the appendix. The paper presents a domain adaptation method for structured output, combining global and patch-level alignments to improve segmentation details. The proposed method achieves a mean IoU of 63.6%, 1.4% higher than the baseline. Further results and comparisons are available in the appendix. The proposed method combines global and patch-level alignments for structured output, improving segmentation details. It involves output space adaptation and learning discriminative patch representations across domains. Extensive experiments validate its effectiveness in semantic segmentation challenges, showing favorable performance compared to baseline methods. The proposed method combines global and patch-level alignments for structured output in semantic segmentation. It involves output space adaptation and learning discriminative patch representations across domains, validated through extensive experiments. The approach utilizes an adversarial learning scheme to align target patch distributions with source ones, showing favorable performance against existing algorithms. The approach involves training the model in an end-to-end manner by sampling one image from each domain in a training iteration. The image and patch sizes are shown in TAB3 for training and testing. BID12 can be used as a loss in the model to push the target feature representation to one of the source clusters, replacing the adversarial loss with an entropy loss on the patch level. The model is trained by sampling one image from each domain in an end-to-end manner. The image is down-sampled to a specific size without cropping. BID12 can be used as a loss to push the target feature representation to a source cluster. An entropy loss is used on the patch level instead of adversarial loss. The model with entropy regularization achieves an IoU of 41.9%, lower than the patch-level adversarial alignment at 43.2%. The model achieves an IoU of 41.9% with entropy regularization, lower than the patch-level adversarial alignment at 43.2%. Our model learns discriminative representations for target patches by pushing them closer to the source distribution in the clustered space guided by the label histogram. Our model learns discriminative representations for target patches by aligning them with the source distribution in a clustered space guided by the label histogram. This patch-level alignment is demonstrated through high similarity between source and target patches in the t-SNE visualization. The complete results for adapting Cityscapes to Oxford RobotCar are presented in TAB4. The effectiveness of patch-level alignment is demonstrated in the clustered space via t-SNE visualization. Results for adapting Cityscapes to Oxford RobotCar are compared with models without adaptation and output space adaptation approach BID31. Visual comparisons for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes scenarios are provided in figures 9 to 11. The proposed method is compared with models without adaptation and output space adaptation BID31 in adapting Cityscapes to Oxford RobotCar. Visual comparisons for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes scenarios are shown in figures 9 to 11, demonstrating that the proposed approach yields better segmentation outputs with more details and less noise. The proposed method often produces better segmentation outputs with more details and less noise compared to models without adaptation and output space adaptation BID31. Results of adapted segmentation for the GTA5-to-Cityscapes setting are shown in figures 9 and 10. The proposed method improves segmentation outputs compared to models without adaptation and output space adaptation BID31. Results for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes settings are shown in figures 9, 10, and 11. The proposed method enhances segmentation results in GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes settings by showing improved outputs compared to models without adaptation and output space adaptation BID31."
}