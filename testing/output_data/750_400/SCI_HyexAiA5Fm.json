{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models successful in modeling high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) using the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the cost-effective transfer of a source measure to a target measure. The formulation is theoretically justified and an algorithm based on stochastic alternating gradient updates is proposed for solving the problem efficiently. The paper introduces a scalable method for unbalanced optimal transport using the generative-adversarial framework. It involves learning a transport map and a scaling factor to optimize the cost-effective transfer of a source measure to a target measure. The algorithm is based on stochastic alternating gradient updates and is demonstrated through numerical experiments in population modeling. The paper presents a scalable method for unbalanced optimal transport using a generative-adversarial framework. It involves learning a transport map and scaling factor for cost-effective transfer of measures. The methodology is demonstrated through numerical experiments in population modeling, focusing on mass variation and transport. The paper discusses modeling mass transport in population modeling using optimal transport methods based on the Kantorovich formulation, which seeks optimal probabilistic coupling between measures. Optimal transport methods aim to push a source distribution to a target distribution efficiently, with recent advancements using the Kantorovich formulation and entropy regularization for improved solving using the Sinkhorn algorithm. Stochastic methods have also been proposed for continuous settings, with applications in various fields. Optimal transport methods, including entropy regularization, have been used to efficiently push a source distribution to a target distribution. Stochastic methods based on the dual objective have been proposed for continuous settings, with applications in various fields such as computer graphics and domain adaptation. Transport maps can also be learned using generative models like GANs to push distributions towards each other. Optimal transport methods, including entropy regularization, have been applied to various areas such as computer graphics and domain adaptation. Transport maps can be learned using generative models like GANs to push distributions towards each other in applications like image translation, natural language translation, domain adaptation, and biological data integration. Training against an adversary in various applications such as image translation, natural language translation, domain adaptation, and biological data integration involves learning a transport map to push source populations towards target populations. One sub-population is growing more rapidly than the others. Monge-like formulations of unbalanced optimal transport aim to learn a transport map and scaling factor to push the source to the target, using deterministic or stochastic transport maps. Previous methods using GANs have focused on enforcing correspondence between original and transported samples, but struggle to handle mass variation. Several methods have been developed to address mass variation in optimal transport problems, including variants of GANs and scaling algorithms that extend the Sinkhorn algorithm for balanced OT. These methods aim to enforce correspondence between original and transported samples while handling unbalanced masses. A class of scaling algorithms has been proposed to extend optimal transport theory to handle unbalanced masses. These algorithms, which generalize the Sinkhorn algorithm, are used to approximate solutions to optimal entropy-transport problems. They relax hard marginal constraints using divergences to allow for mass variation, and have been applied in various fields such as computer graphics, tumor growth modeling, and computational biology. The formulation of unbalanced optimal transport by BID27 corresponds to the Kantorovich optimal transport problem, relaxing hard marginal constraints using divergences to allow for mass variation. These algorithms have been used in computer graphics, tumor growth modeling, and computational biology. However, current methods cannot perform unbalanced optimal transport between continuous measures. Inspired by GANs, a novel framework for unbalanced optimal transport is presented. The curr_chunk discusses a novel framework for unbalanced optimal transport that directly models mass variation in addition to transport. It proposes solving a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport between source and target measures. The novel framework for unbalanced optimal transport directly models mass variation and proposes solving a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport between source and target measures. The text discusses a novel framework for unbalanced optimal transport, generalizing the Monge OT problem and deriving a scalable methodology for solving it. The methodology is applied to population modeling using various datasets, showcasing its practical utility. The methodology proposed in the text offers a novel framework for unbalanced optimal transport, demonstrated through population modeling with various datasets. Additionally, a new scalable method is introduced for solving the optimal-entropy transport problem in the continuous setting. In addition to the proposed methodology for unbalanced optimal transport, a new scalable method (Algorithm 2) is introduced in the Appendix for solving the optimal-entropy transport problem in the continuous setting. This algorithm extends previous work and provides a scalable alternative for large or continuous datasets. Optimal transport (OT) deals with transporting measures in a cost-optimal way. Monge formulated this as a search for deterministic transport maps.\u03c0 X # \u03b3 and \u03c0 Y # \u03b3 are marginals of a joint measure \u03b3 in M + (X \u00d7 Y). Optimal transport (OT) involves cost-optimal transportation between measures. Monge formulated the problem as a search for deterministic transport maps, while Kantorovich OT problem is a convex relaxation that considers probabilistic transport plans. Marginals of a joint measure \u03b3 in M + (X \u00d7 Y) are \u03c0 X # \u03b3 and \u03c0 Y # \u03b3. The Kantorovich OT problem is a convex relaxation of the Monge problem, formulating OT as a search over probabilistic transport plans. Conditional probability distributions \u03b3 y|x specify stochastic maps from X to Y, a \"one-to-many\" version of the deterministic map from the Monge problem. The relaxed problem is a linear program that is always feasible and can be solved in O(n^3) time for discrete \u00b5, \u03bd. The conditional probability distributions \u03b3 y|x specify stochastic maps from X to Y, a \"one-to-many\" version of the deterministic map from the Monge problem. Introducing entropic regularization simplifies the dual optimization problem, solvable efficiently with the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans handling continuous measures. Introducing entropic regularization simplifies the dual optimization problem, leading to efficient solutions using the Sinkhorn algorithm. Stochastic algorithms have been developed for computing transport plans that can handle continuous measures, extending classical optimal transport formulations. Several formulations extend classical optimal transport to handle mass variation, with existing numerical methods based on optimal-entropy transport. This formulation relaxes marginal constraints using divergences to find a measure that minimizes a cost function, allowing for mass variation in the marginals. Optimal entropy-transport minimizes a cost function with measures \u00b5 and \u03bd, finding a measure \u03b3 that allows for mass variation. State-of-the-art algorithms exist for discrete settings, but unbalanced OT between continuous measures in high-dimensional spaces lacks practical algorithms. In the discrete setting, iterative scaling algorithms like BID8 generalize the Sinkhorn algorithm for computing regularized OT plans. However, there are no practical algorithms for unbalanced OT between continuous measures, especially in high-dimensional spaces. A new algorithm is proposed in this section that directly models mass variation for unbalanced OT and can be applied to transport between high-dimensional continuous measures. The goal is to learn a stochastic transport map and scaling factor to push a source to a target. The algorithm proposed in this section directly models mass variation for unbalanced optimal transport (OT) between high-dimensional continuous measures. It aims to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. The algorithm aims to learn a stochastic transport map and scaling factor for unbalanced optimal transport between continuous measures, penalizing mass transport and variation while ensuring exact pushing of the source to the target measure. The algorithm aims to learn a stochastic transport map and scaling factor for unbalanced optimal transport between continuous measures, penalizing mass transport and variation while ensuring exact pushing of the source to the target measure. The transport map T : X \u00d7 Z \u2192 Y and scaling factor \u03be are subject to the constraint T # (\u03be\u00b5 \u00d7 \u03bb) = \u03bd, with a focus on stochastic maps for practical applications like cell biology. The algorithm aims to learn a stochastic transport map and scaling factor for unbalanced optimal transport between continuous measures, focusing on practical applications like cell biology. The problem considers stochastic maps as a more suitable model for many practical problems, such as cell biology, where one cell in a source population can give rise to multiple cells in a target population. The algorithm focuses on learning a stochastic transport map and scaling factor for unbalanced optimal transport between continuous measures, with practical applications in cell biology. It models the transformation from a source measure to a target measure, representing a population at two distinct time points, using the transport map T and scaling factor \u03be. Different transformation models are optimal based on relative costs. The transport map T models the movement of points from a source measure to a target measure, with a scaling factor \u03be representing growth or shrinkage. Different transformation models are optimal based on costs. The transport map T moves points from a source measure to a target measure, with a scaling factor to address class imbalances. The scaling factor adjusts sample weights to balance classes with the target distribution. An unbalanced transport map with a scaling factor can address class imbalances by adjusting sample weights. The optimization challenge lies in satisfying the constraint T # (\u03be\u00b5 \u00d7 \u03bb) = \u03bd, leading to a relaxation using a divergence penalty instead. This relaxation is a Monge-like version of the optimal-entropy transport problem. The relaxation of the optimization problem involves using a divergence penalty instead of an equality constraint to address class imbalances. This approach is a Monge-like version of the optimal-entropy transport problem. The optimization problem involves using a divergence penalty instead of an equality constraint to address class imbalances in the Monge-like version of the optimal-entropy transport problem. The objective function for optimal-entropy transport is obtained by reformulating in terms of a joint measure \u03b3 \u2208 M + (X \u00d7 Y), with a difference in search space due to restrictions on the joint measures specified by (T, \u03be). The optimization problem for optimal-entropy transport involves using a divergence penalty instead of an equality constraint to address class imbalances. The joint measure \u03b3 is restricted by (T, \u03be) to a set of deterministic couplings. Equivalence is established by restricting the support of \u03b3 to supp(\u00b5) \u00d7 Y. The scaling factor \u03be allows mass to grow within the support of \u00b5, but not materialize outside of it. Equivalence is established by restricting the support of \u03b3 to supp(\u00b5) \u00d7 Y. Theorem states that solutions of the relaxed problem converge to solutions of the optimal entropy-transport. Based on the relation between (3) and (6), theoretical results for (6) follow from optimal entropy-transport analysis by BID27. For an appropriate divergence penalty, solutions of (6) converge to solutions of the original problem (5). Theorem 3.4 states conditions for convergence with unique minimization of \u03c8(1) = 0. Based on optimal entropy-transport analysis, solutions of problem (6) converge to solutions of the original problem (5). Theorem 3.4 provides conditions for convergence with unique minimization of \u03c8(1) = 0. The transport map and scaling factor can be learned using stochastic gradient methods with the relaxation of unbalanced Monge OT in (6). The joint measure specified by a minimizer of L converges weakly to \u03b3. The transport map and scaling factor can be learned using stochastic gradient methods with the relaxation of unbalanced Monge OT. The divergence term can be minimized using an adversary function f. The divergence term can be minimized using an adversary function f : Y \u2192 (\u2212\u221e, \u03c8 \u221e ] with the convex conjugate representation. The optimization procedure involves alternating stochastic gradient updates after parameterizing T, \u03be, and f with neural networks, similar to GAN training. The optimization procedure involves alternating stochastic gradient updates after parameterizing T, \u03be, and f with neural networks. The objective is to minimize the divergence between transported samples and real samples from \u03bd, measured by the adversary f. Cost functions c1 and c2 encourage finding the most cost-efficient strategy. The optimization procedure involves minimizing the divergence between transported samples and real samples from \u03bd using neural networks. Cost functions c1 and c2 encourage finding a cost-efficient strategy. The update process includes gradient descent on parameters \u03b8, \u03c6, \u03c9. Further practical considerations for implementation and training are discussed. The optimization procedure involves minimizing divergence using neural networks and gradient descent on parameters \u03b8, \u03c6, \u03c9. Examples of divergences and entropy functions are provided in Table 1. The probabilistic Monge-like formulation is compared to the Kantorovich-like entropy-transport problem. Algorithm 1 solves the non-convex formulation and learns a transport map T and scaling factor \u03be. The probabilistic Monge-like formulation (6) is similar to the Kantorovich-like entropy-transport problem (3) in theory, but they result in different numerical methods in practice. Algorithm 1 solves the non-convex formulation (6) using neural networks to learn a transport map T and scaling factor \u03be, enabling scalable optimization with stochastic gradient descent. The networks are immediately useful for practical applications, requiring only a single forward pass to compute transport and scaling from the source domain to the target. The neural networks parameterize the transport map T and scaling factor \u03be, allowing scalable optimization with stochastic gradient descent. The non-convex optimization problem may not guarantee finding the global optimum, unlike the convex scaling algorithm of BID8. The neural architectures of T, \u03be imbue their function classes with a particular structure, enabling effective learning in high-dimensional settings. Algorithm 1 may not find the global optimum due to non-convexity, while the scaling algorithm of BID8 solves a convex optimization problem but is limited in scalability. A new stochastic method in the Appendix, based on the same dual objective as BID8, can handle transport between continuous measures. The new stochastic method in the Appendix, based on the same dual objective as BID8, can handle transport between continuous measures and overcomes the scalability limitations of BID8. However, the output is in the form of the dual solution, which is less interpretable for practical applications compared to the output of Algorithm 1. The new stochastic method in the Appendix generalizes the approach for handling transport between continuous measures and overcomes the scalability limitations of BID8. However, the output is in the form of the dual solution, which is less interpretable for practical applications compared to the output of Algorithm 1. In the numerical experiments of Section 4, the advantage of directly learning a transport map and scaling factor using Algorithm 1 is demonstrated. In Section 4, the advantage of directly learning a transport map and scaling factor using Algorithm 1 is demonstrated. The problem of learning a scaling factor that balances measures \u00b5 and \u03bd arises in causal inference, where \u00b5 represents the distribution of covariates from a control population and \u03bd represents the distribution from a treated population. In causal inference, the goal is to scale the importance of different members from a control population based on their likelihood to be present in a treated population, to eliminate selection biases in treatment effect inference. BID23 proposed a generative-adversarial method for learning the scaling factor, but did not consider transport. In causal inference, the goal is to scale the importance of different members from a control population based on their likelihood to be present in a treated population, to eliminate selection biases in treatment effect inference. BID23 proposed a generative-adversarial method for learning the scaling factor, but did not consider transport. Figure 3 illustrates Algorithm 1 performing unbalanced optimal transport on MNIST data to eliminate selection biases in treatment effect inference. Algorithm 1 is used to perform unbalanced optimal transport between modified MNIST datasets, simulating class imbalance in the target population. The target dataset consists of regular or dimmed MNIST digits with a class distribution imbalance, simulating a scenario where certain classes become more popular. Algorithm 1 is used to transport the source distribution to the target distribution with a high cost of transport. The scaling factor over each digit class reflects its ratio. Algorithm 1 was evaluated on transporting the source distribution to the target distribution with a high cost of transport, showing that the scaling factor reflects class imbalances. This factor can model growth or decline of different classes in a population, as illustrated in FIG4. The scaling factor learned by Algorithm 1 reflects class imbalances and can model growth or decline of different classes in a population. FIG4 illustrates the reweighting during unbalanced OT, applied from the MNIST dataset to the USPS dataset to model evolution between the two distributions. The unbalanced OT from MNIST to USPS is modeled using Algorithm 1, with transport cost as the Euclidean distance between original and transported images. The evolution of the MNIST distribution to USPS is visualized in FIG1, showing predicted appearances of images in USPS dataset. Arrows represent real MNIST images scaled based on class imbalances. The unbalanced OT model transports MNIST digits to USPS dataset based on Euclidean distance as transport cost. Arrows in FIG1 show how MNIST images evolve in USPS, with some digits maintaining their likeness during the transport. The unbalanced OT model transports MNIST digits to USPS dataset based on Euclidean distance as transport cost. MNIST digits were able to preserve their likeness during the transport, with higher scaling factors resulting in brighter digits covering a larger area of pixels. The model analyzed MNIST digits based on scaling factors, with higher factors resulting in brighter digits covering more pixels. Algorithm 1 was applied to the CelebA dataset for unbalanced OT from young to aged faces, simulating population transformation over time. The USPS digits are brighter and have more pixels compared to MNIST digits. Algorithm 1 was used on the CelebA dataset to perform unbalanced optimal transport from young to aged faces, mimicking population transformation over time. A variational autoencoder was trained on the CelebA dataset to encode samples into a latent space before applying Algorithm 1 for the transport. The study utilized a variational autoencoder on the CelebA dataset to encode faces into a latent space before applying Algorithm 1 for unbalanced optimal transport from young to aged faces. The transported faces maintained key features from the original faces. The study used a variational autoencoder on the CelebA dataset to encode faces into a latent space and applied Algorithm 1 for unbalanced optimal transport from young to aged faces. The transported faces retained key features, with exceptions like gender swaps. Young faces with higher scaling factors were more likely to be male. The study applied a variational autoencoder on the CelebA dataset to encode faces into a latent space and used Algorithm 1 for unbalanced optimal transport from young to aged faces. Young faces with higher scaling factors were significantly enriched for males compared to females, predicting a growth in the prominence of male faces as the population ages. Our model predicts a growth in the prominence of male faces compared to female faces as the CelebA population evolves from young to aged. There is a strong gender imbalance between the young and aged populations, with the young population predominantly female and the aged population predominantly male. The ground truth labels confirm a strong gender imbalance between young and aged populations, with young being predominantly female and aged being predominantly male. Lineage tracing of cells in zebrafish embryogenesis shows unbalanced distributions, highlighting the relevance of learning scaling factors in single-cell gene expression data analysis. During disease progression, transport is applied to unbalanced source and target distributions of cells. Algorithm 1 is used on zebrafish embryogenesis gene expression data from blastulation to gastrulation stages. Results are visualized after dimensionality reduction, showing cells with higher scaling factors from the blastula stage. During zebrafish embryogenesis, cells from blastula and gastrulation stages were analyzed using transport. Cells with higher scaling factors from blastula were found to have upregulated genes associated with mesoderm development. Using differential gene expression analysis, cells with higher scaling factors were found to be enriched for genes associated with mesoderm development. This demonstrates the potential for biological discovery through analysis of scaling factors. Additionally, a stochastic method for unbalanced OT based on the regularized dual formulation of BID7 is presented. The experiment demonstrated that analyzing scaling factors can lead to significant biological discoveries, particularly in the context of mesoderm development. A stochastic method for unbalanced optimal transport based on the regularized dual formulation of BID7 is introduced, which addresses a constrained optimization problem by adding a strongly convex regularization term to the primal objective. The dual formulation of FORMULA2 is given by DISPLAYFORM0 with a supremum over functions u : DISPLAYFORM1. To make the problem unconstrained, a strongly convex regularization term like an entropic term BID13 : DISPLAYFORM2 is added to the primal objective. This term encourages high entropy transport plans. The dual of the regularized problem is given by DISPLAYFORM3 with a supremum over functions u : DISPLAYFORM4. The term BID13 : DISPLAYFORM2 has a \"smoothing\" effect on the transport plan by encouraging high entropy plans. The dual of the regularized problem is given by DISPLAYFORM3 with a supremum over functions u : DISPLAYFORM4. The relationship between the primal optimizer \u03b3 * and dual optimizer (u * , v * ) is given by DISPLAYFORM5. The equation (9) is rewritten in terms of expectations, assuming access to samples from \u00b5, \u03bd, and normalized measures \u03bc, \u03bd, as well as normalization constants. This leads to DISPLAYFORM6. The primal optimizer \u03b3 * and dual optimizer (u * , v * ) relationship is described by DISPLAYFORM5. Equation (9) is rewritten in terms of expectations, assuming access to samples from \u00b5, \u03bd, and normalized measures \u03bc, \u03bd, as well as normalization constants, leading to DISPLAYFORM6. If \u03c8 * 1 , \u03c8 * 2 are differentiable, u, v can be parameterized with neural networks u \u03b8 , v \u03c6 and optimized using stochastic gradient descent in Algorithm 2, a generalization of classical OT to unbalanced OT. Algorithm 2 describes the optimization process for unbalanced optimal transport using neural networks to parameterize u, v. The algorithm is a generalization of classical OT to unbalanced OT, with the dual solution being updated through gradient descent. The dual solution learned from Algorithm 2 for unbalanced optimal transport can reconstruct the primal solution, indicating mass transport between points in X and Y. The marginals of the transport map may not align with the original measures \u00b5 and \u03bd, allowing for mass variation. The dual solution from Algorithm 2 reconstructs the primal solution, showing mass transport between points in X and Y. The transport map may not align with the original measures \u00b5 and \u03bd, allowing for mass variation. Algorithm 3 proposes a stochastic approach to learning a deterministic mapping from X to Y based on the dual solution. Algorithm 3 presents a stochastic method for learning a deterministic mapping from X to Y based on the dual solution, allowing for mass variation between points in X and Y. Algorithm 3 presents a stochastic method for learning a deterministic mapping from X to Y based on the dual solution, allowing for mass variation between points in X and Y. In the update process, the objectives in (6) and (3) are equivalent when reformulated in terms of \u03b3 instead of (T, \u03be). This equivalence is formalized by Lemma 3.3, showing the relationship between the formulations. The relationship between formulations in Algorithm 3 is formalized by Lemma 3.3, showing that formulations are equivalent when restricted to specific joint measures specified by (T, \u03be). The proof demonstrates that L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be), establishing the equivalence between the objectives. The Radon-Nikodym derivative \u03be of \u03c0 X # \u03b3 with respect to \u00b5 is defined by (12). It follows that L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) for any solution \u03b3. By the disintegration theorem, a family of probability measures {\u03b3 y|x} exists. The infimum over the left-hand side yields a result. By the disintegration theorem, a family of probability measures exists, and there are measurable functions that relate to this. The Radon-Nikodym derivative is defined, leading to the conclusion. The Radon-Nikodym derivative is defined by a family of measurable functions, satisfying a specific relation. This implies a certain inequality holds for any measure. The Radon-Nikodym derivative is related to the inequality for any measure, leading to theoretical results on optimal entropy-transport. The proof implies that under certain conditions, the joint measure specified by any minimizer of the function is unique. This result follows from the analysis of optimal entropy-transport. The joint measure specified by any minimizer of the function is unique under certain conditions, as proven in Proposition B.1. The minimizers are restricted to G, ensuring uniqueness of the marginals for any solution \u03b3 of W c1,c2,\u03c8 (\u00b5, \u03bd) when \u03c8 \u221e = \u221e. The minimizers of the function ensure uniqueness of the joint measure under certain conditions. The marginals are uniquely determined for any solution of W c1,c2,\u03c8 (\u00b5, \u03bd) when \u03c8 \u221e = \u221e, as proven in Proposition B.1. The product measure generated by the minimizers of L \u03c8 (\u00b5, \u03bd) is unique, establishing a proper metric between positive measures \u00b5 and \u03bd for certain cost functions and divergences. The uniqueness of the joint measure is ensured by the minimizers of the function. The product measure generated by the minimizers of L \u03c8 (\u00b5, \u03bd) establishes a proper metric between positive measures \u00b5 and \u03bd for certain cost functions and divergences, such as the Hellinger-Kantorovich or Wasserstein-Fisher-Rao metric. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem. This is proven by showing that the convergence of a certain function implies convergence of the equality constraint. Theorem 3.4 proves that solutions of the relaxed problem converge to solutions of the original problem by demonstrating the convergence of a specific function. Theorem 3.4 shows the convergence of solutions from the relaxed problem to the original problem through the convergence of a specific function. By Lemma 3.9 in BID27, it is proven that the limit of a certain function is greater than or equal to a specific value. Additionally, the sequence of minimizers is bounded, and if certain assumptions are met, the sequence is also tight. The sequence of minimizers \u03b3 k is bounded and equally tight if certain assumptions are satisfied. By an extension of Prokhorov's theorem, a subsequence of \u03b3 k weakly converges to some \u03b3. The sequence of minimizers \u03b3 k is bounded and equally tight. By an extension of Prokhorov's theorem, a subsequence of \u03b3 k weakly converges to some \u03b3, which is a minimizer of W c1,c2,\u03b9= (\u00b5, \u03bd). The proof of Lemma 3.3 shows that \u03b3 k is equivalent to the product measure induced by minimizers of L \u03b6 k \u03c8 (\u00b5, \u03bd). The text discusses the minimization of a function W with respect to measures \u00b5 and \u03bd. It introduces the convex conjugate form of \u03c8-divergence to transform the main objective into a min-max problem. Lemma B.2 states a relationship between non-negative finite measures P and Q. The convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem. Lemma B.2 shows a relationship between non-negative finite measures P and Q over T \u2282 R d. The equality condition is defined for measurable functions f, with a simple proof provided. This result has been used in generative modeling and a rigorous proof can be found in a referenced source. The optimal function f is obtained when dP dQ belongs to the subdifferential of \u03c8 * (f). This result has been used in generative modeling and a rigorous proof can be found in a referenced source. The optimal function f is obtained when dP dQ belongs to the subdifferential of \u03c8 * (f). The optimal f over the support of P \u22a5 is equal to \u03c8 \u221e. Proposition B.1 provides conditions for the problem to be wellposed, with the cost of transport often being a convenient choice for c 1. The cost functions c1 and c2 are chosen based on Proposition B.1, with c1 often representing the correspondence between X and Y, and c2 being a convex function that prevents \u03be from becoming too small or too large. The cost functions c1 and c2 are chosen based on Proposition B.1, with c1 often representing the correspondence between X and Y. For the cost of mass adjustment, c2 should be a convex function that prevents \u03be from becoming too small or too large. Any \u03c8-divergence can be used to train generative models to match a generated distribution P to a true data distribution Q. The \u03c8-divergences in Table 1 can be used to train generative models to match a generated distribution P to a true data distribution Q. Jensen's inequality shows that D \u03c8 (P |Q) is minimized when P = Q for probability measures. However, this may not hold true for non-probability measures, as demonstrated in an example from the original GAN paper. In the original GAN paper, the discriminative objective function corresponds to D \u03c8 (P |Q) with a specific entropy function. This divergence is equivalent to the Jensen-Shannon divergence when P, Q are probability measures, but not when they are non-negative measures with unconstrained total mass. In the original GAN paper, the discriminative objective function corresponds to D \u03c8 (P |Q) with a specific entropy function. This divergence is equivalent to the Jensen-Shannon divergence when P, Q are probability measures. If P, Q are non-negative measures with unconstrained total mass, an additional constraint on \u03c8 is required for divergence minimization to match P to Q. The divergence is minimized when P = \u221e and Q = 0. An additional constraint on \u03c8 is required to ensure divergence minimization matches P to Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. Otherwise, P = Q in general when D \u03c8 (P |Q) is minimized. The divergence is minimized when P = \u221e and Q = 0. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. Otherwise, P = Q in general when D \u03c8 (P |Q) is minimized. The second statement discusses conditions where the divergence is equal to or less than P = Q, providing examples of common divergences for unbalanced OT. The choice of function f is important, as it should map from Y to (\u2212\u221e, \u03c8 \u221e ] and can be enforced using a neural network with a final layer mapping to the correct range. Table 1 provides examples of activation layers and neural architectures used in experiments, including fully-connected feedforward networks with ReLU activations and a sigmoid function for the output activation layer. In experiments, fully-connected feedforward networks with ReLU activations were used, along with output activation layers like sigmoid and softplus functions to map pixel brightness and scaling factor weight to specific ranges."
}