{
    "title": "HyeJf1HKvS",
    "content": "This work introduces a two-stage neural architecture for learning structural correspondences between graphs. It utilizes localized node embeddings from a graph neural network to rank soft correspondences initially, followed by synchronous message passing networks to iteratively re-rank and reach a matching consensus in local neighborhoods. The message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, guiding the re-ranking process effectively. This architecture scales well to large, real-world inputs while being purely local and sparsity-aware. Our two-stage neural architecture uses localized node embeddings and message passing networks to establish structural correspondences between graphs. The message passing scheme computes a consensus for corresponding neighborhoods, guiding the re-ranking process effectively. This approach scales well to large inputs and has shown practical effectiveness in computer vision and entity alignment tasks. Graph matching involves establishing structural correspondences between nodes in graphs by considering node and edge similarities. It is crucial for various real-world applications, such as comparing molecules. Our method effectively scales to large inputs and improves upon the current state-of-the-art in tasks like computer vision and entity alignment between knowledge graphs. Graph matching is the process of establishing structural correspondences between nodes in graphs by considering node and edge similarities. This is essential for various real-world applications such as comparing molecules, matching protein networks, linking user accounts in social networks, and tasks in computer vision like tracking objects and recognizing actions. Graph matching is crucial for various applications such as comparing molecules, matching protein networks, linking user accounts in social networks, and tasks in computer vision like tracking objects and recognizing actions. The problem has been extensively studied in theory and practice, often using domain-agnostic distances like graph edit distance and maximum common subgraph problem. Graph matching is a key problem in various applications such as comparing molecules, matching protein networks, and tasks in computer vision. It has been extensively studied in theory and practice, often using domain-agnostic distances like graph edit distance and maximum common subgraph problem. The problem can be formulated as a graph matching problem, which has been investigated in theory and practice, relating it to distances such as graph edit distance and the maximum common subgraph problem. However, solving these approaches to optimality may not be feasible for large-scale instances, and they do not adapt to the given data distribution or consider continuous node embeddings. Recently, neural architectures have been proposed to address graph matching and similarity tasks in a data-dependent manner, incorporating continuous node embeddings for crucial semantic information. These approaches offer a more adaptable solution compared to traditional combinatorial methods, which may not be feasible for large-scale instances. Various neural architectures have been proposed for graph matching and similarity tasks, but they have limitations such as only computing similarity scores between whole graphs, relying on inefficient global matching procedures, and not generalizing to unseen graphs. These approaches may also struggle to match neighborhoods between graphs. Graph matching is often formulated as an edge-preserving, quadratic assignment problem, with the intuition of finding correspondences based on neighborhood consensus. Various neural architectures for graph matching have limitations in computing similarity scores, relying on inefficient global matching procedures, and struggling to match neighborhoods between graphs. Graph matching is typically formulated as an edge-preserving, quadratic assignment problem, aiming to find correspondences based on neighborhood consensus to prevent adjacent nodes from being mapped to different regions. In this work, the problem of supervised and semi-supervised graph matching is addressed by incorporating the concept of neighborhood consensus as an inductive bias into the model. The goal is to find correspondences between graphs while ensuring that adjacent nodes in the source graph are not mapped to different regions in the target graph. The problem of supervised and semi-supervised graph matching is tackled by incorporating neighborhood consensus as an inductive bias into the model. The proposed deep graph matching architecture consists of two stages, aiming to find correspondences between graphs while utilizing complete graph structures. The proposed deep graph matching architecture involves two stages: local feature matching and iterative refinement using synchronous message passing networks. Initial correspondence scores are computed based on node embeddings similarity in the feature matching step, followed by an iterative refinement strategy. The method involves two stages: local feature matching and iterative refinement using synchronous message passing networks. The feature matching step computes initial correspondence scores based on node embeddings similarity, followed by an iterative refinement strategy to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. The method involves two stages: local feature matching and iterative refinement using synchronous message passing networks. It aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. The architecture involves node features being locally matched based on a graph neural network before their correspondence scores are refined based on neighborhood consensus. The method involves local feature matching and iterative refinement using synchronous message passing networks to achieve neighborhood consensus for correspondences. Node features are matched locally based on a graph neural network before refining their correspondence scores through neighborhood consensus. This process involves transferring injective node coloring from the source graph to the target graph and updating based on pair-wise color differences using neural networks. The approach is inspired by related methods and computes similarities between nodes in the source and target graphs based on node embeddings. The method involves local feature matching and iterative refinement using synchronous message passing networks to achieve neighborhood consensus for correspondences. Initial soft correspondences are obtained by computing similarities between nodes in the source and target graphs based on node embeddings, followed by sinkhorn normalization to obtain rectangular doubly-stochastic correspondence matrices. Initial soft correspondences are obtained by computing similarities between nodes in the source and target graphs using node embeddings. Sinkhorn normalization is then applied to obtain doubly-stochastic correspondence matrices fulfilling specific constraints. The i-th row vector S i,: is interpreted as a discrete distribution over potential correspondences in G t for each node i \u2208 V s. \u03a8 \u03b81 is trained in a supervised fashion against ground truth correspondences by minimizing the negative log-likelihood of correct correspondence scores. It is implemented as a Graph Neural Network to obtain localized, permutation equivariant node representations. The Graph Neural Network (GNN) \u03a8 \u03b81 is used to obtain localized, permutation equivariant node representations by aggregating localized information through a neural message passing scheme. Various operators from geometric deep learning and relational representation learning are available for selection. The Graph Neural Network updates node features by aggregating localized information using various operators from geometric deep learning and relational representation learning. The feature matching procedure may lead to false correspondences due to the local nature of node embeddings. The feature matching procedure in graph neural networks can lead to false correspondences due to the local nature of node embeddings. To address this, violations of neighborhood consensus criteria are detected using graph neural networks, and correspondences are iteratively refined. The algorithm aims to detect violations of neighborhood consensus criteria using graph neural networks and iteratively refine correspondences. The soft correspondence matrix is used to pass node functions between graphs. The proposed algorithm utilizes a soft correspondence matrix to map node functions between graphs, allowing for the detection of violations of neighborhood consensus criteria. By iteratively refining correspondences, node indicator functions are mapped from one graph to another using the soft correspondences. Our consensus method maps node indicator functions between graphs using a soft correspondence matrix. Synchronous message passing is performed on both graphs via a shared graph neural network to measure neighborhood consensus between node pairs. Trainable updates of correspondence scores are based on the results of both GNNs. By performing synchronous message passing on both graphs via a shared graph neural network, a vector measuring neighborhood consensus between node pairs can be recovered. This measure is used to update correspondence scores based on an MLP. The process iteratively improves consensus and resolves ambiguities and false matchings. The consensus stage in the process iteratively improves correspondence scores by combining feature matching error and neighborhood consensus error. This two-stage approach resolves ambiguities and false matchings using purely local operators. The importance of the consensus stage is highlighted by the need for an initial matching to test for neighborhood consensus. The two-stage approach resolves ambiguities and false matchings using local operators. The importance of neighborhood consensus is emphasized, with d i,j measuring the matching quality between local neighborhoods in isomorphic graphs. Theorems 1 and 2 demonstrate the effectiveness of permutation equivariant GNNs in encoding soft correspondences between graphs. Theorem 1 and Theorem 2 show the efficacy of permutation equivariant GNNs in capturing soft correspondences between graphs by measuring neighborhood matching quality. The GNN \u03a8 \u03b82 must meet specific criteria to provide equal node embeddings. The GNN \u03a8 \u03b82 must satisfy criteria for equal node embeddings, including permutation equivariance and injectivity. Common GNN architectures are equivariant due to neighborhood aggregators, and injectivity is a discussed topic in recent literature. The GNN must fulfill requirements for equal node embeddings, including permutation equivariance and injectivity. Common GNN architectures are equivariant due to neighborhood aggregators, and injectivity is a discussed topic in recent literature. Theoretical approaches relate to classical graph structures. The GNN must meet requirements for equal node embeddings, including permutation equivariance and injectivity. A powerful GNN, like the Weisfeiler & Lehman heuristic, can distinguish graph structures using sum aggregation with MLPs on neighboring node features. The proposed approach can be related to classical graph matching techniques, such as the graduated assignment algorithm. The proposed approach relates to classical graph matching techniques like the graduated assignment algorithm, which iteratively computes new solutions by solving a linear assignment problem. The softassign operator uses sinkhorn normalization on rescaled inputs to encourage integer solutions. The proposed approach involves iteratively computing new solutions using a linear assignment problem and implementing the softassign operator with sinkhorn normalization to encourage integer solutions. The approach also approximates the linear assignment problem via sinkhorn normalization and utilizes a neighborhood consensus scheme for a non-trainable GNN instantiation. Our approach involves updating correspondence scores via trainable neural networks based on the difference between node attributes, resembling the linear assignment problem approximation with sinkhorn normalization. This deep parameterized model generalizes the graduated assignment algorithm for graph matching. Our approach updates correspondence scores using trainable neural networks based on node attribute differences, akin to the graduated assignment algorithm. This method supports continuous node and edge features through established GNN models, simplifying graph matching computations. Our approach updates correspondence scores using trainable neural networks based on node attribute differences, akin to the graduated assignment algorithm. This method supports continuous node and edge features through established GNN models, simplifying graph matching computations. We experimentally verify the benefits of using trainable neural networks instead of traditional methods and propose optimizations to scale the algorithm for large input domains. The initial correspondences are sparsified by filtering out low score correspondences before neighborhood consensus, improving efficiency. Our proposed algorithm scales to large input domains by sparsifying initial correspondences before neighborhood consensus. Top k correspondences are computed using the KEOPS library, reducing memory footprint and time complexity. See Algorithm 1 in Appendix A for the final optimized algorithm. The proposed algorithm scales to large input domains by sparsifying initial correspondences with the help of the KEOPS library, reducing memory footprint and time complexity. The refinement phase's time complexity is also reduced, optimizing the initial feature matching loss is crucial for accurate results. The algorithm scales to large input domains by sparsifying initial correspondences with the KEOPS library, reducing memory footprint and time complexity. Optimizing the initial feature matching loss is crucial and can be accelerated by training against sparsified correspondences. Node indicator functions are replaced with randomly drawn node functions to improve efficiency. In order to improve efficiency, node indicator functions are replaced with randomly drawn node functions in the algorithm. This refinement strategy helps resolve ambiguities by re-sampling in each iteration, as verified empirically in Section 4.1. Theorem 1 still holds for node indicator functions being injective, while Theorem 2 may not hold anymore. The refinement strategy aims to address ambiguities by re-sampling in each iteration, as shown empirically in Section 4.1. Softmax normalization is efficient for doubly-stochastic solutions but may lead to inconsistent integer solutions early on. Softmax normalization in the refinement strategy aims to relax constraints by applying row-wise softmax normalization, allowing for natural resolution of violations of certain conditions. This approach avoids the inefficiency and risk of vanishing gradients associated with sinkhorn normalization. In the refinement strategy, row-wise softmax normalization is proposed to relax constraints and resolve violations naturally, avoiding inefficiency and vanishing gradients. The number of refinement iterations can vary for training and testing, speeding up runtime. In the refinement strategy, row-wise softmax normalization is proposed to relax constraints and resolve violations naturally. The number of refinement iterations can vary for training and testing, speeding up runtime and encouraging convergence with fewer steps. Decreasing the number of iterations during training does not affect the convergence abilities of the neighborhood consensus procedure during testing. The refinement strategy includes row-wise softmax normalization to relax constraints and resolve violations. Decreasing the number of iterations during training does not impact convergence abilities during testing. The method is validated on synthetic graphs and real-world tasks like supervised keypoint matching and cross-lingual knowledge graph alignment. Our method is validated on synthetic graphs and real-world tasks such as supervised keypoint matching and cross-lingual knowledge graph alignment. Implementation is in PYTORCH using PYTORCH GEOMETRIC and KEOPS libraries, allowing for efficient processing of sparse mini-batches with GPU acceleration and minimal memory usage. Optimization is performed via ADAM. Our method is implemented in PYTORCH using PYTORCH GEOMETRIC and KEOPS libraries for efficient processing of sparse mini-batches with GPU acceleration. Optimization is done via ADAM with a fixed learning rate. Hits@k is used to evaluate and compare the model's performance. In experiments, optimization is done via ADAM with a fixed learning rate. Similar architectures are used for \u03a8 \u03b81 and \u03a8 \u03b82, with dropout omitted in \u03a8 \u03b82. Hits@k is used to evaluate the model's performance on synthetic graphs, aiming to learn a matching for pairs of graphs in a supervised fashion. Each pair consists of an undirected Erd\u0151s & R\u00e9nyi graph G s and a target graph G t. In the first experiment, the method is evaluated on synthetic graphs to learn a matching for pairs of graphs in a supervised fashion. The graphs consist of Erd\u0151s & R\u00e9nyi graphs with different node sizes and edge probabilities. Training and evaluation are conducted on multiple graphs with varying edge removal probabilities. Additional experiments in Appendix E test the approach's robustness to node addition or removal. The target graph G t is constructed from G s by removing edges with probability p s without disconnecting any nodes. Training and evaluation are done on 1 000 graphs for different p s values. Additional experiments in Appendix E test the approach's robustness to node addition or removal. Architecture involves implementing graph neural network operators \u03a8 \u03b81 and \u03a8 \u03b82 with three layers of the GIN operator. Our approach involves implementing graph neural network operators \u03a8 \u03b81 and \u03a8 \u03b82 with three layers of the GIN operator for node addition or removal. The number of layers and hidden dimensionality of all MLPs is set to 2 and 32, respectively, with ReLU activation applied. Input features are initialized with one-hot encodings of node degrees. We use a Jumping Knowledge style concatenation to compute final node representations. Training and testing are conducted with 10 and 20 refinement iterations, respectively. The proposed approach involves using graph neural network operators with three layers of the GIN operator for node addition or removal. The number of layers and hidden dimensionality of all MLPs is set to 2 and 32, respectively, with ReLU activation applied. Input features are initialized with one-hot encodings of node degrees. Training and testing are conducted with 10 and 20 refinement iterations, respectively. The matching accuracy Hits@1 for different choices of |V s | and p is shown, revealing a decrease in performance with increasing structural noise. However, the two-stage architecture proposed can recover all correspondences. The proposed approach involves using graph neural network operators with three layers of the GIN operator for node addition or removal. The number of layers and hidden dimensionality of all MLPs is set to 2 and 32, respectively, with ReLU activation applied. Training and testing are conducted with 10 and 20 refinement iterations, respectively. The matching accuracy Hits@1 for different choices of |V s | and p is shown, revealing a decrease in performance with increasing structural noise. However, the two-stage architecture proposed can recover all correspondences, independent of the applied structural noise. The proposed two-stage architecture can recover all correspondences, regardless of structural noise, by increasing the number of refinement iterations during testing. This highlights the benefits of matching consensus and scalability enhancements made. The proposed two-stage architecture emphasizes the benefits of matching consensus and scalability enhancements. Even when training does not converge, increasing the number of refinement iterations during testing allows for convergence. The refinement strategy performs well on sparsified top k correspondences, although it struggles with poor initial feature matching quality. The refinement strategy performs well on sparsified top k correspondences, converging to the perfect solution with increasing k, making it suitable for scaling the algorithm to large graphs. Experiments were conducted on PASCALVOC and WILLOW-OBJECTCLASS datasets. The experiments were conducted on the PASCALVOC and WILLOW-OBJECTCLASS datasets, following specific experimental setups and dataset pre-filtering criteria. The PASCALVOC dataset contains image categories with labeled keypoint locations, pre-filtered to exclude difficult objects. It has 6,953 training and 1,671 testing images with varying scale, pose, and illumination. The WILLOW-OBJECTCLASS dataset has consistent orientations for each of its five categories. The PASCALVOC dataset has 6,953 training and 1,671 testing images with varying scale, pose, and illumination. The WILLOW-OBJECTCLASS dataset has consistent orientations for its five categories. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 per-class images for training. Graphs are constructed using Delaunay triangulation of keypoints. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 per-class images for training. Graphs are constructed using Delaunay triangulation of keypoints, and input features are obtained from a pre-trained VGG16 on IMAGENET. The architecture and parameters include the adoption of SPLINECNN as the graph neural network operator. The keypoints are obtained from a pre-trained VGG16 on IMAGENET and used for triangulation. The graph neural network operator SPLINECNN is adopted, with B-spline based kernel function conditioned on edge features between node-pairs. Results are evaluated for both isotropic and anisotropic cases. The graph neural network operator SPLINECNN utilizes a trainable B-spline based kernel function \u03a6 \u03b8 (\u00b7) conditioned on edge features between node-pairs. Results are evaluated for both isotropic and anisotropic cases, showing high accuracy across different scenarios. SPLINECNN utilizes edge features as normalized relative distances and 2D Cartesian coordinates. It uses a kernel size of 5, hidden dimensionality of 256, and ReLU activation. The SPLINECNN model utilizes edge features as normalized relative distances and 2D Cartesian coordinates. It includes a kernel size of 5, hidden dimensionality of 256, and ReLU activation. The network architecture consists of two convolutional layers, dropout with probability 0.5, and a final linear layer for training and evaluation. The network architecture includes two convolutional layers with a hidden dimensionality of 256 and ReLU activation. During training, pairs are formed between training examples of the same category, and the model is evaluated using a fixed number of test graph pairs. The model is trained using negative log-likelihood and evaluated using isotropic and anisotropic GNNs for different values of L. The study evaluates the model using negative log-likelihood and isotropic/anisotropic GNNs for different L values. Results for Hits@1 on PASCALVOC and WILLOW-OBJECTCLASS are presented in Tables 1 and 2. The refinement strategy outperforms other methods. The study evaluates GNNs for different L values and includes ablation results using MLP for local node matching. Results for Hits@1 on PASCALVOC and WILLOW-OBJECTCLASS are shown in Tables 1 and 2. The refinement strategy significantly outperforms competing methods and non-refined baselines, reducing errors by half on WILLOW-OBJECTCLASS. The refinement stage improves model performance significantly, reducing errors by half on the WILLOW-OBJECTCLASS dataset. Starting from a weaker initial baseline, the second stage shows improvements of up to 14 percentage points on PASCALVOC. Task-specific GNNs are used for feature matching, and the approach is validated on geometric feature matching using only point coordinates. Improvements of up to 14 percentage points on PASCALVOC are achieved by using task-specific isotropic or anisotropic GNNs for feature matching. The approach is validated on geometric feature matching using only point coordinates and no additional visual features. Training involves generating a synthetic set of graph pairs by sampling source points uniformly and adding Gaussian noise. The experimental training setup follows Zhang & Lee (2019) and tests model generalization on the PASCALPF dataset. Synthetic graph pairs are generated by sampling source points uniformly, adding Gaussian noise, and introducing outliers. Graphs are constructed by connecting nodes with k-nearest neighbors. The unmodified anisotropic keypoint architecture is trained until it has seen 32,000 points. The study generates synthetic graph pairs by adding Gaussian noise and outliers to source points. Graphs are constructed by connecting nodes with k-nearest neighbors. The unmodified anisotropic keypoint architecture is trained on 32,000 synthetic examples and evaluated on the PASCALPF dataset, showing improvement over previous state-of-the-art results. The trained model is evaluated on the PASCALPF dataset, showing improvement over previous state-of-the-art results. Hits@1 results are presented in Table 3, demonstrating the benefits of the consensus architecture. The method also performs well on the DBP15K datasets, even without visual information. The study by Zhang & Lee (2019) outperformed the L = 0 baseline on various categories, highlighting the effectiveness of their consensus stage. Their method also showed success without visual information. Evaluation was done on DBP15K datasets linking entities from different knowledge graphs. The study by Zhang & Lee (2019) outperformed the L = 0 baseline on various categories, showing success without visual information. Evaluation was done on DBP15K datasets linking entities from different knowledge graphs, with each dataset containing 15,000 links split into training and testing. Entity input features were obtained using monolingual FASTTEXT embeddings for each language, aligned into the same vector space, and represented by the sum of word embeddings. We align monolingual FASTTEXT embeddings for each language into the same vector space and use the sum of word embeddings as the final entity input representation. Our graph neural network operator is similar to Xu et al. (2019d), using ReLU followed by dropout with probability 0.5 as our non-linearity \u03c3, and obtaining final node representations via i ]. We employ a three-layer GNN (T = 3) for obtaining node representations. The graph neural network operator in our study is based on Xu et al. (2019d) and uses ReLU with dropout for non-linearity. A three-layer GNN is employed for obtaining node representations with specific architecture and parameters. Training is done using negative log likelihood in a semi-supervised manner. The study uses a three-layer GNN for obtaining initial similarities and refining alignments with dimensionality 256 and 32. Training is done in a semi-supervised manner using negative log likelihood. Results are evaluated using Hits@1 and Hits@10, comparing the model to previous work. The study utilizes a three-layer GNN to establish initial similarities and enhance alignments with dimensionality 256 and 32. The sparse top k correspondence matrix is updated 10 times during the refinement phase. Results show improvements in Hits@1 and Hits@10 compared to previous models, with gains of up to 9.38 percentage points across all categories. Our approach, utilizing a three-layer MLP, improves upon the state-of-the-art with gains of up to 9.38 percentage points across all categories. The refinement strategy significantly enhances Hits@1 results and operates on sparsified top 10 initial correspondences. The scalability allows for multiple refinement iterations while maintaining large hidden feature dimensionalities. The proposed approach consistently improves Hits@1 results significantly by refining sparsified top 10 initial correspondences. The scalability allows for multiple refinement iterations while retaining large hidden feature dimensionalities. Experimental results show effective solving of real-world problems, with the method inheriting the expressive power of GNNs related to graph isomorphism testing. The proposed approach effectively solves real-world problems by refining initial correspondences and retaining large feature dimensionalities. However, limitations related to the WL heuristic for graph isomorphism testing may cause our approach to fail in certain scenarios. One limitation of our approach is that when two nodes are assigned the same color by WL, convergence to a solution may fail due to equal neighborhood sets. Adding noise to initial correspondence distributions could resolve this, but real-world datasets typically have enough feature noise to prevent this scenario. The feature matching procedure may face non-convergence when equal initial correspondence distributions are generated, and adding noise to resolve this is unlikely due to the presence of feature noise in real-world datasets. Various related problems include identifying correspondences between nodes in graphs, such as maximum common subgraph, network alignment, graph edit distance, and graph matching. Identifying correspondences between nodes in graphs has been extensively studied in various domains. Related problems include maximum common subgraph, network alignment, graph edit distance, and graph matching. Recent research has focused on deep graph matching techniques using graph neural networks. For a detailed discussion of related work, refer to Appendices F and G. Graph neural networks have become a key focus in research on deep graph matching techniques. A two-stage neural architecture was presented for learning node correspondences between graphs in a supervised or semi-supervised manner, aiming to reach a neighborhood consensus between matchings. Our proposed graph matching consensus procedure involves a two-stage neural architecture for learning node correspondences between graphs in a supervised or semi-supervised manner. The approach aims to reach a neighborhood consensus between matchings and can scale to large input domains. The algorithm consistently outperformed the state-of-the-art on real-world datasets. Our proposed graph matching consensus procedure involves a two-stage neural architecture for learning node correspondences between graphs in a supervised or semi-supervised manner. The approach aims to reach a neighborhood consensus between matchings and can scale to large input domains. The algorithm consistently outperformed the state-of-the-art on real-world datasets. The final optimized algorithm is provided in Algorithm 1. The proposed graph matching consensus procedure utilizes a two-stage neural architecture to learn node correspondences between graphs. The algorithm aims to achieve a neighborhood consensus between matchings and has shown superior performance on real-world datasets. The final optimized algorithm is detailed in Algorithm 1. The GNN \u03a8 \u03b82 is powerful in distinguishing graph structures and operates on injective node colorings, allowing it to distinguish any graph structure. The GNN \u03a8 \u03b82 can distinguish any graph structure by mapping T-hop neighborhoods around nodes to the same vectorial representation. It utilizes injective node colorings and can identify graph isomorphism relations. If certain conditions are met, it can describe an isomorphism using a permutation matrix. The GNN \u03a8 \u03b82 can distinguish graph structures by mapping T-hop neighborhoods to vectorial representations using injective node colorings. It can identify graph isomorphism relations and describe them using a permutation matrix. If nodes map to the same node, it contradicts the injectivity requirements of AGGREGATE (t) and UPDATE (t) for all t \u2208 {1, . . . , T }. The algorithm extends the graduated assignment algorithm by using trainable parameters to describe isomorphisms between graphs. The impact of a trainable refinement procedure was evaluated by replicating experiments using \u03a8 \u03b82. Our algorithm extends the graduated assignment algorithm by using trainable parameters to describe isomorphisms between graphs. Evaluating the impact of a trainable refinement procedure, experiments were replicated using \u03a8 \u03b82, showing consistent improvements with trainable neural networks compared to fixed-function message passing schemes. Using trainable neural networks \u03a8 \u03b82 consistently improves upon fixed-function message passing schemes, allowing for meaningful similarities between node and edge features to guide the refinement procedure. This approach also offers flexibility in choosing task-dependent GNN operators for learning geometric patterns or fulfilling injectivity requirements, enhancing theoretical expressivity. Our approach in a fixed-function pipeline learns to utilize features for refining procedures and selecting task-dependent GNN operators. Experimental validation on node addition/removal was conducted using synthetic experiments. To validate the robustness of our approach towards node addition or removal, we conducted synthetic experiments using Erd\u0151s & R\u00e9nyi graphs with varying node and edge probabilities. The target graph was constructed by adding noisy nodes to the source graph and generating edges between them and existing nodes. The same network architecture and training procedure were used as described previously. Our consensus stage is robust to node addition or removal, while the first stage struggles with finding the right matching. Unmatched nodes do not affect neighborhood consensus error. Our neural architecture can detect and reduce false positive influence of unmatched nodes in the refinement stage, improving correspondence identification between nodes in graphs. Identifying correspondences between nodes in graphs is a challenging problem studied in various domains. The combinatorial maximum common subgraph isomorphism problem, which seeks the largest common subgraph in two given graphs, is NP-hard and difficult to approximate. Our neural architecture can detect and decrease false positive influence of unmatched nodes, enhancing correspondence identification. In graph theory, the combinatorial maximum common subgraph isomorphism problem is studied, which seeks the largest common subgraph in two given graphs. The problem is NP-hard and difficult to approximate, even in trees, unless the common subgraph is required to be connected. Various techniques have been developed to address this challenging problem. Most variants of the problem are difficult to approximate with theoretical guarantees. Exact polynomial-time algorithms are available for specific problem variants relevant in cheminformatics. Different techniques have been developed in bioinformatics and computer vision for network alignment or graph matching. In graph matching, two graphs of order n with adjacency matrix A s and A t are compared. Different techniques have been developed in bioinformatics and computer vision for network alignment or graph matching, where large networks without specific structural properties are common. In graph matching, two graphs of order n with adjacency matrix A s and A t are compared by minimizing a function involving permutation matrices. In graph matching, the goal is to minimize a function involving permutation matrices for comparing two graphs with adjacency matrices A s and A t. The problem can be solved by minimizing Equation (12) using a Frank-Wolfe type algorithm and projecting the fractional solution to set P. Related work in graph matching has been extensively studied, with recent surveys providing detailed discussions on the topic. In graph matching, research has focused on minimizing Equation (12) using a Frank-Wolfe algorithm and projecting the solution to set P. The applicability of relaxation and projection techniques is not well understood, with limited theoretical results available. The WL heuristic can distinguish between two graphs G s and G t according to a classical result by Tinhofer (1991). The applicability of relaxation and projection techniques in graph matching is still poorly understood, with limited theoretical results available. The WL heuristic distinguishes between graphs G s and G t based on a classical result by Tinhofer (1991). Kersting et al. (2014) modified the Frank-Wolfe algorithm to obtain the WL partition, while Aflalo et al. (2015) showed that the standard relaxation provides correct solutions for specific asymmetric graphs. The Frank-Wolfe algorithm can be modified to obtain the WL partition for graph matching. Different relaxations have been studied, including spectral relaxations and random walks. Other approaches to graph matching exist, such as those based on spectral properties of adjacency matrices. Graph matching is closely related to the quadratic assignment problem (QAP) and can be characterized by the spectral properties of asymmetric graphs. Various relaxations and approaches, such as spectral relaxations and random walks, have been studied in the literature. The problem of graph matching typically involves a weighted version considering node and edge similarities. Graph matching is related to the quadratic assignment problem (QAP) and involves weighted versions considering node and edge similarities. Zhou & De la Torre (2016) proposed factorizing the affinity matrix and incorporating global geometric constraints. Zhang et al. (2019c) studied kernelized graph matching. Lawler's QAP involves an affinity matrix of size n2 \u00d7 n2 and is computationally demanding. Zhou & De la Torre (2016) proposed factorizing the affinity matrix into smaller matrices and incorporating global geometric constraints. Zhang et al. (2019c) studied kernelized graph matching using node and edge similarities as kernels, expressing the problem as Koopmans-Beckmann's QAP in the associated Hilbert space. Swoboda et al. (2017) explored Lagrangean decompositions of graph matching inspired by MAP inference in conditional random fields. In 2019, kernelized graph matching was studied using node and edge similarities as kernels, expressing the problem as Koopmans-Beckmann's QAP in the associated Hilbert space. Swoboda et al. (2017) explored Lagrangean decompositions of graph matching solved by dual ascent algorithms known as message passing, leading to state-of-the-art performance in graph matching tasks. Functional representation for graph matching has been proposed as a generalizing concept. Lagrangean decompositions of the graph matching problem, solved by dual ascent algorithms or message passing, have shown state-of-the-art performance. Functional representation for graph matching has been proposed to avoid constructing the affinity matrix. Graph edit distance measures the cost to transform one graph into another by adding, deleting, and substituting vertices and edges, a concept relevant in pattern recognition. The graph edit distance is a concept in computer vision that measures the minimum cost to transform one graph into another by adding, deleting, and substituting vertices and edges. It has been proposed for pattern recognition tasks over 30 years ago and is closely related to the maximum common subgraph problem and the quadratic assignment problem. Several exact algorithms have been developed to compute the graph edit distance. The computation of the graph edit distance, proposed for pattern recognition tasks over 30 years ago, is NP-hard and closely related to the maximum common subgraph and quadratic assignment problems. Recent exact algorithms have been developed, but heuristics based on the assignment problem are widely used due to limitations with small graphs. The original cubic running time can be reduced to quadratic time. The graph edit distance computation is NP-hard and related to the maximum common subgraph problem. Heuristics based on the assignment problem are widely used for small graphs, with algorithms aiming for quadratic or even linear time complexity. Network alignment involves defining similarity functions between nodes and following a two-step approach. Network alignment involves defining similarity functions between nodes and following a two-step approach. Algorithms can achieve quadratic or linear time complexity for the assignment problem. Singh et al. (2008) proposed ISORANK based on the adjacency matrix of the product graph. In network alignment, similarity functions between nodes are defined and a two-step approach is followed. ISORANK, proposed by Singh et al. (2008), computes an alignment by solving the assignment problem using a node-to-node similarity matrix. The matrix is obtained by applying PAGERANK to a normalized version of the product graph K = A s \u2297 A t. Kollias et al. (2012) proposed an efficient approximation of ISORANK using decomposition techniques to avoid generating the product graph of quadratic complexity. The matrix M is obtained by applying PAGERANK using a normalized version of K as the GOOGLE matrix and node similarities as the personalization vector. Various techniques have been proposed for network alignment, including efficient approximations of ISORANK and solving the problem via integer linear programming. Various techniques have been proposed for network alignment, including generating the product graph of quadratic size, supporting vertex and edge similarities, and solving the problem via integer linear programming. The goal is to find an optimal correspondence between the vertices of two graphs based on a clearly defined objective function. Bayati et al. (2013) developed a message passing algorithm for sparse network alignment, aiming to find an optimal correspondence between vertices of two graphs. Recent approaches focus on learning node and edge similarity functions for specific tasks, such as a cost model for graph edit distance (Cort\u00e9s et al., 2019). Caetano et al. (2009) proposed a method to learn correspondences in a more principled manner. In recent research, methods have been proposed to learn node and edge similarity functions for specific tasks, such as graph edit distance. Different approaches aim to learn correspondences between graphs, including deep graph matching procedures that utilize local node feature matchings and cross-graph embeddings. The idea of refining local feature matchings by enforcing neighborhood consistency has been relevant for matching in images. The method presented in this work is related to deep graph matching procedures, which have been investigated from various perspectives. Refining local feature matchings by enforcing neighborhood consistency has been relevant for matching in images. The functional maps framework aims to solve a similar problem for manifolds. Recently, the problem of graph matching has been heavily investigated in a deep fashion by various researchers. Deep graph matching has been heavily investigated in a deep fashion, with researchers developing supervised deep graph matching networks based on displacement and combinatorial objectives. Different approaches have been taken, such as modeling graph matching affinity via a differentiable spectral graph matching solver. Our matching procedure is fully learnable, in contrast to other methods. Develop supervised deep graph matching networks based on displacement and combinatorial objectives. Our matching procedure is fully learnable, unlike other methods that use node-wise features and dense node-to-node cross-graph affinities. Zhang & Lee (2019) propose a compositional message passing algorithm for mapping point coordinates into a high-dimensional space. In contrast to previous methods using node-wise features and dense node-to-node cross-graph affinities, our work resolves inconsistent neighborhood assignments naturally. Xu et al. (2019b) address graph matching by connecting it to the Gromov-Wasserstein discrepancy and enhancing the optimal transport objective. Our work resolves inconsistent neighborhood assignments naturally by computing pairwise inner products between point embeddings. Xu et al. (2019b) relate graph matching to the Gromov-Wasserstein discrepancy and enhance the optimal transport objective by learning node embeddings to account for noise in graphs. They also extend this concept to multi-graph partitioning and matching by learning a Gromov-Wasserstein barycenter. Our approach works in a supervised fashion for optimal transport between nodes. The optimal transport objective is enhanced by learning node embeddings to account for noise in graphs. Xu et al. (2019a) extend this concept to multi-graph partitioning and matching by learning a Gromov-Wasserstein barycenter. Our approach works in a supervised fashion for sets of graphs and can generalize to unseen instances. Derr et al. (2019) use CYCLEGANs to align NODE2VEC embeddings and find matchings based on nearest neighbors. The task of network alignment has been explored from various perspectives. Different approaches include leveraging CYCLEGANs to align NODE2VEC embeddings, utilizing a fast local matching procedure based on node embedding similarity, and using shared graph neural networks to approximate graph edit distance. In network alignment, deep graph models are designed based on global and local network topology preservation. Different methods include utilizing local node embedding similarity for fast matching, using shared graph neural networks to approximate graph edit distance, and ordering the correspondence matrix for further processing with traditional CNNs. These approaches focus on local node operations. In a follow-up work, Bai et al. (2018) proposed to order the correspondence matrix in a breadth-first-search fashion and process it further with traditional CNNs. Both approaches operate on local node embeddings, enhancing intra-graph node embeddings by inter-graph node embeddings has been heavily investigated in practice. Wang et al. (2019b) enhance the GNN operator. Intra-and inter-graph message passing is heavily investigated in practice. Wang et al. enhance the GNN operator by aggregating information from local neighbors and similar embeddings in the other graph. Xu et al. leverage alternating GNNs to propagate local features between graphs. Wang & Solomon tackle the problem of finding an unknown rigid motion between points. Intra-and inter-graph message passing techniques are explored in various studies. Wang et al. improve the GNN operator by aggregating information from local neighbors and similar embeddings in the other graph. Xu et al. use alternating GNNs to propagate local features between graphs. Wang & Solomon address the challenge of identifying an unknown rigid motion between point clouds. They employ a point cloud matching approach followed by a differentiable SVD module for feature matching based on inner product similarity scores. However, these methods do not consistently achieve matching due to their limited scope. Our approach involves utilizing a point cloud matching problem and a differentiable SVD module to find an unknown rigid motion between point clouds. Intra-graph node embeddings are processed through a Transformer module before feature matching based on inner product similarity scores. While these methods may not consistently achieve matching, they can enhance the initial feature matching process and complement advancements in the field. Neighborhood consensus methods in computer vision improve local feature matching results efficiently. Deep neural networks can enhance initial feature matching procedures. Neighborhood consensus methods in computer vision, such as deep neural networks, have a history of improving local feature matching results efficiently. A recent approach using 4D convolution was proposed, but it cannot be directly applied to the graph domain due to computational limitations. The 4D convolution method proposed by Rocco et al. (2018) cannot be efficiently transferred to the graph domain due to computational limitations. Our algorithm infers errors for the product graph but performs computations on the original graphs, focusing on functional maps between function spaces on manifolds."
}