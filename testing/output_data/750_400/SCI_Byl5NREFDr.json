{
    "title": "Byl5NREFDr",
    "content": "The study focuses on model extraction in natural language processing, where an adversary can reconstruct a victim model using only query access. It is shown that the adversary can successfully mount the attack without real training data or using grammatically correct queries. Random word sequences with task-specific heuristics are effective for model extraction across various NLP tasks. The attacker can extract a model without real training data or using grammatically correct queries, using random word sequences and task-specific heuristics. This exploit is possible due to the shift towards transfer learning in NLP. Defense strategies against model extraction include membership classification and API. The shift towards transfer learning in NLP allows attackers to extract a model with a few hundred dollars, slightly worse than the victim model. Defense strategies like membership classification and API watermarking can be circumvented by clever adversaries. Machine learning models are valuable intellectual property, requiring significant money and effort for training and design. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may attempt to steal models served by APIs to avoid the costly development cycle. Malicious users may attempt to steal machine learning models served by web APIs through \"model stealing\" or \"model extraction\" attacks. This involves issuing a large number of queries to collect (input, output) pairs and train a local copy of the model. Extracted models can leak sensitive information about the training data or be used to generate adversarial examples. NLP APIs based on ELMo and BERT are popular due to their contextualized pretrained representations, which enhance performance and reduce sample complexity. Adversaries can steal machine learning models through model extraction attacks, where they collect (input, output) pairs to train a local copy of the model, potentially leaking sensitive information or creating adversarial examples. In this paper, the focus is on how NLP models obtained by fine-tuning a pretrained BERT model can be vulnerable to model extraction attacks. In this paper, it is demonstrated that NLP models fine-tuned from BERT can be extracted without access to training data. Extraction attacks are possible with randomly sampled word sequences and task-specific heuristics. Extraction attacks on NLP models fine-tuned from BERT can be successful without access to training data or well-formed queries. This contrasts with prior work that required some access to relevant data for large-scale attacks. Extraction attacks on NLP models fine-tuned from BERT can succeed without training data or well-formed queries. The attacks use randomly-sampled sentences from Wikipedia as queries, making them cost-effective. Extraction attacks on NLP models, particularly BERT, can be successful without the need for training data or well-formed queries. By using randomly-sampled sentences from Wikipedia as queries, these attacks are cost-effective. The attacker samples words to form queries and sends them to the victim BERT model, then fine-tunes their own BERT using the victim's outputs as labels. This process is illustrated in Figure 1. The attacker queries a victim BERT model with randomly-sampled sentences and fine-tunes their own BERT using the victim's outputs as labels. Despite the effectiveness of the randomly-generated queries in extracting good models, they are mostly nonsensical and uninterpretable. The randomly-generated queries are effective in extracting good models, despite being nonsensical and uninterpretable. Pretraining on the attacker's side makes model extraction easier. Simple defenses against extraction include membership classification and API watermarking. In the study, it was found that pretraining on the attacker's side facilitates model extraction. Two defenses against extraction - membership classification and API watermarking - were effective against naive adversaries but failed against more sophisticated ones. The research aims to inspire further exploration of stronger defenses against model extraction and a deeper understanding of the vulnerability of models and datasets to such attacks. The study found that defenses like membership classification and API watermarking are effective against naive adversaries but fail against more sophisticated ones. The research aims to inspire stronger defenses against model extraction and a better understanding of why models and datasets are vulnerable to such attacks. Our work relates to prior efforts on model extraction, focusing on computer vision applications. We synthesize queries for extracting models, which also connects to zero-shot distillation and studies of rubbish inputs to NLP systems. Model extraction attacks have been studied empirically and theoretically, mostly against image classification APIs. These attacks involve synthesizing queries in an active learning setup by searching for inputs near the victim classifier's decision boundaries, a method not applicable to text-based systems. The prior work focused on model extraction attacks against image classification APIs, synthesizing queries in an active learning setup. However, this method is not transferable to text-based systems due to the discrete nature of the input space. Prior attempts at extraction on NLP systems have been limited, with Pal et al. (2019) using pool-based active learning for sentence selection from WikiText-2. The prior work focused on model extraction attacks against image classification APIs, while our study explores extraction on modern BERT-large models for tasks expecting pairwise inputs like question answering. Our work is related to data-efficient distillation methods, aiming to distill knowledge from larger models to smaller ones with limited input data. Our work focuses on extracting knowledge from modern BERT-large models for tasks like question answering, using nonsensical inputs. Prior research has explored data-efficient distillation methods to transfer knowledge from larger models to smaller ones with limited data. Unlike model extraction, these methods assume white-box access to the teacher model for generating data impressions. Rubbish inputs, randomly-generated examples yielding high-confidence predictions, have been studied in the model extraction literature. The curr_chunk discusses the limitations of prior work in scaling the idea of using rubbish inputs to extract knowledge from deeper neural networks. Previous studies have shown that unnatural text inputs can lead to overly confident model predictions. The curr_chunk discusses the effectiveness of using unnatural text inputs to train models that perform well on real NLP tasks, even without seeing real examples during training. The study focuses on model extraction on BERT, a Bidirectional Encoder. BERT, a 24-layer transformer model, is studied for model extraction. It converts word sequences into high-quality contextualized vector representations. BERT-large, a 24-layer transformer model, revolutionized NLP with its contextualized vector representations learned through masked language modeling on unlabeled text data. BERT's parameters are learned through masked language modeling on natural text, revolutionizing NLP with state-of-the-art performance on various tasks. A modern NLP system typically uses fine-tuning with task-specific networks to achieve end-to-end learning for specific tasks. A modern NLP system leverages fine-tuning methodology with task-specific networks to construct a composite function. Description of extraction attacks involves reconstructing a local copy of a black-box API model. Using fine-tuning with a small learning rate, a malicious user attempts to reconstruct a local copy of a black-box API model without access to training data by generating nonsensical word sequences as queries. The attacker uses a task-specific query generator to create nonsensical word sequences as queries to the victim model, resulting in a dataset used to train a new model. This extraction attack is demonstrated on four NLP tasks with different input and output spaces. The attacker uses a task-specific query generator to create nonsensical word sequences as queries to the victim model, resulting in a dataset used to train a new model. Extraction attacks are demonstrated on four NLP tasks with diverse input and output spaces. The attacker uses a task-specific query generator to create nonsensical word sequences as queries to the victim model for extraction attacks on NLP tasks like ternary NLI classification, extractive QA, and boolean QA. Two query generators, RANDOM and WIKI, are studied in this context. The study explores the use of query generators RANDOM and WIKI to extract models for tasks requiring complex interactions between different parts of the input. The study examines the use of query generators RANDOM and WIKI to extract models for tasks with complex interactions between input parts. In the WIKI setting, input queries are formed from actual sentences or paragraphs from the WikiText-103 corpus. Task-specific heuristics are applied for tasks like MNLI and SQuAD/BoolQ. For MNLI, three words in the premise are randomly replaced with three random words to construct the hypothesis. The study explores using query generators RANDOM and WIKI for tasks with complex interactions between input parts. Task-specific heuristics are applied, such as randomly replacing words in the premise for MNLI and sampling words from the passage for SQuAD/BoolQ questions. Additional details on query generation can be found in Appendix A.3. The study uses query generators RANDOM and WIKI for tasks with complex interactions between input parts. For SQuAD/BoolQ questions, words are sampled from the passage to form a question. Evaluation is done with different query budgets for each task. Commercial cost estimates are provided. The study evaluates extraction procedure using different query budgets for tasks with complex interactions. Commercial cost estimates are provided using Google Cloud Platform's Natural Language API calculator. High accuracies are achieved for some tasks even at low query budgets. The study evaluates extraction procedure using various query budgets with high accuracies achieved even at low budgets using Google Cloud Platform's Natural Language API calculator. The study evaluates extraction procedure with high accuracies achieved even at low query budgets. Extracted models are accurate on original development sets of all tasks, even when trained with nonsensical inputs. Despite only seeing nonsensical questions during training, extracted SQuAD models recover 95% of original accuracy on WIKI. The study evaluates the extraction procedure, showing high accuracies even with nonsensical inputs. Extracted models are accurate on original development sets of all tasks, with SQuAD models recovering 95% of original accuracy on WIKI despite only seeing nonsensical questions during training. However, the agreement between extracted models is only slightly better than accuracy, and lower on held-out sets constructed using different sampling schemes. The agreement between extracted models is slightly better than accuracy, but lower on held-out sets using different sampling schemes. On SQuAD, extracted models have low agreements despite being trained on the same data, indicating poor functional equivalence. An ablation study with alternative query generation heuristics for SQuAD and MNLI is conducted. An ablation study was conducted with alternative query generation heuristics for SQuAD and MNLI. Classification with argmax labels only was assumed for classification datasets, showing minimal drop in accuracy from experiments with output probability distribution. The study conducted alternative query generation heuristics for SQuAD and MNLI, showing minimal accuracy drop with argmax labels only. Model extraction experiments with argmax outputs also had minimal accuracy drop, indicating full probability distribution access is not crucial for extraction. The study showed that full probability distribution access is not necessary for model extraction. Query efficiency was measured with varying query budgets, showing extraction success even with small budgets. Accuracy gains diminish quickly with more queries. The cost of these attacks can be estimated from the results in Table 2. The study demonstrated that model extraction can be successful without full probability distribution access. Accuracy gains diminish rapidly with more queries, and approximate costs for these attacks can be estimated from Table 2. This raises questions about the effectiveness of nonsensical input queries in model extraction and the performance of extraction without large pretrained language models. In this section, an analysis is performed to understand the effectiveness of nonsensical input queries in model extraction based on BERT. The study examines if different victim models produce the same answer with nonsensical queries and if some queries are more representative of the original data distribution. The focus is on the RANDOM and WIKI extraction configurations for SQuAD. In this section, the study examines if different victim models produce the same answer with nonsensical queries. Five victim SQuAD models are trained on original data with varying random seeds, achieving an F1 score between 90 and 90.5. The average pairwise F1 agreement between the models is then measured. The study explores if different victim models agree on answers to nonsensical queries using RANDOM and WIKI extraction configurations for SQuAD. Five victim models trained on original data show high agreement on SQuAD and development set queries but significantly lower agreement on WIKI and RANDOM queries. The study found that victim models trained on original data agree more on SQuAD and development set queries compared to WIKI and RANDOM queries. The average pairwise F1 agreement drops significantly for WIKI and RANDOM queries, indicating that victim models are less reliable on nonsensical inputs. High-agreement queries are closer to the original data distribution, indicating victim models are more reliable on nonsensical inputs. Extracting models using high-agreement subsets shows large F1 improvements compared to random and low-agreement subsets of the same size. This suggests that agreement between victim models is beneficial for model extraction. The study analyzed agreement between victim models to extract high-quality input-output pairs. High-agreement subsets showed significant F1 improvements, outperforming random and low-agreement subsets. This suggests that victim model agreement is a reliable proxy for extraction quality. Future work could explore the interpretability of high-agreement nonsensical queries to humans. Future work could investigate if high-agreement nonsensical textual inputs have a human interpretation, as prior research has shown deep neural networks can leverage non-robust features for learning classifiers. Prior research has shown deep neural networks can use non-robust features to learn classifiers. Investigating if high-agreement nonsensical textual inputs have human interpretations, three human annotators answered SQuAD questions with unanimous victim model agreement. Annotators matched victim models' answers 23% of the time on the WIKI subset. An investigation was conducted to determine if nonsensical textual inputs could be interpreted by humans. Three human annotators answered SQuAD questions with unanimous victim model agreement. Annotators matched victim models' answers 23% of the time on the WIKI subset. Annotators used a word overlap heuristic to select answer spans. An investigation found that annotators scored lower on RANDOM questions compared to original SQuAD questions. Annotators commonly used a word overlap heuristic to select answer spans, but many nonsensical question-answer pairs remained unexplained. The attacker may not have access to the same information as the victim in practical scenarios. In practical scenarios, the attacker's lack of information about the victim's architecture can impact extraction accuracy. The study explores the effects of different pretraining setups on extraction accuracy, considering variations in base models and the use of pretrained language models like BERT. When the attacker lacks information about the victim's architecture, extraction accuracy can be affected. The study investigates how different pretraining setups impact extraction accuracy, focusing on variations in base models like BERT-large and BERT-base. Results show higher accuracy when the attacker uses BERT-large, even if the victim was initialized with BERT-base. BERT comes in two sizes: BERT-large with 24 layers and BERT-base with 12 layers. Results show higher accuracy when the attacker uses BERT-large, even if the victim was initialized with BERT-base. The study also found that accuracy is better when the victim uses the same model as the attacker. The study found that accuracy is better when the victim uses the same model as the attacker, especially if the attacker starts from BERT-base. Finetuning BERT gives attackers a headstart as only the final layer is randomly initialized. The study found that attackers have an advantage when using BERT-base and fine-tuning the model. Training a QANet model without pretraining led to a significant drop in F1 score when using nonsensical queries. BERT-based models show high accuracy with original SQuAD inputs and BERT-large labels, but F1 score drops significantly with nonsensical queries. Better pretraining helps models start with a good language representation, simplifying extraction. The study shifts focus to defense strategies against model extraction while preserving API utility. BERT-based models show high accuracy with original SQuAD inputs and labels, but F1 score drops with nonsensical queries. Better pretraining simplifies extraction. Defense strategies against model extraction while preserving API utility are explored, focusing on two defenses effective against weak adversaries. The first defense uses membership inference. The first defense against model extraction while preserving API utility uses membership inference to detect nonsensical inputs or adversarial examples. The API uses membership inference to identify nonsensical or adversarial inputs, issuing random outputs to prevent extraction. Membership inference is treated as a binary classification problem, labeling original examples as real and extraction examples as fake. The API issues random outputs to prevent extraction of nonsensical inputs. Membership inference is treated as a binary classification problem, using logits and final layer representations as input features for training the classifier. These classifiers transfer well to a balanced development set. The classifier uses logits and final layer representations of the victim model as input features for training. Results show that the classifiers transfer well to a balanced development set and are robust to query generation processes. An ablation study on input features is provided in the appendix. The classifier is robust to query generation processes like RANDOM or SHUFFLE. Watermarking is another defense strategy where a fraction of queries are modified to return wrong outputs. This defense anticipates the memorization ability of deep neural networks. Watermarking is a defense strategy where a fraction of queries are modified to return wrong outputs, anticipating the memorization ability of deep neural networks. This defense aims to make extracted models vulnerable to post-hoc detection if deployed publicly. Watermarking is a defense strategy to make extracted models vulnerable to post-hoc detection by modifying a fraction of queries to return wrong outputs. Evaluation on MNLI and SQuAD shows high accuracy in predicting watermarked outputs but low accuracy in predicting original labels. The study evaluates the performance of watermarked models on predicting watermarked outputs and original labels. Watermarked WIKI has high WM Label Acc and low Victim Label Acc. Only 0.1% of queries are watermarked to minimize API performance drop. Non-watermarked models perform similarly on the development set but differ on watermarked queries. Watermarking works effectively on predicting watermarked outputs and original labels. Non-watermarked models struggle on watermarked queries, predicting victim model's outputs. Training with more epochs exacerbates these differences. Limitations include the need to use watermarking after an attack has occurred. Watermarking is effective in predicting watermarked outputs and original labels. Non-watermarked models struggle with watermarked queries, predicting the victim model's outputs. Training for more epochs exacerbates these differences. Limitations include the need to use watermarking after an attack has occurred and the assumption that the attacker will deploy the extracted model publicly with black-box query access. Model extraction attacks against NLP APIs serving BERT-based models are effective, assuming the attacker deploys the extracted model publicly with black-box query access. Attackers can prevent detection by using techniques like differentially private training, fine-tuning with different queries, or issuing random outputs on specific queries. Model extraction attacks against NLP APIs serving BERT-based models are effective, especially when attackers fine-tune large pretrained language models. Existing defenses against extraction are generally inadequate. Fine-tuning large pretrained language models simplifies model extraction attacks, making existing defenses inadequate. Further research is needed to develop robust defenses against adaptive adversaries. Further research is necessary to develop robust defenses against adaptive adversaries who anticipate counter-attacks. Future directions include leveraging nonsensical inputs for model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity in active learning setups. The curr_chunk discusses diagnosing dataset complexity using query efficiency as a proxy and investigating agreement between victim models for input distribution proximity in an active learning setup. It also mentions using the cost estimate from Google Cloud Platform's Calculator for Natural Language APIs. The paper used Google Cloud Platform's cost estimate for Natural Language APIs, allowing inputs up to 1000 characters per query. Costs were extrapolated for tasks not covered by Google Cloud APIs. In this study, costs were extrapolated for tasks not covered by Google Cloud APIs, such as natural language inference and reading comprehension. It is challenging to estimate the price of issuing a certain number of queries due to varying API providers and free query allowances. An attacker could potentially exploit multiple accounts to gather data. It is difficult to estimate the cost of issuing a certain number of queries due to varying API providers and free query allowances. An attacker could exploit multiple accounts to collect data in a distributed manner. APIs are often used on webpages and can be easily emulated to extract information at a large scale for free. APIs are freely available on webpages and can be exploited to extract data at a large scale. API costs can vary based on infrastructure or revenue models. It is crucial to focus on the low costs of data extraction rather than estimates. Web scraping and API costs can vary significantly based on infrastructure and revenue models. It is important to focus on the relatively low costs of data extraction rather than estimates. Complex tasks like machine translation and speech recognition are relatively inexpensive. For example, it costs -$430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translation queries. In this section, details on input generation algorithms for datasets like SST2 and RANDOM are provided. A vocabulary is created using wikitext103, preserving the top 10000 tokens based on unigram frequency. In this section, input generation algorithms for datasets like SST2 and RANDOM are detailed. A vocabulary is built using wikitext103, preserving the top 10000 tokens based on unigram frequency. The algorithms involve randomly sampling tokens from the vocabulary for translation queries. The curr_chunk explains the process of sampling tokens from the top-10000 wikitext103 vocabulary for tasks like SST2 and MNLI. It involves randomly selecting words to replace those not in the vocabulary. The process involves sampling tokens from the top-10000 wikitext103 vocabulary for tasks like SST2 and MNLI by randomly selecting words to replace those not in the vocabulary. The process includes sampling tokens from the top-10000 wikitext103 vocabulary for tasks like SST2 and MNLI by randomly replacing words. A vocabulary is created using wikitext103, and paragraph lengths are chosen from this pool. The final paragraph is constructed by sampling tokens from the vocabulary based on unigram probabilities. The final paragraph is constructed by sampling tokens from the wikitext103 vocabulary based on unigram probabilities. A random integer length is chosen to build the question, which is then appended with a ? symbol and prepended with a question starter word randomly chosen from a list. The final paragraph is constructed by sampling tokens from the wikitext103 vocabulary based on unigram probabilities. Questions are built by sampling paragraph tokens up to a chosen length and prepended with a randomly chosen question starter word. In this section, additional query generation heuristics are studied. Table 11 compares various extraction datasets used for SQuAD 1.1, showing general findings. In Table 11, various extraction datasets for SQuAD 1.1 are compared, with findings showing that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. Our general findings from an ablation study on MNLI show that lexical overlap between premise and hypothesis affects model predictions. When overlap is too low, the model predicts neutral or contradiction, while too high overlap leads to shuffled hypotheses. The study on MNLI found that the lexical overlap between premise and hypothesis impacts model predictions. Low overlap leads to neutral or contradiction predictions, high overlap results in entailment predictions, and a few different words create balanced datasets with strong extraction signals. Using frequent words aids extraction. The study found that lexical overlap between premise and hypothesis affects model predictions. Low overlap leads to neutral or contradiction predictions, high overlap results in entailment predictions, and a few different words create balanced datasets with strong extraction signals. Using frequent words aids extraction. Human studies involved fifteen annotators who annotated five sets of twenty questions. The study involved human annotators who annotated five sets of twenty questions, including original SQuAD questions, WIKI questions with high and low agreement among victim models, and RANDOM questions with high and low agreement. The study analyzed inter-annotator agreement among human annotators for five question sets, including original SQuAD questions, WIKI questions with high and low agreement, and RANDOM questions with high and low agreement. The results showed that the average pairwise F1 followed a specific order, indicating the closeness to the actual input distribution. In an ablation study on input features for the membership classifier, two candidates were considered: 1) the logits of the BERT classifier indicating confidence scores, and 2) the last layer representation. The inter-annotator agreement was analyzed, showing that the average pairwise F1 followed a specific order reflecting the input distribution closeness. The ablation study on input features for the membership classifier compared the effectiveness of using the logits of the BERT classifier and the last layer representations. Results showed that the last layer representations were more effective in distinguishing between real and fake inputs, but the best results were achieved by using both feature sets. The ablation study compared the effectiveness of using logits and last layer representations in the membership classifier. Results show last layer representations are more effective in distinguishing real and fake inputs, but best results are achieved by using both feature sets."
}