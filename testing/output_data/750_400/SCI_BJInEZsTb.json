{
    "title": "BJInEZsTb",
    "content": "In this paper, the authors explore representation learning and generative modeling using three-dimensional geometric data in the form of point clouds. They introduce a deep autoencoder network that excels in reconstruction quality and generalization. The learned representations surpass current methods in 3D recognition tasks and enable shape editing through simple algebraic manipulations. Additionally, the study includes an analysis of various generative models, such as GANs and Gaussian mixture models, operating on raw point clouds. The authors introduce a deep autoencoder network that excels in reconstruction quality and generalization for 3D geometric data in point clouds. They surpass current methods in 3D recognition tasks and enable shape editing through algebraic manipulations. The study includes an analysis of various generative models like GANs and Gaussian mixture models operating on raw point clouds, with GMMs trained in the latent space of the autoencoders producing samples of the best fidelity and diversity. The authors propose simple measures of fidelity and diversity for evaluating generative models using 3D representations of real-life objects. Different encodings like view-based projections, volumetric grids, and graphs are discussed, highlighting their effectiveness in various domains but shortcomings in semantics. The text discusses different encodings like view-based projections, volumetric grids, and graphs, which are effective in various domains but lack semantics. High-dimensional representations are not suitable for generative models, requiring construction and manipulation for editing and designing new objects. Recent advances in deep learning offer a data-driven approach that eliminates the need for hand-crafting features and models in domains with abundant data. This contrasts with the limitations of high-dimensional representations for generative models, which often require complex parametric models for object editing and design. Recent advances in deep learning have made it possible to construct and manipulate complex parametric models without the need for manual feature engineering. Deep learning tools like autoencoders and Generative Adversarial Networks are successful at learning complex data representations and generating realistic samples. Recent advances in deep learning have enabled the construction and manipulation of complex parametric models without manual feature engineering. Deep learning architectures like autoencoders and Generative Adversarial Networks are successful at learning complex data representations and generating realistic samples. Point clouds, a relatively unexplored 3D modality, provide a homogeneous, expressive, and compact representation of surface geometry. In this paper, the focus is on 3D point clouds, which are a compact representation of surface geometry. They are commonly generated by range-scanning devices like the Kinect and iPhone. Only a few deep learning architectures exist for 3D point clouds, such as PointNet. Deep architectures for 3D point clouds are attractive for learning due to their surface geometry representation. They are commonly outputted by range-scanning devices like the Kinect and iPhone. Existing architectures include PointNet for classification and segmentation tasks, with others using point clouds in various pipeline steps. This study focuses on using deep architectures to learn representations and generative models. Generative models for point clouds have gained attention in the deep learning community, with the introduction of GANs. However, training GAN-based generative pipelines is challenging and unstable. There is currently no universally accepted method to evaluate generative models. Generative models for point clouds, particularly GAN-based pipelines, have become a focus in deep learning. Training these models is difficult and unstable, with no standard evaluation method. Evaluation criteria include fidelity and coverage, crucial for avoiding mode collapse. Generative models for point clouds, especially GAN-based ones, are challenging to train and evaluate. Fidelity and coverage are key in assessing these models to prevent mode collapse. A new AE architecture inspired by recent classification architectures is introduced to learn compact representations of point clouds with high reconstruction quality, even on unseen samples. The text discusses a new AE architecture designed for learning compact representations of point clouds with high reconstruction quality. The architecture is inspired by recent classification architectures and is capable of improving classification accuracy and enabling meaningful interpolations and semantic operations. Additionally, the text mentions the creation of generative models that can generate point clouds similar to training and test data, providing good coverage. The text introduces a workflow for learning compact representations of point clouds using an AE with a bottleneck layer, followed by training a GAN in the fixed latent representation. This approach improves classification accuracy and enables the generation of point clouds similar to training and test data. The workflow involves training an AE with a compact bottleneck layer to learn a representation, followed by training a GAN in that fixed latent representation. This approach is supported by theory and empirically verified to be easier and more effective in achieving superior reconstruction and coverage compared to training a GAN without a fixed latent representation. Training a GAN in a compact, low-dimensional latent space is easier and more effective, as supported by theory and empirical evidence. Latent GANs outperform raw GANs in reconstruction and coverage. GMMs trained in fixed AEs' latent space show the best performance. Multi-class GANs perform well when trained in the latent space. Various metrics are evaluated for learning good representations. Multi-class GANs trained in a compact latent space perform almost as well as dedicated GANs per object category. Various metrics are evaluated for learning good representations, including the failure of Chamfer distance to distinguish between pathological and good examples. New fidelity and coverage metrics are proposed for generative models. The paper introduces new fidelity and coverage metrics for evaluating generative models, specifically for point cloud generation. It also highlights the limitations of the Chamfer distance metric in discriminating between pathological and good examples. The rest of the paper discusses the background, evaluation metrics, models for latent representations, and the quantitative evaluation of the proposed models. The paper discusses new fidelity and coverage metrics for evaluating generative models in point cloud generation. It also addresses the limitations of the Chamfer distance metric. The rest of the paper covers the necessary background, evaluation metrics, models for latent representations, and the quantitative evaluation of the proposed models. Autoencoders are deep architectures that aim to reproduce their input by compressing data into a low-dimensional representation. Generative Adversarial Networks (GANs) are also discussed in the paper. The code for all models is publicly available for further evaluation. Autoencoders compress data into a low-dimensional representation through a bottleneck layer. The Encoder (E) learns to encode data into a latent representation, z, while the Decoder (D) reproduces the original data from z. Generative Adversarial Networks (GANs) are state-of-the-art models where a generator (G) creates samples indistinguishable from real data by passing a random sample z through G. Generative Adversarial Networks (GANs) involve an adversarial game between a generator (G) and a discriminator (D) to create samples that resemble real data. The generator synthesizes samples by passing a randomly drawn sample through G, while the discriminator distinguishes between synthesized and real samples using specific losses. The parameters for the discriminator and generator networks are denoted as \u03b8 (D) and \u03b8 (G) respectively. Additionally, an improved Wasserstein GAN formulation is also utilized. The discriminator distinguishes between synthesized and real samples using specific losses, with parameters denoted as \u03b8 (D) and \u03b8 (G) for the discriminator and generator networks. The improved Wasserstein GAN formulation is utilized for improved stability during training. Point clouds present unique challenges in network architecture, requiring specific considerations for geometry processing. Improved Wasserstein GAN BID11 is used for stability in training. Point clouds pose unique challenges due to lack of grid structure, making encoding difficult compared to images or voxel grids. Recent work bypasses this issue with PointNet by avoiding 2D convolutions. Point clouds lack a grid structure, making encoding challenging compared to images or voxel grids. Recent work like PointNet bypasses this issue by avoiding 2D convolutions. Point clouds are unordered, complicating comparisons between sets and requiring permutation invariance for encoded features. Point clouds are unordered, making comparisons between sets challenging and requiring permutation invariance for encoded features. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature, including the Earth Mover's distance (EMD). Two permutation-invariant metrics for comparing unordered point sets are the Earth Mover's distance (EMD) and the Chamfer distance (CD). EMD transforms one set to the other through a transportation problem, while CD measures the squared distance between each point in one set to its nearest neighbor in the other set. The Earth Mover's distance (EMD) compares two sets by finding the minimum squared distance between each point in one set to its nearest neighbor in the other set using a bijection. The Chamfer distance (CD) also measures squared distances between points but is more computationally efficient. These metrics are used to evaluate representations and generative models by comparing reconstructed or synthesized point clouds to their ground truth counterparts. In the paper, evaluation metrics are used to compare reconstructed or synthesized point clouds to their ground truth counterparts. This comparison is essential for assessing the quality of representation and generative models, measuring faithfulness, diversity, and potential mode-collapse. Metrics such as Coverage are employed to measure how well a point-cloud distribution matches a ground truth distribution. The quality of a representation model is evaluated by comparing how well it matches the training set or a held-out test set. Metrics like Coverage are used to measure the faithfulness and diversity of generative models and to detect potential mode-collapse. Coverage is determined by finding the closest neighbor in a ground truth distribution for each point-cloud in a given distribution, with closeness measured using CD or EMD. High coverage indicates a good representation of the ground truth distribution within the given distribution. Coverage metrics, COV-CD and COV-EMD, measure the fraction of point-clouds in a distribution that are matched to a ground truth distribution. High coverage indicates a good representation of the ground truth within the given distribution. To capture fidelity, MMD-CD and MMD-EMD measure the average distance between matched point clouds in the distributions. The fidelity of a representation is captured by matching every point cloud of G to the one in A with the minimum distance (MMD), yielding MMD-CD and MMD-EMD. MMD measures the distances in the pairwise matchings, correlating with the realism of elements in A. Jensen-Shannon Divergence (JSD) measures the divergence between marginal distributions in the euclidean 3D space. The Jensen-Shannon Divergence (JSD) measures the degree to which point clouds of A and B occupy similar locations in a 3D space by counting points within voxels. This section describes the architectures of representation and generative models. The text describes the architectures of representation and generative models for point clouds, including an autoencoder design, a GAN tailored to point-cloud data, a pipeline that learns an AE and then a smaller GAN in the latent space, and a generative model based on Gaussian Mixtures. The input to the AE network is a point cloud. The text discusses representation and generative models for point clouds, including an autoencoder design and a GAN architecture tailored to point-cloud data. The AE network takes a point cloud with 2048 points as input and uses 1-D convolutional layers with increasing features to encode each point independently. The AE network encodes a 3D shape point cloud with 2048 points using 1-D convolutional layers and a symmetric function. It consists of 5 conv layers followed by ReLU and batch-norm layers, producing a k-dimensional vector for the latent space. The decoder then transforms this vector with 3 fully connected layers. The implementation uses 5 1-D conv layers with ReLU and batch-norm, leading to a k-dimensional vector for the latent space. The decoder consists of 3 fully connected layers to generate a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, are explored for permutation invariant objectives. The implementation includes 3 fully connected layers with ReLUs to produce a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, are used for permutation invariant objectives. The appropriate size for the latent space was determined to be k = 128 through experimentation with different bottleneck sizes. The appropriate size for the latent space was determined to be k = 128 through experimentation with different bottleneck sizes. The first version of our generative model operates directly on the raw 2048 \u00d7 3 point set input, presenting a GAN for point clouds for the first time. The generative model operates on raw 2048 \u00d7 3 point set input, with k = 128 showing the best generalization error on test data and minimal reconstruction error on train split. The discriminator architecture is similar to the AE, with leaky ReLUs and a sigmoid neuron in the last layer. The generator maps a 128-dimensional noise vector to a 2048 \u00d7 3 output. The discriminator architecture for the GAN is similar to the AE, with leaky ReLUs and a sigmoid neuron in the last layer. The generator takes a 128-dimensional noise vector and produces a 2048 \u00d7 3 output through 5 FC-ReLU layers. In the l-GAN, data is passed through a pre-trained autoencoder before being used by the generator and discriminator. In the l-GAN, data is passed through a pre-trained autoencoder before being used by the generator and discriminator. The architecture is simpler compared to the r-GAN, with shallow designs for both the generator and discriminator. The generator operates on a 128-dimensional bottleneck variable of the AE, and the output is decoded to a point cloud via the AE decoder. The l-GAN uses a pre-trained autoencoder to generate realistic results with a simpler architecture compared to the r-GAN. The generator operates on a 128-dimensional bottleneck variable of the AE, and the output is decoded to a point cloud via the AE decoder. Additionally, Gaussian Mixture Models are trained on the latent spaces learned by the AEs. The l-GAN utilizes a pre-trained autoencoder for realistic results with a simpler architecture. Gaussian Mixture Models are trained on the latent spaces learned by the AEs, which can be turned into point-cloud generators. The GMMs, with both diagonal and full covariance matrices for the Gaussians, can be used as point-cloud generators by sampling the latent-space and using the AE's decoder. Shapes from the ShapeNet repository are reconstructed using class-specific AEs, trained with an 85%-5%-10% split for training/testing/validation sets. The ground truth shapes are compared with shapes produced by encoding and decoding using class-specific AEs trained on ShapeNet data. Models are split into training/testing/validation sets with an 85%-5%-10% split. The performance of the latent features computed by the AE is evaluated using linear models on supervised datasets. The AE was trained on 57,000 models from 55 categories of man-made objects with a bigger bottleneck of 512. Features for input 3D shapes were obtained by feeding them through the AE. For this experiment, the AE was trained on 57,000 models from 55 categories of man-made objects with a bigger bottleneck of 512. Features for input 3D shapes were obtained by feeding them through the network to extract a 512-dimensional bottleneck layer vector. This feature was then processed by a linear classification SVM trained on ModelNet BID32 for 3D classification. Comparing results with the previous state of the art BID31, which used several GAN layers, Table 1 shows the outcomes. The 3D shape input features are extracted by feeding the point-cloud to the network and obtaining a 512-dimensional bottleneck layer vector. A linear classification SVM trained on ModelNet BID32 is then used for processing. Compared to the previous state of the art BID31, which utilized multiple GAN layers, the 512-dimensional feature is more intuitive and parsimonious. The decoupling of latent representation from generation allows flexibility in choosing the AE loss, impacting the learned feature. On ModelNet10, EMD and CD losses perform similarly for larger objects with fewer categories compared to ModelNet40. The decoupling of latent representation from generation in the 512-dimensional feature allows flexibility in choosing the AE loss, impacting the learned feature. On ModelNet10, EMD and CD losses perform similarly for larger objects with fewer categories, while CD produces better results with increased variation within the collection. This is due to its more local and less smooth nature, allowing it to understand rough edges and high frequency geometric details. The EMD and CD losses perform equivalently on ModelNet10 for larger objects with fewer categories. However, CD produces better results with increased variation within the collection due to its more local and less smooth nature, allowing it to understand rough edges and high frequency geometric details. This experiment also demonstrates the domain-robustness of the learned features. The experiment demonstrates the domain-robustness of learned features through qualitative evaluation. Reconstruction results using AEs show the ability to generalize to unseen shapes and enable shape editing applications like interpolations and part editing. The learned representation from the AEs demonstrates the ability to generalize to unseen shapes and enables shape editing applications like interpolations and part editing. The results showcase the generalization ability through qualitative evaluation and quantitative measurements of fidelity and coverage. The AEs demonstrate generalization ability by reconstructing unseen shapes and maintaining quality on training vs. test splits. Five generative models are compared on chair point-cloud data, with two AEs trained with CD or EMD loss. The study compares five generative models on chair point-cloud data, including two AEs trained with CD or EMD loss. Additionally, l-GANs are trained in the latent spaces of the AEs, with further models utilizing the Wasserstein objective with gradient-penalty. The study compares five generative models on chair point-cloud data, including AEs trained with CD or EMD loss. l-GANs are trained in the latent spaces of the AEs, with further models using the Wasserstein objective with gradient-penalty. GANs are trained for 2000 epochs, selecting the final model based on how well synthetic results match the ground-truth distribution. The study compares five generative models on chair point-cloud data, including AEs trained with CD or EMD loss. GANs are trained for 2000 epochs, selecting the final model based on how well synthetic results match the ground-truth distribution using JSD or MMD-CD metrics. The study uses GAN to generate synthetic point clouds and compares them to the validation set using JSD or MMD-CD metrics. Model selection is done every 100 epochs, with GMMs performing better with 32 components. To reduce computational cost, model selection is done every 100 epochs (50 for r-GAN). The JSD criterion is used to select models and determine the number of Gaussian components for GMM, with 32 components being optimal. GMMs perform better with full covariance matrices. Using MMD-CD as the criterion yields similar quality models with 40 Gaussians as optimal. Upon model selection every 100 epochs (50 for r-GAN), the JSD criterion determines the optimal number of Gaussian components for GMM, with 32 components being optimal. GMMs perform better with full covariance matrices, and using MMD-CD as the criterion yields similar quality models with 40 Gaussians as optimal. Comparisons are made with the volumetric approach of BID31, achieving an average classification score of 84.7% for ground-truth point clouds. Table 2 shows the evaluation of 5 generators on the chair dataset using minimal JSD for model selection. The ground-truth point clouds achieved an average classification score of 84.7%. The models were compared based on their ability to generate synthetic samples resembling the ground truth distribution. Upon selecting models, we compare their ability to generate synthetic samples resembling the ground truth distribution. Experiments are conducted to measure the similarity of generated samples to the train and test splits of the distribution. Results are reported in Table 2, along with the average classification probability for samples recognized as chairs using the PointNet classifier BID17. The study evaluates the performance of different generators by comparing their ability to generate synthetic samples resembling the ground truth distribution. Results are reported in Table 2, along with the average classification probability for samples recognized as chairs using the PointNet classifier BID17. The study evaluates the performance of different generators on a test-split of chair dataset using minimal JSD on the validation-split. Results in TAB10 show that training a Gaussian mixture model in the latent space of an EMD-based AE yields the best fidelity and coverage. GMMs are easy to train and perform well in generating synthetic samples. Training a simple Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of fidelity and coverage. GMMs are easy to train and achieve fidelity and coverage close to the reconstruction baseline. The Gaussian mixture models (GMMs) trained in the latent space of the EMD-based AE are easy to train and achieve fidelity and coverage close to the reconstruction baseline. The MMD-EMD values achieved by AE and GMMs are comparable, indicating good generalization ability of the models. The MMD-EMD values achieved by GMMs in the latent space of the...\n The number of synthetic point clouds generated for the train split experiment is equal to the size of the train dataset. For the test split experiment and model selection comparisons, synthetic datasets three times bigger than the ground truth dataset are generated to reduce sampling bias. This is necessary for measuring MMD or Coverage statistics. In TAB10, it is noted that the MMD-CD distance to the test set is relatively small for the r-GANs, showing an advantage of the r-GANs. The MMD-CD distance to the test set is relatively small for the r-GANs, indicating an advantage. However, qualitative inspection shows inadequacy of chamfer distance in distinguishing pathological cases. Examples are showcased in Fig. 3 with synthetic point clouds generated by r-GANs and l-GANs. The chamfer distance is inadequate to distinguish pathological cases, as shown in Fig. 3 with examples of synthetic point clouds generated by r-GANs and l-GANs. The left triplet displays an l-GAN on the AE-CD, and the right triplet shows an l-GAN on the AE-EMD. Nearest neighbors in synthetic sets are found for a ground truth point cloud using chamfer distance. CD values do not reflect that r-GAN results are of lower quality due to point concentration issues. The distances between nearest neighbors in the l-GAN and r-GAN sets are reported using CD and EMD. r-GAN results have lower quality due to point concentration issues, especially in areas likely to be occupied in the shape class. This affects the CD metric's ability to detect partial matches between shapes. The CD metric in r-GAN results is likely to have a small value due to point concentration issues in areas occupied by shapes. This metric is blind to partial matches, leading to a larger coverage metric compared to EMD. EMD promotes one-to-one mapping, correlating more strongly with visual quality and penalizing r-GAN results in terms of MMD and coverage. The CD-based coverage metric in r-GAN results is consistently larger than that reported by EMD, as EMD promotes one-to-one mapping and correlates more strongly with visual quality. Extensive measurements were conducted during training to understand model behavior, as shown in FIG2 with JSD distance and EMD-based MMD. During training, extensive measurements were conducted to understand model behavior. The r-GAN struggles to provide good coverage of the test set, while the l-GAN (AE-CD) performs better in terms of MMD and coverage. The r-GAN struggles to provide good coverage of the test set, while the l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs, but its coverage remains low due to the CD promoting unnatural topologies. The l-GAN (AE-EMD) shows improved coverage and fidelity compared to the r-GAN, with a dramatic improvement in results when using an EMD-based AE for representation. However, both l-GANs still face the issue of mode collapse during training. Switching to a latent WGAN largely eliminates mode collapse in GANs, improving coverage and fidelity. Comparisons to voxel-based methods are also discussed. The text discusses how switching to a latent WGAN can prevent mode collapse in GANs, improving coverage and fidelity. It also compares GANs on point-cloud data to voxel-based methods, specifically focusing on the chair category. In this study, the authors propose using GANs on point-cloud data and compare their models to a voxel-grid based approach. They evaluate the performance in terms of JSD on the training set of the chair category, converting voxel grid output into a point-set with 2048 points for analysis. The authors converted voxel grid output into a point-set with 2048 points using farthest-point-sampling on the isosurface. They compared the performance of r-GAN and l-GANs to BID31, with l-GANs outperforming in terms of diversity and classification score. The r-GAN outperforms BID31 in diversity and realism, while l-GANs perform even better in classification and diversity with fewer training epochs. l-GANs have significantly shorter training times compared to r-GAN due to their smaller architecture. BID31 operates on voxel grids, which may not be as effective for generating point clouds. The l-GAN has shorter training times compared to the r-GAN due to its smaller architecture. The qualitative evaluation shows high-quality results from both l-GANs and a 32-component GMM model trained on the AE-EMD latent space. This highlights the strength of the learned representation in producing good results. In a qualitative evaluation, synthetic results from l-GANs and a 32-component GMM trained on the AE-EMD latent space show high quality. The l-GAN produces crisper results compared to the r-GAN, showcasing the advantage of using a good structural loss on the pre-trained AE. The shapes corresponding to the 32 means of the Gaussian components can be found in the appendix, along with results using the l-GAN. The l-GAN produces clearer results than the r-GAN, showing the advantage of using a good structural loss on the pre-trained AE. Synthetic point clouds were generated by samples produced with l-GAN and a 32-component GMM, both trained on the latent space of an AE using the EMD loss. The study compares point clouds generated by samples from l-GAN and 32-component GMM trained on an AE using EMD loss. Experiments were conducted with a multi-class AE trained on a mixed set of point clouds from 5 categories. The AE was trained for 1000 epochs and compared against class-specific AEs trained for 500 epochs. The study constructed training, testing, and validation datasets for an autoencoder (AE) with 2K models per class for training, 200 models for testing, and 100 for validation. The multi-class AE with a bottleneck size of 128 was trained for 1000 epochs. Class-specific AEs were compared with an 85-5-10 train-val-test-split and trained for 500 epochs. Additionally, six l-WGANs were trained for 2K epochs and evaluated for fidelity/coverage using MMD-CD. The study trained a multi-class AE with a bottleneck size of 128 for 1000 epochs and compared it with class-specific AEs trained for 500 epochs. Additionally, six l-WGANs were trained for 2K epochs and evaluated using MMD-CD for fidelity/coverage. The l-WGANs based on the multi-class AE performed similarly to the dedicated class-specific ones, with minimal sacrifice in visual quality. The study found that l-WGANs based on the multi-class AE performed similarly to dedicated class-specific ones in terms of visual quality. MMD-CD measurements for l-WGANs trained on latent spaces of both types of AEs were compared, showing minimal sacrifice in visual quality with the multi-class AE. Some failure cases of the models were also shown in FIG5. The study stopped training at the two-thousand epoch and used latent spaces of dedicated and multi-class EMD-AEs. The \"average\" measurement is calculated as a weighted average of per-class values. Limitations include failure cases where chairs with rare geometries are not faithfully decoded, high-frequency details may be missed, and the r-GAN struggles to create realistic shapes for some classes. The study found limitations in the decoding of chairs with rare geometries and high-frequency details by the AEs. The r-GAN also struggled to create realistic shapes for certain classes, indicating the need for more robust raw-GANs for point clouds in future research. The study highlighted challenges in decoding chairs with rare geometries and high-frequency details using AEs. While r-GAN struggled with creating realistic shapes for certain classes like cars, suggesting the need for more robust raw-GANs for point clouds in future research. Some recent works focus on training Gaussian mixture models in the latent space of autoencoders, addressing issues like over-regularization in VAEs. In addressing challenges with VAEs, methods have been developed to mitigate over-regularization issues. By fixing the AE before training generative models, good results were achieved in 3D point-cloud representation learning. The novel architectures showed good generalization to unseen data and produced faithful samples. The study found that fixing the AE before training generative models led to good results in 3D point-cloud representation learning. The novel architectures showed good generalization to unseen data and produced faithful samples. The best-performing generative model was a GMM trained in the fixed latent space of an AE, suggesting that simple classic tools should not be overlooked. The best-performing generative model in the study was a GMM trained in the fixed latent space of an AE, indicating that simple classic tools should not be underestimated. Further investigation into the power of simple latent GMMs compared to adversarially trained models would be valuable. The AE used in the experiments had specific filter configurations in its layers. A study found that a GMM trained in the fixed latent space of an AE performed well, suggesting the importance of classic tools. The AE used in the experiments had specific filter configurations in its layers and utilized online data augmentation. The AE used in the experiments had specific filter configurations in its layers and utilized online data augmentation by applying random rotations along the gravity-(z)-axis to the input point-clouds of each batch. The AE was trained for different numbers of epochs with CD loss and EMD, and different setups were used for denoising/regularized AEs. The encoder had varying filter sizes at each layer, while the decoder consisted of 3 FC-ReLU layers. Training was done for a maximum of 500 epochs for single class data and 1000 epochs for 5 shape classes. Different AE setups did not show significant advantages. Dropout layers led to worse reconstructions, while using batch-norm on the encoder improved training speed and generalization error. The discriminator's initial layers were 1D convolutions. The discriminator's first 5 layers consist of 1D convolutions with specific filter sizes and leaky-ReLU activation, followed by a featurewise max-pool. The generator includes 5 FC-ReLU layers with varying numbers of neurons. The discriminator in the r-GAN model has 5 layers of 1D convolutions with {64, 128, 256, 256, 512} filters, followed by leaky-ReLU and max-pooling. The generator consists of 5 FC-ReLU layers with {64, 128, 512, 1024, 2048 \u00d7 3} neurons each. Training was done with Adam optimizer, using a noise vector of 128 dimensions. The r-GAN model used Adam optimizer with a learning rate of 0.0001 and beta 1 of 0.5. The noise vector had 128 dimensions. The discriminator had 2 FC-ReLU layers with 256 and 512 neurons, and a final layer with a sigmoid neuron. The generator had 2 FC-ReLU layers with 128 neurons each. For l-Wasserstein-GAN, a gradient penalty regularizer \u03bb = 10 was used, and the critic was trained for 5 iterations per generator iteration. The generator in the model consists of 2 FC-ReLUs with 128 neurons each, while the final FC layer in the discriminator has a single sigmoid neuron. A gradient penalty regularizer \u03bb = 10 was used for the l-Wasserstein-GAN, with the critic trained for 5 iterations per generator iteration. For classification experiments, a one-versus-rest linear SVM classifier with an l2 norm penalty and balanced class weights was used. The generator's noise distribution and parameters for the classification experiments were similar to those used for the r-GAN. A one-versus-rest linear SVM classifier with an l2 norm penalty and balanced class weights was utilized. Training parameters for SVMs with structural loss in different datasets were specified in Table 5. The training parameters for SVMs with structural loss in different datasets were specified in Table 5, including C-penalty, intercept, and loss functions. The reconstruction quality of CD and EMD-based AEs was compared in terms of JSD with ground truth datasets, showing comparable results between training and test datasets. The reconstruction quality of CD and EMD-based AEs was compared using JSD with ground truth datasets, showing comparable results between training and test datasets. The embedding learned with AE-EMD across all 55 object classes is used for shape editing applications, showcasing its ability to encode features for different shapes. The AE-EMD trained across all 55 object classes can generalize well for shape editing applications, showcasing its ability to encode features for different shapes. Editing parts in point clouds using vector arithmetic on the AE latent space allows for tuning appearance and modifying shapes based on shape annotations. Editing parts in point clouds involves modifying shapes using vector arithmetic on the AE latent space. Shape annotations are used as guidance to make changes, such as tuning the appearance of cars, adding armrests to chairs, and removing handles from mugs. The structural differences between object sub-categories can be modeled using the latent representation. By using latent space representation, structural differences between object sub-categories can be modeled by the difference in their average latent representations. This allows for modifying shapes by transforming the latent representation of an object. Using latent space representation, structural differences between object sub-categories can be modeled by the difference in their average latent representations. This allows for modifying shapes by transforming the latent representation of an object. Interpolating between different point clouds in the latent space representation results in intermediate variants between two shapes. The height of chairs with armrests is on average 13% smaller than chairs without, reflecting in the output of this process. The latent representation allows for morphing between shapes by interpolating their representations. It supports removing and merging shape parts, enabling morphing between shapes of different appearances and classes. The latent representation enables morphing between shapes by interpolating their representations, supporting removing and merging shape parts for morphing between shapes of different appearances and classes. Additionally, it allows for finding analogous shapes through linear manipulations and euclidean nearest neighbor searching. The latent space allows for morphing between shapes of different classes by interpolating their representations. Shape analogies can be found through linear manipulations and euclidean nearest neighbor searching, demonstrating the euclidean nature of the latent space. In this section, the authors demonstrate finding shape analogies in the latent space by manipulating the difference vector between shapes A and B. They use images from meshes to visualize the process and discuss point-cloud generators working with voxel-based autoencoders. The authors present preliminary results of point-cloud generators working with voxel-based autoencoders. They used a full-GMM model with 32 centers on ShapeNet's chair class, comparing the results with \"pure\" point-cloud generators by converting voxel-grids into 2048 points. The authors used a full-GMM model with 32 centers on ShapeNet's chair class to generate voxel-grids, which were then converted into 2048 points for comparison with other models. They also compared their results with Wu et al.'s voxel-based GANs. The latent AE-based GMM models outperformed the \"raw\" GAN architecture for voxel grids. For more details and quantitative results, refer to Table 7. The authors used a full-GMM model with 32 centers on ShapeNet's chair class to generate voxel-grids, which were then converted into 2048 points for comparison with other models. Comparing against Wu et al.'s voxel-based GANs, the latent AE-based GMM models outperformed the \"raw\" GAN architecture significantly. The latent representation (voxel GMM) provided a vast improvement over the \"raw\" voxel GAN architecture, indicating an advantage. AE-based GMM models outperform Wu et al.'s \"raw\" GAN architecture significantly, indicating an advantage of using latent representations for generation in the voxel modality. The performance of the 64^3 voxel-based GMM is comparable to the one operating at 32^3 resolution, suggesting that fidelity of results is not affected by high-frequency details in the ground-truth data. The 64^3 voxel-based GMM performs comparably to the 32^3 resolution model, indicating fidelity is not affected by high-frequency details. Point-cloud-based models outperform voxel-based ones in fidelity to ground truth, as measured by MMD. The coverage boost of voxel-based latent-space models is likely due to how the coverage metric is computed. The voxel-based models frequently produce shapes with missing components, leading to an artificial increase in coverage when matched against ground truth. The voxel-based models often generate shapes with missing components, resulting in poor quality matchings with ground truth models. This artificial increase in coverage is confirmed through qualitative inspection. The histogram in FIG0 shows distances between GMM-generated samples and their closest matches in the ground truth. The voxel-based method has a heavier \"tail\" indicating poor quality matchings. Qualitative inspection confirmed that the voxel-based output covered ground truth models with very poor quality partial shapes. The voxel-based method produced covering from poor quality partial shapes. The models used GMMs with full covariances and different dimensional latent codes. The mesh conversion utilized the marching cubes algorithm. The models utilized GMMs with full covariances and varying dimensional latent codes. Mesh conversion was done using the marching cubes algorithm. Voxel-based AEs were fully-convolutional with specific layer parameters listed. The fully-convolutional voxel-based autoencoders have encoder layers with 3D-Conv-ReLU and decoder layers with 3D-Conv-Relu-transpose. The parameters of consecutive layers are listed, with a stride of 2 for most layers except the last layer of the decoder. Training is done for 100 epochs with Adam optimizer and binary cross-entropy loss. The last layer of the 32 3 decoder does not use a non-linearity. The abbreviation \"bn\" stands for batch-normalization. The autoencoders are trained for 100 epochs with Adam optimizer and binary cross-entropy loss, using a learning rate of 0.001 and a batch size of 64. The reconstruction quality of the voxel AE architectures is compared to the state-of-the-art method BID28 for the ShapeNetCars dataset. The learning rate was 0.001, \u03b2 1 0.9, and batch size 64. Comparison of voxel AE architectures to BID28 for ShapeNetCars dataset in terms of reconstruction quality using a 0.5 occupancy threshold. GMM-generator performance evaluated against a model memorizing training data. The GMM-generator is compared against a model that memorizes training data for the chair class. Different sizes of training sets are considered, and metrics are evaluated between these \"memorized\" sets and the point-clouds of the test split. The coverage/fidelity obtained by the generative models is slightly lower than the equivalent. The model memorizes the training data for the chair class, either considering the entire set or sub-sampling it. Metrics are evaluated between the \"memorized\" sets and test point-clouds, showing slightly lower coverage/fidelity compared to the equivalent case. This validates the metrics, highlighting the advantage of learning the underlying space structure. The model memorizes the training data for the chair class, showing slightly lower coverage/fidelity compared to the equivalent case. This validates the metrics, highlighting the advantage of learning the underlying space structure and generating novel shapes. Some mode collapse is present in the generative results, with a 10% drop in coverage, but the achieved MMD of the generative models is almost identical to that of the memorization case. The text chunk discusses the compact representation of data in generative models, showcasing novel shape generation through interpolations. Despite some mode collapse, the fidelity of the generative models is comparable to memorization cases. Additional comparisons with BID32 models are provided in Tables 10, 11, and 12 for ShapeNet classes. JSD-based and MMD/Coverage comparisons are presented for different models on the test split. The text discusses comparisons with BID32 models for ShapeNet classes, including JSD-based and MMD/Coverage comparisons on the test split. Full GMM/32 represents a GM model trained on the latent space of an AE with EMD structural loss. The l-GAN uses the same adversarial objective as BID31. Generalization error of various GAN models is shown in FIG0. The text provides MMD/Coverage comparisons on the test split following the same protocol as in Table 10, focusing on JSD-based comparison between BID31 and generative models. Generalization error of various GAN models is also discussed, along with GMM model selection based on varying number of Gaussians and covariance type. The text discusses the generalization error of various GAN models using JSD and MMD-CD metrics. It also explores GMM model selection based on the number of Gaussians and covariance type. Models with full covariance matrices show lower JSD compared to those with diagonal covariance, with 30 or more clusters being sufficient for minimal JSD. In GMM model selection, models with full covariance matrices achieve lower JSD compared to diagonal covariance models. 30 or more clusters are sufficient for minimal JSD. The values in a typical covariance matrix of a Gaussian component are shown in pseudocolor, highlighting strong off-diagonal components. The 32 centers of the GMM fitted to the latent codes are decoded using the decoder of the AE-EMD. Evaluation of five generators on test-split of chair data shows consistent model quality regardless of the selection metric used. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. The selected models, determined by minimal MMD-CD on the validation-split, show consistent quality regardless of the metric used. GMM-40-F refers to a GMM with 40 Gaussian components with full covariances. Synthetic point-clouds are sampled for each model, three times the size of the ground truth test dataset, to measure how well they match in terms of MMD-CD. This evaluation complements a previous measure used in FIG2."
}