{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. This approach addresses the issue of vanishing gradients and provides a theoretical justification for its effectiveness in minimizing the training loss. In this paper, the theoretical justification of the concept of STE in training activation quantized neural networks is provided. The unusual \"gradient\" given by the STE-modified chain rule is referred to as coarse gradient, and its correlation with the population gradient is explored. The paper explores the concept of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. The STE-modified chain rule introduces a \"coarse gradient\" that correlates positively with the population gradient when properly chosen. A poor choice of STE can lead to instability in the training algorithm. The paper discusses the use of a coarse gradient descent algorithm for minimizing population loss in deep neural networks. It also highlights the impact of a poor choice of STE on training algorithm stability, as demonstrated in CIFAR-10 experiments. Deep neural networks have been successful in various machine learning applications, but their deployment requires significant memory and computational resources. Recent efforts have focused on training methods to achieve memory savings and energy efficiency during inference. Recent efforts have focused on training coarsely quantized DNN to achieve memory savings and energy efficiency during inference, while maintaining performance. This optimization problem involves minimizing a nonconvex function. Efforts have been made to train coarsely quantized DNN while maintaining performance. Weight quantization of DNN has been extensively studied in the literature. Weight quantization of DNN is a key research area, with efforts to train coarsely quantized DNN while maintaining performance. The gradient in training activation quantized DNN is mostly zero, requiring a non-trivial search direction to address this issue. The gradient in training activation quantized DNN is mostly zero, requiring a non-trivial search direction. The straight-through estimator (STE) is used to replace the zero derivative of quantized activation function in the chain rule. Bengio et al. (2013) also proposed an alternative approach based on stochastic neurons. The straight-through estimator (STE) is a proxy derivative used in the backward pass to replace the zero derivative of quantized activation functions in the chain rule. It originates from the perceptron algorithm for learning single-layer perceptrons. Additionally, Friesen & Domingos (2017) proposed the feasible target propagation algorithm for learning hard-threshold networks via convex combinatorial optimization. The STE is a derivative proxy used in the backward pass to replace zero derivatives of quantized activation functions. It stems from the perceptron algorithm for single-layer perceptrons and has been extensively discussed in the literature for its convergence. The perceptron algorithm for single-layer perceptrons uses a modified chain rule with the derivative of the identity function as a proxy for the original derivative of the binary output function. Convergence of this algorithm has been extensively discussed in the literature. Hinton (2012) extended this idea to train multi-layer networks with binary activations, while Bengio et al. (2013) proposed a variant using the derivative of the sigmoid function. In the literature, various approaches have been proposed for training deep neural networks with binary activations. Hinton (2012) introduced the idea of training multi-layer networks with binary activations, while Bengio et al. (2013) proposed a variant using the derivative of the sigmoid function. Hubara et al. (2016) and (2018) further extended this concept by employing the Straight-Through Estimator (STE) for training DNN with quantized ReLU activations. In training deep neural networks with binary activations, the Straight-Through Estimator (STE) was used to substitute the derivative of the signum activation function in the backward pass. This approach has been applied to DNN with general quantized ReLU activations, including derivatives of vanilla ReLU and clipped ReLU. Despite empirical success, there is limited theoretical understanding of STE in training DNN with stair-case activations. Goel et al. (2018) considers leaky ReLU. ReLU activations, including derivatives of vanilla ReLU and clipped ReLU, have been used in training deep neural networks. Despite empirical success, there is limited theoretical understanding of the Straight-Through Estimator (STE) in DNN with stair-case activations. Recent studies have explored scenarios where certain layers are not ideal for back-propagation. Goel et al. (2018) studied leaky ReLU activation in a one-hidden-layer network and demonstrated the convergence of the Convertron algorithm using the STE in the backward pass. Wang et al. (2018) and Athalye et al. (2018) also discussed scenarios where certain layers are not suitable for back-propagation, proposing solutions such as an implicit weighted nonlocal Laplacian layer and using pre-trained fully connected layers in the backward pass for improved generalization accuracy and adversarial defense. In their study, Athalye et al. (2018) introduced a backward pass differentiable approximation to improve adversarial defense, breaking defenses that rely on obfuscated gradients. They referred to the \"gradient\" of the loss function as coarse gradient using the STE-modified chain rule. The backward and forward passes were noted to not match. The study by Athalye et al. (2018) introduced a backward pass differentiable approximation to improve adversarial defense by breaking defenses relying on obfuscated gradients. They referred to the \"gradient\" of the loss function as coarse gradient using the STE-modified chain rule. The backward and forward passes were noted to not match, leading to questions about the optimization process. The coarse gradient, not matching the standard gradient, raises questions about optimization. The choice of STE is non-unique, leading to the search for a good STE. Three representative STEs are considered for learning a two-linear-layer network with binary activation and Gaussian data. From an optimization perspective, understanding the training of quantized ReLU nets involves exploring different STEs. Three representative STEs for learning a two-linear-layer network with binary activation and Gaussian data are considered: derivatives of the identity function, vanilla ReLU, and clipped ReLUs. Proper choices of STE are shown to lead to descent training algorithms. Proper choices of Straight Through Estimators (STE) for training quantized ReLU nets lead to descent training algorithms. Negative expected coarse gradients based on STEs of vanilla and clipped ReLUs are descent directions for minimizing population loss, resulting in monotonically decreasing energy during training. Identity STE can lead to unstable training near certain local minima. The negative expected coarse gradients of vanilla and clipped ReLUs based on STEs are descent directions for minimizing population loss, leading to monotonically decreasing energy during training. However, the identity STE can cause instability near certain local minima. Empirical performance shows that clipped ReLU STE is the best choice for deeper networks like VGG-11. In empirical performance tests, clipped ReLU STE is found to be the best choice for deeper networks like VGG-11, while identity or ReLU STE can lead to instability at certain local minima during training on CIFAR-10. In CIFAR experiments, it was observed that using identity or ReLU STE can lead to instability at good minima, resulting in higher training loss and decreased generalization accuracy. Poor STEs generate coarse gradients incompatible with the energy landscape, as supported by theoretical findings about the identity STE. Convergence guarantees for the perceptron algorithm and Convertron algorithm were proven for the identity STE. In CIFAR experiments, using identity or ReLU STE can lead to instability at good minima, resulting in higher training loss and decreased generalization accuracy. Poor STEs generate coarse gradients incompatible with the energy landscape. Convergence guarantees for the perceptron algorithm and Convertron algorithm were proven for the identity STE, but do not generalize to networks with two trainable layers. The identity STE is a poor choice in this case, and it is unclear if their analyses can be extended to other STEs. In contrast to previous studies, the identity STE is not suitable for the network with two trainable layers analyzed here. The quantized activation function's monotonicity is crucial for gradient descent, with all three STEs leveraging this property. The clipped ReLU STE is highlighted as a superior choice to avoid instability issues. The quantized activation function's role in gradient descent is crucial, with all three considered STEs exploiting this property. The clipped ReLU STE is recommended to avoid instability issues. Section 2 analyzes the energy landscape of a two-linear-layer network with binary activation, while section 3 presents the main results for STE. Empirical performance comparisons of different STEs in 2-bit and 4-bit activation quantization are discussed in section 4, highlighting instability issues observed in poor STEs during CIFAR training. The energy landscape of a two-linear-layer network with binary activation and Gaussian data is analyzed in section 2. Section 3 presents the main results and mathematical analysis for STE. Section 4 compares the empirical performances of different STEs in 2-bit and 4-bit activation quantization, highlighting instability issues observed in poor STEs during CIFAR experiments. Technical proofs and figures are deferred to the appendix. The text discusses poor STEs observed in CIFAR experiments. Technical proofs and figures are deferred to the appendix. Notations include Euclidean norm, spectral norm, zero vector, one vector, identity matrix, inner product, and Hadamard product. The model considered is similar to Du et al. (2018) with trainable weights w and v, outputting predictions for input Z. The model discussed is similar to Du et al. (2018) with trainable weights w and v, outputting predictions for input Z. The first layer acts as a convolutional layer, treating each row of Z as a patch, and the second layer serves as the classifier. The model discussed has two layers - a convolutional layer treating rows of Z as patches and a linear classifier. The label is generated using true parameters v* and w*. The activation function \u03c3 is a binary function. The second linear layer acts as the classifier, generating labels based on parameters v* and w*. The activation function used is a binary function, not ReLU. The entries of Z are sampled from a Gaussian distribution. The learning task is framed as a population loss minimization problem. The learning task involves minimizing the population loss by framing it as a problem with Gaussian-distributed Z entries. The gradient of the objective function is not directly accessible for network training, so only the expected sample gradient can be used. The learning task involves minimizing the population loss with Gaussian-distributed Z entries. The gradient of the objective function is not directly accessible for network training, so only the expected sample gradient can be used. The idea of STE is to replace the a.e. zero component \u03c3 with a related non-trivial function \u00b5. The idea of STE is to replace the a.e. zero component \u03c3 with a related non-trivial function \u00b5, which is the derivative of some (sub)differentiable function. Using the STE \u00b5 to train a two-linear-layer CNN with binary activation gives rise to the coarse gradient. The coarse gradient descent for learning a two-linear-layer CNN with STE \u00b5 involves using a non-trivial surrogate of the partial gradient. Preliminaries about the population loss function landscape are presented, defining the angle between vectors w and w*. The text discusses the coarse gradient descent for learning a two-linear-layer CNN with STE \u00b5. It presents preliminaries about the population loss function landscape and defines the angle between vectors w and w*. The population loss function and partial gradients of f (v, w) w.r.t. v and w are elaborated. The text elaborates on the analytic expressions of the population loss function and partial gradients of f (v, w) w.r.t. v and w. It discusses the possible local minimizers of the model and conditions for stationary and non-differentiable points. The text discusses the possible local minimizers of the model, including stationary points and non-differentiable points, which are potential global minimizers. It also states that stationary points, if they exist, can only be saddle points, while non-differentiable points are spurious local minimizers. The model has no saddle points or spurious local minimizers otherwise. The text discusses global minimizers and local minimizers of the model, stating that stationary points can only be saddle points, while non-differentiable points are spurious local minimizers. The model has no saddle points or spurious local minimizers otherwise. The population gradient is proven to be Lipschitz continuous on bounded domains. The text discusses the absence of saddle points or spurious local minimizers in the model, proving the Lipschitz continuity of the population gradient on bounded domains. It focuses on the complex case with both types of points present and explores the behavior of gradient descent algorithms using various activation functions. The text explores the behavior of gradient descent algorithms in the presence of saddle points and spurious local minimizers. It shows that using the derivative of vanilla or clipped ReLU activation functions leads to convergence to a critical point, while using the identity function does not. Algorithm 1 using the derivative of ReLU or clipped ReLU converges to a critical point, while the identity function does not. The objective sequence decreases monotonically with a small learning rate, leading to convergence to a saddle point or local minimizer. The convergence guarantee for the coarse gradient descent is established under the assumption of infinite training samples, where the objective sequence decreases monotonically with a small learning rate, leading to convergence to a saddle point or local minimizer. The convergence guarantee for Algorithm 1 with the identity function \u00b5(x) = x near local minimizers satisfying \u03b8(w, w*) = \u03c0 does not hold. Coarse gradient descent converges under the assumption of infinite training samples. With few data, empirical loss descends along negative coarse gradient, gaining monotonicity and smoothness as sample size increases. This explains the success of STE in deep learning with large data. The empirical loss descends along negative coarse gradient as sample size increases, gaining monotonicity and smoothness. This explains the success of STE in deep learning with large data. The same results hold even if the Gaussian assumption on input data is weakened. The proof for the main results involves mathematical analysis, with plots showing empirical loss moving based on sample size and learning rate. Choosing the derivative of ReLU as the STE simplifies the expressions for expected coarse gradient. The expected coarse gradient with non-negative correlation with the population partial gradient is a descent direction for minimizing the population loss. If w = 0 and \u03b8(w, w*) \u2208 (0, \u03c0), then the inner product between them is considered. Lemma 5 states that the coarse partial gradient has non-negative correlation with the population partial gradient, forming a descent direction for minimizing the population loss. If w = 0 and \u03b8(w, w*) \u2208 (0, \u03c0), the inner product between the expected coarse and population gradients w.r.t. w is considered. The inner product between the expected coarse and population gradients w.r.t. w is considered when w * ) \u2208 (0, \u03c0). Additionally, the significance of the estimate (12) in guaranteeing the descent property of Algorithm 1 is highlighted. The coarse gradient descent behaves like gradient descent on f(v, w). The estimate (12) guarantees the descent property of Algorithm 1 by ensuring monotonically decreasing energy until convergence. When Algorithm 1 using ReLU STE converges, it can only converge to a critical point of the population loss. When Algorithm 1 converges, the energy decreases until convergence, and the gradients vanish simultaneously at saddle points. The use of ReLU STE ensures convergence to a critical point of the population loss function. For clipped ReLU STE, results similar to Lemmas 5 and 6 are observed, with positive correlation between the coarse partial gradient and the true partial gradient of the population loss. The use of clipped ReLU STE ensures convergence to critical points of the population loss function, with positive correlation between coarse and true gradients. If certain conditions are met, the coarse gradient only vanishes at critical points. The coarse gradient vanishes at critical points when Algorithm 1 converges, with the derivative of the identity function showing similar results to previous Lemmas. The coarse gradient vanishes at critical points when Algorithm 1 converges, but the derivative of the identity function does not vanish at local minima, potentially preventing convergence. The coarse gradient derived from the identity function may not vanish at local minima, hindering Algorithm 1's convergence. Lemmas 9 and 10 provide conditions where the expected coarse gradient does not vanish at local minimizers, affecting convergence near spurious minimizers. Lemma 10 states that if the inner product between expected coarse and true gradients does not vanish at local minimizers, the descent property may not hold, leading to instability during training. The theory suggests that while vanilla and clipped ReLUs can learn a two-linear-layer CNN, their performance differs on deeper networks. Comparing the identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized data, it is observed that the training loss may increase and instability arises when the descent property does not hold due to the inner product not vanishing at local minimizers. In this section, the performances of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations are compared. The instability issue of using an improper STE in training algorithms is also reported. In this section, the performances of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations are compared. The instability issue of using an improper STE in training algorithms is also reported. The resolution \u03b1 for the quantized ReLU needs to be carefully chosen to maintain full-precision level accuracy. In experiments, weights are kept float. The resolution \u03b1 for quantized ReLU must be carefully chosen for accuracy. A modified batch normalization layer is used without scale and shift to approximate a unit Gaussian distribution. \u03b1 is pre-computed using Lloyd's algorithm on simulated data and fixed during training. In experiments, weights are kept float. The resolution \u03b1 for quantized ReLU must be carefully chosen for accuracy. A modified batch normalization layer is used to approximate a unit Gaussian distribution. \u03b1 is pre-computed using Lloyd's algorithm on simulated data and fixed during training. The optimizer used is stochastic gradient descent with momentum = 0.9. In experiments, weights are kept float. A modified batch normalization layer is used to approximate a unit Gaussian distribution. The optimizer used is stochastic gradient descent with momentum = 0.9. We add batch normalization prior to each activation layer. The quantization approach used is similar to HWGQ, with uniform quantization. Training epochs are 50 for LeNet-5 on MNIST, and 200 for VGG-11 and ResNet-20 on CIFAR-10. Parameters are initialized with pre-trained full-precision counterparts. Learning rate schedule is specified in TAB2 in the appendix. The experiments used gradient descent with momentum = 0.9. Training lasted 50 epochs for LeNet-5 on MNIST, and 200 epochs for VGG-11 and ResNet-20 on CIFAR-10. Parameters were initialized with pre-trained full-precision counterparts. The learning rate schedule is in TAB2 in the appendix. Results in Table 1 show training losses and validation accuracies. Clipped ReLU derivative performed best, followed by vanilla ReLU and then the identity function. The experimental results in Table 1 show that the derivative of clipped ReLU outperforms vanilla ReLU and the identity function. Clipped ReLU performs best in deeper networks, while vanilla ReLU shows comparable performance in shallow networks like LeNet-5. The identity function leads to being repelled from a good minimum in ResNet-20 with 4-bit activations. The experimental results in Table 1 demonstrate that clipped ReLU outperforms vanilla ReLU and the identity function. Clipped ReLU performs best in deeper networks, while vanilla ReLU shows comparable performance in shallow networks like LeNet-5. The identity function leads to being repelled from a good minimum in ResNet-20 with 4-bit activations. In Table 1, coarse gradient descent algorithms using vanilla and clipped ReLUs converge to minima with validation accuracies of 86.59% and 91.24% respectively, while the identity function gives 54.16%. The experimental results in Table 1 show that using the identity function leads to instability in training with 4-bit activations. The coarse gradient descent algorithms using vanilla and clipped ReLUs converge to minima with validation accuracies of 86.59% and 91.24% respectively, while the identity function gives 54.16%. Training is then initialized with the two improved minima using the identity function, and stability is tested with a tiny learning rate of 10^-5. The experimental results show that using the identity function leads to instability in training with 4-bit activations. Training with the identity function gives 54.16% accuracy, while using other functions gives higher accuracies. Training is initialized with the two improved minima using the identity function, but the training loss and validation error increase significantly within the first 20 epochs. The training loss and validation error increase within the first 20 epochs, leading to a switch to a normal learning rate schedule at epoch 20. The identity STE results in a worse minimum due to coarse gradients, while ReLU STE on 2-bit activated ResNet-20 shows instability at good minima. The coarse gradient with identity STE does not vanish at good minima, leading to poor performance of ReLU STE on 2-bit activated ResNet-20. The training algorithm is unstable at good minima, as shown in Figure 4. When initialized with weights from vanilla and clipped ReLUs on ResNet-20 with 4-bit activations, coarse gradient descent using identity STE is repelled. The learning rate is set to 10^-5 until epoch 20, providing theoretical justification for the concept of STE in descent training algorithms. The coarse gradient descent using identity STE is repelled from good minima when initialized with weights from vanilla and clipped ReLUs on ResNet-20 with 4-bit activations. The learning rate is set to 10^-5 until epoch 20, providing theoretical justification for the concept of STE in descent training algorithms. The study compared three different Smoothed Thresholding Estimators (STEs) for training a two-linear-layer CNN with binary activation: derivatives of the identity function, vanilla ReLU, and clipped ReLU. It was found that negative expected coarse gradients based on vanilla and clipped ReLUs are effective for minimizing population loss, while the identity STE is not due to generating incompatible coarse gradients. CIFAR experiments confirmed issues with improper STE choices. Future work aims to enhance understanding of coarse gradient descent for large-scale optimization. The study compared three Smoothed Thresholding Estimators (STEs) for training a two-linear-layer CNN with binary activation: derivatives of the identity function, vanilla ReLU, and clipped ReLU. Negative expected coarse gradients from vanilla and clipped ReLUs are effective for minimizing population loss, while the identity STE generates incompatible gradients. CIFAR experiments confirmed issues with improper STE choices. Future work aims to enhance understanding of coarse gradient descent for large-scale optimization. In experiments, coarse gradient descent using ReLU STE with a 10^-5 learning rate on ResNet-20 with 2-bit activations led to instability and increased errors. The study compared three Smoothed Thresholding Estimators (STEs) for training a two-linear-layer CNN with binary activation. Experiments showed that coarse gradient descent using ReLU STE with a 10^-5 learning rate on ResNet-20 with 2-bit activations led to instability and increased errors. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. It proves three identities related to these vectors, including using the polar representation of two-dimensional Gaussian random variables. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors, proving three identities. Using the polar representation of two-dimensional Gaussian random variables, it is shown that E{z_i | z^Tw>0, z^Tw>0} = 0 for i \u2265 3. Additionally, for Gaussian random vector z with entries sampled from N(0,1) and nonzero vectors w, w with angle \u03b8, E{z_i | 0<z^Tw<1} = 0 for i \u2265 3 and E{z_2 | 0<z^Tw<1, z^Tw>0} = q(\u03b8, w). Lemma 12 states that for Gaussian random vector z with entries sampled from N(0,1) and nonzero vectors w with angle \u03b8, E{z_i | 0<z^Tw<1} = 0 for i \u2265 3. Additionally, E{z_2 | 0<z^Tw<1, z^Tw>0} = q(\u03b8, w). The proof involves rearrangement inequality and trigonometric functions. Lemma 13 states that for Gaussian random vector z with entries sampled from N(0,1) and nonzero vectors w with angle \u03b8, E{z_i | 0<z^Tw<1, z^Tw>0} = q(\u03b8, w). The proof involves rearrangement inequality and trigonometric functions. The proof of Lemma 14 involves Cauchy-Schwarz inequality and trigonometric functions, showing the relationship between vectors w and w*. If w = 0, the population loss f(v, w) is determined. Lemma 1 states that if w = 0, the population loss f(v, w) can be calculated. Lemma 2 shows the partial gradients of f(v, w) with respect to v and w when w = 0 and the angle between w and w* is between 0 and \u03c0. Lemma 2 demonstrates the partial gradients of f(v, w) when w = 0 and the angle between w and w* is between 0 and \u03c0. The proof shows the optimality of the stationary points. The text discusses the optimality of stationary points by analyzing the Hessian matrix and determining that they are saddle points. The objective function is rewritten as DISPLAYFORM14, with its Hessian matrix being indefinite, indicating saddle points. An arbitrary point in the neighborhood of (v, \u03c0) is considered, showing that the perturbed objective value is greater than the original value. The perturbed objective value in the neighborhood of (v, \u03c0) with \u2206\u03b8 \u2264 0 is greater than the original value. This is due to the unique minimizer v = (I m + 1 m 1 m ) \u22121 (1 m 1 m \u2212 I m )v * for the quadratic function f (v, \u03c0). If \u2206v = 0 m, then for sufficiently small \u2206v, \u2206\u03b8 \u00b7 (v + \u2206v) v * > 0. Therefore, f (v + \u2206v, \u03c0 + \u2206\u03b8) > f (v, \u03c0) for small and non-zero (\u2206v, \u2206\u03b8). The perturbed objective value in the neighborhood of (v, \u03c0) with small and non-zero perturbations is greater than the original value. There exists a Lipschitz constant L > 0 depending on certain parameters, validating this claim. The expected partial gradient w.r.t. v and coarse gradient w.r.t. w are also discussed in the context. The expected partial gradient and coarse gradient w.r.t. v and w are discussed in Lemmas 4 and 5, respectively, validating the claims made in the context. The inner product between w and w* is also analyzed in Lemma 5. Lemma 4 proves the first claim using \u00b5 = \u03c3 = 1 {x>0}. Lemma 5 shows that if w = 0n and \u03b8(w, w*) \u2208 (0, \u03c0), the inner product between expected coarse and true gradients w.r.t. w is given. Lemma 5 states that for w = 0n and \u03b8(w, w*) \u2208 (0, \u03c0), the inner product between expected coarse and true gradients w.r.t. w is given. Additionally, if v \u2264 Cv and w \u2265 cw, there exists a constant Arelu > 0 such that saddle points satisfying certain conditions can be identified. Lemma 6 shows that under certain conditions, specific expressions for variables can be derived. If w = 0n and \u03b8(w, w*) \u2208 (0, \u03c0), then a particular formula is satisfied. Lemma 7 provides further conditions and expressions for variables under specific constraints. The inner product between expected gradients is computed, and additional inequalities lead to the existence of a constant A crelu > 0. Lemma 7 introduces conditions and expressions for variables under constraints. The inner product between expected gradients is calculated, leading to the existence of a constant A crelu > 0. Lemma 8 is proven similarly to Lemma 6, with q(\u03b8, w) from Lemma 12 being non-negative and only equal to 0 at \u03b8 = 0, \u03c0, and p(0, w) being greater than or equal to p(\u03b8, w) and p(\u03c0, w) = 0. Lemma 9 states that the expected coarse partial gradient with respect to w is calculated using a specific formula. The proof involves various mathematical identities and equations. Lemma 10 states that if w = 0n and \u03b8(w, w*) \u2208 (0, \u03c0), then the inner product between the expected coarse and true gradients with respect to w is calculated."
}