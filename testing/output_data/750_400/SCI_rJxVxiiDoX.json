{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits. Additionally, combining quantization-aware training with weight matrix factorization reduces model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training, combined with weight matrix factorization, reduces model size and computation for keyword spotting while maintaining performance. Context information is incorporated by stacking frames in the input. Common quantizations in the industry are 16 bit and 8 bit. Quantization degrades performance on device, but can be mitigated through approaches like tuning a threshold. Deployed keyword spotting models are always quantized. Incorporating context by stacking frames in the input is essential for keyword spotting models. Quantization, common in the industry at 16 bit and 8 bit, degrades performance on device. To mitigate this, quantization-aware training is used to optimize weights against quantization errors, resulting in a small-footprint model. Quantization-aware training is used to optimize weights against quantization errors in order to build a small-footprint low-power keyword spotting system. This method enables successful training of 8 bit and 4 bit quantized KWS models. The paper discusses training a small-footprint low-power keyword spotting system using quantization-aware training. Successful training of 8 bit and 4 bit quantized KWS models is achieved. Dynamic quantization approach is used for DNN weight matrices, with inputs quantized row-wise on the fly. The dynamic quantization approach is used for DNN weight matrices in training a small-footprint low-power keyword spotting system. Inputs are quantized row-wise on the fly, and the accuracy loss due to quantization is addressed through quantization-aware training. The keyword 'Alexa' is chosen for experiments, using a 500 hrs far-field corpus for training and a 100 hrs dataset for evaluation. Models are evaluated using end-to-end Detection Error Tradeoff (DET) curves. Loss due to quantization is incorporated via quantization-aware training in the training process. An in-house 500 hrs far-field corpus and a 100 hrs dataset are used for evaluation of 70 models. Training is done using GPU-based distributed DNN training method in 3 stages with a small ASR DNN pre-trained in the 1st stage. The training process involves 3 stages using a GPU-based distributed DNN training method. A small ASR DNN is pre-trained in the 1st stage with full ASR phone-targets. The models' performance is evaluated based on miss rate vs. false accept rate and DET area under curve. The 'naively quantized' models show an improvement in AUC with quantization-aware training. In another 20 epochs, the performance of the 'naively quantized' models is shown in TAB0. AUC improvement of quantized models' performance using quantization-aware training is observed. DET curves for full-precision, quantized, and quantization-aware trained 50k model are shown in Figure 2. DET curves for 16 bit and 8 bit quantized-models are not significantly different from the full-precision model."
}