{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging. A non-statistical framework based on identifying modular organization of the network is proposed, relaxing the requirement of exploiting statistical independence. Experiments show modularity between groups of channels is achieved to a certain degree. The proposed non-statistical framework identifies modular organization in deep generative models, allowing targeted interventions on image datasets for applications like style transfer and robustness assessment in pattern recognition systems. Deep generative models, such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE), have been successful in designing realistic images in various domains. These models enable targeted interventions on image datasets for applications like style transfer and assessing robustness in pattern recognition systems. Non-linear functions mapping latent space to observations, used in designing realistic images in complex domains. State-of-the-art approaches include Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE). Efforts made to create models with disentangled latent representations for controlling interpretable image properties. Models may not be mechanistic or causal in interpreting image properties. Efforts have been made to create models with disentangled latent representations for controlling interpretable image properties. However, these models may not be mechanistic or causal in interpreting image properties down to specific parts of the network architecture. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds, which aligns with human representational capabilities. Extrapolations are essential for human representational capabilities and support adaptability to environmental changes. Leveraging deep generative architectures for extrapolations is challenging due to non-linearities and high dimensionality. In this paper, a causal framework is proposed to explore modularity in deep generative architectures for extrapolations, addressing the challenge of non-linearities and high dimensionality. The framework relates to the causal principle of Independent Mechanisms, stating that causal mechanisms do not influence each other. The paper proposes a causal framework to explore modularity in deep generative architectures, based on the principle of Independent Mechanisms. This framework allows for the study of direct interventions in the network without affecting other causal mechanisms. The study explores the effect of direct interventions in the network by modifying individual mechanisms without affecting each other. It applies this principle to generative models to assess how well they capture causal mechanisms and uses counterfactuals to evaluate the role of specific internal variables in deep generative models. The analysis focuses on disentanglement within a causal framework. The study examines the impact of altering individual mechanisms in generative models to evaluate their ability to capture causal mechanisms. It uses counterfactuals to assess the role of internal variables in deep generative models and analyzes disentanglement within a causal framework. The study demonstrates how VAEs and GANs trained on image databases exhibit modularity in their hidden units, enabling counterfactual editing of generated images. The study analyzes disentanglement in generative models through unsupervised counterfactual manipulations, showing modularity in hidden units of VAEs and GANs. It relates to interpretability of CNNs and highlights the challenges in generative models compared to discriminative architectures. The study explores disentanglement in generative models, emphasizing the challenges compared to discriminative architectures. It introduces the concept of intrinsic disentanglement to uncover internal representations. In contrast to previous works on disentanglement, this study introduces the concept of intrinsic disentanglement to uncover the internal organization of networks. The study introduces the concept of intrinsic disentanglement to reveal the internal structure of networks. It contrasts with other approaches by being more flexible and not requiring semantic information or group representation theory. Our approach to disentanglement, introduced independently, is more flexible than previous proposals and applies to arbitrary continuous transformations. Suter et al. (2018) also take an interventional approach to disentanglement in a classical graphical model setting. In a general framework, a generative model M implements a function g M that maps a latent space Z to a manifold Y M for learned data points. This theory section will be presented informally for high-level understanding, with mathematical details provided in Appendix A. The theory section introduces the concept of disentanglement and links it to causal concepts. A generative model M maps a latent space Z to a manifold Y M where data points are located. The model generates samples by drawing from a prior latent variable distribution in Z. The term representation refers to a mapping from Y M to a representation space R. The model maps latent space Z to manifold Y M in ambient Euclidean space Y. Samples are generated by drawing from a prior latent variable distribution in Z. The term representation refers to a mapping from Y M to representation space R, with M being the latent representation of the data. The generative model is implemented by a non-recurrent neural network with a causal graphical model representation. The latent representation M maps to the representation space R. A causal generative model (CGM) is used to implement the generative model with a non-recurrent neural network. Endogenous variables can be chosen as nodes in the causal graph. The computational graph implements the mapping g M through a series of operations, creating a Causal Generative Model (CGM). Endogenous variables represented by nodes in the causal graph are used to compute the mapping g M. This is not a statistical independence, as the quantities transformed by the mechanisms can influence each other and be statistically dependent. The mapping g M is computed by composing endogenous variable assignment v M with endogenous mapping g M in the causal graph. This is not statistical independence as the quantities transformed by mechanisms can influence each other and be statistically dependent. The endogenous variables are represented by nodes in the causal graph. The endogenous variables in a causal graph are represented by nodes, with the internal representation of the network defined by mild conditions ensuring left-invertibility. The V k 's are constrained to subsets of smaller dimensions, denoted as endogenous image sets. The endogenous variables in a causal graph are represented by nodes, with the internal representation of the network defined by mild conditions ensuring left-invertibility. The V k 's are constrained to subsets of smaller dimensions, denoted as endogenous image sets. In the CGM framework, counterfactuals can be defined following Pearl (2014) by replacing assignments of subset variables with a vector of assignments. The CGM framework allows defining counterfactuals in the network by replacing assignments of a subset of variables with a vector of assignments. This transformation induces a change in the output of the generative model, aligning with the concept of potential outcome. The concept of counterfactuals in a generative model involves transforming the output by replacing variable assignments. Faithfulness of the counterfactual mapping ensures interventions do not generate outputs outside the model's learned distribution. In a generative model, faithfulness of counterfactual mapping ensures interventions stay within the learned distribution. Non-faithful counterfactuals may lead to artifactual outputs or extrapolation to unseen data. The classical notion of disentangled representation posits that latent variables sparsely encode real-world transformations. The classical notion of disentangled representation suggests that latent variables encode real-world transformations. This insight has driven supervised approaches to disentangling representations, while unsupervised learning approaches require different methods. In contrast to supervised approaches, unsupervised learning methods aim to learn real-world transformations from unlabeled data. State-of-the-art approaches encode these transformations through changes in individual latent factors, enforcing conditional independence between them. State-of-the-art approaches in unsupervised learning aim to encode real-world transformations by changes in individual latent factors, enforcing conditional independence between them. However, this statistical approach leads to issues such as imposing independence constraints on latent variables that may not hold for relevant properties like skin and hair color. The statistical approach in unsupervised learning imposes independence constraints on latent variables, which may not hold for relevant properties like skin and hair color. This leads to an ill-posed problem in finding a disentangled representation, hindering downstream tasks. State-of-the-art unsupervised approaches are mostly tested on synthetic datasets. Finding a disentangled representation for complex real-world datasets remains a challenge, as current unsupervised approaches are mostly tested on synthetic datasets. Disentangled generative models struggle to match the visual quality of non-disentangled models on real-world data, such as the CelebA dataset. Disentangled generative models struggle to match visual quality of non-disentangled models on complex real-world datasets beyond MNIST and CelebA. A non-statistical definition of disentanglement involves a transformation T acting on the data manifold YM, corresponding to a transformation T of the latent space. Disentanglement involves a transformation T acting on the data manifold YM, where T corresponds to a transformation of the latent space. It aims to have T act on a single variable z k, leaving other latent variables free to encode other properties. Two transformations T1 and T2 are considered disentangled if they modify different components of the latent representation. Extrinsic disentanglement involves transformations T1 and T2 acting on different components of the latent representation, following the causal principle of independent mechanisms. This notion relies on transformations of the latent representation that are exogenous to the data manifold, allowing for encoding of other properties. Extrinsic disentanglement involves transformations on the latent representation that follow the causal principle of independent mechanisms, allowing for encoding of other properties. This notion is agnostic to subjective choices and statistical independence, but still requires statistical independence between disentangled components in the latent space. The functional definition of disentangled transformation is agnostic to subjective choices and statistical independence. However, in the latent space, it still requires statistical independence between disentangled factors. To uncover statistically related properties that are disentangled according to this definition, a different representation needs to be utilized. Unlike latent variables, properties encoded by endogenous variables in a graphical model may not be statistically independent. In a graphical model, properties encoded by endogenous variables may not be statistically independent due to a common latent cause. This allows for interventions on interesting data properties independently, extending the definition of disentanglement. In a graphical model, properties encoded by endogenous variables may not be statistically independent due to a common latent cause. This allows for interventions on interesting data properties independently, extending the definition of disentanglement to include transformations of internal variables. In a graphical model, properties encoded by endogenous variables may not be statistically independent due to a common latent cause. This allows for interventions on interesting data properties independently, extending the definition of disentanglement to include transformations of internal variables that are intrinsically disentangled with respect to a subset E of endogenous variables. This indicates that finding faithful counterfactuals can be used to learn disentangled transformations, with modularity defined as a structural property of the internal representation. Modularity is defined as a structural property of the internal representation, allowing for the implementation of arbitrary disentangled transformations on a subset of endogenous variables E. If E is modular, then any transformation applied to it within its input domain is disentangled. The proof extends from Proposition 1 and can be extended to multiple modules. Modularity is a structural property of the internal representation that allows for disentangled transformations on a subset of endogenous variables E. Any transformation applied within its input domain is disentangled. The proof extends from Proposition 1 and can be extended to multiple modules. A disentangled representation is defined as a partition of the intermediate representation into modules, where transformations within each module lead to valid transformations in the data space. This partition is necessary for achieving a disentangled representation. The concept of disentangled representation involves partitioning the set of latent variables into modules, where transformations within each module lead to valid transformations in the data space. This additional requirement was not considered in classical approaches to disentanglement, which assumed each scalar variable could be treated as an independent module. This insight is relevant to artificial and biological systems, particularly in understanding the activity of multiple neurons. Our framework introduces the concept of grouping neurons into modules for disentanglement, challenging the idea of single neurons as independent modules. This \"mesoscopic\" level allows for independent intervention on each group. Biological systems require grouping neurons into modules at a \"mesoscopic\" level for independent intervention. Finding a modular structure in the network allows for a broad class of disentangled transformations. Transformations within their input domain are good candidates for disentanglement, and counterfactual interventions define transformations. Propositions 1 and 2 suggest that once a modular structure is identified in a network, various disentangled transformations become available. Transformations that remain within their input domain are considered suitable for disentanglement, and counterfactual interventions implicitly define these transformations. By assigning a constant value to a subset of endogenous variables, counterfactuals can be defined, aiming for faithful representations by constraining the constant value to belong to a specific set. Sampling from the joint marginal distribution of the variables in the subset helps avoid characterizing the set explicitly. The procedure involves assigning a constant value to a subset of endogenous variables to define counterfactuals, aiming for faithful representations by constraining the value to a specific set. Sampling from the joint marginal distribution of the variables in the subset helps avoid explicitly characterizing the set. The hybridization procedure involves selecting a subset of endogenous variables in a neural network. Two original examples of output are generated using independent latent variables. Tuple values are memorized for variables indexed by E when generating the output. The hybridization procedure involves selecting a subset of endogenous variables in a neural network. Two original examples of output are generated using independent latent variables. Tuple values are memorized for variables indexed by E when generating the output, encoding different aspects of the generated images. This allows for the generation of hybrid examples by combining features from the two original examples. The counterfactual hybridization framework assesses how a chosen module affects the output of the generator by generating pairs of latent vectors independently and quantifying the causal effect on the output. The counterfactual hybridization framework evaluates the impact of a module on the generator's output by generating pairs of latent vectors independently and quantifying the causal effect through an influence map calculation. The approach involves generating hybrid outputs from independently sampled vectors to estimate an influence map by calculating the mean absolute effect, representing unit-level causal effects in a potential outcome framework. The method considers the absolute value of causal effects and computes the average treatment effect. The approach involves generating hybrid outputs from independently sampled vectors to estimate an influence map by calculating the mean absolute effect. This represents unit-level causal effects in a potential outcome framework, where the absolute value is considered due to inconsistent signs across units. The result is averaged over different interventions corresponding to various values of z2, and then averaged across color channels to create a single grayscale heat-map pixel map. A scalar quantity is defined to quantify the magnitude of the causal effect, specifically the individual influence of module E, by averaging the influence map. The hybridization approach involves selecting subsets E to intervene on, especially in networks with many units or channels per layer. A fine to coarse approach is used to extract these groups, particularly in convolutional layers. The challenge lies in determining the subsets E to intervene on, which is addressed by estimating elementary influence. In the hybridization approach, subsets E are selected to intervene on in networks with many units or channels per layer. A fine to coarse approach is used to extract these groups, particularly in convolutional layers. Influence maps are grouped by similarity to define modules at a coarser scale. In the hybridization approach, subsets E are selected to intervene on in networks with many units or channels per layer. Influence maps are grouped by similarity to define modules at a coarser scale, as shown in representative EIMs for channels of convolutional layers of a VAE trained on the CelebA face dataset. This suggests that individual channels can be functionally segregated into modules dedicated to specific features like eyes, mouth, background, or hair. The VAE trained on the CelebA face dataset shows functionally segregated channels influencing finer face features like eyes, mouth, and background. Clustering of channels using EIMs as feature vectors is done to group them into modules dedicated to specific aspects of the output. The VAE trained on the CelebA face dataset segregates channels influencing finer face features like eyes, mouth, and background. Clustering channels using EIMs as feature vectors groups them into modules dedicated to specific output aspects through a process involving pre-processing, thresholding, and Non-negative Matrix Factorization. The VAE trained on the CelebA face dataset segregates channels influencing finer face features like eyes, mouth, and background. Clustering channels using EIMs as feature vectors groups them into modules dedicated to specific output aspects through a process involving pre-processing, thresholding, and Non-negative Matrix Factorization. The process involves mapping spatially and thresholding maps to get binary images, followed by Non-negative Matrix Factorization to obtain cluster template patterns and weights for individual maps based on template contributions. The process involves mapping spatially and thresholding maps to get binary images, followed by Non-negative Matrix Factorization to obtain cluster template patterns and weights for individual maps based on template contributions. The choice of NMF is justified by its success in isolating meaningful parts of images in different components. The approach will also be compared to the classical k-means clustering algorithm, and a toy generative model is introduced. Model 1 considers Z as a vector of K i.i.d. uniformly distributed RVs. The approach involves using Non-negative Matrix Factorization (NMF) to isolate meaningful parts of images in different components. The comparison will be made with the k-means clustering algorithm, and a toy generative model is introduced. Model 1 considers Z as a vector of K i.i.d. uniformly distributed RVs and a neural network with hidden layers composed of vector variables. The neural network has one hidden layer with vector variables mapped to the output using matrices. Model parameters are randomly chosen with specific conditions. The neural network's hidden layer has model parameters randomly chosen with specific conditions, including coefficients sampled from distributions and sets of indices enforcing module influence in image areas. The hidden layer of the neural network has model parameters with coefficients sampled from distributions and specific conditions enforcing module influence in image areas. This encodes assumptions about areas in the image being influenced by specific modules. The hidden layer of the neural network encodes assumptions about areas in the image being influenced by specific modules. For Model 1, the partition of the hidden layer corresponds to a disentangled representation, justifying the use of NMF for generating a binary matrix summarizing significant influences on each output pixel. The hidden layer partition using K vectors corresponds to a disentangled representation, justifying NMF for generating a binary matrix summarizing influences on each output pixel. Sliding window application enforces similarity between influence maps in the same module, favoring low-rank matrix factorization. The application of the sliding window enforces similarity between influence maps in the same module, favoring low-rank matrix factorization. Investigating generative models on the CelebFaces Attributes Dataset, a basic architecture like \u03b2-VAE was used. The full procedure included EIM calculations, channel clustering into modules, and hybridization. The study focused on the modularity of generative models trained on the CelebFaces Attributes Dataset (CelebA) using a basic architecture like \u03b2-VAE. The procedure involved EIM calculations, clustering of channels into modules, and hybridization of generator samples. Setting the number of clusters to 3 resulted in highly interpretable outcomes. The study involved clustering channels into modules and hybridizing generator samples. Setting the number of clusters to 3 led to interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed these results. The study clustered channels into modules, leading to interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed these results by assessing robustness of the clustering across the number of clusters. The study clustered channels into modules for background, face, and hair. Cluster stability analysis showed that 3 clusters were a reasonable choice with high consistency (>90%), dropping considerably for 4 clusters. NMF-based clustering outperformed k-means clustering. The consistency results in Supplemental Fig. 9 indicate that 3 clusters are a suitable choice with high consistency (>90%), dropping significantly for 4 clusters. NMF-based clustering outperforms k-means clustering, and the robustness of the clustering was assessed by looking at the cosine distance between templates associated with matching clusters, showing an average cosine similarity of .9 with 3 clusters. The clustering robustness was evaluated by examining the cosine distance between templates of matching clusters, with an average cosine similarity of .9 achieved with 3 clusters. Influence maps showed that some features may spread across different clusters. Applying hybridization to the 3 modules obtained by clustering resulted in feature replacement. The hybridization procedure applied to the 3 modules obtained by clustering leads to a replacement of features associated with the module intervened on, while maintaining the overall structure of the image. For example, facial features from one sample are inserted into another image while preserving certain characteristics. The hybridization procedure replaces features of the module intervened on while preserving the overall image structure. The \u03b2-VAE is designed for extrinsic disentanglement but may not be optimal compared to other approaches, suggesting further investigation into better extrinsic and intrinsic disentanglement methods. Further work has shown that the \u03b2-VAE, designed for extrinsic disentanglement, may not be optimal compared to other approaches, especially in models where disentanglement is not explicitly enforced, such as GAN-like architectures. These architectures typically outperform VAE-like approaches in terms of sample quality in complex image datasets. Investigating intrinsic disentanglement in models where disentanglement is not explicitly enforced, such as GAN-like architectures, which excel in sample quality in complex image datasets. Results were replicated in the tensorlayer DCGAN implementation, indicating applicability to non-optimized models. Pretrained Boundary Equilibrium GAN was utilized after experiments with basic models. Our approach was successfully applied to non-optimized models like the tensorlayer DCGAN implementation and a pretrained Boundary Equilibrium GAN (BEGAN), known for high-quality image generation. The simplicity of BEGAN's generator architecture allowed for minimal modifications to test our hypothesis. Boundary Equilibrium GAN (BEGAN) set a milestone in visual quality for higher resolution face images. Interventions on specific layers showed selective transfer of features, as seen in Fig. 3b. Interventions on layers 5 and 6 resulted in selective transfer of features from Original 2 to Original 1 in the BEGAN model. The model, trained on tightly cropped face images, showed clear hair transfer in one module and encoded different face features in the remaining modules. The BEGAN model, trained on tightly cropped face images, selectively transferred features from Original 2 to Original 1. One module showed clear hair transfer, while the other modules encoded different face features. The image quality was mildly affected by the hybridization procedure, as evaluated using the Frechet Inception Distance. The hybridization procedure mildly affected image quality, as assessed by the Frechet Inception Distance. The approach was tested on high-resolution generative models using the BigGAN-deep architecture pretrained on the ImageNet dataset. The hybridization procedure has a mild effect on image quality compared to original samples. The approach was tested on high-resolution generative models using the BigGAN-deep architecture pretrained on the ImageNet dataset. The architecture comprises 12 Gblocks with convolutional layers and skip connections. The conditional GAN architecture on the ImageNet dataset consists of 12 Gblocks with convolutional layers and skip connections. Hybrids were generated by mixing features of different classes, and modifying backgrounds while maintaining high quality counterfactuals. Intervening on two successive layers within a Gblock was found to be more effective in generating hybrids with different features. Examples in Fig. 4 demonstrate the ability to create high-quality counterfactuals with modified backgrounds while maintaining similar foreground objects. Even in challenging scenarios with objects of different nature, meaningful combinations are still produced, such as a teddy bear in a tree or a \"teddy-koala\" merging textures and colors. The ability to create high-quality counterfactual images with modified backgrounds while maintaining similar foreground objects is demonstrated. Meaningful combinations are still produced even in challenging scenarios with objects of different nature, such as a teddy bear in a tree or a \"teddy-koala\" merging textures and colors. The study investigates how counterfactual images can improve classifier robustness. Several pretrained classifiers were tested on recognizing original classes like teddy-bear or koala. Recognition rates increase with layer depth, especially when intervening closer to the output. The study tested pretrained classifiers on recognizing original classes like teddy-bear or koala. Recognition rates increase with layer depth, especially when intervening closer to the output. Inception resnet performed better than others at intermediate blocks 5-6. Non-consensual classification results were also observed. At intermediate blocks 5-6, Inception resnet outperformed other classifiers. Non-consensual classification results suggest different classifiers rely on different aspects of image content for decision-making. The study introduced a mathematical definition of disentanglement and related it to the causal notion of counterfactual. Evidence was found for interpretable modules of internal variables in generative models trained on real-world datasets, offering a better understanding of complex generative architectures and applications like style transfer. Our framework reveals interpretable modules in generative models trained on real-world datasets, enhancing understanding of complex architectures and applications like style transfer and object recognition system robustness assessment. This research direction focuses on improving the interpretability of deep neural networks and utilizing them for tasks they were not originally trained for, leading to more sustainable Artificial Intelligence research in the future. The goal is to enhance the controllable properties of generated images and assess the robustness of object recognition systems to contextual changes. The research aims to improve the interpretability of deep neural networks and utilize them for new tasks, promoting sustainable Artificial Intelligence research. It suggests using trained generator architectures as mechanistic models that can be manipulated independently, with a focus on structural causal models for mathematical representation. The research proposes using trained generator architectures as mechanistic models that can be manipulated independently. Structural causal models rely on structural equations to represent these models, denoting the assignment of values to variables based on other variables and exogenous influences. Structural causal models use uppercase letters for variables and lowercase for specific values, remaining valid even after interventions. They model operations in computational graphs of neural networks, depicting interdependent modules with dependencies represented by a directed acyclic graph G. The right-hand side variables change due to interventions and model operations in computational graphs of neural networks. These graphs depict structural causal models made of interdependent modules with dependencies represented by a directed acyclic graph G. A Causal Generative Model (CGM) captures computational relations between input latent variables, generator's output Y, and endogenous variables forming an intermediate representation. A Causal Generative Model (CGM) captures computational relations between input latent variables, generator's output Y, and endogenous variables forming an intermediate representation. The CGM decomposes the generator's output into two successive steps: {Z k } \u2192 {V k } \u2192 Y in a feed-forward neural network. The CGM M = G(Z, S, G) comprises a directed acyclic graph G and a set S of N + 1 deterministic continuous structural equations that assign values to endogenous variables and latent inputs, aligning with Pearl's definition of a deterministic structural causal model. The CGM M = G(Z, S, G) consists of a directed acyclic graph G and a set S of N + 1 deterministic continuous structural equations. An example CGM is shown in Fig. 2b with 3 endogenous variables, 2 latent inputs, and the output, following Pearl's definition of a deterministic structural causal model. CGMs have specificities allowing for variable assignments with or without latent/exogenous variables in their right-hand side, enabling modeling of feed-forward networks. In CGMs, variable assignments may involve latent/exogenous variables, allowing for modeling feed-forward networks with deterministic operations. This ensures unambiguous assignment of endogenous variables once z is chosen. Feed-forward networks involve a cascade of deterministic operations in downstream layers, ensuring unambiguous variable assignment. The latent distribution covers the whole latent space, while internal variables and outputs typically live on manifolds of smaller dimension. The output Y is assigned once z is chosen or an appropriate subset of variables is assigned. Latent and endogenous mappings are defined by operations of the graph. The embedded CGM is defined by operations of the graph on manifolds of smaller dimension than their ambient space, with latent and endogenous mappings assigning values from latent and endogenous variables. The mappings are constrained to subsets of their euclidean ambient space, assuming proper embeddings that are invertible. The embedded CGM involves latent and endogenous mappings on manifolds of smaller dimension, constrained to subsets of their euclidean ambient space. The mappings are proper embeddings that are invertible, ensuring unambiguous computation of outputs from inputs. The embedded CGM involves mappings on manifolds of smaller dimension, ensuring unambiguous computation of outputs from inputs. The image set Y M of a trained model should approximate the support of the data distribution we want to model. The image sets (V M, Y M, ...) of defined codomains are constrained by the parameters of M and are difficult to characterize. The image set Y M of a trained model should closely match the support of the target data distribution, a key goal for generative models. Topology of Y M is respected in transformations, using embeddings as the basic structure. Learning the generator parameters to match the support of the target data distribution is a key goal for generative models. Embeddings are used to manipulate the output while respecting the topology of the output space. Injectivity of the function is a crucial requirement for embedded CGMs. Generative models use embeddings as the basic structure, with injectivity of the function being a key requirement for embedded CGMs. If the latent space is compact and injective, the model is considered embedded. Generative models use embeddings with injectivity as a key requirement for embedded CGMs. If the latent space is compact and injective, the model is considered embedded. Proposition 4 states that a CGM is embedded if its compact, and the proof is in Appendix B. This applies to generative models with uniformly distributed latent variables. VAEs' latent space is typically not compact, but can be restricted to intervals to approximate an embedded CGM. The latent space of VAEs is typically not compact, but can be restricted to intervals to approximate an embedded CGM. This framework allows defining counterfactuals in the network following Pearl (2014). The CGM framework allows defining counterfactuals in the network following Pearl (2014). It involves replacing structural assignments for variables with specific values to generate unit-level counterfactuals. This concept aligns with potential outcome theory (Imbens & Rubin, 2015) and induces a transformation in the generative model's output. The unit-level counterfactuals in a CGM involve replacing structural assignments for variables with specific values, aligning with potential outcome theory. This induces a transformation in the generative model's output, relating counterfactuals to disentanglement and intrinsic transformations within the network. Our approach connects counterfactuals to disentanglement in a generative model by allowing transformations of internal variables. Intrinsic disentanglement in a CGM involves a transformation that only affects specific endogenous variables, preserving the unambiguous assignment of values. In a generative model, intrinsic disentanglement involves a transformation that affects specific endogenous variables, preserving the unambiguous assignment of values. This concept relates to a causal interpretation of the model's structure, showing robustness to perturbations in its subsystems. Fig. 2d illustrates intrinsic disentanglement in a generative model, showing how values are computed before applying a transformation. Counterfactuals can be disentangled if faithful. Armstrong (2013) states that a continuous and injective embedding exists due to Z being compact and the codomain of g M being Hausdorff. Counterfactuals can be disentangled if faithful, as stated in Armstrong (2013). A continuous and injective embedding exists due to Z being compact and the codomain of g M being Hausdorff. This implies that g M is an embedding, and the respective V M 's are also embeddings. The proof of equivalence between faithful and disentangled transformations is discussed. If a transformation is disentangled, it is an endomorphism of Y M, making the counterfactual mapping faithful. Conversely, assuming a faithful Y E h, the mapping from V to the output shows that T is disentangled with respect to E in M. The proof shows that a disentangled transformation is an endomorphism of Y M, ensuring a faithful counterfactual mapping. By assuming a faithful Y E h, it is demonstrated that T is disentangled with respect to E in M. This implies non-overlapping subsets of latent variables, A and B, guaranteeing that T is an endomorphism of V M for any choice of endomorphism T. The proof demonstrates that a disentangled transformation is an endomorphism of Y M, ensuring a faithful counterfactual mapping. Non-overlapping subsets of latent variables, A and B, guarantee that T is an endomorphism of V M for any choice of endomorphism T, leading to a disentangled representation in the hidden layer. The proof shows that subsets of latent variables ensure T is an endomorphism of V M, leading to a disentangled representation. The choice of increasing dimensions and i.i.d. sampling guarantee an injective mapping and counterfactual hybridization results in an influence map covering I k. The choice of increasing dimensions and i.i.d. sampling ensure an injective mapping for a disentangled representation. Counterfactual hybridization results in an influence map covering I k, guaranteeing a rank K binary factorization of matrix B. The uniqueness of this factorization is guaranteed by classical NMF identifiability results. The \u03b2-VAE architecture is similar to DCGAN, with hyperparameters specified in Table 1. The method proposed by Berthelot et al. (2017) was used for the CelebA dataset with a pre-trained model. The \u03b2-VAE architecture, similar to DCGAN, consists of three blocks of convolutional layers with skip connections to enhance image sharpness. The pre-trained model used has the same architecture as in the paper by Berthelot et al. (2017). Consult Figure 1 in their paper for architectural details. The architecture used in the study is based on the same model as Berthelot et al. (2017), with three blocks of convolutional layers and upsampling layers. Skip connections are included to improve image sharpness. The pretrained model, sourced from Tensorflow-hub, is the BigGan-deep architecture by Brock et al. (2018) on 256x256 ImageNet, consisting of ResBlocks as the generator's building blocks. The model was not retrained. The study utilized the BigGan-deep architecture by Brock et al. (2018) as a pre-trained model on 256x256 ImageNet. The architecture consists of ResBlocks with BatchNorm-ReLU-Conv Layers and skip connections for signal enhancement. The ResBlocks in the generator architecture contain BatchNorm-ReLU-Conv Layers and upsampling transformations with skip connections for signal enhancement. The concept of influence maps generated by a VAE on the CelebA dataset is discussed, along with FID analysis of BEGAN hybrids showing distances between different pairs of classes. In the context of the generator architecture with ResBlocks and upsampling transformations, influence maps generated by a VAE on the CelebA dataset are discussed. FID analysis of BEGAN hybrids shows distances between different pairs of classes, indicating closeness of hybrids to generated data. Hybrids have a small distance to the generated data and to each other, indicating closeness of the distribution of Hybrids to that of generated data. This suggests that Hybridization produces visually plausible images. The entropy is computed using the probabilistic output for the 10 classes receiving top ranking across all hybrids, normalized to provide a total probability of 1. The entropy values for hybrids based on interventions on Gblock 4 are smaller, indicating better quality in object texture rendering. In contrast, hybrids from Gblock 6 show larger entropy values for modules with poorer quality. The results suggest that object texture is crucial for the classifier's decision, with hybrids from Gblock 4 showing better quality. Larger entropy values are seen in hybrids from Gblock 6 with poorer quality modules. The entropy values in hybrids from Gblock 4 are important for the classifier's decision. The hybrids between classes \"cock\" and \"ostrich\" show a larger collection for the BIGAN. Each panel corresponds to different Gblocks, with entropy computed for the top ranking classes normalized to total probability. The NMF algorithm extracted 3 clusters for hybrids, with entropy computed for top ranking classes. The first module of Gblock 5 showed large entropy, generating a bird with properties of both cock and ostrich. Classification outcomes of discriminative models for koala+teddy hybrids were investigated. The experiment investigated the classification outcomes of discriminative models for koala+teddy hybrids, aiming to assess the robustness of classifiers. The resultant hybrids were a mix of teddy bear and koala, with the need for classifiers to focus on the object present in the scene rather than contextual information. The experiment focused on assessing classifier robustness using a proposed intervention procedure. The hybrids created were a mix of teddy bear and koala, with the need for classifiers to prioritize the object in the scene over contextual information. Nasnet large showed greater robustness to context changes compared to other classifiers."
}