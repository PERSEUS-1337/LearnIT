{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations and utilizing a fully autoregressive graph decoder. The model is evaluated on multiple molecular optimization tasks, demonstrating its effectiveness. Our graph decoder for molecular optimization creates multi-resolution representations by encoding substructure components and atom-level details of the original molecular graph. The decoder is fully autoregressive, improving compound properties by adding new substructures and resolving their attachment to the molecule. Evaluation shows significant performance gains over previous baselines in various optimization tasks. Our model significantly outperforms previous state-of-the-art baselines in molecular optimization tasks. The task involves translating input molecular graphs into better forms to improve biochemical properties. This is challenging due to the vast candidate space and complex dependencies in graph generation. The model is trained to translate input molecular graphs into better forms with improved chemical properties. Graph generation is computationally challenging due to complex dependencies, similar to machine translation. Prior work proposed a junction tree encoder-decoder using valid chemical substructures. The junction tree encoder-decoder approach for generating molecular graphs utilized valid chemical substructures as building blocks. However, the encoding of trees and graphs was done separately, limiting the overall success of the method. The approach of generating molecular graphs using valid chemical substructures as building blocks had limitations. The encoding of trees and graphs was done separately, leading to non-autoregressive attachment prediction and inconsistent substructure attachments. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation involves non-autoregressive attachment prediction and interleaved prediction of substructure components with their attachments. This approach aims to address limitations in generating molecular graphs using valid chemical substructures as building blocks. The multi-resolution, hierarchically coupled encoder-decoder proposed for graph generation addresses inconsistent substructure attachments in the junction tree. It utilizes an auto-regressive decoder to predict substructure components and their attachments to the molecule being generated. The encoder represents molecules at different resolutions to match the decoding process, enabling the modeling of strong dependencies between successive attachments and substructure choices. The target graph is unraveled through triplet predictions, enabling strong dependencies between attachments and substructure choices. The encoder represents molecules at different resolutions, with each layer capturing essential information for decoding steps. Graph convolution supports attachment and substructure predictions at different levels. Compared to prior work, this approach improves graph generation. The encoding of molecules involves three levels, with each layer capturing key information for decoding. Graph convolution at different levels supports attachment and substructure predictions. The decoding process is more efficient by breaking down generation steps into smaller hierarchical steps to avoid combinatorial explosion. The method can handle conditional translation by taking desired criteria as input. Our decoding process efficiently decomposes generation steps into smaller hierarchical steps to avoid combinatorial explosion. It can handle conditional translation by taking desired criteria as input, allowing for different combinations of criteria at test time. Interleaving tree and graph decoding steps prevents the generation of invalid junction trees, ensuring accurate substructure prediction. Our method addresses issues with generating invalid junction trees and inconsistent local substructure attachments by interleaving tree and graph decoding steps. We propose an autoregressive decoder that improves substructure prediction accuracy. The new model is evaluated on various molecular optimization tasks, surpassing previous state-of-the-art graph generation methods. Our new model proposes an autoregressive decoder that improves substructure prediction accuracy and addresses issues with inconsistent local substructure attachments. It outperforms previous state-of-the-art graph generation methods in discovering molecules with desired properties, showing significant improvements on QED and DRD2 optimization tasks. Our model significantly outperforms state-of-the-art graph generation methods in discovering molecules with desired properties, showing improvements on QED and DRD2 optimization tasks. It runs 6.3 times faster during decoding and utilizes hierarchical decoding and multi-resolution encoding. Additionally, conditional translation can succeed even with minimal training data on molecular pairs with desired target property. Our model runs 6.3 times faster than previous methods during decoding and utilizes hierarchical decoding and multi-resolution encoding. Conditional translation can succeed with minimal training data on molecular pairs with desired target property. Various approaches have been adopted for generating molecular graphs based on SMILES strings. Previous work has used different methods to generate molecular graphs based on SMILES strings. Some models output adjacency matrices and node labels at once, while others decode molecules node by node. In 2018, various generative models were developed to create molecules based on SMILES strings. Different approaches included generating graphs with adjacency matrices and node labels simultaneously, decoding molecules node by node, and using hypergraph grammar for molecule generation. Jin et al. (2018) also focused on generating molecules based on substructures using a two-stage procedure. Our work is closely related to Jin et al. (2018) who generated molecules based on substructures using a two-stage procedure. Their method involved creating a junction tree with substructures as nodes in the first step and then specifying how the substructures should be attached in the second step. Our method differs from Jin et al. (2018) as we adopt a two-stage procedure for realizing graphs. The first step generates a junction tree with substructures as nodes, capturing their coarse relative arrangements. The second step resolves the full graph by specifying how the substructures should be attached to each other. However, our method jointly predicts the substructures and their attachments with an autoregressive decoder, unlike the two-stage process used by Jin et al. Our method differs from Jin et al. (2018) by using an autoregressive decoder to jointly predict substructures and their attachments, unlike their two-stage procedure. Graph neural networks have been extensively studied for graph encoding, and our method is related to graph encoders for molecules. Our method represents molecules as hierarchical graphs, spanning from atom-level graphs to substructure-level trees. It is closely related to previous work that learns to represent graphs hierarchically. Our method represents molecules as hierarchical graphs, different from previous approaches. It is closely related to work that learns to represent graphs hierarchically, utilizing graph coarsening algorithms to construct multiple layers of graph hierarchy. Defferrard et al. (2016) and Ying et al. (2018) used graph coarsening algorithms to create graph hierarchy layers. Gao & Ji (2019) proposed learning graph hierarchy during encoding. These methods aim to represent graphs as a single vector for regression. Our encoder represents molecules across three levels (atom, attachment, and substructure layers) to capture relevant information for decoding. Unlike methods focusing on graph representation for regression or classification, our approach is on graph generation. Molecules are encoded into multiple sets of vectors at different resolutions, dynamically aggregated by decoder attention modules. The graph translation task aims to learn a function mapping a molecule to a graph. Our focus is on graph generation, where molecules are encoded into multiple sets of vectors at different resolutions and dynamically aggregated by decoder attention modules. The graph translation task aims to learn a function that maps a molecule to another molecule with improved chemical properties using an encoder-decoder with neural attention. The decoder adds new substructures in each generation step and determines how they should be attached to the current graph. The task involves learning a function F that maps a molecule X to another molecule G with better chemical properties using an encoder-decoder with neural attention. The decoder adds new substructures in each generation step and determines their attachment to the current graph through a two-step attachment prediction process. To support hierarchical generation, a matching encoder is needed to represent molecules at multiple resolutions. The attachment prediction process involves predicting attaching points in a new substructure and their corresponding attaching points in the current graph. To support hierarchical generation, a matching encoder is designed to represent molecules at multiple resolutions. A molecule is represented by a hierarchical graph with substructure, attachment, and atom layers. Our model represents molecules using a hierarchical graph with substructure, attachment, and atom layers. The nodes in the graph are encoded into substructure, attachment, and atom vectors for decoding. The encoder is tailored for the decoder, with the decoder utilizing these vectors for prediction steps. Our model encodes nodes in the graph into substructure, attachment, and atom vectors for decoding. The decoder utilizes these vectors for prediction steps, with attention mechanisms and substructures defined as subgraphs of molecules. The paper discusses the use of substructures in encoding molecules for prediction, with a focus on rings and bonds. Substructures are defined as subgraphs of molecules, and a substructure tree is used to characterize them. The vocabulary of substructures is constructed from the training set, with high coverage on test sets. The paper discusses the use of substructures in encoding molecules for prediction, focusing on rings and bonds. Substructures are defined as subgraphs of molecules, and a substructure tree is used to characterize their connections. The vocabulary of substructures is constructed from the training set, ensuring high coverage on test sets. The paper discusses encoding molecules using substructures, focusing on rings and bonds. A substructure tree is constructed to show connections between substructures. The graph decoder generates molecules by expanding the substructure tree in a depth-first order. Predictions are made based on the encoding of input X. The graph decoder generates a molecule by incrementally expanding its substructure tree in a depth-first order. Predictions are made for new substructures and how they should be attached to the graph. Topological predictions are made to determine if a new substructure should be attached, with the model backtracking if not. The graph decoder generates a molecule by incrementally expanding its substructure tree in a depth-first order. It predicts new substructures and their attachment to the graph. Topological predictions determine if a new substructure should be attached, with the model backtracking if not. The decoder then moves to the next step of predicting substructure type and attachment. The model predicts new substructures and their attachments in a molecule by using a MLP with attention over substructure vectors. If the probability is greater than 0.5, a new substructure is created and its type is predicted. The attachment between the new and existing substructures is determined by predicting atom pairs. The model predicts new substructures in a molecule using an MLP with attention over substructure vectors. It then determines the attachment between new and existing substructures by predicting atom pairs. This is done by outputting a distribution over the vocabulary of substructures. The model predicts new substructures in a molecule using an MLP with attention over substructure vectors. It then determines the attachment between new and existing substructures by predicting atom pairs. This involves predicting attaching atoms from a fixed graph and finding corresponding atoms in the substructure. The probability of a candidate attachment is computed based on the predicted configurations. The model predicts new substructures in a molecule using an MLP with attention over substructure vectors and determines attachments by predicting atom pairs. The prediction involves finding corresponding atoms in the substructure and computing the probability of a candidate attachment based on predicted configurations. The predictions give an autoregressive factorization of the distribution over the next substructure and its attachment. Each decoding step depends on the previous outcome, and predicted attachments influence subsequent predictions. The model predicts new substructures in a molecule using an MLP with attention over substructure vectors and determines attachments by predicting atom pairs. A candidate attachment M is computed based on atom representations learned by the decoder. The predictions provide an autoregressive factorization of the distribution over the next substructure and its attachment. The decoding steps depend on previous outcomes, and predicted attachments influence subsequent predictions. During training, teacher forcing is applied with a depth-first traversal over the ground truth substructure tree. Attachment enumeration is tractable due to small substructure sizes. The model predicts new substructures in a molecule using an MLP with attention over substructure vectors and determines attachments by predicting atom pairs. During training, teacher forcing is applied with a depth-first traversal over the ground truth substructure tree. The encoder represents a molecule X by a hierarchical graph to support the decoding process, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The encoder represents a molecule X with a hierarchical graph for decoding, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The hierarchical graph includes an atom layer and an attachment layer derived from the substructure tree of molecule X. The encoder represents molecule X with a hierarchical graph for decoding, consisting of an atom layer and an attachment layer derived from the substructure tree. Each atom node is associated with a label indicating its type and charge, while the attachment layer represents specific attachment configurations of substructures. This layer provides necessary information for attachment prediction. The attachment layer in the hierarchical graph represents specific attachment configurations of substructures, providing essential information for attachment prediction. Each node in this layer is a particular attachment configuration of a substructure, illustrated in the attachment vocabulary for a ring. The substructure layer, similar to the substructure tree, also provides crucial information. The attachment layer connects atoms to their substructures, with each node representing a specific attachment configuration. The substructure layer provides information for substructure prediction, with edges connecting atoms and substructures between layers to propagate information. The attachment layer connects atoms to substructures, while the substructure layer provides information for prediction. Edges connect atoms and substructures between layers to propagate information in the hierarchical graph H X for molecule X, encoded by a hierarchical message passing network (MPN). The encoder contains three MPNs for each layer, following the MPN architecture from Jin et al. (2019). The hierarchical graph H X for molecule X is encoded by a hierarchical message passing network (MPN) with three layers. The atom layer MPN encodes the atom layer of H X by propagating message vectors between atoms for T iterations. The hierarchical graph H X for molecule X is encoded by a hierarchical message passing network (MPN) with three layers. The MPN encoding process is denoted as MPN \u03c8 (\u00b7) with parameter \u03c8. The Atom Layer MPN encodes the atom layer of H X by propagating message vectors between atoms for T iterations to output the atom representation h v for each atom v. The Attachment Layer MPN encodes the input features of nodes and edges in the attachment layer H a X. The Attachment Layer MPN encodes input features for nodes and edges in the attachment layer by concatenating embeddings and atom vectors, and running T iterations of message passing to compute substructure representations. The hierarchical encoder computes substructure representations by running message passing over node and substructure layers, resulting in vectors that represent a molecule at multiple resolutions. The hierarchical encoder computes substructure representations by concatenating embedding and node vectors, running message passing over substructure layers to obtain molecule representations. The hierarchical MPN architecture is used during decoding to encode the hierarchical graph, generating substructure and atom vectors for prediction. During decoding, the hierarchical MPN architecture is used to encode the hierarchical graph, generating substructure and atom vectors for prediction. The training set contains molecular pairs where each compound can have multiple outputs, allowing for diverse output generation. During decoding, the hierarchical MPN architecture encodes the hierarchical graph for diverse output generation. A variational translation model is used to generate molecule Y given compound X and an additional input z, sampled from a Gaussian prior during testing. The model is trained using variational inference. The method extends to a variational translation model F with an additional input z, sampled from a Gaussian prior during testing. Training involves variational inference, sampling z from the posterior Q(z|X, Y), encoding X and Y into representations, computing structural changes vector \u03b4 X,Y, and sampling z using reparameterization trick. During training, a latent code z is sampled from the posterior Q(z|X, Y) to encode structural changes from molecule X to Y. This code is used in the decoder to reconstruct output Y. The model follows a conditional VAE framework, allowing for conditional translation without knowledge of optimized properties. During training, a latent code z is used to reconstruct output Y in a conditional VAE framework. However, in a multi-property optimization setting, the model may have limitations as it does not know what properties are being optimized during translation. To address this, the method is extended to handle conditional translation where desired criteria are also inputted for the translation process. In a multi-property optimization setting, the method is extended to handle conditional translation by inputting desired criteria for the translation process. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with an additional input g X,Y, augmenting the latent code as [z, g X,Y]. Users can specify criteria in g X,Y during testing to control the outcome. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with an additional input g X,Y. The latent code is augmented as [z, g X,Y] and passed to the decoder. Users can specify criteria in g X,Y during testing to control the outcome. The model is evaluated on single-property optimization tasks following Jin et al. (2019) experimental design. A novel conditional optimization task is constructed where desired criteria are fed as input to the translation process to prevent the model from ignoring input X. The translation model is evaluated on single-property optimization tasks based on Jin et al. (2019) experimental design. A novel conditional optimization task is created where desired criteria are input to prevent the model from ignoring input X. Molecular similarity between input X and output Y must be above a certain threshold at test time. The model is trained on single-property optimization tasks with a focus on molecular similarity between input X and output Y above a certain threshold at test time. The tasks include LogP Optimization, measuring solubility and synthetic accessibility of compounds. The dataset consists of four tasks where the model is trained without g X,Y as input. In LogP Optimization, the model aims to improve both drug-like and DRD2-active properties of compounds. Two similarity thresholds are experimented with: \u03b4 = {0.4, 0.6}. The model aims to improve the drug-like and DRD2-active properties of compounds by translating input X into output Y with logP(Y) > logP(X). Two similarity thresholds of \u03b4 = {0.4, 0.6} are experimented with, where Y should be both drug-like and DRD2-active. Different criteria can be encoded as vector g for conditional translation training. The model focuses on improving drug-like properties of compounds by translating input X into output Y with logP(Y) > logP(X). DRD2 is considered an off-target that may cause side effects, so only drug-likeness should be enhanced. Evaluation metrics include translation accuracy and diversity, with a similarity constraint of sim(X, Y) \u2265 0.4 for both settings. In \u00a73.3, a similarity constraint of sim(X, Y) \u2265 0.4 is imposed for both single-property tasks. Evaluation metrics measure translation accuracy and diversity. Each test molecule X i is translated K = 20 times with different latent codes. The final translation Y i is selected based on the highest property improvement and sim(X i, Y i) \u2265 \u03b4. The average property improvement is reported, along with the translation success rate for other tasks. Successful translation requires satisfying all similarity and property constraints. HierG2G is compared against baselines like GCPN, MMPA, and Seq2Seq for translation tasks, measuring property improvement and success rates. Successful translation requires meeting similarity and property constraints, with diversity measured by Tanimoto distance between compounds. HierG2G is compared against baselines like GCPN, MMPA, Seq2Seq, and JTNN for molecule generation tasks. Diversity is measured using the average pairwise Tanimoto distance between translated compounds. HierG2G is compared against Seq2Seq, JTNN, and CG-VAE for molecule generation tasks. AtomG2G is developed as a baseline for atom-based generation. The AtomG2G model predicts atom types and bond types in molecule generation, using an atom-based translation approach. The encoder encodes the atom-layer graph, and the decoder attention focuses on atom vectors. The decoding process in AtomG2G completes by predicting atom and bond types to capture edge dependencies. The model achieves state-of-the-art results in translation tasks, outperforming JTNN in accuracy and output diversity. Our model achieves state-of-the-art results on four translation tasks, outperforming JTNN in accuracy and output diversity. It runs 6.3 times faster during decoding and shows over 10% improvement on the DRD2 task compared to AtomG2G. Our model outperforms AtomG2G on three datasets, with over 10% improvement on the DRD2 task. It runs 6.3 times faster during decoding and shows advantages in translation accuracy and output diversity compared to other models like JTNN and AtomG2G. Our hierarchical model outperforms other translation methods like Seq2Seq, JTNN, and AtomG2G in both translation accuracy and output diversity, achieving a 10% improvement on the DRD2 task. Training on examples with strong constraints resulted in low success rates, highlighting the effectiveness of our conditional translation approach. Our model outperforms other translation methods in accuracy and diversity. Training on examples with strong constraints resulted in low success rates, showing the effectiveness of our conditional translation approach. Ablation studies were conducted to understand the importance of different architecture choices. The conditional translation setup transfers knowledge from other pairs with g X,Y = [1, 0], [0, 1]. Ablation studies on the QED and DRD2 tasks show that replacing the hierarchical decoder with an atom-based decoder decreases model performance by 0.8% and 10.9%. The DRD2 task benefits more from structure-based decoding due to biological target binding dependencies. In an experiment to test the benefits of structure-based decoding, the model's performance decreased by 0.8% and 10.9% on two tasks. The DRD2 task showed more improvement with structure-based decoding due to biological target binding dependencies. Another experiment removing hierarchies in the encoder and decoder resulted in a slight drop in translation accuracy by 0.8% and 2.4%. Our second experiment reduced hierarchies in the encoder and decoder MPN, impacting translation accuracy. Removing top substructure layer led to a slight drop of 0.8% and 2.4%, while further removing attachment layer significantly degraded performance on both datasets. The model struggled without substructure information, needing to infer substructures and their construction. Details of these ablations are in the appendix. In the study, removing the attachment layer caused a significant performance drop on both datasets as substructure information was lost. The LSTM MPN was chosen over the GRU MPN, resulting in a slight decrease in translation performance but still outperforming JTNN. Our hierarchical graph-to-graph translation model, utilizing LSTM MPN architecture, outperforms previous models in generating molecular graphs using chemical substructures. The model is fully autoregressive, learning coherent multi-resolution representations, and shows superior performance in various settings. Our model, utilizing LSTM MPN architecture, outperforms previous models in generating molecular graphs using chemical substructures. The model is fully autoregressive, learning coherent multi-resolution representations, and shows superior performance in various settings. The message passing network MPN \u03c8 over graph H is defined as Algorithm 3 LSTM MPN with T message passing iterations for all edges in H simultaneously. Our attention layer is a bilinear attention function with parameter \u03b8. Illustration of AtomG2G decoding process is shown in Figure 7. The attention layer in our model is a bilinear function with parameter \u03b8. The AtomG2G decoding process involves predicting new atoms and bond types sequentially. The AtomG2G model predicts new atoms and bond types sequentially in each step, adding them to the queue Q. It is an atom-based translation method directly comparable to HierG2G, representing molecules as molecular graphs. Training set size and substructure vocabulary size for each dataset are shown in Table 3. AtomG2G is an atom-based translation method that adds new atoms to the queue Q. Molecules are represented as molecular graphs, not hierarchical graphs with substructures. Training set sizes and substructure vocabulary sizes for each dataset are listed in Table 3. The multi-property optimization dataset is created by combining QED and DRD2 optimization tasks. The test set includes 780 compounds that are not drug-like and DRD2-inactive. The optimization datasets are downloaded from a provided link. The training and substructure vocabulary sizes are listed in Table 3. A multi-property optimization dataset is created by combining QED and DRD2 tasks. The test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters for HierG2G and AtomG2G are set accordingly. The training and test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters are set for HierG2G and AtomG2G models, with specific dimensions and regularization weights. Both models are trained using Adam optimizer with default parameters. CG-VAE official implementation is used for experiments. For AtomG2G, the hidden layer and embedding layer dimensions are set to 400 with \u03bb KL = 0.3 and T = 20 message passing iterations. CG-VAE models are trained for logP, QED, and DRD2 tasks, with compounds translated at test time following a specific procedure. At test time, compounds are translated using a CG-VAE model to generate molecules and predict properties from the latent space. Gradient ascent is performed over the latent representation to maximize the property score, decoding multiple molecules and selecting the best one with property improvement within a similarity constraint. Keeping KL regularization is found to be necessary. The study involves embedding a molecule X into a latent representation z and using gradient ascent to maximize the predicted property score. Decoding multiple molecules from the latent vectors and selecting the best one with property improvement is done within a similarity constraint. Keeping the KL regularization weight low is crucial for meaningful results. Ablation studies were conducted, including changing the decoder to AtomG2G's atom-based decoder. In the ablation study, experiments were conducted to modify the decoder and reduce the number of hierarchies in the encoder and decoder MPN. The KL regularization weight was emphasized to be kept low for meaningful results. In experiments with the AtomG2G decoder, modifications were made to include both atom and substructure vectors in the decoder attention. The hidden layer and embedding layer dimensions were set to 300 to match the original model size. Two experiments reduced the number of hierarchies in the encoder and decoder MPN. In the one-layer model, molecules are represented by cX = cGX and predictions are based on atom vectors. In a two-layer model, molecules are represented by cX = cGX \u222a cAX, with topological predictions based on hidden vector hAk. The substructure layer is removed, and the hidden layer dimension is adjusted to match the original model size. In the one-layer model, molecules are represented by cX = cGX, and predictions are based on atom vectors."
}