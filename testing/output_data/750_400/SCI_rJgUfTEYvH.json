{
    "title": "rJgUfTEYvH",
    "content": "Generative models for video prediction face the challenge of high uncertainty in future outcomes. Existing probabilistic models are either computationally expensive or do not optimize data likelihood directly. This work introduces multi-frame video prediction using normalizing flows, enabling direct optimization. Our work proposes multi-frame video prediction with normalizing flows, allowing for direct optimization of data likelihood and high-quality stochastic predictions. Flow-based generative models offer a competitive approach to video generative modeling, leveraging advancements in computational hardware. Flow-based generative models, such as normalizing flows, enable direct optimization of data likelihood and high-quality stochastic predictions. These models offer a competitive approach to video generative modeling, leveraging advancements in computational hardware. The advancement of computational hardware has propelled machine learning into the mainstream, leading to improvements in various capabilities like image classification, machine translation, and game-playing agents. However, the application of machine learning is mostly limited to scenarios with ample supervision or accurate environment simulations. The application of machine learning technology has been constrained to situations with supervision or accurate simulations. An alternative is to use large unlabeled datasets with generative models to predict future events by building an internal representation of the world. Utilizing large unlabeled datasets with predictive generative models allows for effective prediction of future events by building an internal representation of the world, such as in game-playing agents. This approach enables the modeling of complex real-world phenomena without the need for labeled examples, leveraging videos of real-world interactions readily available. A predictive generative model can learn about real-world phenomena by analyzing videos of interactions without labeled examples. This model can be trained on large unlabeled datasets to predict future events and create internal representations of the world. It can be useful for downstream tasks or direct applications involving future prediction. A large generative model can be trained on unlabeled video datasets to learn about real-world phenomena and predict uncertain futures. This model can be used for downstream tasks or applications like robotics for effective decision making and control. In this paper, the focus is on stochastic prediction in conditional video prediction, aiming to synthesize raw RGB video frames based on a short sequence of observations. The challenge lies in the uncertainty of the future, with various possible outcomes. Previous works have explored probabilistic models for uncertain futures, but they are either computationally expensive or do not directly optimize data likelihood. In this paper, the focus is on stochastic prediction in conditional video prediction, aiming to synthesize raw RGB video frames based on a short sequence of observations. The proposed new class of video prediction models can provide exact likelihoods, generate diverse stochastic futures, and accurately synthesize realistic and high-quality video frames. The proposed new class of video prediction models extends flow-based generative models into conditional video prediction, aiming to synthesize realistic and high-quality video frames. This approach can generate diverse stochastic futures and provide exact likelihoods for raw RGB video frames. Our approach extends flow-based generative models into conditional video prediction, addressing the challenges of modeling high-dimensional video sequences by learning a latent dynamical system model. This induces Markovian dynamics on the latent state, enabling the prediction of future values. Conditional video generation using VideoFlow, a flow-based model inspired by Glow, predicts future values of the latent state in high-dimensional video sequences, achieving competitive results with state-of-the-art methods. VideoFlow, a flow-based video prediction model inspired by Glow, achieves competitive results in stochastic video prediction on the BAIR dataset, rivaling the best VAE-based models. It produces high-quality qualitative results and avoids common artifacts seen in models using pixel-level mean-squared-error for training. VideoFlow is competitive in stochastic video prediction on the BAIR dataset, producing high-quality results without common artifacts. It achieves faster test-time image synthesis, making it practical for real-time applications like robotic control. VideoFlow is a competitive model for stochastic video prediction, achieving fast test-time image synthesis suitable for real-time applications like robotic control. It directly optimizes the likelihood of training videos, allowing for evaluation based on likelihood values. VideoFlow is a model for stochastic video prediction that optimizes training video likelihood without using a variational lower bound. Previous work focused on deterministic predictive models with architectural changes and different generation objectives. Boots et al. (2014) researched deterministic models focusing on architectural changes, pixel transformations, predictive coding architectures, generation objectives, and disentangling representations. The next challenge is to address stochastic environments by building models that reason over uncertain futures. The next challenge after deterministic models is to address stochastic environments by building models that can effectively reason over uncertain futures in real-world videos. These videos are inherently stochastic due to random events or unobserved factors, requiring models that can handle uncertainty. Real-world videos are stochastic due to random events or unobserved factors, requiring models that can handle uncertainty. Deterministic models struggle with this, leading to blurry predictions or disregarding potential futures. Various methods, like variational auto-encoders, have been used to incorporate stochasticity in modeling uncertain futures. Models can generate one future, but often produce blurry predictions or disregard potential futures. To address this, methods like variational auto-encoders, generative adversarial networks, and autoregressive models have been used to incorporate stochasticity in modeling uncertain futures. Among these, variational autoencoders, which optimize an evidence lower bound on the log-likelihood, have been widely explored. Auto-regressive models are a class of video prediction models that directly maximize the log-likelihood of the data, generating future predictions. These models have been explored alongside variational autoencoders, generative adversarial networks, and other techniques to incorporate stochasticity in uncertain future modeling. The log-likelihood is widely explored in video prediction models, with auto-regressive models being the only class that directly maximizes it. However, synthesis with these models is typically sequential and inefficient on modern hardware. Prior work has aimed to speed up training and synthesis with auto-regressive models. The log-likelihood is a key aspect in video prediction models, with auto-regressive models focusing on maximizing it. However, these models are often sequential and inefficient on modern hardware. Previous efforts have tried to enhance training and synthesis with auto-regressive models. In contrast, a proposed VAE model shows improved predictions, especially for longer timeframes, with faster sampling and high-quality long-term predictions. The VAE model outperforms auto-regressive models in video prediction by producing sharper and less noisy predictions, especially for longer timeframes. It exhibits faster sampling and directly optimizes log-likelihood for high-quality long-term predictions. The VAE model uses a multi-scale architecture with stochastic variables, while auto-regressive models encode input into multiple levels of stochastic variables sequentially. The flow model in generative models uses a multi-scale architecture with stochastic variables for high-quality long-term predictions. It employs an autoregressive latent-dynamic prior to encode input into multiple levels of stochastic variables sequentially. Flow-based generative models offer advantages such as exact latent-variable inference, log-likelihood evaluation, and parallel sampling. Flow-based generative models have unique advantages like exact latent-variable inference, log-likelihood evaluation, and parallel sampling. These models infer latent variables by transforming data through invertible functions, allowing for exact log-likelihood computation. Parameters are learned by maximizing log-likelihood. Flow-based generative models use tractable priors over latent variables and invertible transformations to compute log-likelihood exactly. Parameters are learned by maximizing log-likelihood, allowing for exact latent-variable inference and parallel sampling. A generative flow for video is proposed using a multi-scale flow architecture. A generative flow model for video is proposed, utilizing a multi-scale flow architecture to break up the latent space into separate variables per timestep. The model allows for generating samples from the data distribution by sampling from a prior distribution and computing the corresponding frame of video through an invertible transformation. The model breaks up the latent space into separate variables per timestep, with each variable being an invertible transformation of a frame of video. A multi-scale architecture is used for encoding information about each frame at different scales. The invertible transformations used in the architecture are briefly described. The latent variable z t is composed of multiple levels encoding information about frame x t at different scales using invertible transformations with simple Jacobian determinants. The latent variable z is formed using invertible transformations with simple Jacobian determinants, such as triangular, diagonal, or permutation matrices. Techniques like Actnorm and Coupling are applied to the input data for scaling and splitting operations. The determinant for triangular and diagonal Jacobian matrices is the product of diagonal terms. Techniques like Actnorm, Coupling, SoftPermute, and Squeeze are applied to reshape and split the input data for flow operations. The text discusses the application of techniques like SoftPermute and Squeeze to reshape input data for flow operations in deep networks. It also explains the process of inferring latent variables at different levels using Split and enabling flows at higher levels to operate on lower dimensions and larger scales. The multi-scale architecture described enables flows at different levels to operate on varying dimensions and scales. The latent variables are inferred for each frame of the video using this architecture. Our multi-scale architecture f \u03b8 (x t ) utilizes flows at multiple levels to infer latent variables for each frame of the video. The latent prior is chosen using an autoregressive factorization, with conditional prior specified based on previous latent variables. The latent prior in our multi-scale architecture is chosen using an autoregressive factorization, with conditional prior specified based on previous latent variables. The latent variables at different timesteps and levels are factorized, and a conditionally factorized Gaussian density is used. The architecture of the model includes a deep 3-D residual network with dilations and gated activation units to predict the mean and log-scale of a conditionally factorized Gaussian density. The log-likelihood objective has two parts, with the invertible multi-scale architecture contributing via the sum of log Jacobian determinants. Detailed architecture and ablations are described in the appendix. The architecture of the model includes a deep 3-D residual network with dilations and gated activation units to predict the mean and log-scale of a conditionally factorized Gaussian density. The log-likelihood objective has two parts, with the invertible multi-scale architecture contributing via the sum of log Jacobian determinants. The parameters of the multi-scale architecture and latent dynamics model are jointly learned by maximizing this objective. The architecture includes a deep 3-D residual network with dilations and gated activation units to predict the mean and log-scale of a conditionally factorized Gaussian density. The log-likelihood objective involves the sum of log Jacobian determinants from invertible transformations mapping video frames to latent variables. The parameters of the architecture and latent dynamics model are learned by maximizing this objective. The prior models temporal dependencies in the data, while the flow operates on separate video frames. Realism of generated trajectories is compared using a real-vs-fake test with SAVP-VAE and SV2P. The prior p \u03b8 (z) models temporal dependencies in the data, while the flow g \u03b8 acts on separate video frames. Realism of generated trajectories is compared using a real-vs-fake test with SAVP-VAE and SV2P. VideoFlow model is conditioned on the frame at t = 1 and displays generated trajectories at t = 2 and t = 3 for different shapes. The VideoFlow model is conditioned on the frame at t = 1 and generates trajectories at t = 2 and t = 3 for different shapes. Using 3-D convolutional flows was found to be computationally expensive compared to an autoregressive prior. Due to memory limits, SGD is only feasible with a small number of sequential frames per gradient step. Using 2-D convolutions in our flow f \u03b8 with autoregressive priors allows us to synthesize long sequences without introducing temporal artifacts. Border colors in generated videos indicate conditioning frames. Visit the website to view all generated videos and qualitative results. VideoFlow uses 2-D convolutions with autoregressive priors to generate long sequences without temporal artifacts. Border colors in the videos indicate conditioning frames. The Stochastic Movement Dataset is modeled, with shapes moving in eight directions on a gray background. Visit the website to view all generated videos and results. VideoFlow models the Stochastic Movement Dataset with shapes moving in eight directions on a gray background. The shape's position at (t + 1) th step can be predicted using its position at the t th step. Random temporal patches of 2 frames are extracted for analysis. The deterministic model in VideoFlow averages out all eight possible directions in pixel space by looking back at just one frame. The model achieves a low 0.04 bits-per-pixel on the holdout set and can predict the position of the shape at the (t + 1) th step using only the position at the t th step. The VideoFlow model extracts random temporal patches of 2 frames from each video and maximizes the loglikelihood of the second frame given the first. It achieves a low 0.04 bits-per-pixel on the holdout set and predicts the future trajectory of the shape in one of eight random directions. The model is compared with SV2P and SAVP-VAE models for video generation quality assessment. VideoFlow model consistently predicts future trajectory of shape in one of eight random directions. Compared with SV2P and SAVP-VAE models for video generation quality using Tensor2Tensor implementation. Outperforms baselines in fooling rate, generating plausible trajectories at a greater rate. VideoFlow outperforms baselines in generating plausible \"real\" trajectories in a real vs fake Amazon Mechanical Turk test using the action-free version of the BAIR robot pushing dataset. The task of video generation is unsupervised with multiple plausible trajectories due to partial observability and stochasticity of robot actions. VideoFlow outperforms baseline models in generating plausible trajectories using the action-free BAIR robot pushing dataset. The task of video generation is unsupervised with multiple plausible trajectories due to partial observability and stochasticity of robot actions. The models are trained to generate 10 target frames conditioned on 3 input frames, with VideoFlow achieving a log-likelihood of Bits-per-pixel 1.87. VideoFlow is trained to generate 10 target frames conditioned on 3 input frames, maximizing log-likelihood. Models have seen a total of 13 frames during training. Realism and diversity are measured using a 2AFC test and mean pairwise cosine distance in VGG perceptual space. VideoFlow outperforms SAVP-VAE and SV2P models on bits-per-pixel, as shown in Table 2. High values of bits-per-pixel in baselines are attributed to their optimization objective. VideoFlow outperforms SAVP-VAE and SV2P models on bits-per-pixel, as shown in Table 2. The high values of bits-per-pixel in baselines are due to their optimization objective, which does not directly optimize the variational bound on log-likelihood. The optimization objective of VideoFlow does not directly optimize the variational bound on log-likelihood due to a \u03b2 = 1 term in their objective and scheduled sampling. They compare video generation models based on PSNR, SSIM, and VGG perceptual metrics, selecting the best sample from 100 videos for each model. The BAIR robot-pushing dataset used for training and testing is highly stochastic. The video generation models are evaluated based on PSNR, SSIM, and VGG perceptual metrics to select the best sample. The BAIR robot-pushing dataset used is highly stochastic, leading to a high number of plausible futures. The generated videos may be realistic but can differ perceptually from the ground truth video. The evaluation metrics from prior work are followed to assess the model. The robot-pushing dataset is stochastic, leading to many plausible futures. Evaluation of the model is done using metrics from prior work to compare generated videos to the ground truth using PSNR, SSIM, and cosine similarity. In 2018, the model was evaluated using conditioning frames in the BAIR action-free test-set. 100 videos were generated from each stochastic model, and the closest ones to the ground truth were compared using PSNR, SSIM, and cosine similarity metrics. The findings were reported in Figure 4, indicating if the true future lies within the set of plausible futures according to the video model. In prior work, researchers effectively tuned pixel-level variance as a hyperparameter and removed pixel-level noise to improve sample quality in their video model. This procedure resulted in higher quality videos at the cost of diversity by sampling videos at a lower temperature. In the VideoFlow model, pixel-level noise is removed to enhance video quality by tuning the hyperparameter and sampling at a lower temperature. This is achieved by scaling the standard deviation of the latent gaussian distribution with a factor of T. In VideoFlow model, pixel-level noise is reduced by adjusting hyperparameters and sampling at a lower temperature. Results are reported with different temperatures, and low-temperature sampling negatively impacts performance for some models. The optimal temperature for VGG similarity metrics was determined on the validation set. Low-temperature sampling negatively affected performance for SV2P and SAVP-VAE models. The best hyperparameters for SAVP-VAE models include disappearing arms. The model with optimal temperature performs well on VGG-based similarity metrics compared to other models. Our model with optimal temperature outperforms SAVP-VAE and SVG-LP models on VGG-based similarity metrics, correlating well with human perception and SSIM. It is also competitive with state-of-the-art video generation models on these metrics, despite differences in optimization objectives. Our model with temperature T = 1.0 correlates well with human perception and SSIM, outperforming SAVP-VAE and SVG-LP models on VGG-based similarity metrics. PSNR is a pixel-level metric, while VideoFlow models the conditional probability of joint distribution of frames, leading to underperformance on PSNR. Diversity and quality in generated samples are assessed by generating 10 videos for each set of conditioning frames in the test set and computing the mean distance in VGG perceptual space. VideoFlow outperforms diversity values reported in prior work while being competitive in realism. It has the highest fooling rate at T = 0.6. VideoFlow outperforms diversity values reported in prior work (Lee et al., 2018) while being competitive in realism. At T = 0.6, VideoFlow has the highest fooling rate and is competent with state-of-the-art VAE models in diversity. Lower temperatures result in less random arm behavior and static background objects, achieving higher realism scores. Higher temperatures lead to more stochastic arm motion. At T = 0.6, VideoFlow has the highest fooling rate and competes with state-of-the-art VAE models in diversity. Lower temperatures result in less random arm behavior and static background objects, leading to higher realism scores. Higher temperatures result in more stochastic arm motion and noisier background objects, causing a drop in realism. Interpolations between different shapes are displayed in Figure 6. Interpolations between different shapes are displayed in Figure 6, showing high diversity scores with the arm motion interpolated in a temporally cohesive fashion between initial and final positions in the BAIR robot pushing dataset. The BAIR robot pushing dataset involves encoding input and target frames into a latent space using VideoFlow. Interpolations show cohesive arm motion between initial and final positions. Multi-level latent representation allows for interpolating background objects at lower levels and arm motion at higher levels. The multi-level latent representation in VideoFlow allows for interpolating background objects at lower levels and arm motion at higher levels. Different shapes with fixed type but varying size and color are encoded into the latent space, showing smooth interpolation of shape size. Colors are sampled from a uniform discrete distribution during training, resulting in all interpolated colors lying within the set of colors used. During training, shapes with fixed type but varying size and color are encoded into the latent space, showing smooth interpolation of shape size. Colors are sampled from a uniform discrete distribution, resulting in all interpolated colors lying within the set of colors used. In experiments, generated videos show frames with and without occlusions, and VideoFlow is used to detect the plausibility of temporally inconsistent frames occurring in the future. The generated frames remain within the image manifold, maintaining temporal consistency even 100 frames into the future. In the presence of occlusions, the arm remains sharp while background objects become noisier. Our VideoFlow model can generate 100 frames into the future, maintaining temporal consistency within the image manifold. Despite occlusions, the arm remains sharp while background objects may become noisier and blurrier. The model's bijection between z t and x t means that information in the latent state z t is limited to what is present in frame x t, potentially leading to forgetting objects if occluded for a few frames. In future work, the VideoFlow model aims to address the issue of forgetting objects when occluded for a few frames by incorporating longer memory, such as using recurrent neural networks in the autoregressive prior or more memory-efficient backpropagation algorithms for invertible neural networks. The VideoFlow model aims to address occlusion by incorporating longer memory, such as using recurrent neural networks or memory-efficient backpropagation algorithms. The model is conditioned on 3 frames to detect the plausibility of a temporally inconsistent frame in the immediate future. The VideoFlow model is conditioned on 3 frames to detect the likelihood of a temporally inconsistent frame occurring in the immediate future. The model assigns a decreasing log-likelihood to frames further in the future, as shown in Figure 7. The VideoFlow model predicts future frames by assigning decreasing log-likelihood to frames further in the future. It is inspired by the Glow model for image generation and introduces a latent dynamical system model for flow-based video prediction. VideoFlow is a flow-based video prediction model inspired by the Glow model for image generation. It introduces a latent dynamical system model for predicting future values of the flow model's latent state. Empirical results show competitive performance with VAE models in stochastic video prediction. The model optimizes log-likelihood directly for faster synthesis compared to pixel-level autoregressive video models. Our empirical results demonstrate that VideoFlow competes with state-of-the-art VAE models in stochastic video prediction by optimizing log-likelihood directly for faster synthesis. Future work includes incorporating memory for long-range dependencies and applying the model to challenging tasks. In future work, VideoFlow plans to incorporate memory to model long-range dependencies in videos and apply the model to challenging tasks. The dataset consists of 8-bit videos with added uniform noise to match its discretization level. This noise is necessary to prevent the empirical distribution from becoming infinite. Incorporating memory to model long-range dependencies in videos is a future plan for VideoFlow. The dataset consists of 8-bit videos with added uniform noise to prevent infinite densities in the empirical distribution. Additive noise is necessary for optimization of log-likelihood and allows for minimization of KL divergence. Applying low temperature to latent gaussian priors of SV2P and SAVP-VAE was evaluated empirically. Applying low temperature to latent gaussian priors of SV2P and SAVP-VAE was evaluated empirically, showing that decreasing temperature from 1.0 to 0.0 decreases the performance of VAE models. The VideoFlow model benefits from low-temperature sampling by balancing noise removal from the background with reduced stochasticity. The VAE models' performance decreases when temperature is decreased from 1.0 to 0.0. Lower temperature reduces stochasticity of the arm motion, impacting performance. Training progression correlates with video quality. The VAE models show a decrease in performance as temperature is lowered from 1.0 to 0.0, reducing stochasticity in arm motion. Training progression is linked to video quality, with lower bits-per-pixel resulting in higher quality videos generated by the VideoFlow model. The VideoFlow model generates high-quality videos by learning to model the structure and motion of the arm as bits-per-pixel decreases. Training includes a learning rate schedule and optimization with the Adam optimizer. Models were tuned using the VGG cosine similarity metric with ground-truth data. The VideoFlow model generates high-quality videos by learning the structure and motion of the arm. Hyperparameters include a learning rate schedule, linear warmup for the first 10000 steps, and a linear-decay schedule for the last 150000 steps. Models are trained for 300K steps using the Adam optimizer. Tuning is done using the maximum VGG cosine similarity metric with ground-truth data. Different values of latent loss multiplier are used. For the SAVP-VAE model, linear decay is applied on the learning rate for the last 100K steps. SAVP-GAN involves tuning the gan loss multiplier and learning rate on a logscale. The VideoFlow model generates high-quality videos by learning the structure and motion of the arm. Hyperparameters include a learning rate schedule, linear warmup for the first 10000 steps, and a linear-decay schedule for the last 150000 steps. Models are trained for 300K steps using the Adam optimizer. Tuning involves comparing P(X4 = Xt|X<4) and VGG cosine similarity between X4 and Xt for t = 4 to 13. Results are reported for every similarity metric with the ground-truth across 100 decodes, using different values of latent loss multiplier. For SAVP-VAE, linear decay is applied on the learning rate for the last 100K steps. SAVP-GAN tuning includes adjusting the gan loss multiplier and learning rate on a logscale. In Figure 12, correlation between VGG cosine similarity and bits-per-pixel is analyzed using a pretrained VGG network and VideoFlow model. A weak correlation of -0.51 is observed. Additionally, evaluations are repeated with a smaller VideoFlow model with 4x parameter reduction, remaining competitive with SVG-LP. In Figure 13, results for every video in the test set are reported, showing a weak correlation of -0.51 between VGG perceptual metrics and bits-per-pixel. Evaluations are repeated with a smaller version of the VideoFlow model, maintaining competitiveness with SVG-LP."
}