{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are vulnerable to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, adversarial perturbations are decomposed into model-specific and data-dependent components, with the latter contributing significantly to transferability. The proposed noise reduced gradient (NRG) method targets the data-dependent component to craft adversarial examples, leading to enhanced transferability across various ImageNet classification models. It is observed that low-capacity models outperform high-capacity models in attack capability when test performance is comparable, providing valuable insights for improving adversarial robustness. Experiments show that the new approach significantly improves transferability in various ImageNet classification models. Low-capacity models are found to have stronger attack capability compared to high-capacity models with similar test performance. These findings can guide the construction of successful adversarial examples and aid in developing effective defense strategies against black-box attacks in neural network applications like speech recognition and computer vision. Insights from experiments show that adversarial examples can be constructed successfully to fool neural network models, leading to incorrect outputs. This phenomenon poses a challenge for defense strategies against black-box attacks in applications like speech recognition and computer vision. Recent works have shown that adversaries can manipulate inputs to fool models, creating adversarial examples. Understanding this phenomenon and effectively defending against such attacks are still open questions. Adversarial examples can transfer across different models, allowing them to fool multiple models with high probability, a property known as transferability. The phenomenon of adversarial vulnerability and defending against adversarial examples are still open questions. Adversarial examples can transfer across different models, fooling them with high probability, a property known as transferability. This can be leveraged to attack black-box systems. The adversarial vulnerability phenomenon was first introduced and studied, with different methods proposed to generate adversarial examples. These methods include box-constraint L-BFGS optimization, fast gradient sign method (FGSM), and DeepFool method. The primary cause of adversarial instability was attributed to the linear nature and high dimensionality of deep neural networks. Several methods have been proposed to generate adversarial examples, including the fast gradient sign method (FGSM) and the DeepFool method. These methods exploit the linear nature and high dimensionality of deep neural networks to create adversarial instability. Transferability of adversarial examples has been analyzed, with ensemble-based approaches suggested for effective black-box attacks. In BID6 and BID16, the iterative gradient sign method is effective for white-box attacks but not for black-box attacks. BID8 proposed ensemble-based approaches for effective black-box attacks by analyzing the transferability of adversarial examples. BID3 showed that high-confidence adversarial examples have strong transferability. Various works focus on crafting adversarial examples for attacks and developing defense mechanisms, such as defensive distillation by BID12 and adversarial training method introduced by BID5, examined on ImageNet by BID6 and BID16. BID9 utilized image transformation for defense. Several works have focused on crafting adversarial examples for attacks and developing defense mechanisms. BID12 proposed defensive distillation, while BID5 introduced the adversarial training method, examined on ImageNet by BID6 and BID16. BID9 utilized image transformation to mitigate the harm of adversarial perturbations. Some works attempted to detect adversarial examples for manual processing, but these methods can be easily broken by designing stronger attacks. In this work, the transferability of adversarial examples is explained, and insights are used to enhance black-box attacks by decomposing adversarial perturbations into model-specific and data-dependent components. The transferability of adversarial examples is explained in this work, with insights used to enhance black-box attacks by decomposing perturbations into model-specific and data-dependent components. The model-specific component is noisy and represents behavior off the data manifold, while the data-dependent component approximates the ground truth on the data manifold, contributing mainly to the transferability of adversarial perturbations. The transferability of adversarial examples is enhanced by decomposing perturbations into model-specific and data-dependent components. The data-dependent component, which approximates the ground truth on the data manifold, mainly contributes to transferability. Adversarial examples are constructed using the noise-reduced gradient (NRG) method, showing promising results on the ImageNet validation set. The proposed noise-reduced gradient (NRG) method enhances the transferability of adversarial examples by utilizing the data-dependent component of the gradient. This method, when combined with other known techniques, significantly improves the success rate of black-box attacks on the ImageNet validation set. The success rate of these attacks is influenced by model-specific factors such as capacity and accuracy. The proposed noise-reduced gradient method, when combined with other techniques, increases the success rate of black-box attacks on the ImageNet validation set. Model-specific factors like capacity and accuracy impact the success rate of these attacks. Models with higher accuracy and lower capacity are better at attacking unseen models, guided by transferability principles. The high accuracy and lower capacity of models enhance their ability to attack unseen models, guided by transferability principles. The model function f : R d \u2192 R K is defined by minimizing empirical risk over the training set, with d 1 and K = o(1). The high dimensionality of f (x) makes it vulnerable to adversarial perturbations. In this work, the focus is on studying adversarial examples in deep neural networks, where the high dimensionality of the model makes it susceptible to imperceptible perturbations. These perturbations, called adversarial perturbations, can lead to the creation of adversarial examples. Adversarial perturbations are small imperceptible changes that create adversarial examples in deep neural networks. These examples can be used in non-targeted attacks to misclassify input data. Targeted attacks aim to produce a specific wrong label chosen by the adversary. In non-targeted attacks, the adversary aims to misclassify input data without control over the output. In contrast, targeted attacks aim to produce a specific wrong label chosen by the adversary. In black-box attacks, the adversary has no knowledge of the target model and cannot query it directly. In a black-box attack, the adversary has no knowledge of the target model and cannot query it directly. They can construct adversarial examples on a local model trained on a similar dataset and use them to fool the target model. This type of attack is different from white-box attacks, where the target is the source model itself. Crafting adversarial examples involves creating perturbations on a local model trained on a similar dataset to fool the target model in a black-box attack. The process is based on an optimization problem with a loss function J measuring prediction discrepancies and a constraint for image data. Adversarial perturbation is modeled as an optimization problem with a loss function J measuring prediction discrepancies and a constraint for image data. The commonly used loss function is cross entropy, and the measurement of distortion is typically done using \u221e and 2 norms. BID2 introduced a loss function that manipulates output logit directly, commonly used in many works. Distortion is best measured by human eyes but commonly quantified using \u221e and 2 norms. Ensemble-based approaches, like BID8, suggest using a large ensemble of source models to improve adversarial examples. Averaging predicted probabilities is a commonly used ensemble strategy. Ensemble-based approaches like BID8 recommend using a large ensemble of source models to enhance adversarial examples. A common ensemble strategy involves averaging the predicted probabilities of each model. Different optimizers can be used to solve the problem, with a focus on normalized-gradient based methods in this paper. The paper focuses on ensemble-based approaches for enhancing adversarial examples by averaging predicted probabilities of source models. It discusses using normalized-gradient based optimizers, specifically the Fast Gradient Based Method, for solving non-targeted and targeted attacks efficiently. The paper discusses using normalized-gradient based optimizers, such as the Fast Gradient Based Method and Iterative Gradient Method, for efficient non-targeted and targeted attacks. The Fast Gradient Based Method performs one-step iteration using a normalized gradient vector, while the Iterative Gradient Method performs normalized-gradient ascent for k steps. Both methods are empirically shown to be fast with good transferability. The Iterative Gradient Method performs normalized-gradient ascent for k steps, using a projection operator to enforce constraints and a step size \u03b1. The method includes the iterative gradient sign method for \u221e-attack and g q (x) for q-attack, with the fast gradient based method as a special case when \u03b1 = \u03b5, k = 1. The transferability of adversarial examples between models is crucial for black-box attacks and defense strategies. Only a few articles, such as BID8 and BID17, have explored why this transferability occurs, suggesting it stems from similarities in decision boundaries. The transferability of adversarial examples between models is crucial for black-box attacks and defense strategies. Previous works suggest that transferability comes from similarities in decision boundaries, especially in the direction of transferable adversarial examples. The transferability of adversarial examples between models is influenced by similarities in decision boundaries, particularly in the direction of transferable adversarial examples. Models A and B, despite having high performance on the same dataset, may exhibit different behaviors off the data manifold due to factors such as model architectures and random initializations. The transferability of adversarial examples between models A and B is influenced by similarities in decision boundaries. Factors such as model architectures and random initializations can lead to different behaviors off the data manifold. Decomposing the perturbation into data-dependent and model-specific components can help understand the transferability from A to B. The transferability of adversarial examples between models A and B is influenced by similarities in decision boundaries. Decomposing the perturbation into data-dependent and model-specific components can help understand the transferability from A to B. The data-dependent component mainly contributes to transferability, capturing shared information between models, while the model-specific component has little impact due to different behaviors off the data manifold. The transferability of adversarial examples between models A and B is influenced by similarities in decision boundaries. Decomposing the perturbation into data-dependent and model-specific components can help understand the transferability from A to B. The data-dependent component mainly contributes to transferability, capturing shared information between models, while the model-specific component has little impact due to different behaviors off the data manifold. The adversarial perturbation crafted from model A can mislead both model A and B, with \u2207f A almost aligned to the inter-class deviation. In model A, the perturbation can mislead both model A and B, with \u2207f A almost aligned to the inter-class deviation. The transfer from A to B is mainly influenced by the data-dependent component, while the model-specific component has little impact. The NRG method aims to enhance black-box adversarial attacks by reducing model-specific noise and increasing the data-dependent component, as shown in the decision boundaries of resnet34 (model A) and densenet121 (model B) for ImageNet. The NRG method aims to reduce model-specific noise and enhance the data-dependent component for black-box adversarial attacks, specifically focusing on densenet121 (model B) for ImageNet. The method proposes using local average to remove high-frequency noise inherited from random initialization, while preserving the low-frequency knowledge learned from training data. The Noise-reduced gradient (NRG) method aims to reduce model-specific noise by applying local average to remove high-frequency noise inherited from random initialization. This results in capturing more data-dependent information compared to the ordinary gradient \u2207f. The Noise-reduced gradient (NRG) method uses local average to remove noisy model-specific information, capturing more data-dependent information than the ordinary gradient \u2207f. Visualizing NRG for different sample sizes shows that larger m leads to a smoother and more data-dependent gradient, with m = 100 capturing semantic information effectively. This helps prevent overfitting to specific source models and improves generalization. The Noise-reduced gradient (NRG) method visualizes NRG for different sample sizes, showing that larger m leads to a smoother and more data-dependent gradient. Using \u2207f in Eq.(8) instead of \u2207f can drive the optimizer towards more data-dependent solutions. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) is a special case of the iterative gradient sign method. The proposal suggests using \u2207f in Eq.(8) instead of \u2207f for attacks to drive the optimizer towards more data-dependent solutions. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) is a special case of the iterative gradient sign method, with a noise-reduced version for q-attack. The noise-reduced fast gradient sign method (nr-FGSM) is a special case for q-attack, with a noise-reduced version replacing the second equation. The effectiveness of NRG for enhancing transferability is analyzed using ImageNet dataset models. The dataset used is the ImageNet ILSVRC2012 validation set with 50,000 samples, randomly selecting 5,000 images for each attack experiment. To analyze the effectiveness of NRG for enhancing transferability, classification models trained on ImageNet dataset are used. The dataset consists of 50,000 samples from the ImageNet ILSVRC2012 validation set, with 5,000 images randomly selected for each attack experiment using pre-trained models like resnet and vgg provided by PyTorch. For targeted attack experiments, random wrong labels are assigned to each image. Various pre-trained models from PyTorch are used, including resnet and vgg series, to ensure experiment reliability. Only selected models are used for specific experiments to save computational time. The Top-1 and Top-5 accuracies of various pre-trained models from PyTorch, including resnet, vgg, densenet, alexnet, and squeezenet, can be found on a website. To increase experiment reliability, all models are used, but only selected ones are chosen for specific experiments to save computational time. White-box attack performance is evaluated using a set of adversarial examples, with targeted attacks assessed based on Top-1 success rate. The white-box attack performance is evaluated using a set of adversarial examples, with targeted attacks assessed based on the Top-1 success rate. The distortion is measured by two distances: \u221e norm and scaled 2 norm. The cross entropy 2 is chosen as the loss function, and both FGSM and IGSM are used as optimizers. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with commonly-used methods like FGSM and IGSM. Success rates of FGSM and nr-FGSM are compared in this section. Combining noise-reduced gradient technique with commonly-used methods like FGSM and IGSM shows improved success rates. Nr-FGSM consistently outperforms original FGSM in both blackbox and white-box attacks. The results of combining noise-reduced gradient technique with FGSM and IGSM show improved success rates in both blackbox and white-box attacks. Nr-FGSM consistently outperforms original FGSM, even under the same number of gradient calculations. The combination of noise-reduced gradient technique with FGSM and IGSM improves success rates in both blackbox and white-box attacks. Nr-IGSM generates more easily transferable adversarial examples compared to IGSM, indicating that noise-reduced gradient guides the optimizer towards data-dependent solutions. Table 2 shows that adversarial examples generated by nr-IGSM transfer more easily than those generated by IGSM. Large models like resnet152 are more robust to adversarial transfer. Model-specific components also contribute to transfer across models with similar architectures. Large models like resnet152 are more robust to adversarial transfer. Model-specific components also contribute to transfer across models with similar architectures. In most cases, IGSM generates stronger adversarial examples than FGSM, except for attacks against alexnet. This contradicts previous claims but aligns with higher confidence adversarial examples. IGSM generates stronger adversarial examples than FGSM, except for attacks against alexnet. This contradicts previous claims but aligns with higher confidence adversarial examples. The inappropriate choice of hyperparameters may lead to underfitting when attacking the alexnet model. The inappropriate choice of hyperparameters, such as \u03b1 = 1 and k = min(\u03b5 + 4, 1.24\u03b5) in BID6, may lead to underfitting when attacking the alexnet model, causing the solution to overfit. Multiple iterations of IGSM overfit more than FGSM, resulting in a lower fooling rate. This indicates a lack of trust in the objective in Eq. (2). The AlexNet model differs significantly from source models in architecture and test accuracy. Multiple iterations of IGSM lead to overfitting, with a lower fooling rate compared to FGSM. This highlights the need to not fully trust the objective in Eq. (2). Our noise reduced gradient technique removes model-specific information, allowing for better cross-model generalization. Our noise reduced gradient technique regularizes the optimizer by removing model-specific information, leading to better cross-model generalization. The NRG method is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images. FGSM, IGSM, and their noise reduced versions are tested for non-targeted attacks, with IGSM showing nearly saturated success rates. In this study, the NRG method is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images. Both non-targeted attacks using FGSM, IGSM, and their noise reduced versions, as well as targeted attacks, are tested. The Top-1 success rates of IGSM attacks are nearly saturated, so the corresponding Top-5 rates are reported to demonstrate method improvements more clearly. For targeted attacks, generating adversarial examples predicted by unseen target models as a specific label is much harder than non-targeted examples. Single-model based approaches are ineffective in generating targeted adversarial examples, so they were not considered in the study. Targeted adversarial examples are sensitive to the step size \u03b1 used in optimization procedures. In targeted attacks, generating specific label adversarial examples is more challenging than non-targeted ones. Single-model approaches are ineffective for targeted examples. Targeted adversarial examples are sensitive to step size \u03b1, with larger steps needed for success. More detailed analysis can be found in Appendix A. In targeted attacks, a large step size is necessary for generating strong adversarial examples. NRG methods outperform normal methods by a large margin in both targeted and non-targeted attacks. Top-5 success rates are reported in Table 3 for ensemble-based approaches. The Top-5 success rates of ensemble-based approaches are compared between normal methods and NRG methods for targeted and non-targeted attacks. NRG methods significantly outperform normal methods in this scenario. The corresponding Top-1 success rates can be found in Appendix C. Sensitivity of hyper parameters m is explored in this part. In this part, the sensitivity of hyper parameters m and \u03c3 is explored when applying NRG methods for black-box attacks using the nr-FGSM approach. Larger m leads to a higher fooling rate for any distortion level \u03b5 due to better estimation. Four attacks are considered, and the results are shown in FIG6. The sensitivity of hyperparameters m and \u03c3 in NRG methods for black-box attacks using the nr-FGSM approach is explored. Larger m leads to a higher fooling rate, while an optimal value of \u03c3 induces the best performance. Overly large \u03c3 introduces bias, and extremely small \u03c3 is ineffective in removing noisy model-specific information. In exploring the sensitivity of hyperparameters in NRG methods for black-box attacks, an optimal value of \u03c3 induces the best performance. The optimal \u03c3 varies for different source models, being about 15 for resnet18 and 20 for densenet161. Additionally, the robustness of adversarial perturbations to image transformations is preliminarily explored. In this experiment, the optimal \u03c3 varies for different source models, with a value of about 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is explored to determine if adversarial examples can survive in the physical world. The influence of image transformations on adversarial examples is quantified using the destruction rate, which measures the fraction of adversarial images that are no longer misclassified after a transformation. Densenet121 and resnet34 are chosen as the source and target models, with four image transformations considered. The study evaluates the impact of image transformations on adversarial examples using the destruction rate. Densenet121 and resnet34 are the source and target models, with four transformations considered. Results show that NRG-based methods generate more robust adversarial examples compared to vanilla methods. In this section, image transformations such as rotation, Gaussian noise, Gaussian blur, and JPEG compression are considered. The study analyzes the decision boundaries of different models, including Resnet34 as the source model and nine target models. The results show that adversarial examples generated by NRG methods are more robust than those generated by vanilla methods. The study analyzes decision boundaries of different models using Resnet34 as the source model and nine target models. Adversarial examples generated by NRG methods are found to be more robust than those from vanilla methods. The \u2207f is estimated with m = 1000, \u03c3 = 15, and each point (u, v) in a 2-D plane corresponds to an image perturbed by u and v along sign \u2207f and sign (\u2207f \u22a5 ). A randomly selected image recognized by all models is used to show decision boundaries. Various source models and images were tested, yielding similar results. The study analyzes decision boundaries of different models using Resnet34 as the source model and nine target models. Adversarial examples generated by NRG methods are found to be more robust than those from vanilla methods. The direction of sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for the source model resnet34, but other target models are more sensitive along sign \u2207f. Removing \u2207f \u22a5 from gradients can be seen as penalizing. The study compares decision boundaries of different models using Resnet34 as the source model and nine target models. Adversarial transfer is more sensitive along sign \u2207f than sign (\u2207f \u22a5 ) for most target models, except AlexNet. Penalizing the optimizer along the model-specific direction helps avoid overfitting to the source model. The minimal distance u to produce adversarial transfer varies for different models, with complex models requiring significantly larger distances. The study compares decision boundaries of different models using Resnet34 as the source model and nine target models. Adversarial transfer is more sensitive along sign \u2207f than sign (\u2207f \u22a5 ) for most target models, except AlexNet. Penalizing the optimizer along the model-specific direction helps avoid overfitting to the source model. The minimal distance u to produce adversarial transfer varies for different models, with complex models requiring significantly larger distances. Gradients penalize the optimizer along the model-specific direction to avoid overfitting and source model-overfitting solutions that transfer poorly to other target models. The distances for complex models are larger than those of small models, providing a geometric understanding of why big models are more robust. Adversarial examples crafted from AlexNet generalize worst across models. The comparison between different models shows that complex models require significantly larger distances for adversarial transfer. Adversarial examples crafted from AlexNet generalize worst across models, while attacks from DenseNet121 consistently perform well for any target model. This indicates that different models can exhibit varying levels of robustness against adversarial attacks. The performance of different models in attacking a target model varies. For example, AlexNet's attacks generalize poorly across models, while DenseNet121 consistently performs well. This observation suggests that the choice of a local model for generating adversarial examples can impact the success of attacking a remote black-box system. Different models can exhibit varying performances in attacking a target model. To find the principle behind this phenomenon, we selected vgg19 bn and resnet152 as target models and used different models for FGSM and IGSM attacks. Results show that models with strong attack capability are concentrated in the bottom left corner, with low fooling rates. The results of using different models for FGSM and IGSM attacks are summarized in FIG11, showing that models with powerful attack capability are concentrated in the bottom left corner. Smaller test error and lower capacity models have stronger attack capability, as explained in Section 3.1 regarding transferability. The smaller test error and lower capacity models have stronger attack capability, as models with small \u2207f \u22a5 and large \u2207f can provide strong adversarial examples that transfer more easily. This phenomenon is explained in Section 3.1 regarding transferability. A smaller test error indicates lower bias for approximating the ground truth, while a less complex model allows the data-dependent factor to dominate. Models with small \u2207f \u22a5 and large \u2207f can produce strong adversarial examples that transfer easily. In this paper, it is shown that adversarial perturbations can be broken down into model-specific and data-dependent components, with the latter being the main contributor to transferability. The proposed noise-reduced gradient (NRG) based methods are used to create adversarial examples. In this study, it is demonstrated that adversarial perturbations consist of model-specific and data-dependent components, with the latter being crucial for transferability. The noise-reduced gradient (NRG) based methods are introduced for crafting more effective adversarial examples. Additionally, models with lower capacity and higher test accuracy exhibit stronger capabilities for black-box attacks. Future work will explore combining NRG-based methods with adversarial training to enhance defense against black-box attacks. The study introduces NRG-based methods for crafting more effective adversarial examples and highlights the importance of data-dependent components for transferability. Models with lower capacity and higher test accuracy show stronger capabilities for black-box attacks. Future research will explore combining NRG-based methods with adversarial training for defense against black-box attacks. The study discusses black-box attacks and the transferability of data-dependent components. It suggests that black-box attacks may be defensible due to their low-dimensional nature, while white-box attacks are more challenging to defend against. Future research may focus on learning stable features for transfer learning using the NRG strategy. The impact of hyperparameters like iteration number and step size on targeted black-box attacks is also explored. Future research may focus on learning stable features for transfer learning using the NRG strategy. The impact of hyperparameters like iteration number and step size on targeted black-box attacks is explored, with success rates evaluated on 1,000 randomly selected images using resnet152 and vgg16 bn as target models. The performance is assessed by the average Top-5 success rate over three ensembles. In this experiment, the influence of step size \u03b1 on the quality of adversarial examples generated is explored. The optimal step size is found to be very large, around 15, compared to the allowed distortion \u03b5 = 20. Both too large and too small step sizes harm the attack performances. Interestingly, a small step size of \u03b1 = 5 with a large number of iterations performs worse than a small number of iterations. In Table 3 (b), the optimal step size \u03b1 is very large at around 15, compared to the allowed distortion \u03b5 = 20. Both too large and too small step sizes can harm attack performances. With a small step size of \u03b1 = 5, a large number of iterations performs worse than a small number of iterations, possibly due to overfitting. A large step size can prevent overfitting and encourage exploration of model-independent areas, making more iterations beneficial. To further confirm the influence of model redundancy on attack capability, an additional experiment was conducted on the MNIST dataset using fully connected networks of varying depths. The Top-1 success rates of cross-model attacks were reported in the experiment. In an experiment on the MNIST dataset, models of different depths were tested for their attack capability. Results showed that low-capacity models had stronger attack capabilities compared to large-capacity models. The results in TAB5 show that low-capacity models have stronger attack capabilities than large-capacity models, consistent with previous observations. Adversarial attacks were tested on resnet152 using FGSM and IGSM attacks with k=20, \u03b1=5, and \u03b5=15 distortion."
}