{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions, such as Q-values, are essential in reinforcement learning algorithms like SARSA and Q-learning. A new concept of action value is proposed, defined by a Gaussian smoothed version of the expected Q-value used in SARSA. These smoothed Q-values still satisfy a Bellman equation and can be learned from experience sampled from an environment. The gradients of expected reward with respect to a parameterized Gaussian policy can be obtained from the gradient and Hessian of the smoothed Q-value function, leading to the development of new algorithms for training a Gaussian policy directly from a learned Q-value. The gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. New algorithms have been developed for training a Gaussian policy directly from a learned Q-value approximator, achieving strong results on continuous control benchmarks. The approach involves training a Gaussian policy directly from a learned Q-value approximator, allowing for learning both mean and covariance during training. This method achieves strong results on continuous control benchmarks by alternating between policy evaluation and policy improvement processes. Model-free reinforcement learning algorithms involve policy evaluation and policy improvement processes. Different notions of Q-value lead to distinct families of RL methods, such as SARSA and Q-learning. SARSA uses the expected Q-value, while Q-learning exploits a hard-max notion of Q-value. Different notions of Q-value in reinforcement learning algorithms lead to distinct families of methods. SARSA uses the expected Q-value based on the current policy, while Q-learning utilizes a hard-max notion of Q-value for an optimal policy. Soft Q-learning and PCL employ a soft-max form of Q-value for an optimal entropy regularized policy, impacting algorithm outcomes by restricting policy types and determining algorithm types. In this work, a new notion of action value is introduced: the smoothed action value function Q \u03c0. Unlike previous notions, which assign a value to a specific action at each state, the smoothed Q-value assigns a value to a distribution over actions. The smoothed action value function Q \u03c0 introduces a new notion of action value by associating a value with a distribution over actions, rather than a specific action at each state. It defines the smoothed Q-value of a state-action pair as the expected return of taking an action sampled from a normal distribution centered at a, followed by actions sampled from the current policy. This concept can be seen as a Gaussian-smoothed version of the expected Q-value. The smoothed Q-value of a state-action pair is defined as the expected return of taking an action sampled from a normal distribution centered at a, followed by actions sampled from the current policy. Smoothed Q-values have properties that make them useful in RL algorithms, such as satisfying single-step Bellman consistency and allowing bootstrapping for training function approximators. For Gaussian policies, the optimization objective is the expected return. Smoothed Q-values have properties that make them attractive for RL algorithms, such as single-step Bellman consistency and compatibility with Gaussian policies. The optimization objective for Gaussian policies can be expressed in terms of smoothed Q-values, allowing for efficient policy parameter updates. Smoothed Q-values are beneficial for RL algorithms due to their properties like single-step Bellman consistency and compatibility with Gaussian policies. The optimization objective for Gaussian policies can be expressed using smoothed Q-values, enabling efficient policy parameter updates. The Smoothie algorithm utilizes derivatives of a trained smoothed Q-value function to train a policy, avoiding the high variance of stochastic updates in standard policy training. Smoothie algorithm utilizes derivatives of a trained smoothed Q-value function to train a policy, allowing for exploratory behavior with a non-deterministic Gaussian policy. This approach avoids high variance in standard policy training and reduces the need for excessive hyperparameter tuning. The Smoothie algorithm utilizes a non-deterministic Gaussian policy with mean and covariance parameters, allowing for exploratory behavior without excessive hyperparameter tuning. It can incorporate proximal policy optimization techniques by penalizing KL-divergence from previous policies, which is not feasible in standard DDPG algorithms. Smoothie can easily incorporate proximal policy optimization techniques by adding a penalty on KL-divergence from previous policies, improving stability and performance. Results on continuous control benchmarks are competitive, especially for challenging tasks with limited data. The algorithm operates in a model-free RL framework where an agent interacts with a stochastic environment. Our formulation improves stability and performance in the standard model-free RL framework. Results on continuous control benchmarks are competitive, especially for challenging tasks with limited data. The goal is to find an agent that achieves maximal cumulative discounted reward in a Markov decision process (MDP) with state space S and action space A. In a stochastic black-box environment, an agent interacts with the environment by observing states, taking actions, and receiving rewards. The goal is to maximize cumulative discounted reward in a Markov decision process (MDP) with state space S and action space A. The agent's behavior is modeled using a stochastic policy \u03c0 that produces a distribution over feasible actions at each state. The agent interacts with the environment in space A, encountering states and taking actions based on a stochastic policy \u03c0. The optimization objective is the expected discounted return, expressed in terms of the expected action value function Q \u03c0 (s, a). The Bellman equation recursively defines Q \u03c0 (s, a) with a discount factor \u03b3. The policy gradient theorem expresses the gradient of the optimization objective with respect to the tunable parameters of a policy. It is a key concept in reinforcement learning algorithms. The policy gradient theorem BID23 expresses the gradient of the optimization objective with respect to the tunable parameters of a policy, involving trade-offs between variance and bias in estimating Q \u03c0 (s, a) using function approximation. In this paper, the focus is on multivariate Gaussian policies for continuous action spaces, represented by a mean and covariance function. The observed state of the MDP is represented as a feature vector, and the policy gradient algorithms trade off variance and bias in estimating Q \u03c0 (s, a) using function approximation. Discounted rewards are calculated using a single Monte Carlo sample for multivariate Gaussian policies over continuous action spaces. The policy is parametrized by mean and covariance functions mapping the observed state to a Gaussian distribution. New RL training methods are developed for this policy family, with potential generalization to other policy families. Prior work on learning Gaussian policies is reviewed. The text discusses new RL training methods for parametric policies mapping the environment state to a Gaussian distribution. It reviews prior work on learning Gaussian policies, including the deterministic policy gradient for Gaussian policies with a small policy covariance. BID21 introduces the deterministic policy gradient for Gaussian policies with a small policy covariance, allowing for estimating expected future returns and optimizing the policy parameters. BID21 introduces the deterministic policy gradient for Gaussian policies with a small policy covariance, allowing for estimating expected future returns and optimizing the policy parameters. Under a deterministic policy, one can estimate the expected future return from a state and optimize the value function approximator by minimizing the Bellman error. In the limit of \u03a3 \u2192 0, the Bellman equation can be re-expressed, allowing for optimization of a value function approximator by minimizing the Bellman error for transitions sampled from a dataset. Algorithms like DDPG alternate between improving the value function and policy through gradient descent. To improve sample efficiency, off-policy distributions are used in place of on-policy distributions. In practice, algorithms like DDPG alternate between improving the value function and policy through gradient descent. To enhance sample efficiency, off-policy distributions are utilized. BID5 and BID21 replace the on-policy state distribution with an off-policy distribution based on a replay buffer, which has been found to work well in practice. This paper introduces smoothed action value functions, whose gradients effectively optimize the parameters of a Gaussian policy. In this paper, smoothed action value functions are introduced to optimize Gaussian policy parameters. Smoothed Q-values, denoted Q \u03c0 (s, a), differ from ordinary Q-values by not assuming the first action is fully specified, but only the mean of the distribution. This allows for an effective signal for parameter optimization. Optimizing Gaussian policy parameters involves using smoothed Q-values, denoted Q \u03c0 (s, a), which do not assume the first action is fully specified but only the mean of the distribution. This approach differs from prior work by performing an expectation of Q \u03c0 (s, \u00e3) for actions \u00e3 drawn near a. Smoothed action values are defined as Q \u03c0 (s, a) for a Gaussian policy \u03c0, allowing direct learning of Q \u03c0 (s, a) without the need for sampling. This approach is differentiated by its Bellman consistency and avoids the need for function approximators. Directly learning smoothed Q-values, Q \u03c0 (s, a), for Gaussian policies eliminates the need for sampling by leveraging Bellman consistency. The one-step Bellman equation for Q-values is derived from sampled r and s, enabling the use of derivatives to learn policy parameters \u00b5 and \u03a3. Directly learning smoothed Q-values for Gaussian policies eliminates sampling by leveraging Bellman consistency. The one-step Bellman equation for Q-values is derived from sampled r and s, enabling the use of derivatives to learn policy parameters \u00b5 and \u03a3. Parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 in terms of mean and covariance parameters allows for direct optimization of Q \u03c0 through the gradient of the objective w.r.t. mean parameters. The Bellman equation enables direct optimization of Q \u03c0 for a Gaussian policy parameterized by mean and covariance parameters. Estimating the derivative of the objective w.r.t. covariance parameters is challenging, but the second derivative of Q \u03c0 w.r.t. actions allows for exact computation of the derivative w.r.t. \u03a3. Estimating the derivative of the objective with respect to covariance parameters is challenging, but the second derivative of Q \u03c0 with respect to actions allows for exact computation of the derivative with respect to \u03a3. A proof of this identity is provided in the Appendix, derived using standard matrix calculus. The full derivative with respect to \u03c6 can be optimized in two ways for Q 2 using multiple samples. The proof in the Appendix shows how to optimize the full derivative with respect to \u03c6 for Q 2 using standard matrix calculus. Two approaches are considered for optimizing Q 2, with the second approach using a single function approximator for Q \u03c0 w (s, a) for a simpler implementation in experimental evaluation. The second approach for optimizing Q 2 involves using a single function approximator for Q \u03c0 w (s, a) and drawing a phantom action to minimize a weighted Bellman error. This approach simplifies the implementation in experimental evaluation. The second approach for optimizing Q \u03c0 w (s, a) involves drawing a phantom action to minimize a weighted Bellman error, assuming a sampling distribution with full support. The training procedure reaches an optimum when Q \u03c0 w (s, a) satisfies the recursion in the Bellman equation. When optimizing Q \u03c0 w (s, a), the training procedure reaches an optimum when Q \u03c0 w (s, a) satisfies the recursion in the Bellman equation. It is unnecessary to track probabilities q(\u00e3 | s) when using target networks, as the replay buffer provides a near-uniform distribution of actions conditioned on states. In practice, it is unnecessary to track probabilities q(\u00e3 | s) when using target networks, as the replay buffer provides a near-uniform distribution of actions conditioned on states. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods to mitigate this issue. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods to mitigate this issue. Stabilizing techniques like constraining gradient steps within a trust region or penalizing KL-divergence from a previous policy have not been applicable to deterministic policy algorithms like DDPG. The proposed formulation in this paper is easily amenable to trust region methods. The paper proposes a formulation easily amenable to trust region optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. This optimization is straightforward due to the analytical expression of the KL-divergence of two Gaussians. This paper introduces a trust region optimization approach by penalizing KL-divergence from a previous policy. The optimization is straightforward due to the analytical expression of the KL-divergence of two Gaussians. It builds on previous work using Q-value functions to learn a stable policy and utilizes gradient information to train the policy. The paper introduces a trust region optimization approach by penalizing KL-divergence from a previous policy. It builds on previous work using Q-value functions to learn a stable policy and utilizes gradient information to train the policy. The proposed method is a generalization of deterministic policy gradient, with updates for training the Q-value approximator and policy mean. The proposed method in the paper is a generalization of deterministic policy gradient, with updates for training the Q-value approximator and policy mean. It is a trust region optimization approach that penalizes KL-divergence from a previous policy. The method is compared to other approaches like policy gradient BID21 and Stochastic Value Gradient (SVG) BID8. The proposed method in the paper generalizes deterministic policy gradient by updating the Q-value approximator and policy mean. Stochastic Value Gradient (SVG) also trains stochastic policies similar to DDPG but lacks updates for covariance. Updating the covariance along the gradient of expected reward is crucial for trust region and proximal optimization. The paper introduces a method that updates the Q-value approximator and policy mean, extending deterministic policy gradient. Expected policy gradients (EPG) generalize DDPG by updating the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. This approach is crucial for trust region and proximal optimization techniques. The paper introduces a method called expected policy gradients (EPG) that updates the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. This approach avoids approximate integrals and simplifies updates compared to previous techniques like DDPG. The novel training scheme proposed for learning the covariance of a Gaussian policy relies on properties of Gaussian integrals and avoids approximate integrals, making the updates simpler. It directly estimates the smoothed Q-value function using neural network function approximators, unlike previous techniques like BID4 which rely on a quadratic Taylor expansion. The paper proposes a novel training scheme for learning the covariance of a Gaussian policy based on Gaussian integrals. It directly estimates the smoothed Q-value function using neural network function approximators, different from previous methods that rely on a quadratic Taylor expansion. The perspective presented in the paper focuses on Q-values representing the averaged return of a distribution of actions, distinct from recent advances in distributional RL. The paper introduces a new training scheme for learning the covariance of a Gaussian policy based on Gaussian integrals. It focuses on estimating the smoothed Q-value function using neural network function approximators, which is different from previous methods. The perspective presented in the paper emphasizes Q-values representing the averaged return of a distribution of actions, contrasting recent advances in distributional RL. The paper introduces a new RL algorithm, Smoothie, which utilizes insights from Gaussian policies to train a parameterized Q function and a Gaussian policy. The algorithm focuses on estimating the smoothed Q-value function using neural network function approximators. Smoothie is a new RL algorithm that maintains a parameterized Q function and a Gaussian policy. It uses the gradient and Hessian of the Q function approximation to train the policy. The algorithm involves collecting experience, updating parameters, and utilizing a replay buffer. Smoothie is a new RL algorithm that maintains a parameterized Q function and a Gaussian policy. It utilizes gradient and Hessian of the Q function to train the policy by collecting experience, updating parameters, and using a replay buffer. The algorithm is evaluated against DDPG, a standard algorithm known for good performance on continuous control benchmarks. Smoothie is evaluated against DDPG, a baseline algorithm known for good performance on continuous control benchmarks. The evaluation starts with a simple synthetic task involving a single-action one-shot environment with a reward function based on two Gaussians. The policy mean is initialized at the worse Gaussian for study purposes. Smoothie and DDPG are compared in a synthetic task with a reward function based on two Gaussians. Smoothie learns both mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima. Smoothie learns both the mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima, unable to progress far from the initial mean. Smoothie successfully solves the task by learning both the mean and variance, while DDPG struggles to escape local optima and cannot progress far from the initial mean. Smoothie's smoothed reward function guides \u00b5 \u03b8 towards the better Gaussian, unlike DDPG which is limited by the derivative of Q \u03c0 w at the current mean. Smoothie successfully adjusts the covariance during training, with \u03a3 \u03c6 decreasing initially and then increasing before approaching the global optimum. During training, Smoothie adjusts the covariance \u03a3 \u03c6, initially decreasing and then increasing before approaching the global optimum. The learnable policy mean and standard deviation for Smoothie and DDPG on a synthetic task are compared, showing Smoothie's ability to escape lower-reward local optima. Smoothie successfully escapes lower-reward local optima by adjusting its policy variance based on the smoothed reward function's convexity/concavity. In contrast, DDPG maintains a constant exploratory noise during training. The implementation utilizes feed forward neural networks for policy and Q-values, with the covariance parameterized as a diagonal given by e \u03c6. Smoothie adjusts its policy variance based on the reward function's convexity/concavity, competing with DDPG in continuous control benchmarks. Feed forward neural networks are used for policy and Q-values, with covariance parameterized as a diagonal. Exploration for DDPG is determined by an Ornstein-Uhlenbeck process. Average reward and standard deviation are shown in plots after selecting best hyperparameters. Smoothie competes with DDPG in continuous control benchmarks by adjusting its policy variance based on reward function convexity/concavity. The covariance is parameterized as a diagonal, and exploration for DDPG is determined by an Ornstein-Uhlenbeck process. Plots display average reward and standard deviation after selecting best hyperparameters, showing Smoothie's competitive performance with DDPG, especially in challenging tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient across all tasks. Smoothie competes with DDPG in continuous control benchmarks by adjusting its policy variance based on reward function convexity/concavity. It learns the optimal noise scale during training and outperforms DDPG in final reward performance, especially in challenging tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient as a competitive baseline. Results are compared in FIG2 after hyperparameter search for actor learning rate, critic learning rate, and reward scale. TRPO is not sample-efficient as a competitive baseline. Smoothie and DDPG are compared in FIG2 after hyperparameter search for actor learning rate, critic learning rate, and reward scale. DDPG extends the search to consider exploratory noise scale and damping, while Smoothie adjusts a hyperparameter for KL-penalty weight. Despite DDPG's exploration advantage through hyperparameter search, Smoothie performs well without supervision. Smoothie and DDPG were compared after hyperparameter search for actor learning rate, critic learning rate, and reward scale. Smoothie outperformed DDPG across all tasks, with significant improvements in Hopper, Walker2d, and Humanoid environments. Notably, the average reward for Hopper doubled, showcasing Smoothie's competitive performance without supervision. Smoothie performs competitively or better across all tasks, with a slight advantage in Swimmer and Ant, and significant improvements in Hopper, Walker2d, and Humanoid environments. The average reward for Hopper doubled, showcasing Smoothie's superior performance without supervision. Smoothie achieves impressive results with only millions of environment steps, outperforming TRPO which requires tens of millions of steps for similar rewards. Introducing a KL-penalty further enhances Smoothie's performance, particularly on challenging tasks. The introduction of a KL-penalty improves Smoothie's performance, especially on harder tasks. This penalty encourages stability and addresses the inherent instability in DDPG training. The benefits of using a proximal policy optimization method are observed, particularly in Hopper and Humanoid tasks, where performance significantly improves without sacrificing sample efficiency. Smoothie introduces a new Q-value function, Q \u03c0, which is a Gaussian-smoothed version of the standard expected Q-value. This function improves stability in DDPG training and shows significant performance improvements in tasks like Hopper and Humanoid. The algorithm successfully learns using Q \u03c0's gradient and Hessian relationship with the Gaussian policy's mean and covariance. The new Q-value function, Q \u03c0, is a Gaussian-smoothed version of the standard expected Q-value. It improves training stability and performance in tasks like Hopper and Humanoid. The algorithm, Smoothie, successfully learns mean and covariance using Q \u03c0's gradient and Hessian relationship with the Gaussian policy. The new Q-value function, Q \u03c0, successfully learns mean and covariance during training, matching or surpassing DDPG performance. Smoothed Q-values make the reward surface smoother, easier to learn, and have a direct relationship with the expected return objective. Future work should explore applying these concepts to other policies. The smoothed Q-values of Q \u03c0 make the reward surface smoother and have a direct relationship with the expected return objective. Future work should investigate applying these concepts to other policies. The specific identity mentioned can be derived using standard matrix calculus. The specific identity for Gaussian integrals can be derived using standard matrix calculus. The equations omit certain variables for succinctness, and the proof involves manipulating the left-hand side and right-hand side of the formula."
}