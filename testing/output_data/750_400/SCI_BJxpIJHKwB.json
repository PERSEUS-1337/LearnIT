{
    "title": "BJxpIJHKwB",
    "content": "Few shot image classification involves learning a classifier from limited labeled data. Generating classification weights is common in meta-learning for few shot image classification, but it's challenging to create exact weights for diverse query samples with very few training samples. The proposed Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) addresses this by generating different classification weights for each query sample, allowing them to attend to the entire support set. AWGIM introduces Attentive Weights Generation for few shot learning by generating different classification weights for diverse query samples. This approach maximizes mutual information between the generated weights and query/support data, unifying information maximization into few shot learning. AWGIM introduces Attentive Weights Generation for few shot learning by maximizing mutual information between generated weights and query/support data. This unifies information maximization into few shot learning, achieving state-of-the-art performance on benchmark datasets. AWGIM introduces Attentive Weights Generation for few shot learning by maximizing mutual information between generated weights and query/support data, achieving state-of-the-art performance on benchmark datasets. Deep learning methods have been successful in various domains but are limited by the need for large amounts of labeled data, unlike humans who can learn from limited data. Few shot learning and meta learning are proposed to address this limitation. Few shot learning and meta learning are popular approaches for addressing the limitations of deep learning methods that require large amounts of labeled data. Meta learning allows models to quickly adapt to new tasks by extracting high-level knowledge across different tasks. This approach enables deep models to learn from very few samples, unlike traditional deep learning methods. Meta learning is a popular approach for few shot problems, where models extract high-level knowledge across tasks to quickly adapt to new tasks. Different methods include gradient-based and metric-based approaches, with weights generation showing effectiveness. Weights generation methods have shown effectiveness in few shot learning, with the challenge of fixed classification weights for different query samples within one task being sub-optimal. Introducing Attentive Weights aims to address this issue. In this work, Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) is introduced to address the limitations of fixed classification weights for different query samples within one task. The classification weights are generated for each query sample specifically through two encoding paths where the query sample attends to the task context. In this work, Attentive Weights Generation (AWGIM) is introduced to address limitations in few shot learning. AWGIM generates classification weights specifically for each query sample through two encoding paths, allowing the query sample to attend to the task context. The proposed method maximizes mutual information between generated weights and query, support data to ensure diverse query data is accurately classified. In experiments, simple cross attention between query samples and support set fails to ensure classification weights fitted to diverse query data. To address this, AWGIM proposes maximizing mutual information between generated weights and query, support data. AWGIM introduces Variational Information Maximization in few shot learning, with minimal computational overhead. It eliminates inner update without compromising performance and is evaluated on two benchmark datasets. AWGIM is the first to introduce Variational Information Maximization in few shot learning, with minimal computational overhead. By maximizing mutual information, it eliminates inner updates without performance compromise. AWGIM shows state-of-the-art performance on benchmark datasets and detailed analysis validates its components. The AWGIM model introduces Variational Information Maximization in few shot learning, achieving state-of-the-art performance on benchmark datasets. Previous works in few shot learning include meta learning approaches such as gradient-based methods and meta-learner LSTM. In gradient-based approaches, optimal initialization is learned for all tasks (Finn et al., 2017). Ravi & Larochelle (2016) trained a meta-learner LSTM for few-shot classification tasks. Sun et al. (2019) optimized task-specific transformations for each layer. Metric-based methods learn similarity metrics between query and support samples (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018; Li et al., 2019a). Some works also consider spatial information or local image descriptors for richer similarities (Lifchitz et al., 2019; Li et al.). In metric-based methods, a similarity metric is learned between query and support samples. Some works also consider spatial information or local image descriptors for richer similarities. Generating classification weights directly has been explored by some works. Some works explore generating classification weights directly, using methods like linear combinations of weights, activations of a trained feature extractor, or \"fast weights\" from loss gradients. Graph neural network denoising autoencoders are also utilized in this context. In (2018), classification weights are generated from a trained feature extractor's activations. Graph neural network denoising autoencoders are used in (Gidaris & Komodakis, 2019). Munkhdalai & Yu (2017) proposed generating \"fast weights\" from loss gradients for each task. Other methods for few-shot classification include generative models to create more data (Wang et al., 2018; Chen et al., 2019) and closed-form solutions for few-shot classification by Bertinetto et al. (2019). Generative models and closed-form solutions are used for few-shot classification. Attention mechanism is effective in modeling interactions between queries and key-value pairs in various contexts. In this work, attention mechanism is used for few-shot image classification by encoding task and query-task information using self and cross attention. This approach is similar to Attentive Neural Processes, which also utilizes attention for the same purpose. In this work, attention is used for few-shot image classification by maximizing mutual information between keys and queries. The approach contrasts with regression tasks that focus on stochastic processes and variational objectives. Mutual information measures the decrease in uncertainty of one random variable when another is known. In contrast to regression tasks focusing on stochastic processes, this work utilizes attention for few-shot image classification by maximizing mutual information between keys and queries. Mutual information measures the decrease in uncertainty of one random variable when another is known. Mutual information measures the decrease in uncertainty of one random variable when another is known. It is widely applied in various applications such as Generative Adversarial Networks, self-supervised learning, and visual question generation. The attentive path equips the query sample with task knowledge through an attention mechanism. The attentive path equips query samples with task knowledge through an attention mechanism. It involves concatenating Xap with Xcp and generating classification weights specific for X using a weight generator. These weights can predict class labels and reconstruct inputs of the generator. The weight generator g concatenates Xap with Xcp to generate classification weights specific for X, predicting class labels and reconstructing inputs. The model's objective function maximizes mutual information between variables. The weight generator g concatenates Xap with Xcp to generate classification weights specific for X, predicting class labels and reconstructing inputs. The lower bound of mutual information is maximized to force g to generate sensitive classification weights for different query samples. The problem formulation, proposed model, objective function, and theoretical analysis are provided in this section. The problem is formulated under episodic training paradigm for few-shot classification tasks. In Sec. 3.4, the problem is formulated under episodic training paradigm for few-shot classification tasks. An N-way K-shot task includes support set S and query set Q. Support set S contains N K labeled samples, while query set Q includes x for which label \u0177 needs to be predicted based on S. Meta-loss is estimated on Q during meta-training to optimize the model, and the performance of meta-learning method is evaluated on Q during meta-testing. During meta-training and meta-testing, the model is optimized and evaluated on query set Q, given the labeled support set S. The classes used in meta-training and meta-testing are disjoint to facilitate knowledge transfer across tasks and quick adaptation to novel tasks. Our proposed approach follows a general framework for generating classification weights. Our proposed approach follows a general framework for generating classification weights, with a feature extractor outputting image feature embeddings. The meta-learner generates classification weights for different tasks, with Latent Embedding Optimization (LEO) being a related method for weight generation. In the framework for generating classification weights, a feature extractor outputs image feature embeddings. The meta-learner generates weights for tasks using Latent Embedding Optimization (LEO), where a latent code z is generated by h conditioned on a support set S. Classification weights w can be decoded from z with l. The loss is computed on the support set using w and z is updated accordingly. In Latent Embedding Optimization (LEO), a latent code z is generated by h conditioned on support set S. Classification weights w can be decoded from z with l. The loss is computed on the support set using w, and z is updated accordingly. The objective function of LEO minimizes a function involving \u03b8 parameters of h and l. LEO avoids updating high-dimensional w in the inner loop by learning a. The Latent Embedding Optimization (LEO) method generates a latent code z from support set S using h and decodes classification weights w with l. LEO minimizes an objective function involving \u03b8 parameters of h and l, avoiding inner updates for w by learning a lower-dimensional latent space. In contrast to AWGIM, LEO does not require inner updates to adapt the model, focusing on learning a latent space that can generate w efficiently. The Latent Embedding Optimization (LEO) method generates a latent code z from support set S using h and decodes classification weights w with l. LEO avoids inner updates for w by learning a lower-dimensional latent space. In contrast to AWGIM, LEO does not require inner updates to adapt the model, focusing on learning a latent space that can generate w efficiently. The most significant difference between LEO and AWGIM is that AWGIM is a feedforward network trained to maximize mutual information for different tasks. AWGIM learns to generate optimal classification weights for each query sample, while LEO generates fixed weights conditioned on the support set within one task. LEO generates fixed weights based on the support set within a task, while AWGIM learns optimal classification weights for each query sample. LEO can be seen as a special case of AWGIM under certain conditions. The framework includes a feature extractor processing images into d-dimensional vectors, with contextual and attentive paths for encoding task context and individual query samples. The feature extractor processes images into d-dimensional vectors. Two paths encode task context and query samples. Generated classification weights predict labels and maximize mutual information. The encoding process involves two paths: contextual and attentive. The outputs are concatenated for classification weight generation, predicting labels and maximizing mutual information. The encoding process includes two paths: contextual and attentive. The contextual path focuses on learning representations for the support set using a multi-head self-attention network. Existing weight generation methods may result in sub-optimal classification weights due to limited labeled data. The encoding process involves two paths: contextual and attentive. Existing weight generation methods may lead to sub-optimal classification weights due to limited labeled data. The attentive path allows individual query examples to attend to the task context for generating classification weights. The attentive path introduces adaptive classification weights by allowing individual query examples to attend to the task context, ensuring better adaptation to different query samples and awareness of the task context. A multi-head self-attention network is employed in the attentive path to encode global task information, different from the contextual path's focus on weight generation. The classification weights are adaptive to different query samples and task context. A multi-head self-attention network is used in the attentive path to encode global task information, while the contextual path focuses on generating classification weights. Sharing the same self-attention networks may limit the expressiveness of learned representations in both paths. The attentive path uses a multi-head self-attention network to encode global task information, while the contextual path focuses on generating classification weights. Sharing the same self-attention networks may limit the expressiveness of learned representations in both paths. The cross attention network is applied on each query sample and task-aware support set to produce comprehensive and expressive representations. The cross attention network is applied on each query sample and task-aware support set to produce comprehensive and expressive representations using multi-head attention with h heads. X cp\u2295ap \u2208 R |Q|\u00d7N K\u00d72d h is obtained by concatenating X cp and X ap after replication and reshaping. The cross attention network utilizes multi-head attention with h heads to generate X cp\u2295ap \u2208 R |Q|\u00d7N K\u00d72d h, which represents query samples with their own latent representations for support set, enabling the generation of specific classification weights. These weights are task-context aware and adaptive to individual query samples, decoded by the weights generator g : R 2d h \u2192 R 2d. The classification weights, represented as W, are generated for each query sample in a way that is task-context aware and adaptive. These weights follow a Gaussian distribution with diagonal covariance, and are sampled from a learned distribution during meta-training. To simplify, the mean value of the classification weights for each class is computed to obtain the final representation W final. The weights are generated following a Gaussian distribution with diagonal covariance. The classification weights are sampled from a learned distribution during meta-training and represented as W \u2208 R |Q|\u00d7N K\u00d7d. To simplify, the mean value of the classification weights for each class is computed to obtain the final representation W final. The prediction for query data is computed by XW finalT. The support data X is replicated for |Q| times and reshaped as X s \u2208 R |Q|\u00d7N K\u00d7d. The prediction for support data can also be computed as X s W finalT. The prediction for query and support data is computed using specific classification weight matrices. Two decoders reconstruct X cp and X ap based on generated weights. Reconstruction serves as auxiliary tasks for analysis. The weights generator g produces weights used by two decoders to reconstruct X cp and X ap. Reconstruction is used as auxiliary tasks for analysis. The weights generator produces weights for two decoders to reconstruct X cp and X ap, serving as auxiliary tasks for analysis. The analysis shows that query-specific weights do not outperform weights conditioned only on S, indicating insensitivity of generated weights to different query samples. The information from the attentive path is not well retained during the weight generation process. The weights generator produces weights for two decoders to reconstruct X cp and X ap, serving as auxiliary tasks for analysis. However, simply using query-specific weights does not outperform weights conditioned only on S, indicating insensitivity of generated weights to different query samples. To address this limitation, the proposal is to maximize the mutual information between generated weights w and support as well as query data. The proposal aims to maximize mutual information between generated weights and support/query data to improve weight generation. This involves using Variational Information Maximization to compute a lower bound due to unknown true posterior distributions. The proposal utilizes Variational Information Maximization to compute a lower bound for the mutual information between generated weights and support/query data. This approach approximates the true posterior distributions to maximize the lower bound as a proxy for the true mutual information. The proposal uses Variational Information Maximization to compute a lower bound for mutual information between generated weights and support/query data. This maximizes the log likelihood of labels for support and query data with respect to network parameters. The new objective function involves maximizing the log likelihood of labels for support and query data by minimizing cross entropy. Gaussian distributions are assumed for p \u03b8 (x|w) and p \u03b8 (x|w), with r 1 and r 2 approximating their means. The loss function for training the network involves reconstructing x cp and x ap with L2 loss. The loss function for training the network involves maximizing the log likelihood of labels for support and query data by minimizing cross entropy. Gaussian distributions are assumed for p \u03b8 (x|w) and p \u03b8 (x|w), with r 1 and r 2 approximating their means. The generated classification weights are forced to carry information about the support data and the specific query sample through hyper-parameters \u03bb 1 , \u03bb 2 , \u03bb 3. The loss function for training the network involves deciding weightage for different terms using hyper-parameters \u03bb 1 , \u03bb 2 , \u03bb 3. The generated classification weights carry information about support data and query samples. LEO computes inner update loss as cross entropy on support data, but weight generation does not involve specific query samples, making reconstruction impossible. The encoding process in LEO results in computational complexity O((N K) 2) due to self-attention, while the attentive path complexity is O((N K) 2 + |Q|(N K)). The total complexity is O((N K) 2 + |Q|(N K)). AWGIM is a special case of the proposed method, with contextual path and \u03bb 2 = \u03bb 3 = 0. The computational complexity is O((N K) 2 + |Q|(N K)), but the overhead is usually negligible in few-shot learning. The complexity of AWGIM is O((N K) 2 + |Q|(N K)), with the value of (N K) 2 usually negligible in few-shot learning. The computational overhead from cross attention is minimal due to parallel implementation via matrix multiplication. Inner updates are avoided without compromising performance, reducing training and inference time significantly. Empirical evaluation is presented in A.3.4, with experiments conducted on miniImageNet and tieredImageNet datasets to compare with other methods. The empirical evaluation is conducted on miniImageNet and tieredImageNet datasets, comparing with other methods. miniImageNet has 100 classes with 600 images each, while tieredImageNet is larger. The datasets are subsets of ILSVRC-12 dataset. tieredImageNet is a larger dataset with 608 classes and 779,165 images, selected from 34 higher level nodes in ImageNet hierarchy. 351 classes are used for meta-training, 97 for meta-validation, and 160 for meta-testing. Image features from LEO are utilized for analysis. In tieredImageNet, there are 608 classes and 779,165 images selected from 34 higher level nodes in ImageNet hierarchy. 351 classes are used for meta-training, 97 for meta-validation, and 160 for meta-testing. Image features from LEO are employed for analysis, with a 28-layer Wide Residual Network used for training. Each image is represented by a 640-dimensional vector for input to the model in N-way K-shot experiments. In LEO (Rusu et al., 2019), a 28-layer Wide Residual Network was trained on the meta-training set. Each image is represented by a 640-dimensional vector for input to the model in N-way K-shot experiments. During meta-testing, 600 N-way K-shot tasks are sampled from the meta-testing set, and the average accuracy for the query set is reported with a 95% confidence interval. During meta-testing, 600 N-way K-shot tasks are sampled from the meta-testing set. The average accuracy for the query set is reported with a 95% confidence interval. TensorFlow is used to implement the method with a feature embedding dimension of 640. The number of heads in the attention module is set to 4, with 2-layer MLPs for g, r1, and r2. Lambda values are determined by meta-validation performance. The study uses TensorFlow to implement a method with a feature embedding dimension of 640. The number of heads in the attention module is set to 4, with 2-layer MLPs for g, r1, and r2. Lambda values are determined by meta-validation performance. Various models are compared based on their performance metrics. Various models are compared based on their performance metrics, with Resnets-12 achieving 61.20% accuracy and 75.50% in meta-validation performance. WRN-28-10 models also show high accuracy, with DAE-GNN achieving 62.96% accuracy and 78.85% in meta-validation performance. Table 2 shows accuracy comparison of different approaches on tieredImageNet, with our AWGIM model achieving 63.12% accuracy in 5-way 1-shot tasks and 78.40% in 5-way 5-shot tasks. The results are averaged on 600 tasks from meta-testing set with 95% confidence interval. The AWGIM model achieved 63.12% accuracy in 5-way 1-shot tasks and 78.40% in 5-way 5-shot tasks on tieredImageNet, outperforming other approaches such as MAML, Prototypical Nets, and Relation Nets. The results were averaged on 600 tasks from the meta-testing set with a 95% confidence interval. MetaOptNet Resnets (2017) is utilized to optimize the network with weight decay 1 \u00d7 10 \u22126. The initial learning rate is set to 0.0002 for 5-way 1-shot and 0.001 for 5-way 5-shot, decayed by 0.2 every 15,000 iterations. The model is trained for 50,000 iterations with batch sizes of 64 for 5-way 1-shot and 32 for 5-way 5-shot. The model is first trained on a meta-training set to select optimal hyper-parameters based on validation results before further training. The model is trained on meta-training and meta-validation sets together using fixed hyper-parameters. The performance of AWGIM is compared with state-of-the-art methods on two datasets. The model is trained on meta-training and meta-validation sets using fixed hyper-parameters. AWGIM's performance is compared with state-of-the-art methods on two datasets, including tieredImageNet and miniImageNet with WRN-28-10 as the feature extractor. Results are shown in Table 1 and 2, with the backbone network structure included for reference. The results on miniImageNet and tieredImageNet are presented in Table 1 and 2, showcasing various meta learning categories and classification weights generation approaches, including AWGIM which outperforms all methods. The backbone network structure of the feature extractor is also included for reference. The top half of Table 1 and 2 outlines different meta learning categories like metric-based, gradient-based, and graph-based methods. The bottom part discusses classification weights generation approaches, including AWGIM, which outperforms all other methods. AWGIM shows competitive performance, ranking best on tieredImageNet and near state-of-the-art on miniImageNet. All methods use WRN-28-10 as the backbone network. AWGIM outperforms all other classification weights generation methods, ranking best on tieredImageNet and near state-of-the-art on miniImageNet. The comparison is fair as all methods use WRN-28-10 as the backbone network. In particular, AWGIM surpasses LEO in all settings. Analysis of AWGIM is provided in Table 3, with the top half comparing with LEO and the bottom part offering ablation analysis of different components. Random shuffling of generated classification weights demonstrates the effectiveness of AWGIM. AWGIM outperforms LEO in all settings, with detailed analysis provided in Table 3. The comparison is fair as both use WRN-28-10 as the backbone network. Ablation analysis of different components is also included, along with the effectiveness of randomly shuffled classification weights. In Table 3, a detailed analysis of AWGIM is presented, comparing it to LEO with a focus on the effect of the attentive path and different generators. The \"Generator conditioned on S only\" is highlighted as being similar to \"Generator in LEO\" without inner updates. The study compared AWGIM to LEO, focusing on the attentive path and different generators. The \"Generator conditioned on S with IM\" incorporates cross entropy and reconstruction loss for the support set. The \"Generator conditioned on S only\" is trained with cross entropy on the query set, similar to \"Generator in LEO\" without inner updates. Results show that self-attention is as effective as relation networks in LEO. The generator with information maximization outperforms LEO slightly. Attention modules were replaced to investigate their impact. The study compared AWGIM to LEO, focusing on different generators. The generator with information maximization outperforms LEO slightly, even without attention modules. The effect of attention was investigated by replacing the modules with 2-layer MLPs. The study investigated the impact of replacing attention modules with 2-layer MLPs, known as \"MLP encoding\". Even without attention, MLP encoding achieved accuracy close to LEO, highlighting the importance of information maximization. Ablation analysis was conducted on the effect of information maximization by varying \u03bb 1, \u03bb 2, and \u03bb 3. The study compared MLP encoding to LEO for information maximization. Ablation analysis varied \u03bb 1, \u03bb 2, and \u03bb 3, showing the importance of maximizing mutual information between weights and support. In the study, the importance of maximizing mutual information between weights and support was highlighted by varying \u03bb 1, \u03bb 2, and \u03bb 3. The classification weights in AWGIM are tailored for each query sample, with \u03bb 1 = 0 significantly affecting performance. The study highlighted the importance of maximizing mutual information between weights and support by varying \u03bb values. The classification weights in AWGIM are tailored for each query sample, with \u03bb 1 = 0 significantly affecting performance. The classification weights are shuffled between query samples to study their adaptability. The classification weights in AWGIM are tailored for each query sample, with \u03bb 1 = 0 significantly affecting performance. Shuffling the weights between query samples within and between classes shows that random shuffle between classes degrades accuracy, while shuffle in class has minimal impact. In AWGIM, classification weights are customized for each query sample, with \u03bb 1 = 0 impacting performance significantly. Shuffling weights between query samples within and between classes reveals that random shuffle between classes reduces accuracy, while shuffle in class has minimal effect. In AWGIM, classification weights are customized for each query sample, with \u03bb 1 = 0 impacting performance significantly. Larger support sets lead to more diverse and specific classification weights for each query sample in 5-way 5-shot setting. Random shuffle between classes reduces accuracy, while shuffle within classes has minimal effect. In this work, Attentive Weights Generation via Information Maximization (AWGIM) is introduced for few-shot image classification. AWGIM learns to generate optimal classification weights for each query sample by maximizing the mutual information between generated weights and query, support data. The larger support set leads to more diverse and specific classification weights for each query sample in the 5-way 5-shot setting. More analysis is provided in Appendix A.3. Attentive Weights Generation via Information Maximization (AWGIM) is introduced for few shot image classification. It learns to generate optimal classification weights for each query sample by maximizing mutual information between generated weights and query, support data. AWGIM utilizes mutual information techniques for few shot learning and demonstrates state-of-the-art performance on benchmark datasets. AWGIM is the first work to use mutual information techniques for few shot learning, achieving state-of-the-art performance on benchmark datasets. The multi-head attention involves matrices representing support samples and task information, utilizing self-attention networks for encoding global task information. The attentive path in AWGIM involves using attention and self-attention networks to encode global task information and compute cross-attention between query and context-aware support samples. The outputs of the model are represented by matrices, with a focus on contextual information for each sample. The cross attention in AWGIM involves computing attention between query and context-aware support samples. Classification weights follow a Gaussian distribution during meta-training. AWGIM can be adapted for few shot regression tasks by setting the number of classes N equal to 1. During meta-training, classification weights follow a Gaussian distribution. For few shot regression tasks, AWGIM is modified by setting N=1 and using mean square error instead of cross entropy loss. Weight and bias parameters are generated for a three layer MLP with hidden dimension 40, consistent with LEO's experimental setting for few shot regression tasks. In few shot regression tasks, the number of classes is set to 1 and cross entropy loss is adapted to mean square error. Data points (x, y) are used as inputs to AWGIM to generate weight and bias parameters for a three layer MLP with hidden dimension 40. The tasks are constructed as sinusoidal or linear regression tasks, with multi-head attention replaced by single-head attention in two paths. 5-way 1-shot and 5-way 5-shot experiments are conducted on miniImageNet dataset, showing that multi-head attention improves performance. In few shot regression tasks, the number of classes is set to 1 and cross entropy loss is adapted to mean square error. Data points (x, y) are used as inputs to AWGIM to generate weight and bias parameters for a three layer MLP with hidden dimension 40. The tasks are constructed as either sinusoidal or linear regression tasks. The results of 5-way 1-shot and 5-way 5-shot experiments on miniImageNet dataset show that multi-head attention improves performance. Comparing AWGIM with LEO in terms of convergence speed, single head attention struggles when data are extremely scarce. In comparison to LEO, AWGIM shows faster convergence and better performance in 5-way 1-shot miniImageNet experiments. Single head attention struggles with scarce data, while AWGIM induces minimal computational overhead. AWGIM converges faster and outperforms LEO in 5-way 1-shot miniImageNet experiments. Inference time is minimal with AWGIM, using \"MLP encoding\" with time complexity O(N K + |Q|). Experiments on miniImageNet with batch size 64 show minimal computational overhead. The usage of self-attention and cross attention in AWGIM incurs negligible overhead compared to MLP encoding due to small values of N, K, and |Q|. Classification weights are visualized using t-SNE after processing 400 tasks. The visualization of classification weights using t-SNE after processing 400 tasks from the meta-validation set of 5-way 1-shot miniImageNet experiment is shown in Figure 4. The inputs to the generator g are also plotted for comparison. In a study by Hinton (2008), 400 tasks were sampled from the meta-validation set of a 5-way 1-shot miniImageNet experiment. Each task included 5 query samples from 5 different classes, resulting in a total of 10,000 weight vectors to visualize. The comparison between the inputs to the generator g and the generated classification weights showed that the decoded weights for each class were clustered closer together in the generated weights. The visualization results are displayed in Figure 4, with red and blue dots representing classification weights for two query samples from two classes within one task. The comparison between the inputs to the generator g and the generated classification weights showed that the decoded weights for each class were clustered closer together in the generated weights. This is consistent with the results suggesting that query samples from different classes have distinct classification weights. The generator g can produce adapted weights for different query samples, as seen in Table 3. t-SNE visualization in Figure 4 shows distinct classification weights for query samples from different classes. Blue and red dots represent classification weights for two query samples in the same task."
}