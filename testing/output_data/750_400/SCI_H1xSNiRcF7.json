{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models, allowing learning from uncertain supervision and inferring soft-inclusions among concepts. Building on the Box Lattice model of Vilnis et al. (2018), which uses high-dimensional hyperrectangles to model soft-inclusions, this approach maintains the geometric inductive bias of hierarchical embedding models. In this work, a novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian. In this work, a novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions. The approach improves optimization robustness in the disjoint case and maintains desirable properties of the original lattice. Performance on WordNet hypernymy prediction and Flickr is increased or matched. Our approach involves parameterized density functions using Gaussian convolutions over boxes, serving as an alternative surrogate to the original lattice measure. It enhances optimization robustness in the disjoint case and maintains desirable properties of the original lattice. Improved performance is demonstrated on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset, especially in cases of sparse data. Embedding methods, such as Word2Vec, have been crucial in machine learning by converting semantic problems into geometric ones. They have shown significant improvements in cases of sparse data, where conditional probabilities are low. Recent years have seen a renaissance in embeddings, with a focus on structured or geometric representations like density functions and convex cones instead of simple points for objects like images, words, and knowledge base concepts. Recent years have seen an interest in structured or geometric representations, associating objects with complex geometric structures like density functions, convex cones, or axis-aligned hyperrectangles. These geometric objects better express asymmetry, entailment, ordering, and transitive relations than simple points in a vector space, providing a strong inductive bias for tasks. The focus is on the probabilistic Box Lattice model for its strength. In this work, the focus is on the probabilistic Box Lattice model BID22, known for its strong empirical performance in modeling transitive relations and complex joint probability distributions, including negative correlations. Box embeddings (BE) are a generalization of order embeddings (OE) BID20. The Box Lattice model BID22 is favored for its empirical performance in modeling transitive relations and complex joint probability distributions, including negative correlations. Box embeddings (BE) generalize order embeddings (OE) BID20 by using overlapping boxes instead of vector lattice ordering. However, the \"hard edges\" of boxes can cause issues for gradient-based methods. The Box Lattice model BID22 is known for its empirical performance in modeling transitive relations and complex joint probability distributions, including negative correlations. Box embeddings (BE) generalize order embeddings (OE) BID20 by using overlapping boxes instead of vector lattice ordering. However, the \"hard edges\" of boxes can pose challenges for gradient-based optimization, especially with (pseudo-)sparse data where correcting overlap discrepancies is crucial. The Box Lattice model BID22 is effective in modeling transitive relations and complex joint probability distributions. However, the \"hard edges\" of boxes can hinder gradient-based optimization, particularly with (pseudo-)sparse data. Disjoint boxes in the model with overlap in the ground truth present challenges for optimization, especially in scenarios like market basket models and entailment tasks. To address this issue, BID22 introduces an ad-hoc surrogate function, while we see it as an opportunity for a new approach. The Box Lattice model BID22 is effective in modeling transitive relations and complex joint probability distributions. However, the \"hard edges\" of boxes can hinder gradient-based optimization, particularly with sparse data. To address challenges in scenarios like market basket models and entailment tasks, a new approach is proposed based on smoothing density functions with Gaussian convolution. This approach demonstrates superiority in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. The new approach proposes smoothing density functions with Gaussian convolution to model transitive relations more effectively than the Box Lattice model. It outperforms existing methods in WordNet, Flickr caption entailment, and MovieLens-based market basket dataset tasks. The new approach improves on existing methods in modeling transitive relations by smoothing density functions with Gaussian convolution. It outperforms current state-of-the-art results in tasks such as WordNet, Flickr caption entailment, and MovieLens-based market basket dataset. Relevant related work includes order embeddings of BID20, probabilistic extensions by BID9, and box lattice models by BID22. Another hyperrectangle-based generalization of order embeddings was proposed by BID18, known as box embeddings. In a vector space, the probabilistic extension of the model by BID9 and the box lattice model of BID22 are discussed. Another hyperrectangle-based generalization of order embeddings, known as box embeddings, was proposed by BID18. The difference lies in the interpretation: the former is probabilistic while the latter is deterministic. Methods based on embedding points in hyperbolic space have also been explored. The difference between the two models lies in their interpretation: one is probabilistic, assigning conditional probabilities based on overlap degrees, while the other is deterministic, considering an edge present only if one box entirely encloses another. Hyperbolic space-based methods for learning hierarchical embeddings have also been proposed recently, optimizing an energy function and biased towards learning tree structures. Recently, embedding points in hyperbolic space for learning hierarchical embeddings has been proposed. These models optimize an energy function and are biased towards learning tree structures. However, the constant curvature of hyperbolic space makes them less suitable for learning non-treelike DAGs. A common approach to smoothing the energy landscape of the model is using Gaussian convolution, which is increasingly being incorporated into machine learning models. Our approach involves smoothing the energy landscape of the model using Gaussian convolution, a technique common in mollified optimization and continuation methods. This method is being integrated into machine learning models such as Mollifying Networks, diffusion-trained networks, and noisy activation functions. Our focus is on embedding orderings and transitive relations, a subset of knowledge graph embedding, with a probabilistic approach to learning an embedding model. Our focus is on embedding orderings and transitive relations in machine learning models like Mollifying Networks and diffusion-trained networks. The probabilistic approach aims to map concepts to subsets of event space, suited for transitive relations and fuzzy concepts of inclusion and entailment. The text discusses learning an embedding model that maps concepts to subsets of event space, with an inductive bias for transitive relations and fuzzy concepts of inclusion and entailment. It introduces methods for representing ontologies as geometric objects using order theory and vector and box lattices. The focus is on embedding orderings and transitive relations in machine learning models. The text introduces vector and box lattices as a formalism for describing ontologies, specifically non-strict partially ordered sets (posets). Posets generalize the concept of totally ordered sets by allowing some elements to be incomparable. Lattices are posets where any subset of elements has a unique least upper bound and greatest lower bound. A lattice is a poset where any subset of elements has a single unique least upper bound and greatest lower bound. In a bounded lattice, there are two additional elements, (top) and \u22a5 (bottom), which denote the least upper bound and greatest lower bound of the entire set. The lattice is equipped with two binary operations, \u2228 (join) and \u2227 (meet), where a\u2228b denotes the least upper bound of a, b \u2208 P, and a \u2227 b denotes their greatest lower bound. In a bounded lattice, the set P contains two additional elements, (top) and \u22a5 (bottom), denoting the least upper bound and greatest lower bound. The lattice has binary operations \u2228 (join) and \u2227 (meet) where a\u2228b is the least upper bound and a \u2227 b is the greatest lower bound. Bounded lattices must satisfy specific properties. The extended real numbers and sets partially ordered by inclusion form bounded lattices under certain operations. A bounded lattice must satisfy specific properties, including having a greatest lower bound. The extended real numbers and sets partially ordered by inclusion form bounded lattices under certain operations. The dual lattice can be obtained by swapping the meet and join operations. A semilattice has either a meet or join operation, but not both. The fourth property of absorption in lattices is illustrated by special cases where \u2227 and \u2228 operations can be swapped to create a valid lattice known as the dual lattice. A semilattice only has either a meet or join operation. A vector lattice, also called a Riesz space or Hilbert lattice, is a vector space with a lattice structure. A vector lattice, also known as a Riesz space or Hilbert lattice, is a vector space with a lattice structure. It uses the product order from the underlying real numbers for the partial order, with meet and join operations being pointwise min and max. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order. The vector lattice R^n uses the product order from real numbers, with meet and join operations as pointwise min and max. Order Embeddings of BID20 represent partial orders as vectors using the reverse product order, with objects becoming more specific as they move away from the origin. FIG0 shows a two-dimensional example of this representation. The Order Embedding vector lattice represents partial orders as vectors using the reverse product order, with objects becoming more specific as they move away from the origin. Vilnis et al. introduced a box lattice where each concept in a knowledge graph is associated with two vectors, representing the minimum and maximum coordinates of an axis-aligned hyperrectangle. The box lattice introduced by Vilnis et al. represents concepts in a knowledge graph with two vectors, minimum and maximum coordinates of an axis-aligned hyperrectangle. The lattice structure is based on set inclusion between boxes, creating a natural partial order and lattice structure. The box lattice structure represents concepts in a knowledge graph using minimum and maximum coordinates of an axis-aligned hyperrectangle. It involves set inclusion between boxes, creating a natural partial order and lattice structure. The lattice structure includes least upper bounds, greatest lower bounds, and operations for finding the largest box contained within two boxes and the smallest box containing both. Marginal probabilities of events are associated with the volume of boxes. The box lattice structure involves set inclusion between boxes, creating a natural partial order and lattice structure. The lattice meet is the largest box contained entirely within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are associated with the volume of boxes under a suitable probability measure, with the probability p(x) given by n i (x M,i \u2212 x m,i ) for event x. The probability measure for events in the box lattice structure is determined by the volume of boxes under a suitable measure. The uniform measure assigns probabilities based on the interval boundaries of associated boxes. The use of gradient-based optimization for learning box embeddings faces challenges. The probability measure for events in the box lattice structure is determined by the volume of boxes under a suitable measure. When using gradient-based optimization to learn box embeddings, an issue arises when two concepts are incorrectly labeled as disjoint, leading to zero gradient signal flow. The problem identified in the original work is that when two concepts are incorrectly labeled as disjoint, no gradient signal can flow due to zero intersection volume. This issue is especially problematic for sparse lattices where most boxes have little to no intersection. The authors propose a surrogate function to optimize when dealing with disjoint intervals in sparse lattices, improving optimization and final model quality. The authors propose a surrogate function to optimize in cases of disjoint intervals in sparse lattices, aiming to improve optimization and final model quality. They suggest using a more principled framework to develop alternate measures that avoid this issue, illustrated in Figure 2 with a one-dimensional example showing the impact of a smoothing kernel on overlapping intervals. The approach seeks to address the gradient sparsity caused by the \"hard edges\" of standard box embeddings by relaxing this assumption while maintaining desirable properties. The authors propose a surrogate function to optimize in cases of disjoint intervals in sparse lattices, aiming to improve optimization and final model quality. They seek a relaxation of the assumption of \"hard edges\" in standard box embeddings to address gradient sparsity issues. The area under the purple product curve indicates the degree of overlap between intervals before and after applying a smoothing kernel. The results extend from 1-dimensional intervals to boxes represented as products of intervals. The authors propose a surrogate function to optimize in cases of disjoint intervals in sparse lattices, aiming to improve optimization and final model quality. They seek a relaxation of the assumption of \"hard edges\" in standard box embeddings to address gradient sparsity issues. The measure of joint probability between intervals x = [a, b] and y = [c, d] can be rewritten as an integral of the product of indicator functions, with support only where the intervals overlap. The joint probability between intervals x = [a, b] and y = [c, d] can be expressed as an integral of indicator functions, with support in overlapping areas. Kernel smoothing, specifically convolution with a Gaussian kernel, is used to replace indicator functions with functions of infinite support for optimization and energy improvement. Kernel smoothing with a Gaussian kernel is used to replace indicator functions with functions of infinite support for optimization and energy improvement. This approach is demonstrated in one dimension, where the joint probability between intervals x and y with associated smoothed indicators admits a closed form solution. The approach involves mollified optimization and energy smoothing using Gaussian kernel smoothing in one dimension. The solution to the equation involves the antiderivative of the standard normal CDF and the softplus function. The integral has a closed form solution involving the antiderivative of the standard normal CDF and the softplus function. The formula is derived in the zero-temperature limit, leading to the original equation. In the zero-temperature limit, the formula is derived involving the antiderivative of the standard normal CDF and the softplus function. The last line of the equation is the original equation expected from convolution with a zero-bandwidth kernel. The formula derived in the zero-temperature limit involves the antiderivative of the standard normal CDF and the softplus function. The equation expected from convolution with a zero-bandwidth kernel is true for both the exact formula and the softplus approximation. However, multiplication of Gaussian-smoothed indicators does not give a valid meet operation on a function lattice, violating the idempotency requirement. Treating the outputs of p \u03c6 as probabilities complicates applications that train on conditional probabilities. By modifying equation 3, a function p can be obtained such that p(x \u2227 x) = p(x), maintaining smooth optimization properties of the Gaussian model. This adjustment addresses the issue of idempotency violation in function lattices when multiplying Gaussian-smoothed indicators. By modifying equation 3, a function p can be obtained such that p(x \u2227 x) = p(x), while preserving the smooth optimization properties of the Gaussian model. This identity holds true for the hinge function m h and two intervals (a, b) and (c, d), as shown in DISPLAYFORM9. The equation with a similar functional form as equation 6 is valid for both the hinge function and the softplus function. The commutativity of min and max with monotonicity is applicable for two intervals x = (a, b) and y = (c, d). In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for intervals x = y = (a, b) = (c, d), while equation 3 is not. This leads to the definition of probabilities p(x) and p(x, y). In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for intervals x = y = (a, b) = (c, d), while equation 3 is not. This leads to defining probabilities p(x) and p(x, y) using a normalized version of equation 7. Softplus upper-bounds the hinge function, allowing values greater than 1, requiring normalization. In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for intervals x = y = (a, b) = (c, d), while equation 3 is not. This leads to defining probabilities p(x) and p(x, y) using a normalized version of equation 7. Softplus upper-bounds the hinge function, allowing values greater than 1, requiring normalization. In experiments, two approaches to normalization are used, allowing boxes to learn unconstrained and dividing dimensions by the measured size of the global minimum and maximum. In experiments, two approaches to normalization are used for data with a small number of entities. Boxes are allowed to learn unconstrained and dimensions are divided by the measured size of the global minimum and maximum. The final probability is calculated as a product over dimensions. The final probability p(x) is calculated by projecting onto the unit hypercube, normalizing by m soft, and taking the product over dimensions. This approach retains the inductive bias of the original box model and satisfies the necessary condition that p(x, x) = p(x). Our approach retains the inductive bias of the original box model, is equivalent in the limit, and satisfies the necessary condition that p(x, x) = p(x). A comparison of different functions in FIG2 shows that the softplus overlap performs better for highly disjoint boxes than the Gaussian model, while also preserving the meet property. The Gaussian model needs to lower its temperature significantly to achieve high overlap, leading to vanishing gradients in the tails. In experiments on the WordNet hypernym prediction task, improvements in the softplus overlap function show better behavior for disjoint boxes compared to the Gaussian model. The WordNet hypernym hierarchy contains 837,888 edges after transitive closure. The WordNet hypernym hierarchy contains 837,888 edges after transitive closure. Experimental results show that the smoothed box model performs nearly as well as the original box lattice in terms of test accuracy. The model requires less hyper-parameter tuning than the original. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, requiring less hyper-parameter tuning. Further experiments are needed to confirm its performance in a sparse regime. In further experiments, the smoothed box model is compared to the original box lattice and order embeddings on a task with a higher degree of sparsity. The training data consists of the transitive reduction of the WordNet mammal subset, with varying numbers of positive and negative examples. In further experiments, different numbers of positive and negative examples from the WordNet mammal subset are used to compare the box lattice, smoothed approach, and order embeddings (OE) as a baseline. The training data is the transitive reduction of the mammal WordNet subset, while the dev/test data is the transitive closure of the training data. The models nearly match the full transitive closure with balanced data, but as the number of negative examples increases, differences emerge. The training data consists of 1,176 positive examples, while the dev and test sets have 209 positive examples. Negative examples are generated randomly. Models like OE baseline, Box, and Smoothed Box perform well with balanced data, but Smoothed Box outperforms the others on imbalanced data. This is crucial for real-world entailment graph learning where negatives outnumber positives. The performance of the Smoothed Box model remains superior to OE and Box models on imbalanced data, as shown in experiments on the WordNet mammal subset and Flickr entailment dataset. This is crucial for real-world entailment graph learning where negatives greatly outnumber positives. We conduct experiments on the Flickr entailment dataset, a large-scale caption entailment dataset with 45 million image caption pairs. Experimental details are provided in Appendix D.3, and we report KL divergence and Pearson correlation on the full test data and unseen pairs. We use the same dataset as BID22, constraining boxes to the unit cube and applying the softplus function before calculating box volume. Experimental details are in Appendix D.3. We report KL divergence and Pearson correlation on full test data, unseen pairs, and unseen captions. Results in TAB2 show slight performance gain, especially on unseen captions. Our method is applied to a market-basket task using the MovieLens dataset. In a market-basket task using the MovieLens dataset, the method aims to predict users' movie preferences based on strong ratings. The model shows a slight performance improvement, particularly on unseen captions. The dataset is pruned to include only movies with over 100 user ratings for statistical significance. The task involves predicting users' preference for movie A based on their liking of movie B. A subset of movies with over 100 user ratings is selected from the MovieLens-20M dataset, resulting in 8545 movies. The conditional probability P(A|B) is calculated and compared with various baseline methods. Separate embeddings are used for target and training matrices due to their asymmetry. We compare the conditional probability P(A|B) of movie preferences using different models like low-rank matrix factorization and complex bilinear factorization. Separate embeddings are used for target and conditioned movies due to the asymmetric training matrix. Evaluation metrics include KL divergence, Pearson correlation, and Spearman correlation. Experimental details are provided in Appendix D.4. The study compared different models for movie preferences, using separate embeddings for target and conditioned movies. A complex bilinear model with an additional vector for the \"imply\" relation was evaluated using KL divergence, Pearson correlation, and Spearman correlation. Results showed that the smoothed box embedding method outperformed other baselines, especially in Spearman correlation, a key metric for recommendation tasks. Additional analysis on the model's robustness to initialization conditions was also conducted. The smoothed box embedding method outperforms the original box lattice and other baselines in Spearman correlation, a crucial metric for recommendation tasks. The model's energy and optimization landscape are smoothed, making it easier to train with fewer hyper-parameters, achieving state-of-the-art results on various datasets. The approach to smoothing energy and optimization landscape of probabilistic box embeddings reduces hyper-parameters, making it easier to train and achieving state-of-the-art results on multiple datasets. This model is effective with sparse data and robust to poor initialization, addressing challenges in learning from geometrically-inspired embedding models. The model is effective with sparse data and robust to poor initialization, addressing challenges in learning from geometrically-inspired embedding models. Research in this area is ongoing, with a focus on complex embedding structures like unions of boxes or non-convex objects. The study will explore function lattices and constraint-based learning approaches. The study focuses on exploring function lattices and constraint-based learning approaches to address challenges in learning from complex embedding structures like unions of boxes or non-convex objects. A proof of Gaussian overlap formula is presented for evaluating lattice elements with smoothed indicators. The study presents a proof of the Gaussian overlap formula for evaluating lattice elements with smoothed indicators, using the MovieLens dataset as an example. The integral of the product of two functions is evaluated using equation 8 and the identity BID8 BID21. Fubini's theorem is applied, leading to equation 9 and \u03c3 = \u03c4 \u22121. The MovieLens dataset is suitable for optimization by the smoothed model due to its distribution of probabilities. Experiments test the robustness of the smoothed box model to initialization. The MovieLens dataset is suitable for optimization by the smoothed model due to its distribution of probabilities. Additional experiments test the model's robustness to initialization, specifically focusing on the potential for disjoint boxes. The MovieLens dataset is used for optimization by the smoothed model, which is tested for robustness to initialization. The model's initialization is adjusted to determine its robustness to disjoint boxes. The smoothed box model is tested for robustness to initialization by adjusting the width parameter to control the percentage of disjoint boxes. Results show that the smoothed model performs well in the disjoint regime, unlike the original box model which degrades significantly. This suggests that the strength of the smoothed model lies in its ability to optimize smoothly in the disjoint condition. The smoothed box model performs well in the disjoint regime, unlike the original box model which degrades significantly. The strength of the smoothed model lies in its ability to optimize smoothly in the disjoint condition. Detailed methodology and hyperparameter selection methods for each experiment can be found at https://github.com/Lorraine333/smoothed_box_embedding. For the WordNet experiments, the model is evaluated every epoch on the development set for a large fixed number of epochs, and the best development model is used to score the test set. Baseline models are trained using the parameters of BID22, with the smoothed model using hyperparameters determined on the development set. The same routine as the WordNet experiments section is followed to select the best parameters for the 12 experiments conducted in this section. For the experiments conducted in this section, negative examples are randomly generated based on the ratio for each batch of positive examples. A parameter sweep is done for all models to choose the best result for each model. The experimental setup uses the same architecture as BID22 and BID9, a single-layer LSTM that reads captions and produces a box embedding parameterized by min and max. The experimental setup uses a single-layer LSTM to read captions and produce box embeddings. Models are trained for a fixed number of epochs and tested on development data. Hyperparameters are determined on the development set. The model produces box embeddings using feedforward networks on the output of the LSTM. It is trained for a fixed number of epochs and tested on development data. Hyperparameters are determined on the development set, and the best model is used to score the test set. Evaluation is done every 50 steps on the development set, with optimization stopping if no improvement is seen after 200 steps."
}