{
    "title": "S1e-0kBYPB",
    "content": "In this work, the authors highlight two issues with current explanatory methods for AI systems. They point out that different perspectives on explanations lead to different instance-wise explanations, and that post-hoc explainers have only been validated on simple models like linear regression, not on real-world neural networks. The authors highlight issues with current explanatory methods for AI systems, including different perspectives on explanations and lack of validation on real-world neural networks. They introduce a verification framework for explanatory methods based on a non-trivial neural network architecture. The authors introduce a verification framework for explanatory methods in neural networks, focusing on feature selection perspective. They aim to provide a publicly available evaluation tool for explainers, showcasing their efficacy by highlighting current failure modes. The authors introduce a verification framework for explanatory methods in neural networks, focusing on feature selection perspective. They aim to provide a publicly available evaluation tool for explainers, showcasing their efficacy by highlighting current failure modes. The framework aims to offer guarantees on the inner workings of a trained neural network architecture. Various post-hoc explanatory methods have been developed to shed light on black-box machine learning models. A variety of post-hoc explanatory methods have been developed to explain accurate, black-box machine learning models. Two widely used perspectives on explanations are feature-additivity and feature-selection, which are detailed below. These methods shed light on the model's behavior. Two widely used perspectives on explanations in post-hoc explanatory methods for black-box machine learning models are feature-additivity and feature-selection. These perspectives provide different explanations for predicting a single input in isolation. Explanatory methods adhering to these perspectives are being compared in practice. In comparing explanatory methods for black-box machine learning models, different perspectives lead to fundamentally different explanations for predicting a single input. Comparisons between feature-selection and feature-additivity explainers may not be coherent due to their different explanation targets. In comparing L2X with LIME and SHAP, et al. (2018) and Yoon et al. (2019) highlight the differences in explanation targets. Current explanatory methods successfully identify biases but their reliability on the target model remains uncertain. Current explanatory methods are successful in pointing out biases but their reliability on the target model remains uncertain due to the unknown decision-making process of neural networks. Evaluating explainers on complex neural networks trained on real-world datasets assumes that the target models behave reasonably. The target model, which is less biased, is difficult to evaluate due to the unknown decision-making process of neural networks. Evaluating explainers on complex neural networks assumes that the target models behave reasonably. The authors address the issue of unreliable assumptions in evaluating explainers on neural networks, highlighting the presence of spurious correlations in human-annotated datasets. Recent works have shown spurious correlations in human-annotated datasets, making it unreliable to penalize explainers for pointing to insignificant tokens. A framework is proposed to generate evaluation tests for explanatory methods, focusing on feature selection. The model can identify tokens with zero contribution to the model's prediction. The framework proposed aims to generate evaluation tests for explanatory methods by focusing on feature selection. It identifies tokens with zero contribution to the model's prediction and tests if explainers rank them higher than relevant tokens. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. The framework aims to evaluate explanatory methods by focusing on feature selection. It identifies tokens with zero contribution to the model's prediction and tests if explainers rank them higher than relevant tokens. The test was conducted on three pairs of (target model, dataset) for multi-aspect sentiment analysis. The framework evaluates explanatory methods by focusing on feature selection, testing if explainers rank irrelevant tokens higher than relevant ones. The test was conducted on three pairs of models trained independently for multi-aspect sentiment analysis. The test is not sufficient to conclude the power of explainers in full generality, as the ground-truth behavior of the models is unknown. The framework generates necessary evaluation tests and penalizes explainers only when they produce errors under certain guarantees. The framework evaluates explanatory methods by focusing on feature selection and testing explainers for critical failures. It introduces an automatic evaluation test that does not rely on speculations about the target model's behavior. The framework generates necessary evaluation tests and penalizes explainers only when errors are guaranteed. L2X, a feature-selection explainer, is evaluated under this test. The framework introduces an automatic evaluation test for explanatory methods, focusing on feature selection and critical failures. LIME and SHAP, popular explainers, are tested under this method, with LIME and SHAP performing better most of the time. In comparing explanatory methods, LIME and SHAP outperform L2X most of the time. Error rates are provided to highlight potential failures in feature selection explanations. In Section 5, it is explained why LIME and SHAP generally outperform L2X. Error rates are shared to reveal potential failures in feature selection explanations. The findings suggest that explainers may inaccurately identify the most relevant token as having zero contribution. A generic evaluation methodology is introduced for testing explanatory methods in feature selection perspectives. The text discusses the evaluation of explanatory methods in feature selection perspectives, highlighting the potential inaccuracies in identifying the most relevant token with zero contribution. The methodology introduced is generic and can be applied to various tasks or research areas. Feature-based explainers are categorized into two types: feature-additive, which provides signed weights for each input feature. The most common instance-wise explanatory methods are feature-based, explaining predictions in terms of input unit-features. There are two major types of feature-based explainers: feature-additive, providing signed weights for each input feature, and feature-selective, identifying a subset of features responsible for the prediction. The curr_chunk discusses different types of explanatory methods for model predictions, including providing signed weights for input features and identifying a subset of features responsible for the prediction. Other methods include example-based explanations and human-level explanations. In this work, the focus is on verifying feature-based explainers, which are the majority of current works in explaining model predictions. Many explainers have been proposed, but it remains an open question on how to thoroughly evaluate them. In this work, the focus is on verifying feature-based explainers, which are the majority of current works in explaining model predictions. The evaluation of explainers involves testing on interpretable target models like linear regression and decision trees. The evaluation of explainers involves testing on interpretable target models such as linear regression and decision trees, but these simple models may not fully represent the complex neural networks used in practice. Synthetic setups are also used for evaluation, controlling the set of important features in tasks like L2X. The evaluation of explainers involves testing on interpretable target models like linear regression and decision trees, which may not fully represent complex neural networks. Synthetic setups, such as L2X, control important features but may lead to target models learning simpler functions than needed for real-world applications. In synthetic setups like L2X, target models may learn simpler functions than needed for real-world applications, making it easier for explainers. In another approach, intuitive heuristics are assumed for high-performing models, like relying on specific linguistic patterns in sentiment analysis. Crowd-sourcing evaluation is used to assess model behavior. In real-world applications, explainers may find it easier to assess model behavior by assuming intuitive heuristics for high-performing models, such as relying on specific linguistic patterns in sentiment analysis. Crowd-sourcing evaluation is used to confirm if the explainer's features align with the model's predictions, but neural networks may still produce unexpected results despite high accuracy. The evaluation of explainers' faithfulness to the model's predictions may not be reliable due to unexpected artifacts in neural networks. Another evaluation involves humans predicting model behavior based on explanations provided by different explainers. The evaluation of explainers' faithfulness to the model's predictions may not be reliable due to unexpected artifacts in neural networks. Evaluating if explanations help humans predict the model's behavior involves presenting humans with predictions and explanations from different explainers to infer the model's outputs. This method is a good proxy for real-world usage evaluation but can be costly. Our evaluation framework is fully automatic and applies to complex real-world neural network models. It guarantees the inner-workings of the non-trivial neural network model, unlike the costly and human effort-intensive method of comparing explainers E1 and E2 for predicting model outputs. Our evaluation framework is fully automatic and guarantees the inner-workings of complex real-world neural network models. Unlike costly and human effort-intensive methods, our test filters for the fidelity of the explainer to the target model, making it more challenging and robust. The evaluation framework introduced is more challenging than previous methods, requiring explainers to provide different explanations for real data compared to randomized data. Explanations are based on feature-additivity, where contributions from each feature approximate the prediction. Explanatory methods adhere to the perspective of feature-additivity, where explanations consist of contributions from each feature to approximate the prediction. LIME learns weights through linear regression on the neighborhood of the instance, while Lundberg & Lee unified this method by demonstrating its effectiveness. Many explanatory methods, such as LIME and Shapley values from game theory, approximate predictions by considering feature-additive contributions. Lundberg & Lee showed that Shapley values satisfy desired constraints for accurate explanations. The Shapley values from game theory are the only feature-additive contributions that meet desired constraints for accurate explanations. Each feature's contribution in an instance is an average over a neighborhood of the instance, typically including all perturbations. The contribution of each feature in an instance is an average over a neighborhood of the instance, with the choice of neighborhood being critical. The explanation of a model for an instance consists of a sufficient set of features. The explanation of a model for an instance involves selecting a subset of features that lead to a similar prediction as the original model. Different perspectives exist on how to choose these features, with the choice of neighborhood being a critical factor. The explanation of f(x) involves selecting a small subset S(x) of features that lead to a similar prediction as the original model. Different perspectives exist on how to choose these features, with the choice of neighborhood being a critical factor. Chen et al. (2018), Carter et al. (2018), and Ribeiro et al. (2018) adhere to this perspective. L2X (Chen et al., 2018) learns S(x) by maximizing mutual information between S(x) and the prediction, but it assumes the number of important features per instance is known, which is usually not the case. The L2X method maximizes mutual information between S(x) and the prediction, assuming the number of important features per instance is known. However, in practice, this is often not the case. The model may not always rely on a small subset of features, as opposed to using all features, but this can be true for tasks like sentiment analysis. Instance-wise explanations for a sentiment analysis regression model are provided in Figure 1 to illustrate the differences between perspectives. In Figure 1, instance-wise explanations are provided for a hypothetical sentiment analysis regression model, highlighting differences between perspectives. The model behavior closely resembles real-world neural networks, which may heavily rely on specific tokens in the input due to dataset biases. In a hypothetical sentiment analysis regression model, the behavior closely resembles real-world neural networks, which may heavily rely on specific tokens in the input due to dataset biases. For example, natural language inference neural networks trained on SNLI may rely on specific tokens like \"outdoors\" for entailment, \"tall\" for neutral, and \"sleeping\" for contradiction. The differences between perspectives are evident in the provided instance-wise explanations. The behavior of sentiment analysis regression models may heavily rely on specific tokens in the input, such as \"outdoors\" for entailment, \"tall\" for neutral, and \"sleeping\" for contradiction. Instance-wise explanations show the differences between perspectives, with certain features like \"nice\" and \"good\" contributing to the model's score. The feature-additive explanation highlights the relevance of \"nice\" and \"good\" in the model's scoring. It shows that \"nice\" had a weight of 0.4 and \"good\" contributed 0.3. The model relied on \"nice\" for a score of 0.7, but would have used \"good\" for 0.6 if \"nice\" was absent. This perspective aims to explain the model's behavior on a neighborhood of instances, while the feature-selective perspective focuses on individual features like \"nice\" for a specific instance. The feature-selective perspective ranks \"good\" and \"nice\" as important features on instance x 1, while on instance x 2, \"very\" and \"good\" are prioritized. This difference highlights the varying importance of features based on the perspective used. The difference between the two perspectives is evident in instance x 2, where the ranking of features varies. While the feature-selective explanation prioritizes \"good\" and \"nice\", the model relies on \"very\" and \"good\" in isolation. This suggests that one perspective may be more suitable for different real-world scenarios. The paper introduces a verification framework for the feature-selection perspective of instance-wise explanations. In the paper, a verification framework is proposed for the feature-selection perspective of instance-wise explanations using the RCNN model architecture. The dataset is pruned to identify irrelevant and relevant features for each datapoint. The framework leverages the RCNN model architecture introduced by Lei et al. (2016) and prunes the dataset to identify irrelevant and relevant features for each datapoint. It introduces metrics to measure explainers' failure to rank irrelevant tokens lower than relevant ones. The RCNN consists of a generator and an encoder instantiated with recurrent convolutional neural networks. The RCNN model architecture (Lei et al., 2016) includes a generator and an encoder using recurrent convolutional neural networks. The generator selects a subset of tokens from input text x, which is then passed to the encoder for making predictions. The RCNN model architecture (Lei et al., 2016) consists of a bidirectional network with a generator and an encoder. The generator selects a subset of tokens from the input text x based on a Bernoulli distribution, which is then passed to the encoder for making predictions. The training process involves joint training of the generator and encoder with supervision only on the final prediction. Regularizers are used to encourage the generator to select a short sub-phrase and to discourage disconnected tokens. The RCNN model architecture involves a generator selecting tokens from input text based on a Bernoulli distribution, passed to an encoder for predictions. Training includes joint training of generator and encoder with supervision on final prediction. Regularizers encourage selecting short sub-phrases and fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. The RCNN model uses a generator to select tokens from input text, with a focus on encouraging the selection of fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. The model may have learned an internal communication protocol that encodes information from non-selected tokens via selected tokens, known as a handshake. The RCNN model may have learned an internal communication protocol called a handshake, where non-selected tokens encode information via selected tokens. The goal is to eliminate handshakes by ensuring that non-selected tokens have zero contribution to the model's prediction. The RCNN model learned a handshake protocol, where non-selected tokens encode information via selected tokens. The goal is to eliminate handshakes by ensuring non-selected tokens have zero contribution to the model's prediction. In an example, the model selects \"very\" in the sentence \"The movie was very good\" and predicts a score of 1, but selecting only \"very\" returns a score of 0.5. The RCNN model aims to eliminate handshakes by ensuring non-selected tokens have zero contribution to the prediction. In an example, selecting \"very\" in the sentence \"The movie was very good\" resulted in a score of 1, while selecting only \"very\" returned a score of 0.5. Equation 7 captures the handshake in this scenario. Equation 7 captures the handshake in the scenario where selecting \"very\" in a sentence resulted in a score of 1, while selecting only \"very\" returned a score of 0.5. Non-selected tokens are considered irrelevant or zero-contribution. To avoid confusion, instances where S Sx = S x are retained in the dataset to eliminate handshakes. The dataset is pruned to eliminate handshakes by retaining instances where S Sx = S x, ensuring non-selected tokens have no contribution to the prediction. However, it is uncertain if all non-selected tokens are relevant, as some may be noise selected for inclusion. After pruning the dataset to eliminate handshakes and ensuring non-selected tokens have no contribution to the prediction, further pruning is done to remove noise tokens that may not be relevant to the prediction. This ensures that at least one selected token is meaningful for the explanation. After eliminating irrelevant tokens and ensuring non-selected tokens do not contribute to the prediction, further pruning is done to remove noise tokens. This guarantees that at least one selected token is clearly relevant for the explanation. After eliminating irrelevant tokens and ensuring non-selected tokens do not contribute to the prediction, further pruning is done to remove noise tokens. To ensure a selected token is clearly relevant, the absolute change in prediction when removing the token must be higher than a significant threshold. Selected tokens are then partitioned into clearly relevant tokens and tokens for which relevance is unknown. If the absolute change in prediction when removing a token from S x is greater than a threshold \u03c4, then the token is considered relevant. Selected tokens are divided into clearly relevant tokens (SR x) and tokens with unknown relevance (SDK x). Simply because a token alone did not significantly impact the prediction does not mean it is irrelevant, as it may be important in combination with other tokens. The procedure ensures that tokens changing the prediction by a high threshold are important and should be ranked higher. The dataset is pruned to retain instances with at least one clearly relevant token. The dataset is pruned to retain instances with at least one clearly relevant token, ensuring important tokens are ranked higher. The procedure does not provide an explainer but guarantees that all tokens in N x are ranked lower than any token in SR x. The procedure guarantees that all tokens in N x are ranked lower than any token in SR x, without providing an explainer or contribution weights. The first most important token has to be in S x, and error metrics are defined based on the ranking of features by an explainer. The error metrics are defined based on the ranking of features by an explainer, including the percentage of instances where important tokens are not selected and the average number of non-selected tokens ranked higher than relevant ones. The error metrics are defined based on the ranking of features by an explainer, including the percentage of instances where important tokens are not selected and the average number of non-selected tokens ranked higher than relevant ones. Metrics (A), (B), and (C) assess the failure rates and errors in the explanation provided by the explainer. The framework is instantiated on the RCNN model trained on the BeerAdvocate corpus, where the lowest rank of relevant tokens is determined. Metrics (A), (B), and (C) evaluate the failure rates and errors in the explanation provided by the explainer. The RCNN model trained on the BeerAdvocate corpus evaluates the number of zero-contribution features ranked higher than relevant ones. The corpus contains 100K human-generated beer reviews with aspects like appearance, aroma, and palate. The RCNN predicts ratings rescaled between 0 and 1. The RCNN model trained on the BeerAdvocate corpus analyzes human-generated beer reviews with aspects like appearance, aroma, and palate. It predicts ratings rescaled between 0 and 1. Three separate RCNNs are trained for each aspect independently. The model identifies relevant tokens by considering non-selected tokens with zero contribution to the prediction. Three separate RCNNs are trained for each aspect independently, with the same default settings. The datasets are gathered for each aspect, and non-selected tokens have zero contribution to the model's prediction. A threshold of \u03c4 = 0.1 is chosen to identify clearly relevant tokens. Various statistics of the datasets are provided in Appendix A, including average review lengths and selected token information. The threshold of \u03c4 = 0.1 is chosen to identify clearly relevant tokens, with an average of 1 or 2 relevant tokens per datapoint. This strict threshold ensures high guarantees on the evaluation test. Various statistics of the datasets are provided in Appendix A, including average review lengths and selected token information. The study used a strict threshold of 0.1 to identify clearly relevant tokens, resulting in 1 or 2 relevant tokens per datapoint. Three popular explainers, LIME, SHAP, and L2X, were evaluated using their default code. The study evaluated three popular explainers: LIME, SHAP, and L2X, using default code with some modifications. The study evaluated three popular explainers: LIME, SHAP, and L2X, with modifications in the original repositories. LIME and SHAP outperformed L2X in practice, despite L2X being a feature selection explainer. A major limitation of L2X is the need to know the number of important features per instance. In practice, LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature selection explainer. A major limitation of L2X is the requirement to know the number of important features per instance. L2X learns a distribution over features by maximizing mutual information with the response variable. K is assumed to be known, but in practice, the number of features per instance is usually unknown. Testing L2X with K as the average number of tokens highlighted by human annotators, we obtained averages of 23, 18, and 13 for three aspects. In Table 1, explainers often identify irrelevant features as most relevant, with LIME at 14.79% and L2X at 12.95% in the aroma aspect. In a study by McAuley et al. (2012), the average number of highlighted tokens by human annotators was 23, 18, and 13 for three aspects. The explainers often misidentify irrelevant features as most relevant, with LIME at 14.79% and L2X at 12.95% in the aroma aspect. This is considered a significant failure in the ranking process. In the ranking process, both LIME and L2X have shown significant failures in identifying irrelevant features as most relevant, with LIME at 14.79% and L2X at 12.95% in the aroma aspect. Metric (B) indicates that both explainers can make mistakes in the predicted ranking by placing zero-contribution tokens higher than clearly relevant features. Metric (C) shows that SHAP and L2X have different behaviors in ranking zero-contribution tokens ahead of relevant ones across different aspects. SHAP and L2X differ in the number of zero-contribution tokens placed ahead of relevant ones, with SHAP placing fewer tokens for the first two aspects and around 9 tokens for the third aspect, while L2X places around 3-4 tokens for all three aspects. The explainers' rankings on an instance from the palate aspect are shown in Figure 4, with the heatmap indicating the ranking of tokens. The heatmap in Figure 6 shows the ranking of tokens for the palate aspect in the evaluation dataset. The explainers attribute importance to nonselected tokens, with differences in the number of zero-contribution tokens placed ahead of relevant ones. The heatmap displays the top 10 ranked tokens for the palate aspect. Both LIME and SHAP prioritize nonselected tokens, ranking \"mouthfeel\" and \"lacing\" as most important. In contrast, L2X highlights \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as key tokens, with \"gorgeous\" not making the top 13 for L2X. The study compared the explanations provided by LIME, SHAP, and L2X for a model's predictions. LIME and SHAP ranked \"mouthfeel\" and \"lacing\" as most important, while L2X highlighted \"taste\", \"great\", \"mouthfeel\", and \"lacing\". The evaluation test introduced sheds light on the differences between the two perspectives of explanations. The study compared explanations from LIME, SHAP, and L2X for a model's predictions. LIME and SHAP prioritized \"mouthfeel\" and \"lacing\", while L2X emphasized \"taste\", \"great\", \"mouthfeel\", and \"lacing\". An evaluation test was introduced to highlight differences in explanation perspectives. The framework offers guarantees on a real-world neural network's behavior, showcasing error rates for popular explanatory methods. The study presented error rates on different metrics for three popular explanatory methods in a verification framework for neural networks. The methodology is adaptable to various tasks and areas, such as computer vision. The methodology presented is adaptable to various tasks and areas, such as computer vision. It can be applied to train neural networks for making predictions based on selected features, with a focus on checking for zero contribution of non-selected features. The core algorithm in current post-hoc explainers is domain-agnostic, allowing for a broad evaluation scope. The methodology is adaptable to different tasks like computer vision, focusing on predicting based on selected features and checking zero contribution of non-selected features. The core algorithm in post-hoc explainers is domain-agnostic, providing a broad evaluation scope. Statistics of the dataset are provided in Table 2, including the number of instances retained, average review length, and average numbers of selected tokens. The evaluation provides insights into the limitations of explainers, with statistics on dataset instances, average review length, and selected tokens. Percentages show instances eliminated and datapoints with zero contribution. The evaluation analyzes the impact of selected tokens on predictions, highlighting instances where tokens have a significant effect on the final outcome. The percentage of eliminated instances and datapoints with zero contribution are also discussed. The evaluation examines the influence of selected tokens on predictions, focusing on instances where tokens have a notable impact on the final result. It discusses the percentage of eliminated datapoints due to the absence of a token with a significant effect on the prediction. The evaluation focuses on the impact of selected tokens on predictions, determining if a token is crucial for a handshake. If the model gives the same prediction after removing non-selected tokens, it indicates no handshake. This leads to the conclusion that certain tokens have zero contribution to the final result. The evaluation determines the importance of selected tokens for predictions, indicating a lack of handshake if the model's prediction remains the same after removing non-selected tokens. This implies that certain tokens have zero contribution to the final result. The evaluation shows that certain tokens have zero contribution to the final prediction, indicated by no change in the model's output when these tokens are removed. This is demonstrated through the proof that having S Sx = S x satisfies a condition in Equation 11. The beer described is a dark, fizzy brew with fruity aromas and minimal alcohol taste. The beer is poured from a brown \"grolsch\" bottle, with a dark fizzy yellow color and fruity aromas of apple and blueberry. It has minimal alcohol taste and a slight warming sensation. Overall, it is better than most American lagers and very smooth. The beer, poured from a brown \"grolsch\" bottle, has a dark fizzy yellow color with fruity aromas of apple and blueberry. It has minimal alcohol taste and a slight warming sensation, better than most American lagers and very smooth. The beer, poured from a brown \"grolsch\" bottle, has a dark fizzy yellow color with fruity aromas of apple and blueberry. It has minimal alcohol taste and a slight warming sensation, better than most American lagers and very smooth. No mouthfeel, small initial alcohol taste, tastes of fruit, smooth, better than most American lagers. The beer, poured from a brown \"grolsch\" bottle, has a dark fizzy yellow color with fruity aromas of apple and blueberry. It has minimal alcohol taste and a slight warming sensation, better than most American lagers and very smooth. No mouthfeel, small initial alcohol taste, tastes of fruit, smooth, better than most American lagers."
}