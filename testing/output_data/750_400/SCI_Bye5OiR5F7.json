{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experimental results show improved speed and stability in training GANs, measured by wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. Generative Adversarial Networks (GANs) involve a discriminator distinguishing real data from generated data by a generator in an adversarial game. The text discusses training GANs using optimal transport in implicit deep generative models, improving speed and stability in training measured by wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. GANs involve a discriminator distinguishing real data from generated data in an adversarial game. The adversarial game in GANs involves the generator trying to fool the discriminator by recreating the density distribution from the real source. Matching a target density is done by minimizing a discrepancy measure like the Kullback-Leibler (KL) divergence, which can be challenging with low dimensional support sets. An alternative approach is needed for applications with structured data and high dimensional sample spaces. The problem of matching a target density can be formulated as minimizing a discrepancy measure. The Kullback-Leibler (KL) divergence is challenging with low dimensional support sets. Optimal transport, such as Wasserstein distance, has been used as an alternative approach. It has been applied in defining loss functions for generative models like Wasserstein GAN. The Wasserstein distance, also known as Earth Mover's distance, is used to define a discrepancy measure between densities and has been applied in defining loss functions for generative models like Wasserstein GAN. Optimal transport can introduce structures for optimization, such as the Wasserstein steepest descent flow, which is derived for deep generative models in GANs using the Wasserstein-2 metric function. In this paper, the Wasserstein steepest descent flow for deep generative models in GANs is derived using the Wasserstein-2 metric function, which allows for a Riemannian structure and natural gradient. Natural gradients, like the Fisher-Rao natural gradient induced by KL divergence, offer advantages over Euclidean gradients in learning problems. In GANs, the Fisher-Rao natural gradient induced by the KL divergence is problematic due to low dimensional support sets. To address this, the gradient operator induced by the Wasserstein-2 metric is proposed for GANs, offering advantages over the Euclidean gradient. The proximal operator for the generators of GANs is computed using the squared constrained Wasserstein-2 regularization. We propose using the gradient operator induced by the Wasserstein-2 metric for GANs, which simplifies computation by introducing a relaxed proximal operator for generators. The constrained Wasserstein-2 distance can be approximated by a neural network in practice, offering a simple structure for implicit generative models. The constrained Wasserstein-2 distance can be approximated by a neural network in practice, simplifying computation for implicit generative models. Generalizing the metric introduces a relaxed proximal operator for generators, streamlining parameter updates and making it an easy-to-implement regularizer for generator updates. The relaxed proximal operator simplifies parameter updates for generators in Wasserstein natural gradient. It can be easily implemented as a regularizer for generator updates. The paper introduces a Wasserstein proximal method and demonstrates its effectiveness in GAN experiments. Related work on optimal transport and proximal operators is briefly reviewed. In Algorithm 1, a Wasserstein proximal method is introduced for optimizing GANs. Section 3 demonstrates the method's effectiveness in various GAN experiments. Optimal transport and its proximal operator on a parameter space are briefly presented in Section 4. Optimal transportation defines distance functions between probability densities, with W p as the Wasserstein-p distance. This paper focuses on the case p = 2, denoted as W 2 or W. The Wasserstein-2 distance is described as a trajectory minimizing kinetic energy to transport densities \u03c1 0 to \u03c1 1. In this paper, the focus is on extending the classic theory of the Wasserstein-2 distance to cover parameterized density models. The parameterized probability \u03c1(\u03b8, x) is considered, where \u03c1(\u03b8, x) is locally injective from parameter space \u0398 to P2(Rn). This extension allows for the constraint of the density path within a parametrized model. The classic theory of the Wasserstein-2 distance is extended to parameterized density models, where the parameterized probability \u03c1(\u03b8, x) is locally injective from parameter space \u0398 to P2(Rn). The constrained Wasserstein-2 metric function is formulated with feasible Borel potential functions and continuous parameter paths. The constrained Wasserstein-2 metric function d W on parameter space \u0398 is defined with feasible Borel potential functions and continuous parameter paths. This metric can be used for steepest descent optimization schemes. The constrained Wasserstein-2 metric on parameter space allows for steepest descent optimization schemes, defined with feasible Borel potential functions and continuous parameter paths. The metric can be used to formulate a Riemannian structure, such as the Fisher natural gradient, leading to the constrained Wasserstein-2 gradient, also known as the Wasserstein natural gradient. The constrained Wasserstein-2 metric allows for a Riemannian structure, leading to the Wasserstein natural gradient for optimization. The Wasserstein gradient operator is defined for a loss function on parameter space. The Wasserstein natural gradient is defined for a loss function on parameter space using the constrained Wasserstein metric. The steepest descent flow and gradient descent iteration are also discussed. The steepest descent flow and gradient descent iteration in the context of the constrained Wasserstein metric involve the natural gradient operator, Euclidean gradient operator, and Wasserstein Riemannian metric matrix G. The computation of matrix G(\u03b8) \u22121 can be challenging, leading to the use of the proximal operator in the backward Euler method, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme, provides a numerical scheme for equation 2 using the proximal operator. The distance of the parameter update acts as a regularization to the loss function, and the d W distance can be approximated locally by a second order Taylor expansion. This allows for the derivation of other first order schemes, such as the Semi-Backward Euler. The Semi-Backward Euler method for the gradient flow of loss function F is easier to approximate than the forward Euler method. It involves a second order Taylor expansion to approximate the parameter update distance, acting as a regularization to the original loss function. The Semi-Backward Euler method for the gradient flow of loss function F is easier to approximate than the forward Euler method. It does not require computing and inverting G(\u03b8) and is simpler than the backward Euler method. The method involves a constrained optimization over \u03a6, making it more tractable than the timedependent constraint involved in computing d W. The Semi-Backward Euler method in implicit generative models simplifies parameter updates by avoiding complex computations and constraints. The generator function maps noise input to output samples, with parameters in R^d, latent space in R^m, and sample space in R^n. The implicit generative model uses a generator function g(\u03b8, z) to map noise input to output samples, with parameters in R^d, latent space in R^m, and sample space in R^n. The update in Proposition 3 involves applying a neural network to approximate variable \u03a6, leading to a simpler formulation of the constrained Wasserstein-2 metric. This reformulation allows for the definition of the relaxed Wasserstein metric and introduces a straightforward algorithm for proximal operations. The update in Proposition 3 simplifies the constrained Wasserstein-2 metric in implicit generative models by using a neural network to approximate variable \u03a6. This reformulation leads to the relaxed Wasserstein metric and a simple algorithm for proximal operator on generators. Proposition 4 defines the constrained Wasserstein-2 metric with infimum among feasible Borel potential functions and continuous parameter paths. The constrained Wasserstein-2 metric in implicit generative models requires the generator's derivative to be a gradient vector field of a potential function. Computational difficulties arise in finding this function, especially in higher-dimensional sample spaces. The generator's derivative must be a gradient vector field of a potential function for the constrained Wasserstein-2 metric in implicit generative models. Computational challenges arise in determining this function, particularly in higher-dimensional sample spaces. To address this, a relaxed Wasserstein metric is considered on the parameter space, leading to an approximation of the Wasserstein proximal operator. The text discusses difficulties in fitting the gradient constraint for computing the Wasserstein proximal operator. To simplify computations, a relaxed Wasserstein metric is considered on the parameter space, leading to an approximation of the proximal operator. This approach regularizes the generator by the expectation of squared differences in the sample space, especially in high-dimensional scenarios. The text discusses a relaxed Wasserstein Proximal approach for regularizing the generator in high-dimensional scenarios. It involves minimizing a parameterized function and using a proximal step-size and batch size. The text presents a toy example illustrating the effectiveness of Wasserstein proximal operator in GANs, focusing on a family of distributions with two weighted delta measures. The approach involves minimizing a parameterized function and using a proximal step-size and batch size. In a toy example illustrating Wasserstein proximal operator in GANs, a family of distributions with two weighted delta measures is considered. The proximal regularization for a loss function is defined, and statistical distance functions between parameters are checked. The proximal regularization for a loss function involves statistical distance functions like Wasserstein-2, Euclidean, and Kullback-Leibler divergences. These distances help measure differences between probability models, with Wasserstein-2 and Euclidean distances being effective. The Euclidean distance is independent of model structure, while the constrained Wasserstein-2 metric depends on it. The Wasserstein-2 and Euclidean distances are effective in measuring differences between probability models. The Euclidean distance is model-independent, while the constrained Wasserstein-2 metric depends on the model structure. The Wasserstein proximal solution decreases the objective function more than the Euclidean proximal solution. The Wasserstein-1 metric is used as the loss function in GAN optimization. Numerical experiments show that Wasserstein-2 proximal outperforms Euclidean proximal for this loss function. The Relaxed Wasserstein Proximal algorithm provides faster speed in Wasserstein gradient-descent on various GANs. The Relaxed Wasserstein Proximal (RWP) algorithm improves speed and stability in training GANs by applying regularization on the generator. It outperforms Euclidean proximal for the Wasserstein-1 loss function, providing better speed and convergence in GAN training. The Relaxed Wasserstein Proximal (RWP) algorithm enhances GAN training speed and stability by regularizing the generator, introducing novel modifications to the update rule. The Relaxed Wasserstein Proximal (RWP) algorithm improves GAN training by modifying the generator update rule, introducing hyperparameters for iteration control. The Relaxed Wasserstein Proximal (RWP) algorithm introduces hyperparameters for iteration control in GAN training. It tests this regularization on various GAN types using different datasets and architecture. The quality of generated samples is measured using Fr\u00e9chet Inception Distance (FID). We test the Relaxed Wasserstein Proximal regularization on different GAN types using CIFAR-10 and CelebA datasets with DCGAN architecture. Fr\u00e9chet Inception Distance (FID) is used to measure sample quality and convergence of GAN training. The Relaxed Wasserstein Proximal regularization improves the speed and stability of GAN training, measured using Fr\u00e9chet Inception Distance (FID) on CIFAR-10 and CelebA datasets with DCGAN architecture. Samples are evaluated every 1000 outer-iterations for CIFAR-10 and every 10,000 outer-iterations for CelebA. Hyperparameter choices are detailed in Appendix C. The Relaxed Wasserstein Proximal regularization improves GAN training speed and stability, measured by Fr\u00e9chet Inception Distance (FID) on CIFAR-10 and CelebA datasets with DCGAN architecture. Hyperparameter choices for training are provided in Appendix C. The regularization enhances convergence speed and achieves lower FID for all GAN types, with a notable 20% improvement in DRAGAN. The regularization using Relaxed Wasserstein Proximal improves GAN training speed and stability, achieving lower FID for all GAN types, with a significant 20% improvement in DRAGAN. Multiple generator iterations may hinder Standard GANs on CelebA initially, but restarting the algorithm leads to successful learning. In DRAGAN, there is a 20% improvement in sample quality based on FID. Similar results are seen in the CelebA dataset as well. Multiple generator iterations can sometimes hinder Standard GANs on CelebA initially, but restarting the algorithm leads to successful learning. The issue may be resolved with a more stable loss function like WGAN-GP or different parameters. The impact of multiple generator updates versus discriminator updates is also examined. FIG2 focuses on successful runs and predicts that the defect can be rectified with a more stable loss function like WGAN-GP or by adjusting parameters. The effect of multiple generator updates compared to discriminator updates is also explored, showing that using regularization in RWP leads to more stable FID convergence. The effect of not using regularization in GAN training is examined, showing that FID has high variance without it. RWP regularization leads to more stable FID convergence and lower FID scores. Latent space walks demonstrate that RWP does not cause the GAN to memorize. RWP also improves speed and achieves better results in terms of wallclock time. Using RWP regularization improves speed and achieves lower FID scores in GAN training. Multiple generator iterations may cause initial learning to fail, but once successful, it remains stable. Successful runs are shown in Figure 4. RWP regularization improves speed and achieves lower FID scores in GAN training. Multiple generator iterations may cause initial learning to fail, but once successful, it remains stable. Successful runs are shown in Figure 4, demonstrating the effect of performing 10 generator iterations per outer-iteration with and without RWP regularization. With RWP, convergence and lower FID are obtained after 1,000,000 outer-iterations. Figure 4 shows an experiment comparing the effect of 10 generator iterations per outer-iteration with and without RWP regularization. With RWP, convergence and lower FID scores are achieved after 1,000,000 outer-iterations. The experiment on the CIFAR-10 dataset using the Semi-Backward Euler method shows comparable training to the standard WGAN-GP loss. The experiment on the CIFAR-10 dataset using the Semi-Backward Euler method shows comparable training to the standard WGAN-GP loss. The training is more stable with convergence and lower FID scores achieved. The algorithm and hyperparameter settings are detailed in the appendix. The Semi-Backward Euler (SBE) method for training on the CIFAR-10 dataset is comparable to the standard WGAN-GP loss. The algorithm and hyperparameter settings are detailed in the appendix, with stable training and lower FID scores achieved. Further investigation into the SBE method is left for future work. The Semi-Backward Euler method for training on CIFAR-10 is comparable to standard WGAN-GP loss. Many studies apply Wasserstein distance as the loss function in optimal transport and GANs. In the literature, optimal transport and GANs often use the Wasserstein distance as the loss function due to its statistical properties and ability to compare probability distributions on lower dimensional sets. The Wasserstein distance is a statistical estimator used in GANs, specifically in Wasserstein GAN, where the loss function is the Wasserstein-1 distance. The discriminator must satisfy the 1-Lipschitz condition, leading to studies on regularization techniques for the discriminator. The Wasserstein-2 metric in GANs provides a metric tensor structure, forming an infinite dimensional Riemannian manifold known as the density manifold. The gradient flow in the density manifold is linked to various studies on regularization techniques for the discriminator. The regularization of the discriminator in GANs is focused on satisfying specific conditions. The Wasserstein-2 metric creates a metric tensor structure, forming an infinite dimensional Riemannian manifold called the density manifold. The gradient flow in this manifold is connected to transport-related partial differential equations, such as the Fokker-Planck equation. The density manifold links with transport-related partial differential equations like the Fokker-Planck equation. Learning communities explore leveraging gradient flow in probability space and nonparametric models like the Stein gradient descent method. Many groups leverage gradient flow in probability space and study stochastic gradient descent. Nonparametric models like the Stein gradient descent method are also explored, along with an approximate inference method for computing Wasserstein gradient flow. The Wasserstein structure can be constrained on parameter space as well. The Wasserstein gradient flow can be constrained on parameter space, with studies focusing on Gaussian families and elliptical distributions. Previous works have explored the constrained Wasserstein gradient with fixed mean and variance, while our approach applies the Wasserstein gradient in Gaussian family settings. Our approach applies the constrained Wasserstein gradient to implicit generative models, focusing on regularizing the generator instead of the discriminator. This is a departure from previous works that have mainly concentrated on Gaussian families and elliptical distributions. The constrained Wasserstein gradient is applied to implicit generative models, focusing on regularizing the generator. This method improves convergence speed and minimizes FID in Wasserstein GAN. The proposed method computes the Wasserstein-2 gradient flow on parameter space, leading to faster convergence speeds and better minimizers in terms of FID. It introduces a Riemannian structure in density space and considers smooth and strictly positive probability densities. Feasible Borel potential functions and continuous density paths satisfying the continuity equation introduce a Riemannian structure in density space. The set of smooth and strictly positive probability densities is considered, with the tangent space of P+ given by certain functions. The elliptic operator identifies functions modulo additive constants with tangent vectors in the space of densities. The tangent space of smooth real valued functions in P+ is defined by certain functions. The elliptic operator identifies functions with tangent vectors in the space of densities. The Wasserstein gradient operator in (P+, gW) is given for a loss function F: P+ \u2192 R. The gradient flow satisfies certain conditions. The Wasserstein gradient operator in (P+, gW) is defined for a loss function F: P+ \u2192 R, and the gradient flow satisfies specific conditions. More analytical results on the Wasserstein-2 gradient flow are provided in BID3. The Wasserstein-2 metric and gradient operator are then constrained on statistical models defined by a triplet (\u0398, Rn, \u03c1). Analytical results on the Wasserstein-2 gradient flow are provided in BID3. The Wasserstein-2 metric and gradient operator are constrained on statistical models defined by a triplet (\u0398, Rn, \u03c1), where \u03c1 is a locally injective parameterization function. A Riemannian metric is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. A Riemannian metric is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The Wasserstein statistical manifold is defined with a smooth and positive definite metric tensor G(\u03b8), forming a smooth Riemannian manifold. The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2. The proof of Theorem 1 involves the constrained Wasserstein gradient operator in parameter space, where the distance can be expressed as an action function on the Wasserstein statistical manifold. The gradient operator on a Riemannian manifold is defined accordingly. The proof involves the constrained Wasserstein gradient operator in parameter space, expressed as an action function on the Wasserstein statistical manifold. The gradient operator on a Riemannian manifold is defined accordingly. The derivation of the proposed semi-backward method is presented, along with the proof of a claim involving a geodesic path. The derivation of the proposed semi-backward method involves proving equations 6 and 7, with a focus on the maximizer \u03a6 * satisfying equation 8. This leads to the proof of a claim regarding the geodesic path on the Wasserstein statistical manifold. The derivation of the Semi-backward method involves proving equations 6 and 7, with a focus on the maximizer \u03a6 * satisfying equation 8. This leads to the proof of a claim regarding the geodesic path on the Wasserstein statistical manifold. The proof of Proposition 4 is also presented for the completion of the paper. The Semi-backward method is derived by proving equations related to the maximizer \u03a6 * on the Wasserstein statistical manifold. Proposition 4 is proven in BID23, with the implicit model given by a push-forward relation. The probability density transition equation of g(\u03b8(t), z) satisfies a constrained continuity equation. The probability density transition equation of g(\u03b8(t), z) satisfies the constrained continuity equation, proven through various equations involving gradient and divergence operators. The text discusses the computation of the proximal operator explicitly using equations involving gradient and divergence operators. The equations are proven through integration by parts and the push forward relation. The Wasserstein and Euclidean proximal operators are calculated separately. The text discusses the computation of the proximal operator explicitly using equations involving gradient and divergence operators. The Wasserstein and Euclidean proximal operators are calculated separately, with hyperparameter settings provided for the experiments. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 include batch size, optimizer details, latent space dimension, and generator iterations for different models like WGAN-GP, Standard, and DRAGAN on CIFAR-10 dataset. The hyperparameter settings for experiments on CIFAR-10 and CelebA datasets include optimizer details, latent space dimension, and generator iterations for different models like WGAN-GP, Standard, and DRAGAN. The hyperparameter settings for experiments on CIFAR-10 and CelebA datasets include optimizer details, latent space dimension, and generator iterations for different models like WGAN-GP, Standard, and DRAGAN. For both the generator and discriminator, the Adam optimizer with specific parameters was used. The Relaxed Wasserstein Proximal regularization method is highlighted as an easy-to-implement approach. The Relaxed Wasserstein Proximal is a simple regularization method meant for easy implementation. An example is provided for applying it to Standard GANs with specific optimizer choices and batch sizes. The algorithm involves updating the discriminator, performing Adam gradient descent, and repeating until a stopping condition is met. The algorithm for training GANs with Relaxed Wasserstein Proximal involves updating the discriminator, performing Adam gradient descent, and repeating until a stopping condition is met. The key differences lie in the terms involving generator iterations and the specific optimization choices. The Relaxed Wasserstein Proximal method for training GANs involves specific optimization choices and updating the discriminator. Samples generated from Standard GAN with RWP had an FID of 17.105, while WGAN-GP with RWP had an FID of 38.3 on CIFAR-10 dataset. In FIG4, samples from Standard GAN with RWP on CelebA dataset had an FID of 17.105. FIG5 shows samples from WGAN-GP with RWP on CIFAR-10 dataset with an FID of 38.3. LATENT SPACE WALK BID34 indicates smooth transitions in GANs with RWP regularization, suggesting no memorization. Specific hyperparameters for SBE on WGAN-GP include a batch size of 64 and DCGAN architecture. Walking in the latent space can detect if a generator is memorizing, as shown in FIG6 and FIG7 for GANs with RWP regularization. Specific hyperparameters for Semi-Backward Euler (SBE) on WGAN-GP include a batch size of 64, DCGAN architecture, and Adam optimizer with a learning rate of 0.0002. The DCGAN architecture was used for the discriminator and generator, with a one-hidden-layer fully connected network for the potential \u03a6 p. The Adam optimizer with specific parameters was utilized for both the generator, discriminator, and potential \u03a6 p. The latent space dimension was set to 100, and updates were made to the discriminator, generator, and potential in each outer-iteration loop. In each outer-iteration loop, the discriminator was updated 5 times, the generator once, and the potential 5 times using a latent space dimension of 100 and h = 0.2."
}