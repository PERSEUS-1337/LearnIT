{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions but current training techniques require fully-observed samples. In scenarios where obtaining full samples is costly or impossible, learning implicit generative models from partial, noisy observations is considered. It is shown that the true distribution can be recovered even with per-sample information loss using certain measurement models. This leads to a new method for training Generative models. Generative models can be trained with partial, noisy observations using certain measurement models. A new method called AmbientGAN improves Generative Adversarial Networks (GANs) performance on benchmark datasets, achieving higher inception scores. AmbientGAN is a new method for training Generative Adversarial Networks (GANs) that shows significant improvements in qualitative and quantitative results on benchmark datasets. Models trained with AmbientGAN achieve 2-4 times higher inception scores compared to baselines by passing the generator output through a simulated random measurement function. This approach addresses the challenge of collecting enough data for training when sensing is expensive. The generator output is passed through a simulated random measurement function in AmbientGAN. The discriminator distinguishes between real and generated measurements. This work addresses the challenge of collecting enough data for training generative models by directly training from noisy or incomplete samples. The framework allows for recoverable unknown distributions from various types of measurements. Training a generative model directly from noisy or incomplete samples is addressed in this work. The framework allows for recoverable unknown distributions from various types of measurements, with the critical assumption that the measurement process is known and meets specific conditions. The approach, called AmbientGAN, introduces a new way of training GANs. Our framework, AmbientGAN, introduces a new way of training GANs by distinguishing real measurements from simulated ones of a generated image. It has been shown to be effective on various datasets and measurement models. The AmbientGAN approach involves training a GAN by distinguishing real measurements from simulated ones of a generated image. It has shown effectiveness on different datasets and measurement models, even with noisy observations and low-dimensional projections. The method produces high-quality generative models, as demonstrated visually and quantitatively through inception scores comparisons. Our method constructs good generative models from noisy observations and low-dimensional projections with information loss. Theoretical results show that noisy measurements can be inverted using convolutions and noise addition, leading to high-quality samples. The distribution of measured images uniquely determines the distribution of original images in the presence of noise, implying a pure Nash equilibrium for the GAN game must match the true distribution. Similar results are shown for a dropout measurement model and random projection. In the presence of noise, the distribution of measured images uniquely determines the distribution of original images, leading to a pure Nash equilibrium for the GAN game that matches the true distribution. Results are also shown for dropout and random projection measurement models. Incorporating measurement process into GAN training improves inpainting results on celebA dataset with randomly placed occlusions. In FIG1, the celebA dataset of celebrity faces is considered under randomly placed occlusions. In FIG2, learning from noisy, blurred versions of images from the celebA dataset is discussed. Incorporating the measurement process into GAN training improves sample quality. Incorporating the measurement process into GAN training improves sample quality by learning from noisy, blurred images. The models can produce cleaner samples compared to denoised images using Wiener deconvolution. Additionally, a generative model is considered for 2D images in the MNIST dataset from pairs of 1D projections. Incorporating the measurement process into GAN training improves sample quality by learning from noisy, blurred images. A generative model is considered for 2D images in the MNIST dataset from pairs of 1D projections, where AmbientGAN recovers underlying structure but cannot identify distribution up to rotation or reflection. The AmbientGAN model recovers underlying structure from 2D images in the MNIST dataset using pairs of 1D projections. Two variants are considered, one forgetting the choice of line and the other including it. Neural network based implicit generative models can be constructed using autoregressive or adversarial approaches. The adversarial framework is powerful for modeling complex data distributions like images, video, and 3D models. Neural network based implicit generative models can be constructed using autoregressive or adversarial approaches. The adversarial framework is powerful for modeling complex data distributions like images, video, and 3D models. Generative priors are explored in papers to solve inverse problems, and GANs are used to make synthetic data more realistic. Translation of images between domains is also demonstrated. A generative model, such as GANs, can be used for various applications like solving inverse problems and translating images between domains. Training stability can be connected to low dimensional projections of samples. The text discusses using GANs to translate images between domains and the connection between training stability and low dimensional projections of samples. It also mentions related works such as creating 3D object shapes from 2D projections. Our work is closely related to BID10, where authors create 3D object shapes from 2D projections using different low-dimensional projections of the data to improve stability. The AmbientGAN framework is mentioned, where measurements create 2D projections using weighted sums of voxel occupancies. The notation 'r' denotes real distribution, 'g' for generated distributions, 'x' for underlying space, and 'y' for measurements. Lossy measurements are observed on samples from the real underlying distribution over R^n, with each measurement size denoted as 'm'. The text discusses using measurements from a real underlying distribution over R^n, with each measurement being an output of a measurement function parameterized by \u03b8. The measurements are denoted as y = f\u03b8(x), where \u0398 can be sampled from a distribution p\u03b8. The text discusses measurements as outputs of a function parameterized by \u03b8, with \u0398 sampled from a distribution p\u03b8. The task involves an unknown distribution prx and a known distribution p\u03b8, with IID realizations {y1, y2}. Our goal is to create an implicit generative model of prx using a set of IID realizations from pry, combining the measurement process with adversarial training. Our main idea is to combine the measurement process with the adversarial training framework to learn a generator G that can sample from the distribution prx. This involves using a random latent vector Z and the distribution of Xg to achieve our goal of creating a generative model close to prx. Unlike the standard GAN setting, we do not have access to the desired objects X from prx. In the standard GAN setting, the goal is to learn a generator G to match the distribution pgx with prx. However, in this scenario, only measurements from samples Y from pry are available. To address this, random measurements are simulated on generated objects Xg using a measurement function f\u0398 sampled from p\u03b8. The main idea is to simulate random measurements on generated objects Xg using a measurement function f\u0398 sampled from p\u03b8. The discriminator distinguishes real measurements from fake ones by predicting if a given y is from the real measurement distribution pry or the generated one pg y. The quality function q(\u00b7) is used to define the objective based on the discriminator output. The discriminator distinguishes between real and generated measurements by predicting if a given y is from the real measurement distribution p r y or the generated one p g y. The AmbientGAN objective is defined based on the discriminator output, with differentiable functions f \u03b8 and feedforward neural networks G and D implemented for end-to-end differentiability. The AmbientGAN objective is defined based on the discriminator output, with differentiable functions and feedforward neural networks used for end-to-end differentiability in training. The model is trained using a standard gradient-based GAN training procedure, with stochastic gradients computed by backpropagation in each iteration. Our model is end-to-end differentiable and trained using a gradient-based GAN training procedure. Stochastic gradients are computed by sampling Z, \u0398, and Y r in each iteration. Updates alternate between parameters of D and G. The approach is compatible with GAN improvements and can incorporate additional information like per sample labels. Our model is end-to-end differentiable and trained using a gradient-based GAN training procedure. Updates alternate between parameters of D and G. The approach is compatible with various GAN improvements and can easily incorporate additional information like per sample labels. The learning framework involves conditional versions of the generator and discriminator, demonstrated in experiments with various GAN models. Measurement models are tailored for 2D images, but the AmbientGAN framework is versatile for other data formats. The measurement models discussed in this section are tailored for 2D images, but the AmbientGAN framework is versatile and can be applied to other data formats as well. The models include Block-Pixels, Convolve+Noise, and Block-Patch. Measurement function (x) is a 2D image. Different measurement models include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, and Pad-Rotate-Project. The measurement function involves various models like Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, Pad-Rotate-Project, and PadRotate-Project-\u03b8, each with different operations on image patches and pixels. The measurement function involves different models like Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, Pad-Rotate-Project, and PadRotate-Project-\u03b8, where images are rotated by a random angle and pixels are summed along the vertical axis to create measurement vectors. Gaussian-Projection involves projecting onto a random Gaussian vector to recover the true underlying distribution for certain measurement models. The chosen angle and projection values are included in the measurements. Gaussian-Projection involves projecting onto a random Gaussian vector to recover the true underlying distribution for certain measurement models. The approach aims to show a unique distribution consistent with observed measurements, making the mapping of distributions invertible. The broad approach aims to demonstrate a unique distribution consistent with observed measurements, making the mapping of distributions invertible. This consistency guarantee is provided by Lemma 5.1, which states that for a given data distribution and distribution over parameters of measurement functions, there is a unique probability distribution that induces the given measurement. Lemma 5.1 provides a consistency guarantee with the AmbientGAN training procedure, assuming uniqueness of the true underlying distribution given the measurement distribution. The theorem shows that this assumption is satisfied. The previous lemma assumes uniqueness of the true underlying distribution given the measurement distribution. The following theorems demonstrate that this assumption holds for Gaussian-Projection, Convolve+Noise, and Block-Pixels measurement models, allowing recovery of the true distribution with the AmbientGAN framework. The required conditions are easily met for Gaussian blurring with additive noise. The AmbientGAN framework can recover the true underlying distribution under Gaussian-Projection, Convolve+Noise, and Block-Pixels measurement models. The required conditions are easily satisfied for Gaussian blurring with additive noise, and the assumption of a finite discrete set of pixel values holds in practical scenarios. The AmbientGAN framework can recover the true underlying distribution under various measurement models, including Gaussian-Projection and Convolve+Noise. The assumption of a finite discrete set of pixel values is common in practical scenarios, allowing for consistency guarantees and sample complexity results in learning distributions. The AmbientGAN framework can recover the true distribution under different measurement models like Gaussian-Projection and Convolve+Noise. For a finite set of pixel values, consistency guarantees and sample complexity results are provided. Theorem 5.4 states that with a unique distribution inducing the measurement distribution, an optimal generator must satisfy certain conditions. The study explores a model with a probability of blocking a pixel, showing that a unique distribution can induce the measurement distribution. Three datasets were used for experiments: MNIST for handwritten digits, CelebA for face images of celebrities, and CIFAR-10 for RGB images from different classes. For our experiments, we used three datasets: MNIST for handwritten digits, CelebA for face images of celebrities, and CIFAR-10 for RGB images from different classes. Two GAN models were used for the MNIST dataset, a conditional DCGAN and an unconditional model. More details on architectures and hyperparameters can be found in the appendix. For our experiments, we used generative models for MNIST, CelebA, and CIFAR-10 datasets. Models included conditional DCGAN, unconditional Wasserstein GAN, unconditional DCGAN, and Auxiliary Classifier Wasserstein GAN with gradient penalty. Architecture details can be found in the appendix. For the CIFAR-10 dataset, an Auxiliary Classifier Wasserstein GAN with gradient penalty (ACWGANGP) is used, following the residual architecture in BID12. Discriminator architectures remain the same for 2D outputs, while fully connected discriminators are used for 1D projections. For measurements with 2D outputs, the same discriminator architectures as in the original work are used. Fully connected discriminators are used for 1D projections. The architecture for the fully connected discriminator on the MNIST dataset was 25-25-1 and for the celebA dataset was 100-100-1. Baseline approaches were implemented to evaluate the performance of the AmbientGAN framework on IID samples from the measurement distribution. The fully connected discriminator architecture for the MNIST dataset was 25-25-1 and for the celebA dataset was 100-100-1. Baseline approaches were implemented to evaluate the performance of the AmbientGAN framework on IID samples from the measurement distribution, aiming to create an implicit generative model for p r x. A crude baseline approach involves learning a generative model directly on the measurements to approximate the true distribution p r x. The goal is to create an implicit generative model for p r x by learning directly on measurements. A stronger baseline involves inverting measurement functions to obtain full-samples for generative model learning. The \"ignore\" baseline approximates the true distribution p r x. In the AmbientGAN setting, the measurement functions may not be invertible, and we may not observe \u03b8 i. Despite these violations, we can try to approximate an inverse function to train a generative model using inverted samples. In the AmbientGAN setting, measurement functions may not be invertible, and \u03b8 i may not be observed. To train a generative model, an inverse function can be approximated using inverted samples. To train a generative model in the AmbientGAN setting, approximate inverse functions are used with inverted samples. For Block-Pixels measurements, blurring the image or using total variation inpainting can fill in zero pixels. For Convolve+Noise measurements, a Gaussian kernel and additive Gaussian Noise are applied. For Block-Pixels measurements, blurring or using total variation inpainting can fill in zero pixels. For Convolve+Noise measurements, a Wiener deconvolution is used as an approximate inverse function. Navier Stokes based inpainting method BID2 is used for Block-Patch measurements. Other measurement models do not have clear approximate inverse functions. For Block-Patch measurements, the Navier Stokes based inpainting method BID2 is used to fill in zero pixels. Inverting other measurement models to obtain an approximate inverse function is unclear. Keep-Patch measurement model does not have known pixels outside a box, making inpainting methods unsuitable. Extract-Patch measurements are challenging as the position of the patch is lost. Pad-Rotate-Project-\u03b8 measurements require sampling many angles and using techniques for inverting the Radon transform. Inverting Pad-Rotate-Project measurements is challenging due to the lack of information about \u03b8. Only results with AmbientGAN models are reported in this subset of experiments, along with samples generated by baselines and our models. In this subset of experiments, only results with AmbientGAN models are reported. Samples generated by baselines and our models are shown for each experiment, along with samples from the dataset of measurements available for training. Additional results can be found in the appendix. In this subset of experiments, samples from the dataset of measurements, baselines, and our models are shown for selected parameter settings. Results on MNIST are in the appendix. Results on celebA with DCGAN and CIFAR-10 with ACW-GANGP are shown in FIG5 and FIG7, respectively. The samples are degraded in our measurement process, making it challenging for baselines to produce good samples. Our models, however, are able to generate images with good visual quality. The samples from the dataset are heavily degraded in the measurement process, making it challenging for baselines to produce good samples. Our models, using Convolve+Noise with Gaussian kernel and IID Gaussian noise, are able to generate images with good visual quality. Our models, using Convolve+Noise with Gaussian kernel and IID Gaussian noise, can produce high-quality images. Results on celebA with DCGAN show that our models create coherent faces by observing only parts of one image at a time. Pad-Rotate-Project and Pad-Rotate-Project-\u03b8 measurement models result in drastic signal degradation, losing most of the information in a sample during the measurements process. Our models can create coherent faces on celebA with DCGAN by observing only parts of one image at a time. 1D projections show signal degradation, with most information lost during measurements. Results on MNIST with DCGAN reveal that the model learns up to rotation and reflection, generating digits with similar orientations and chirality within each class without explicit incentive. The experiments on MNIST with DCGAN show that the model can learn rotation and reflection, generating digits with consistent orientations and chirality within each class. The second measurement model produces upright digits by including the rotation angle. Despite lower visual quality, the method demonstrates the ability to generate digit images from 1D projections. Our method shows that we can generate digit images from 1D projections, even though the quality may be lower. The model trained on celebA dataset with Pad-Rotate-Project-\u03b8 measurements with a DCGAN produces crude outlines of faces, lacking details. This highlights the challenge of learning complex distributions with only 1D projections and the need for better distribution recovery understanding. The model trained on celebA dataset with Pad-Rotate-Project-\u03b8 measurements with a DCGAN generates crude outlines of faces, lacking details. This emphasizes the challenge of learning complex distributions with 1D projections and the need for better distribution recovery understanding. Inception scores are used to quantify the quality of generative models in the AmbientGAN framework. Inception scores are used to quantify the quality of generative models learned in the AmbientGAN framework. For the CIFAR-10 dataset, an Inception model trained on the ImageNet dataset is used. A classification model with high accuracy is trained on the MNIST dataset. Different models with varying probabilities of blocking pixels are trained for Block-Pixels measurements on MNIST. The final test set accuracy of a classification model with two conv+pool layers and two fully connected layers on MNIST was 99.2%. Several models were trained with different probabilities of blocking pixels for Block-Pixels measurements on MNIST, and the inception scores were computed. The plot of inception scores as a function of the probability of blocking pixels showed that AmbientGAN models outperformed baseline models as the probability increased. After training models with varying probabilities of blocking pixels on MNIST, the inception scores were computed. The plot shows that AmbientGAN models outperformed baseline models as the probability increased. Additionally, models were trained on MNIST with Convolve+Noise measurements, varying the additive Gaussian noise standard deviation \u03c3. The inception score was plotted against \u03c3, revealing performance trends. The AmbientGAN models outperform Wiener deconvolution and the \"ignore\" baseline as noise levels increase in Convolve+Noise measurements on MNIST. Inception scores show that AmbientGAN models maintain high performance even with higher noise levels. For 1D projection measurements, inception scores are reported for samples generated by AmbientGAN models. The AmbientGAN models outperform Wiener deconvolution and the \"ignore\" baseline as noise levels increase. Inception scores show that AmbientGAN models maintain high performance even with higher noise levels. For 1D projection measurements, inception scores are reported for samples generated by AmbientGAN models, with different models achieving varying scores. The Pad-Rotate-Project model produces poorly aligned digits with an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements achieves a score of 8.12. The second model trained on 1D projections comes close to the performance of the fully-observed case. The second model, trained on 1D projections, achieves an inception score of 8.12, close to the fully-observed case's score of 8.99. The total variation inpainting method is slow, with performance similar to the unmeasure-blur baseline on MNIST. The model's superiority over baselines is evident in the plots. The total variation inpainting method is slow and performs similarly to the unmeasure-blur baseline on MNIST. Inpainting baselines are not run on the CIFAR-10 dataset. The plots show the superiority of the approach over baselines, with the inception score displayed in Fig. 8. Generative models require a large, high-quality dataset, but the approach demonstrates learning from incomplete, noisy data. The approach demonstrates learning from incomplete, noisy data to construct new generative models of distributions without requiring a large, high-quality dataset. The text discusses constructing new generative models of distributions from incomplete, noisy data without needing a high-quality dataset. It mentions the data distribution, distribution over parameters, and induced measurement distribution. The vanilla GAN model is referenced in relation to an optimal Discriminator. The text discusses constructing generative models from incomplete data using the vanilla GAN model. It mentions the unique probability distribution inducing the measurement distribution and the importance of covering all possible projection directions. The text discusses the unique probability distribution needed to match all 1D marginals obtained with Gaussian projection measurements, emphasizing the importance of covering all possible projection directions. The Cramer-Wold theorem states that a unique probability distribution can match all 1D marginals obtained with Gaussian projection measurements. If the support of the Fourier transform of the convolution kernel and additive noise distribution is empty, then there is a unique distribution that can induce the measurement distribution. The Convolve+Noise measurement model with Gaussian projection measurements states that if the support of the Fourier transform of the convolution kernel and additive noise distribution is empty, then there is a unique distribution that can induce the measurement distribution. This implies a bijective map between X and Z. The Convolve+Noise measurement model with Gaussian projection measurements implies a bijective map between X and Z, where the pdfs of X and Z are related by a Jacobian function. The bijective map between X and Z implies a continuous transformation, allowing Z to be written as h(X) where h is a bijective, differentiable function. The pdfs of X and Z are related by a Jacobian function. Y's pdf is a convolution of individual pdfs, leading to a reverse map from the measurement distribution p y to a sample distribution p x. The pdf of Y is a convolution of individual pdfs, leading to a reverse map from the measurement distribution p y to a sample distribution p x. This reverse map uniquely determines the true underlying distribution p x. The reverse map from the measurement distribution p y to the sample distribution p x uniquely determines the true underlying distribution p x. The empirical version of the vanilla GAN objective is defined for a dataset of measurement samples, with an optimal discriminator determined by the empirical distribution of samples. The empirical version of the vanilla GAN objective is defined for a dataset of measurement samples, with an optimal discriminator determined by the empirical distribution of samples. The optimal generator must satisfy the empirical distribution of samples. The optimal generator in GANs must satisfy the empirical distribution of samples. For a dataset with measurement samples, the discriminator needs to be fixed optimally. If the probability of blocking a pixel is less than 1, there exists a unique distribution. Theorem 5.4 states that if the probability of blocking a pixel is less than 1, there exists a unique distribution for the Block-Pixels measurement model. This is proven by applying a discrete distribution to random measurement functions. In the Block-Pixels model, a discrete distribution is applied to random measurement functions. A transition matrix A is used to relate the distributions p x and p y of measurements. If A is invertible, p x can be recovered from p y. The sample complexity is then considered. The distribution over measurements can be written in terms of a matrix A, allowing recovery of p x from p y if A is invertible. The sample complexity is determined by the minimum eigenvalue magnitude of A, with the formula s = t^2 / (2\u03bb^2) * 2 log(2t/\u03b4) derived using union bound and Chernoff inequalities. The sample complexity is determined by the minimum eigenvalue magnitude of matrix A, denoted as \u03bb. The dataset of measurements is denoted as {y1, y2, ... ys}. For any > 0, the formula s = t^2 / (2\u03bb^2) * 2 log(2t/\u03b4) is derived using union bound and Chernoff inequalities. The optimal generator must satisfy p, and with probability \u2265 1 \u2212 \u03b4, it equals . In the specific case of Block-Pixels measurement, images are divided into n + 1 classes based on the number of zero pixels they have. In the specific case of Block-Pixels measurement, images are divided into classes based on the number of zero pixels they have. The transition matrix A is lower triangular, ensuring no image in a class can produce another image in the same class after measurements. The transition matrix A for Block-Pixels measurement is lower triangular, ensuring no image in a class can produce another image in the same class after measurements. Each image has at least (1 \u2212 p) n chance of being unaffected by the measurements, forming diagonal entries in the transition matrix. The transition matrix A for Block-Pixels measurement is lower triangular. Every image has a (1 \u2212 p) n chance of being unaffected by measurements, forming diagonal entries in the transition matrix. The diagonal entries are strictly positive with a minimum value of (1 \u2212 p) n, proving A is invertible with the smallest eigenvalue being (1 \u2212 p) n. The DCGAN model on MNIST uses a transition matrix A that is lower triangular. The diagonal entries of A are strictly positive with a minimum value of (1 \u2212 p) n. The model's architecture includes a generator with 100-dimensional noise input sampled from a Uniform distribution, and a discriminator with convolutional layers. The DCGAN model on MNIST uses a generator with 100-dimensional noise input sampled from a Uniform distribution and a discriminator with convolutional layers. The architecture includes linear and deconvolutional layers, with labels concatenated at each step. Batch-norm is applied in both generator and discriminator. The WGANGP model on MNIST uses a generator with a 128-dimensional latent vector sampled from a Uniform distribution. The discriminator in the WGANGP model on MNIST uses two convolutional layers and two linear layers, with labels concatenated at each step. Batch-norm is applied in both the generator and discriminator. The generator takes a 128-dimensional latent vector sampled from a Uniform distribution and applies one linear layer and three deconvolutional layers. The discriminator in the unconditional DCGAN model on celebA uses three convolutional layers and one linear layer, without batch-norm. The latent vector for celebA has 100 dimensions sampled from a Uniform distribution. The ACWGANGP model on CIFAR-10 follows a residual architecture with a latent vector of 128 dimensions sampled from a Uniform distribution. The generator uses one linear layer and four deconvolutional layers, while the discriminator employs four convolutional layers and a linear layer with batch-norm applied in both. The ACWGANGP model on CIFAR-10 utilizes a residual architecture with a latent vector of 128 dimensions sampled from a standard Gaussian distribution. The generator consists of a linear layer followed by three residual blocks, each containing conditional batch normalization, nonlinearity, and upconvolution layers. The generator in the ACWGANGP model on CIFAR-10 has 128 dimensions with coordinates sampled from a standard Gaussian distribution. It includes a linear layer, three residual blocks with conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator has one residual block with two convolutional layers, followed by three more residual blocks and a final linear layer. Additional results for various measurement models are presented. The discriminator in the ACWGANGP model consists of one residual block with two convolutional layers, followed by three more residual blocks and a final linear layer. The current analysis explores scenarios where the parameter distribution is only approximately known, aiming for robust training processes. In the ACWGANGP model, the discriminator includes residual blocks and convolutional layers. The study investigates scenarios with approximately known parameter distributions for robust training. The AmbientGAN approach is shown to be robust to mismatches in parameter distributions when using the Block-Pixels measurement model on the MNIST dataset. The AmbientGAN approach is robust to systematic mismatches in parameter distributions when using the Block-Pixels measurement model on the MNIST dataset. The experiment demonstrates the quality of the learned generator, even when the parameter distribution is not exactly known. The AmbientGAN approach is robust to parameter distribution mismatches when using the Block-Pixels measurement model on the MNIST dataset. The generator learned through this approach captures the data distribution well, as shown by the peak in the inception score at p = 0.5. The AmbientGAN approach is robust to parameter distribution mismatches when using the Block-Pixels measurement model on the MNIST dataset. The generator learned through this approach captures the data distribution well. Generative models have been shown to improve sensing over sparsity-based approaches. The GAN learned using this procedure was used for compressed sensing, showing a plot of reconstruction error. Using the AmbientGAN approach with Block-Pixels measurement model on MNIST dataset, a plot comparing Lasso with AmbientGAN shows a reduction in the number of measurements for compressed sensing. The generator trained with corrupted samples outperformed a regular GAN trained with fully observed samples."
}