{
    "title": "HJcjQTJ0W",
    "content": "Massive data on user local platforms cannot support deep neural network (DNN) training due to resource constraints. Cloud-based training poses privacy risks from excessive data collection. To enable cloud-based DNN training while protecting data privacy, we propose splitting DNNs and deploying them separately on local platforms and the cloud. Local NN generates feature representations without local training to protect data privacy. Cloud NN is utilized for training. To enable cloud-based DNN training while protecting data privacy, the proposal suggests splitting DNNs and deploying them separately on local platforms and the cloud. The local NN generates feature representations without local training to safeguard data privacy, while the cloud NN is used for training based on the extracted intermediate representations. The idea is validated by analyzing the relationship between privacy loss and classification accuracy on the local NN topology for an image classification task. PrivyNet is proposed to optimize accuracy for target learning tasks while considering privacy loss, local computation, and storage constraints. It is demonstrated with the CIFAR-10 dataset, showing efficiency and effectiveness in handling massive data generated by sensors, cameras, and mobile devices. Based on the characterization, PrivyNet is proposed to optimize accuracy for target learning tasks while considering privacy loss, local computation, and storage constraints. It is demonstrated with the CIFAR-10 dataset, showing efficiency and effectiveness in handling massive data generated by sensors, cameras, and mobile devices. Cloud-based services offer an alternative for deep model training but rely on excessive user data collection, which poses privacy concerns. Cloud-based services provide an alternative for deep model training but rely on excessive user data collection, posing privacy concerns. To protect user data privacy, different data pre-processing schemes are proposed, where transformed representations are generated locally and uploaded for learning tasks. To enable cloud-based training while protecting user data privacy, data pre-processing schemes generate transformed representations locally for learning tasks. These representations must meet utility and privacy requirements, ensuring accurate task completion without leaking private information. The intermediate representations for cloud-based training must meet utility and privacy requirements, ensuring accurate task completion without leaking private information. The transformation scheme should be flexible for different platforms and data types. The privacy and utility trade-off is a key question in privacy research. The transformation scheme for cloud-based training must meet utility and privacy requirements for different platforms and data types. Various measures of privacy and utility have been proposed to explore the trade-off between them. Different measures of privacy and utility have been proposed based on rate-distortion theory, statistical estimation, and learnability. Various transformations, such as syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness, have been suggested to protect sensitive attributes in databases. However, applying syntactic anonymization to high-dimensional continuous data is challenging due to difficulties in defining quasi-identifiers and sensitive attributes. Differential privacy offers a formal privacy guarantee by adding noise to data, but it does not limit total information leakage from released representations. Differential privacy provides a formal privacy guarantee by adding noise to data, but it does not limit total information leakage from released representations. Existing works often require local platforms to be involved in the backward propagation process, making deployment on lightweight platforms challenging. Non-invertible linear and non-linear transformations are also proposed for data anonymization. Existing works in differential privacy often involve local platforms in the backward propagation process, hindering deployment on lightweight platforms. Non-invertible linear and non-linear transformations are proposed for data anonymization, with linear transformations relying on covariance or linear discriminant analysis for filtering training data. However, linear transformations may lack sufficient privacy protection as original data can be reconstructed from released representations. Recent nonlinear transformations based on minimax filter offer an alternative approach. The existing linear transformations for data anonymization lack sufficient privacy protection as original data can be reconstructed from released representations. Nonlinear transformations like minimax filter or Siamese networks offer better privacy protection but require an iterative training scheme between cloud and local platforms. The proposed PrivyNet framework offers a flexible DNN training approach for fine-grained control of privacy and utility trade-off. It utilizes nonlinear transformations like minimax filter or Siamese networks for better privacy protection in the inference stage, requiring an iterative training scheme between cloud and local platforms. PrivyNet is a DNN training framework that divides the model into local and cloud parts for privacy and utility trade-off control. The local NN generates intermediate representations, while the cloud NN is trained based on these representations, ensuring privacy protection through transformation. PrivyNet divides a DNN model into local and cloud parts for privacy protection. The local NN generates intermediate representations using non-linear operations like convolution and pooling. The cloud NN is trained based on these representations to avoid local training. Initial layers of a DNN extract general features that are not application specific. The local NN, derived from pre-trained NNs, utilizes non-linear operations like convolution and pooling to generate intermediate representations. By extracting general features from the initial layers of a DNN, useful features can be embedded in the released representations while privacy is protected by controlling the specific features to release. PrivyNet is a framework that splits DNN models for cloud-based training with fine-grained privacy control. It characterizes privacy loss and utility using CNN as the local NN, identifying key factors for the trade-off. PrivyNet is a novel framework that splits DNN models for cloud-based training with fine-grained privacy control. It characterizes privacy loss and utility using CNN as the local NN, identifying key factors for the trade-off and proposing a hierarchical strategy to optimize utility. The framework is validated using CNN-based image classification, demonstrating efficiency and effectiveness. A hierarchical strategy is proposed to determine the topology of the local NN to optimize utility considering constraints on local computation, storage, and privacy loss. PrivyNet is validated using CNN-based image classification, demonstrating efficiency and effectiveness in leveraging pre-trained NN for intermediate representation generation. The idea of using pre-trained neural networks for generating intermediate representations is validated through a detailed utility and privacy analysis. A CNN-based image classification example is used to illustrate the process. Feature representations are generated by a feature extraction network (FEN) from original data, which is then used to train an image classification network (ICN) for the target learning task. The utility is measured by the accuracy of the learning task, while privacy is assessed by the distance metric. An image classification network (ICN) is trained using feature representations from a pre-trained feature extraction network (FEN). An image reconstruction network (IRN) is also trained to reconstruct original images from the features. Privacy is measured by the distance between reconstructed and original images, assuming the transformation (FEN) is unknown. This aligns with an adversarial model described in later sections. The IRN is trained to reconstruct original images from known feature representations, measuring privacy by the distance between the reconstructed and original images. The adversarial model is detailed in later sections. The text describes the transformation induced by the FEN on training instances, with details on the label indicator vector and output feature representations. The depth of the output feature representations is D, with dimensions W \u00d7 H. The transformation is parameterized by the number of FEN layers and selected filters for each layer. The transformation induced by the FEN is parameterized by the number of layers and selected filters. The output feature representations have a depth of D and dimensions W \u00d7 H. The utility is evaluated by learning a classifier with minimized empirical risk for the target learning task. The utility of transformed representations is evaluated by learning a classifier with minimized empirical risk for the target learning task. The accuracy achieved by the classifier indicates the utility, with better accuracy implying better utility. The utility of transformed representations is evaluated by the accuracy achieved by a classifier, with better accuracy indicating better utility. Privacy is evaluated by minimizing the distance between reconstructed images and original images, measured by the peak signal-to-noise ratio (PSNR). The model minimizes the distance between reconstructed and original images using a loss function based on pixelwise Euclidean distance. Privacy loss is measured by the peak signal-to-noise ratio (PSNR) of the reconstructed images. Larger PSNR implies larger privacy loss. The impact of FEN topology on privacy and utility of transformed representations is characterized. The FEN topology's impact on privacy and utility of transformed representations is characterized by using CNN to construct h for image classification and g for reconstruction. The FEN is derived from VGG16 pre-trained on Imagenet dataset, with architectures of VGG16, ICN, and IRN shown in detail in Appendix B. The FEN topology is detailed in Appendix B, derived from VGG16 pre-trained on Imagenet. CNN constructs h for image classification and g for reconstruction. Evaluation of factors like number of layers and output depth forms the basis for PrivyNet framework. The PrivyNet framework evaluates the impact of the number of FEN layers and output depth on utility and privacy. Changes in these factors affect both utility and privacy differently, as shown in FIG2. The FEN topology impacts utility and privacy differently. Privacy loss is reduced with fewer FEN layers or increased output depth, while utility degradation is minimal with fewer FEN layers and larger output depth. The trade-off between accuracy and privacy loss is shown in FIG2 (c), where smaller PSNR of reconstructed images indicates less privacy loss with reduced output depth or increased FEN layers. Accuracy remains stable with larger output depth or fewer FEN layers, but significant degradation occurs with a large number of FEN layers and small output depth. When the number of FEN layers is large and the output depth is small, there is a significant accuracy degradation. The trade-off between accuracy and PSNR is illustrated in FIG2 (c), showing that FEN with different topologies have similar utility when privacy loss is high, while FEN with more layers provide better utility when privacy loss is low. The PrivyNet framework observes that FEN with different topologies have similar utility when privacy loss is high, but FEN with more layers provide better utility when privacy loss is low. The number of FEN layers, output channel depth, and selected subset of output channels impact privacy and utility. Comparing utility and privacy loss for transformed representations induced by each single channel helps understand the impact of channel selection. The selected subset of output channels, along with the number of FEN layers and output channel depth, affects the privacy and utility of output representations. Comparing utility and privacy loss for representations generated by each channel helps understand the impact of channel selection. When the FEN consists of 4 VGG16 layers, the best channel achieves around 4 times the utility of the worst channel. When the FEN consists of 4 VGG16 layers, the utility achieved by the best channel is around 4 times that of the worst channel, with a privacy loss difference of 6 dB. Similar discrepancies are observed with 6 VGG16 layers. The privacy loss for the best channel is 6 dB less compared to the worst channel. Large differences are observed when using 6 VGG16 layers to generate the FEN. Output channel selection is compared with the number of FEN layers and output depth. Different sets of output channels are evaluated for privacy and utility. The impact of output channel selection, number of FEN layers, and output depth on privacy and utility is compared using VGG16 layers. Results show that privacy and utility depend more on FEN layers and output depth than on output channel selection. The privacy and utility of representations generated from pre-trained CNN-based transformation can be controlled by the number of FEN layers and output channel depth. Leveraging the pre-trained CNN allows for exploring the trade-off between utility and privacy by adjusting the FEN topology. Based on the representations generated from the pre-trained CNN-based transformation, key observations include leveraging the CNN to build the FEN and exploring the trade-off between utility and privacy by adjusting the FEN topology. The privacy and accuracy trade-off can be controlled by the number of FEN layers, output channel depth, and output channel selection, with larger dependence on the first two factors for both privacy and utility. This leads to the proposal of PrivyNet framework in the next section to optimize utility under privacy constraints. In the next section, the framework PrivyNet is proposed to optimize utility under privacy constraints by determining the FEN topology. The trade-off between privacy and utility can be controlled by adjusting factors such as the number of FEN layers and output channel depth. PrivyNet framework is designed to optimize utility under privacy constraints by determining the FEN topology, considering local computation and storage constraints. The trade-off between privacy and utility can be controlled by adjusting factors such as the number of FEN layers and output channel depth. The PrivyNet framework optimizes utility under privacy constraints by determining FEN topology, considering local computation and storage constraints. Privacy characterization is done using cloud-based services, and NN performance profiling is conducted on local platforms to determine the FEN's number of layers and output depth. The PrivyNet framework optimizes utility under privacy constraints by determining FEN topology through privacy characterization and performance profiling. A supervised channel pruning step is then conducted based on private data to remove ineffective channels, considering local computation and storage constraints. The FEN output depth is determined considering privacy, local computation capability, and storage constraints. A supervised channel pruning step removes ineffective channels based on private data. Output channels are randomly selected to determine the FEN topology, assuming availability of original images for worst-case privacy evaluation. The FEN topology is determined based on selected output channels, assuming availability of original images for worst-case privacy evaluation. The assumption of original image availability is crucial for assessing privacy loss induced by releasing feature representations. In our adversarial model, we assume the attackers do not know the transformation induced by the FEN, derived from pre-trained NNs. This anonymity is crucial to limit privacy loss in released representations. The attackers may inject images into a database to obtain corresponding FEN-generated representations, making it difficult to evaluate and restrict privacy loss. To protect anonymity and limit privacy loss, it is crucial to safeguard the Feature Extraction Network (FEN) from attackers who may have access to the architecture and weights of pre-trained NNs. Detailed steps for FEN anonymity protection are outlined in Section 4, including performance and storage profiling on local and cloud platforms. The pre-characterization stage involves performance and storage profiling on local platforms and cloud-based privacy characterization for pre-trained NNs. Different platforms have varying computation capability and storage configurations, which impact the FEN topology. Privacy characterization for pre-trained NNs is detailed in Section 2. The privacy characterization of pre-trained NNs involves profiling on local platforms and leveraging cloud-based services. The reconstruction network is trained on publicly available data of the same dimension and distribution. Verification is done by characterizing privacy for different datasets like CIFAR-10 and CIFAR-100. The reconstruction network for NNs is trained on publicly available data of the same dimension and distribution. Privacy characterization is done for different datasets like CIFAR-10 and CIFAR-100, with experiments showing similar results for FEN with different topologies. Less than 1000 samples are needed for characterization with data augmentation. Experiments were conducted on CIFAR-10 and CIFAR-100 datasets to compare PSNR for FEN with different topologies. Less than 1000 samples are needed for an accurate characterization with data augmentation. Detailed PSNR values can be less accurate for privacy characterization, reducing the training sample requirement. The focus now is on determining the topology for the FEN. In PrivyNet, the topology for the FEN is determined based on the impact of FEN layers and output channel depth on privacy and accuracy. This decision is made considering constraints on local computation and results from pre-characterization. Less than 1000 samples are needed for accurate characterization, with detailed PSNR values being less critical for privacy assessment. In PrivyNet, the FEN topology is determined based on the impact of FEN layers and output channel depth on privacy and accuracy. This decision considers constraints on local computation and results from pre-characterization. The relation between privacy, local computation, and storage on a mobile class CPU is shown in Figure 8, guiding the strategy for high privacy requirements. Based on the impact of FEN layers and output channel depth on privacy and accuracy, the FEN topology is determined in PrivyNet. The strategy for high privacy requirements involves selecting the deepest layer for the FEN while considering constraints on computation and storage. The output depth is then determined by the privacy constraints. For high privacy requirements, the deepest layer of the FEN is selected in PrivyNet, with output depth determined by privacy constraints. In contrast, for low privacy requirements, a shallow FEN is chosen to achieve the desired privacy level, with output channel depth adjusted accordingly. This approach minimizes local computation and storage consumption. PSNR is used to select a shallow FEN for achieving the required privacy level, with output channel depth adjusted accordingly. This minimizes local computation and storage consumption based on the allowed PSNR levels. After determining the number of layers and output depth for privacy requirements based on PSNR levels, output channel selection is crucial for balancing utility and privacy. Large discrepancies in utility and privacy are observed for a single channel. Output:\nOutput channel selection is crucial for balancing utility and privacy in FENs. Large variances in utility and privacy are observed when selecting output channels directly from the whole set. This may result in poor utility with significant privacy leakage. When selecting output channels in FENs, there is a significant impact on utility and privacy. Directly choosing from the whole set can lead to poor utility with high privacy leakage. Additionally, the correlation between utility and privacy loss for a single channel is shown to be negligible. In channel pruning for FENs, the correlation between utility and privacy loss for a single channel is negligible. This allows for optimizing utility while suppressing privacy loss simultaneously. The process considers both utility and privacy in selecting output channels. From the largest privacy loss, 4 channels are among the 32 channels with the worst utility. Negligible correlation is observed for different output channel depths and FEN layers, enabling optimization of utility while suppressing privacy loss. Channel pruning considers both utility and privacy, with Fisher's LDA used to identify channels with the worst utility. The privacy loss for each channel can be acquired from offline pre-characterization to prune channels with the largest privacy loss. Fisher's LDA is used to identify channels with the worst utility based on distance measurements in representations. Fisher's LDA scheme measures the distance between representations of images within the same class and different classes using the covariance matrix. It is a good criterion for identifying ineffective channels based on distance measurements in representations. Fisher's LDA scheme measures distance between image representations in the same and different classes using the covariance matrix. It is effective for identifying ineffective channels based on distance measurements in representations. The notations and calculations for Fisher's linear discriminability are detailed for evaluating the effectiveness of output channels. Fisher's LDA scheme evaluates the effectiveness of output channels by measuring distance between image representations in the same and different classes using covariance matrices. The maximum discriminability value is achieved when the eigenvector corresponding to the largest eigenvalue of S w S b is used. Fisher's linear discriminability is computed using between-class and within-class variance. The maximum value is achieved with the eigenvector corresponding to the largest eigenvalue of S w S b. Evaluating Fisher's discriminability helps identify channels with poor utility for pruning, improving accuracy. The effectiveness of LDA-based supervised channel pruning is verified in experiments. The effectiveness of LDA-based supervised channel pruning algorithm is verified by determining channels with poor utility using Fisher's discriminability. The experimental setup involves using the first 6 VGG16 layers to prune 32 output channels with the worst utility. Using Fisher's discriminability, the LDA-based supervised pruning method prunes 69.7% of the worst 32 channels on average, compared to only 50.3% with random pruning. This results in a 33.5% reduction in the probability of selecting a bad channel randomly. When 64 channels are pruned, our method allows for pruning 69.7% of the worst 32 channels on average, compared to only 50.3% with random pruning. This results in a 33.5% reduction in the probability of selecting a bad channel randomly. Additionally, similar results are observed when pruning the 64 channels with the worst utility. The LDA-based pruning process shows that changing the mini-batch number does not significantly affect the pruning rate for both 32 and 64 channels. The computation complexity scales with the number of samples, but the extra computation introduced is minimal. The experimental results show that the computation complexity of the LDA-based pruning process is minimal and scales with the number of samples. The effectiveness of supervised channel pruning is demonstrated by setting the layer of FEN to 6 and the output depth to 8. Three settings for comparing privacy and utility are random selection, channel pruning based on privacy and utility characterization results. In supervised channel pruning, the layer of FEN is set to 6 and output depth to 8. Three settings for comparing privacy and utility are random selection, channel pruning based on privacy and utility characterization results. Pruning process involves removing 64 channels with worst utility and 32 channels with largest privacy loss. Channel pruning involves removing 64 channels with the worst utility and 32 channels with the largest privacy loss. Results show that random selection without pruning has higher average PSNR compared to pruning based on privacy and utility characterization. After conducting experiments with random selection for each setting, the results in FIG2 (a) show that pruning can lead to better utility and less privacy leakage. The LDA-based pruning strategy (the third setting) achieves on average 1.1% better accuracy and 1.25 dB compared to random selection without pruning. Detailed statistics are listed in Table 13 (b). Our supervised pruning strategy, as shown in Table 13 (b), demonstrates better accuracy and smaller privacy loss compared to random selection without pruning. The LDA-based pruning strategy achieves 1.1% better accuracy and 1.25 dB smaller PSNR. Additionally, our method shows similar accuracy to the characterization-based pruning strategy but with slightly less privacy loss. This confirms the effectiveness of our approach. Our supervised pruning strategy, compared to random selection without pruning, demonstrates better accuracy and smaller privacy loss. The LDA-based pruning strategy achieves 1.1% better accuracy and 1.25 dB smaller PSNR. Our method shows similar accuracy to the characterization-based pruning strategy but with slightly less privacy loss, confirming the effectiveness of our approach. The adversarial model adopted in the paper is discussed in detail in this section. In this section, a comparison is made for three settings: random selection without pruning, random selection after pure characterization-based pruning, and random selection after LDA-based pruning. The adversarial model adopted in the paper assumes the transformation induced by the FEN is unknown to attackers, providing better privacy protection. However, strategies are needed to protect the anonymity of the FEN derived from pre-trained NNs, whose structure and weights are available to attackers. In Section 3, the FEN transformation is assumed unknown to attackers for better privacy protection. Strategies are needed to protect the anonymity of the FEN derived from pre-trained NNs. One method is to build a pool of pre-trained NNs like VGG16, VGG19, ResNet, Inception, making it harder for attackers. In the framework, two methods are considered to protect the anonymity of the FEN: building a pool of pre-trained NNs like VGG16, VGG19, ResNet, Inception, and applying channel selection to output and intermediate channels to make it harder for attackers to guess how the FEN is derived. By enlarging the pool of pre-trained NNs like VGG16, VGG19, ResNet, Inception, it becomes harder for attackers to guess how the FEN is derived. Applying channel selection to output and intermediate channels further increases the difficulty for attackers to identify the channels forming the FEN. The privacy and utility of the intermediate channel selection are verified empirically, ensuring that utility is not sacrificed and privacy loss is not increased. In verifying the privacy and utility of intermediate channel selection, the depth of output channel is set to 8 using the first 6 layers of VGG16. The channel depth of the first convolution layer is gradually reduced from 64 to 16, showing minimal impact on privacy and utility. After gradually reducing the channel depth of each convolution layer, privacy and utility remain similar with a significant decrease in runtime. After reducing the channel depth for each convolution layer, privacy and utility are maintained with a significant decrease in runtime. Channel selection for intermediate layers enhances anonymity protection in PrivyNet, a flexible framework. PrivyNet is a flexible framework designed for cloud-based training with fine-grained privacy protection. It ensures anonymity by allowing channel selection for intermediate layers, making it difficult for attackers to determine the structure of the network. This framework is beneficial for resource-constrained platforms like modern hospitals that require detailed patient information. PrivyNet is a framework for cloud-based training with privacy protection, beneficial for resource-constrained platforms like hospitals. It allows hospitals to release informative features instead of original patient data for disease diagnosis and treatment. PrivyNet offers a framework for hospitals to release informative features from patient data for disease diagnosis and treatment while protecting privacy. It is also useful for mobile platforms to collect and analyze health-related information. PrivyNet is a simple, platform-aware framework that enables mobile platforms to upload collected data to the cloud while protecting private information. It is generally applicable for different end-users in various situations. PrivyNet is a lightweight framework that allows mobile platforms to upload data to the cloud while protecting private information. It offers fine-grained privacy control and is applicable for different end-users. The CIFAR-10 dataset contains 60000 color images in 10 classes, while CIFAR-100 has images of objects in 100 classes. The CIFAR-10 dataset has 60000 color images in 10 classes, with 50000 training images and 10000 test images. CIFAR-100 consists of images in 100 classes, with 600 images per class. VGG16, pre-trained on ImageNet, is used for privacy and accuracy characterization. The CIFAR-100 dataset contains 600 images per class across 100 classes. VGG16, pre-trained on ImageNet, is utilized for privacy and accuracy analysis. CNN is used for image classification (h) and a generative NN architecture based on ResNet blocks is used for image reconstruction (g). In image classification, CNN is used to construct h, while a generative NN architecture based on ResNet blocks is used for image reconstruction as g. The ResNet blocks cluster consists of 8 blocks each, following the structure shown in Figure 18. The training process utilizes a gradient descent optimizer. In image reconstruction tasks, an image IRN is constructed using ResNet blocks. Each ResNet block cluster contains 8 blocks. The training process uses a gradient descent optimizer with a learning rate of 0.003 and a mini-batch size of 128 for 100 epochs. For image classification, the initial learning rate is 0.05 with a mini-batch size of 128. The learning rate decreases by a factor of 0.1 at 100 and 200 epochs. For image reconstruction, the IRN is trained with a learning rate of 0.003 and a mini-batch size of 128 for 100 epochs. For image classification, the initial learning rate is 0.05 with a mini-batch size of 128, decreasing by a factor of 0.1 at 100 and 200 epochs for a total of 250 epochs. Data augmentation includes normalization, brightness, and contrast adjustments following Tensorflow example. Topology of the IRN is determined before characterization for accurate privacy evaluation. The IRN is trained for 100 epochs with a learning rate of 0.003 and a mini-batch size of 128 for image reconstruction. Data augmentation involves normalization, brightness, and contrast adjustments. The topology of the IRN is determined before characterization to ensure accurate privacy evaluation. Image recovery capability is influenced by the number of ResNet block clusters. PSNR of reconstructed images saturates with increasing ResNet block clusters. In image reconstruction experiments, the number of ResNet block clusters determines the image recovery capability of IRN. The quality of reconstructed images saturates as the number of clusters increases. For the experiments, 2 ResNet block clusters with 8 blocks each were chosen. Performance and storage characterization were conducted by profiling different pre-trained NNs. In experiments, 2 ResNet block clusters with 8 blocks each were chosen to analyze image reconstruction. Performance profiling of VGG16 on different CPUs and storage requirements with increasing VGG16 layers were also examined. The results show that as the number of VGG16 layers increases, local computation and storage needs rise rapidly. In experiments, VGG16 performance profiling on different CPUs and storage requirements with increasing layers was analyzed. The increase in VGG16 layers led to a rapid rise in local computation and storage needs, with convolution layers contributing to most of the computation and fully connected layers to storage. The increase in VGG16 layers results in a rapid rise in local computation and storage needs. Convolution layers contribute to most of the computation, while fully connected layers account for a significant portion of storage, especially with larger input image sizes. Different platforms may have varying bottlenecks, and runtime differences highlight the need for a flexible framework considering local computation and storage differences. The complexity of the second part of the computation is determined by the number of samples and the output dimensions. The complexity for different calculations varies, with O(KW^2H^2) for Sb, O(NLDAW^2H^2) for Sw, and O(W^3H^3) for W^-1 and the largest eigenvalue of W^-1B. The complexity of the second part of the computation is mainly determined by the number of samples N LDA and the output dimensions W \u00d7 H. The complexity varies for different calculations, with O(KW^2H^2) for Sb, O(NLDAW^2H^2) for Sw, and O(W^3H^3) for W^-1 and the largest eigenvalue of W^-1B. The overall complexity is O((K + NLDA)W^2H^2 + W^3H^3), where NLDA plays a key role in determining the extra computation needed. The complexity of the second part of the computation is O((K + N LDA)W^2H^2 + W^3H^3), with N LDA being a key factor determining additional computation. Small N LDA typically suffices for good pruning results, resulting in minimal overall computation overhead."
}