{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution in the spectral domain. They outperform WaveNet baselines on automated and human evaluation metrics, generating audio much faster than autoregressive models. Neural audio synthesis involves training generative models to produce high-fidelity audio with global structure, requiring modeling temporal scales over a wide range. Autoregressive models like WaveNet have made significant advancements but suffer from slow sampling speed due to generating waveforms one sample at a time. WaveNet models focus on the finest scale possible for audio generation, relying on external conditioning signals for global structure. However, they have slow sampling speed and require methods like training secondary networks to speed up generation. Autoencoder variants of these models can only model local latent structure due to memory constraints. Generative Adversarial Networks (GANs) have been successful in generating high resolution images efficiently by conditioning transposed convolutions on a latent vector. This approach contrasts with WaveNet models, which focus on fine-scale audio generation and rely on external conditioning signals for global structure. However, WaveNet models have slower sampling speeds and may require training secondary networks to speed up generation. Generative Adversarial Networks (GANs) have achieved success in generating high resolution images efficiently by conditioning transposed convolutions on a latent vector. While WaveNet models focus on fine-scale audio generation, they may require external conditioning signals for global structure and have slower sampling speeds. Attempts to adapt image GAN architectures for audio waveform generation have not reached the same level of perceptual fidelity. Attempts to adapt image GAN architectures for audio waveform generation have not reached the same level of perceptual fidelity as their image counterparts. Frame-based techniques, such as transposed convolutions or STFTs, have a given frame size and stride for audio waveform estimation. Frame-based techniques for audio waveform estimation, like transposed convolutions or STFTs, struggle with aligning audio periodicity and output stride, impacting the fidelity of generated audio compared to images. Transposed convolutional filters face challenges in covering all necessary frequencies and phase alignments to preserve coherence. For STFT, unwrapping phase over 2\u03c0 boundary and taking its derivative gives instantaneous radial frequency, showing the constant relationship between audio and frame frequency. The STFT method involves unwrapping phase over a 2\u03c0 boundary to obtain instantaneous radial frequency, showcasing the relationship between audio and frame frequency. GAN researchers have made significant progress in image modeling by evaluating models on focused datasets with limited degrees of freedom. The NSynth dataset was introduced with the motivation to focus on audio modeling, similar to how GAN researchers made progress in image modeling by starting with limited datasets and gradually expanding to less constrained domains. The dataset contains spectra for a trumpet note and aims to improve the generation of realistic texture and fine-scale features in audio. The NSynth dataset, like CelebA for images, focuses on individual notes from musical instruments, aligned and cropped to enhance fine-scale details like timbre and fidelity. Each note is accompanied by attribute labels for further analysis. NSynth consists of individual notes from musical instruments with attributes for conditional generation. The dataset includes autoregressive WaveNet and bottleneck spectrogram autoencoders for analysis. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms, such as speech and music. It builds on previous efforts that have explored various approaches for conditional generation in NSynth, including autoregressive WaveNet and bottleneck spectrogram autoencoders. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms, focusing on maintaining the regularity of periodic signals. The text discusses the importance of maintaining the regularity of periodic signals in audio waveforms, as human perception is sensitive to discontinuities. Convolutional filters trained on this data form frequency selective filter banks, and the alignment of frames with a waveform's periodicity affects the phase over time. This poses a challenge for synthesis due to the presence of multiple frequencies in a signal. The alignment of frames with a waveform's periodicity affects phase over time, posing a challenge for synthesis networks to learn appropriate frequency and phase combinations for coherent waveform production, similar to short-time Fourier transform (STFT) observations. Phase precession is a challenge for synthesis networks as they must learn frequency and phase combinations to produce coherent waveforms. This phenomena is similar to short-time Fourier transform (STFT) observations. Another approach to generating coherent waveforms involves unwrapping the phase of a pure tone. Phase precession occurs in situations where filterbanks overlap, generating coherent waveforms inspired by the phase vocoder. Unwrapping the phase causes it to grow linearly, with the derivative representing the instantaneous angular frequency. This is a time-varying measure of the true signal. The unwrapped phase grows linearly, with the derivative representing the instantaneous angular frequency, a time-varying measure of the true signal oscillation. The pure harmonic frequencies of a trumpet cause the wrapped phase spectra to oscillate at different rates while the unwrapped phase smoothly diverges. The instantaneous frequency (IF) is a time-varying measure of the true signal oscillation. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Generating log-magnitude spectrograms and phases directly with GANs can produce more coherent waveforms than directly generating waveforms with strided. In this study, the interplay of architecture and representation in synthesizing coherent audio with GANs is explored. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherent audio, and the importance of preventing harmonics from overlapping. Increasing STFT frame size and switching to mel frequency scale can improve performance by creating more separation between lower harmonic frequencies. Estimating IF spectra leads to more coherent audio than estimating phase. Increasing STFT frame size and switching to mel frequency scale improve performance by creating more separation between lower harmonic frequencies. GANs can outperform WaveNet in generating audio examples on the NSynth dataset. Global conditioning on latent and pitch vectors allow GANs to generate smooth audio. The NSynth dataset contains 300,000 musical notes from 1,000 different instruments, with diverse timbres and pitches. GANs can generate examples faster than WaveNet and produce smooth audio with global conditioning on latent and pitch vectors. The NSynth dataset contains 300,000 musical notes from 1,000 different instruments, with diverse timbres and pitches. It is structured with labels for pitch, velocity, instrument, and acoustic qualities. Samples are four seconds long, sampled at 16kHz, and include human evaluations on audio quality. Training is restricted to acoustic instruments and fundamental pitches ranging from MIDI 24-84. The NSynth dataset contains 300,000 musical notes from 1,000 different instruments, structured with labels for pitch, velocity, instrument, and acoustic qualities. Samples are four seconds long, sampled at 16kHz. Training is restricted to acoustic instruments and fundamental pitches ranging from MIDI 24-84, resulting in 70,379 examples from strings, brass, woodwinds, and mallets. A new test/train 80/20 split was created from shuffled data to avoid the original split based on instrument type. The NSynth dataset contains 300,000 musical notes from 1,000 instruments, with samples restricted to acoustic instruments and pitches ranging from MIDI 24-84. A new test/train 80/20 split was created from shuffled data to avoid the original split based on instrument type. The model adapts progressive training methods to generate audio spectra. The model adapts progressive training methods from image generation to generate audio spectra by sampling a random vector and using transposed convolutions to upsample and generate output data. The discriminator network estimates the divergence measure between real and generated distributions. The model utilizes progressive training methods to generate audio spectra by sampling a random vector and using transposed convolutions. A discriminator network estimates the divergence measure between real and generated distributions, with the use of gradient penalty and pixel normalization for better convergence time and sample diversity. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector, aiming for independent control of pitch and timbre. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. This allows for independent control of pitch and timbre. Additionally, an auxiliary classification BID24 loss is added to the discriminator to predict the pitch label. Spectral representations are computed using TensorFlow's STFT implementation with specific parameters. The method involves adding pitch information to the latent vector for independent control of pitch and timbre. An auxiliary classification BID24 loss is used to predict the pitch label. Spectral representations are computed using TensorFlow's STFT with specific parameters, resulting in an \"image\" of size (256, 512, 2) representing magnitude and phase. The text discusses the process of creating an \"image\" with specific parameters for spectral representations, including magnitude and phase information. The magnitude is logged and scaled to match the generator network's nonlinearity, while the phase angle is also scaled. Additionally, there is an option to unwrap the phase angle and calculate the finite difference to create \"instantaneous frequency\" models. The phase angle and magnitude are scaled to match the generator network's nonlinearity. There is an option to unwrap the phase angle and calculate the finite difference to create \"instantaneous frequency\" models. Maintaining 75% overlap allows for doubling the STFT frame size and stride, resulting in high-frequency resolution variants. The performance is sensitive to frequency resolution at the lower range. Doubling the STFT frame size and stride with 75% overlap results in high-frequency resolution variants. Log magnitudes and instantaneous frequencies are transformed to a mel frequency scale without compression, known as \"IF-Mel\" variants. Converting back to linear STFTs using an approximate inverse linear transformation does not significantly harm audio quality. Comparisons are made against strong models. To compare against strong baselines, WaveGAN is adapted to accept pitch conditioning and retrained on a subset of the NSynth dataset. Independent training of waveform generating GANs achieves similar performance to WaveGAN without progressive training. WaveGAN, the current state of the art in waveform generation with GANs, is adapted to accept pitch conditioning and retrained on a subset of the NSynth dataset. Our own waveform generating GANs achieve similar performance to WaveGAN without progressive training. WaveNet is currently the state of the art in generative modeling of audio. WaveNet is the current state of the art in generative audio modeling, using a WaveNet autoencoder to interpolate between sounds on the NSynth dataset. The architecture is adapted to accept one-hot pitch conditioning signals for strong baselines. The 8-bit mu law model is more stable and outperforms the 16-bit mixture of logistics. The WaveNet architecture is adapted to accept one-hot pitch conditioning signals for strong baselines. The 8-bit mu law model is more stable and outperforms the 16-bit mixture of logistics in audio generation evaluation. The 8-bit model is more stable and outperforms the 16-bit model in audio generation evaluation. Evaluation of generative models is challenging due to the difficulty in formalizing goals, leading to heuristic evaluation metrics with blind spots. Human evaluation is used as a gold standard for audio quality assessment, as it is hard to automate. We evaluate models using diverse metrics to capture different aspects of performance. Human evaluation is used as a gold standard for audio quality assessment due to the sensitivity of human perception to phase irregularities. Amazon Mechanical Turk was used for comparison tests on all models in TAB0. The study used Amazon Mechanical Turk to compare models in TAB0 based on audio quality. Participants evaluated pairs of 4s examples on a Likert scale. 3600 ratings were collected, with each model involved in 800 comparisons. The study collected 3600 ratings from participants who evaluated pairs of 4s examples on audio quality using a Likert scale. The diversity of generated examples was measured using the Number of Statistically-Different Bins (NDB) metric, where examples were clustered into Voronoi cells and compared for statistical significance. The diversity of generated examples was measured using the NDB metric proposed by BID27, which clusters examples into Voronoi cells in log-spectrogram space. In addition, the Inception Score (IS) proposed by BID28 evaluates GANs by calculating the mean KL divergence between generated examples and a pretrained Inception classifier. The Inception Score (IS) is a metric for evaluating GANs by measuring the mean KL divergence between generated examples and a pretrained Inception classifier. It penalizes models with examples that are not easily classified into a single class or belong to only a few possible classes. The Inception Score (IS) measures the mean KL divergence between imageconditional output class probabilities and the marginal distribution. It penalizes models with examples that are not easily classified into a single class or belong to only a few possible classes. The metric replaces Inception features with features from a pitch classifier trained on spectrograms of an acoustic NSynth dataset. Additionally, Pitch Accuracy (PA) and Pitch Entropy (PE) are used to separately measure the accuracy of the pretrained pitch classifier. The Inception Score measures KL divergence between output class probabilities and the marginal distribution. It penalizes models with examples that are not easily classified. Features from a pitch classifier trained on spectrograms of an acoustic NSynth dataset are used. Pitch Accuracy (PA) and Pitch Entropy (PE) measure the accuracy of the pretrained pitch classifier. Fr\u00e9chet Inception Distance (FID) evaluates GANs based on the distance between multivariate Gaussians fit to features extracted from a pretrained Inception classifier. The Fr\u00e9chet Inception Distance (FID) metric evaluates GANs based on the distance between multivariate Gaussians fit to features extracted from a pretrained Inception classifier. It correlates with perceptual quality and diversity on synthetic distributions. The accuracy of a pretrained pitch classifier on generated examples (PA) and the entropy of its output distribution (PE) are also measured. Human evaluation shows a clear trend of decreasing quality. The Fr\u00e9chet Inception Distance (FID) metric correlates with perceptual quality and diversity on synthetic distributions. Results show that quality decreases as output representations move from IF-Mel to Waveform. The WaveNet baseline produces high-fidelity sounds but occasionally breaks down into feedback and self oscillation. The highest quality model, IF-Mel, was slightly inferior to real data. WaveNet baseline produces high-fidelity sounds but occasionally breaks down. Sample diversity correlates with audio quality, with high frequency resolution improving the NDB score. WaveNet baseline receives the worst NDB score. The NDB score is comparable to the IF GANs and correlates with sample diversity and audio quality. High frequency resolution improves the NDB score across model types. The WaveNet baseline receives the worst NDB score due to a lack of diversity in autoregressive sampling. FID provides a similar story to the first. The autoregressive sampling in the model tends to gravitate towards the same type of oscillation for each pitch conditioning, resulting in a lack of diversity. FID scores are significantly lower for models with high frequency resolution. Mel scaling has less impact on FID compared to listener studies. Phase models exhibit high FID, indicating poor sample quality. Classifier metrics show that many models perform well. Histograms in the appendix display peaky distributions for different models. FID scores are significantly lower for models with high frequency resolution. Mel scaling has less impact on FID compared to listener studies. Phase models exhibit high FID, indicating poor sample quality. Classifier metrics show that many models perform well on IS, Pitch Accuracy, and Pitch Entropy. High-resolution models generate examples classified with similar accuracy to real data. The classifier metrics of IS, Pitch Accuracy, and Pitch Entropy show that high-resolution models generate examples classified similarly to real data. However, due to distribution issues like mode collapse, there is little discriminative information to gain from differences among high scores. Low frequency models seem to have less reliable pitch generation. The metrics provide a rough measure of model reliability in generating classifiable pitches, with low frequency models and baselines showing less reliable pitch generation. Visualizing qualitative audio concepts is recommended, along with listening to accompanying audio examples for a better understanding. The WaveGAN and PhaseGAN models show phase irregularities in the waveform, creating a blurry web of lines, while the IFGAN model is more coherent with only small variations. Listen to accompanying audio examples for a better understanding. The WaveGAN and PhaseGAN models exhibit phase irregularities in the waveform, creating a blurry web of lines. In contrast, the IFGAN model shows more coherence with only small variations. The Rainbowgrams display coherent waveforms for real data and IF models, resulting in strong consistent colors for each harmonic, while PhaseGAN shows speckles due to phase discontinuities and WaveGAN is irregular. FIG1 visualizes the phase coherence of examples from different GAN variants. The Rainbowgrams show that the real data and IF models have coherent waveforms with consistent colors for each harmonic, while PhaseGAN has speckles due to phase discontinuities and WaveGAN is irregular. The WaveGAN and PhaseGAN models exhibit phase irregularities, while the IFGAN model shows more coherence with small variations. The Rainbowgrams depict the coherence of examples from different GAN variants. Real data and IF models show consistent waveforms, while PhaseGAN has phase discontinuities and WaveGAN is irregular. The visualization highlights clear phase coherence in real data and IFGAN, while PhaseGAN shows speckled noise and WaveGAN appears largely incoherent. The Rainbowgrams visualize the coherence of different GAN variants. Real data and IF models display clear phase coherence, PhaseGAN shows noise, and WaveGAN is largely incoherent. GANs allow conditioning on the same latent vector for the entire sequence, unlike WaveNet autoencoders which have limited scope. WaveNet autoencoders learn local latent codes for generation on a millisecond scale but have limited scope. Comparing interpolations between raw waveform, latent code, and global code of an IF-Mel GAN shows perceptual equivalence in mixing amplitudes. In Figure 4, a pretrained WaveNet autoencoder is compared to an IF-Mel GAN in terms of interpolating between examples in the raw waveform, latent code, and global code. WaveNet improves mixing in the space of timbre but struggles with complex prior on latents, resulting in intermediate sounds that tend to fall apart, oscillate, and whistle. WaveNet improves mixing in the space of timbre but struggles with a complex prior on latents, leading to intermediate sounds that tend to fall apart, oscillate, and whistle. The GAN model has a spherical gaussian prior which is decoded for global interpolation. The GAN model with a spherical gaussian prior allows for global interpolation, resulting in more realistic examples compared to WaveNet's local conditioning and lack of a compact prior. The IF-Mel GAN with global conditioning allows for smooth interpolation in perceptual attributes, producing high-fidelity audio examples. Spherical interpolation stays aligned with the prior, resulting in realistic sounds without additional feedback harmonics. Interpolating in perceptual attributes along the prior produces high-fidelity audio examples with smooth changes. Timbre morphs smoothly between instruments while pitches follow a musical example. In the audio examples, timbre morphs smoothly between instruments while pitches follow a musical example, demonstrating high-fidelity audio with smooth changes. In audio examples, timbre morphs smoothly between instruments while pitches follow a musical example, demonstrating high-fidelity audio with smooth changes. The timbral identity of the GAN remains largely intact, creating a unique instrument identity for the given point in latent space. The Bach prelude rendered with a single latent vector has a consistent harmonic structure across a range of pitches. The GAN maintains its timbral identity, creating a unique instrument identity in latent space. Using upsampling convolutions allows for parallel processing of training and generation, reducing latency on modern GPU hardware. The IF-Mel GAN allows for parallel processing of training and generation on modern GPU hardware, significantly reducing latency compared to WaveNet. This opens up the possibility for real-time neural audio synthesis. The IF-Mel GAN is around 53,880 times faster than the WaveNet baseline, reducing synthesis latency to 20 milliseconds. This allows for real-time neural network audio synthesis on device, expanding the range of expressive sounds available to users. Previous work on deep generative models for audio has focused on speech synthesis datasets with variable length conditioning. Real-time neural network audio synthesis on device offers a broader range of expressive sounds compared to speech synthesis models. Adapting GANs for variable-length conditioning in music generation remains a challenge for future research. In comparison to speech, audio generation for music is relatively under-explored. Previous work has focused on autoregressive models for synthesizing musical instrument sounds, but these models have slow generation times. While GANs have shown promising results in audio generation, they have not yet achieved the same level of audio fidelity as autoregressive models. Adaptation of GANs for variable-length conditioning in music generation is a challenging task for future research. Our work proposes a modification to GANs for audio generation, improving training stability and architectural robustness. This builds on recent advances in GAN literature, including progressive training for better results. Our work proposes a modification to GANs for audio generation, improving training stability and architectural robustness. It builds on recent advances in GAN literature, including progressive training for better results. The NSynth dataset was introduced as a \"CelebA of audio\" and used WaveNet autoencoders to interpolate between timbres of musical instruments. The NSynth dataset was first introduced as a \"CelebA of audio\" and used WaveNet autoencoders to interpolate between timbres of musical instruments. BID23 expanded on this work by incorporating an adversarial domain confusion loss to achieve timbre transformations between a wide range of audio sources. BID5 achieved significant sampling speedups by training a frame-based regression model to map from pitch and instrument labels to raw waveforms. BID5 achieved significant sampling speedups by training a frame-based regression model to map from pitch and instrument labels to raw waveforms, yielding good frequency estimation but lacking phase coherency and handling multimodal distributions efficiently. Their architecture requires a large number of channels, slowing down sample generation and training. By controlling the audio representation for generative modeling, high-quality audio generation with GANs on the NSynth dataset has been achieved, surpassing WaveNet baseline fidelity and generating samples much faster. This study marks a significant advancement in audio generation with GANs. This study demonstrates high-quality audio generation with GANs on the NSynth dataset, surpassing WaveNet baseline fidelity and generating samples much faster. Further research is needed to validate and expand this to other types of natural sound signals. This study focused on high-quality audio generation with GANs on a specific dataset, highlighting the need for further validation and expansion to different types of natural sound signals. Possible applications of adversarial losses to audio and addressing issues like mode collapse and diversity were also discussed. The study discussed the need for validation and expansion in high-quality audio generation with GANs, addressing issues like mode collapse and diversity. Further work is suggested to combine adversarial losses with encoders or regression losses for better data distribution capture. Training models with different parameters showed that a learning rate of 8e-4 and classifier loss of 10 performed the best across all variants. The models were trained with the ADAM optimizer and tested with different learning rates and classifier loss weights. A learning rate of 8e-4 and classifier loss of 10 performed the best for all variants. The networks used box upscaling/downscaling and pixel normalization. The discriminator also included the standard deviation of the minibatch. The models were trained with the ADAM optimizer using a learning rate of 8e-4 and classifier loss of 10, which performed the best. Both networks utilized box upscaling/downscaling and pixel normalization. The discriminator included the standard deviation of the minibatch activations. Additionally, a Tanh output nonlinearity was used for the generator, and real data was normalized before passing to the discriminator. The generator uses a Tanh output nonlinearity and real data is normalized before passing to the discriminator. Training each GAN variant takes 4.5 days on a single V100 GPU with a batch size of 8. For nonprogressive models, training is done on \u223c5M examples, while progressive models train on 1.6M examples per stage (7 stages). The GAN variants are trained for 4.5 days on a single V100 GPU with a batch size of 8. For nonprogressive models, training is done on \u223c5M examples, while progressive models train on 1.6M examples per stage (7 stages). The WaveNet baseline also uses a Tensorflow implementation with a decoder composed of 30 layers of dilated convolution. The GAN variants are trained for 4.5 days on a single V100 GPU with a batch size of 8. The progressive models train on \u223c11M examples, using a decoder with 30 layers of dilated convolution. The audio encoder stack is replaced with a conditioning stack operating on a one-hot pitch conditioning signal distributed in time. The model consists of 3 stacks of 10 layers each with increasing dilation rates, a conditioning stack with 5 layers of dilated convolution and 3 layers of regular convolution, all with 512 channels. The conditioning signal is passed through a 1x1 convolution for each layer of the decoder and added to the output. The conditioning stack in the model consists of 5 layers of dilated convolution and 3 layers of regular convolution, all with 512 channels. The conditioning signal is passed through a 1x1 convolution for each layer of the decoder and added to the output. The model uses mulaw encoding for the 8-bit model and a quantized mixture of 10 logistics for the 16-bit model. WaveNets converged to 150k iterations in 2 days with 32 V100 GPUs trained with synchronous SGD with batch size 1 per GPU."
}