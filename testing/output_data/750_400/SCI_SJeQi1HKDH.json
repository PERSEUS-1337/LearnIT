{
    "title": "SJeQi1HKDH",
    "content": "Animals develop new skills through environmental interaction and social influence. This study incorporates social influence into reinforcement learning, allowing agents to learn from peers. A metric is defined to measure policy distance and uniqueness. The Interior Policy Differentiation (IPD) algorithm encourages agents to learn unique policies while solving tasks. The Interior Policy Differentiation (IPD) algorithm enforces social uniqueness as a constraint for agents to learn distinct policies while solving tasks, leading to performance improvement and diverse behavior. Inspired by cognition and animal studies, Reinforcement Learning involves learning through interaction with the environment to maximize rewards. Policy Differentiation (IPD) in Reinforcement Learning aims to enhance performance by implementing a collection of policies with distinct behaviors. This approach is inspired by cognition and animal studies, focusing on learning through interaction with the environment to maximize rewards. The importance of behavioral diversity in ecology and species evolution is highlighted, with previous works exploring ways to promote diverse behaviors in RL. Behavioral diversity is crucial for species evolution and is a rising topic in Reinforcement Learning (RL). Previous works have focused on encouraging diverse behaviors in RL through interactive environments and different approaches. Heess et al. (2017) demonstrated that rich environments enable agents to learn various skills using standard RL algorithms. However, designing complex environments manually limits diversity. Two approaches to increase diversity in RL: designing rich environments for agents to learn different skills, and motivating agents to explore beyond maximizing rewards. Zhang et al. (2019) proposed maximizing a novelty metric between policies, but final performance is not guaranteed. In this work, the focus is on policy differentiation in RL to enhance agent diversity while maintaining task-solving abilities. Drawing inspiration from social influence in animal societies, the goal is to improve behavioral diversity beyond just maximizing rewards. The study addresses policy differentiation in RL to increase agent diversity while maintaining task-solving abilities. Inspired by social influence in animal societies, the concept of social influence is applied in the reinforcement learning paradigm. The target agent not only maximizes rewards but also differentiates its actions from other agents. The study introduces the concept of social influence in reinforcement learning to increase agent diversity. The target agent aims to maximize rewards while differentiating its actions from other agents. Social uniqueness motivation is implemented as a constrained optimization problem, with a focus on policy distance metric and optimization constraints. The curr_chunk discusses implementing social influence in reinforcement learning through social uniqueness motivation and an optimization constraint using a policy distance metric. The Interior Policy Differentiation (IPD) method is introduced to encourage agents to be different from each other. The curr_chunk introduces a novel method called Interior Policy Differentiation (IPD) for constrained policy optimization in reinforcement learning. It includes a metric for immediate feedback and encourages agents to take unique actions compared to other agents. The curr_chunk introduces a method called Variational Information Maximizing Exploration (VIME) for tackling sparse reward problems in reinforcement learning. It includes an intrinsic reward term based on maximizing exploration. The Variational Information Maximizing Exploration (VIME) method, designed to address sparse reward problems in reinforcement learning, adds an intrinsic reward term based on information gains to encourage exploration. Curiosity-driven methods define intrinsic rewards based on prediction errors of neural networks. In VIME, an intrinsic reward term based on information gains is added to RL algorithms to promote exploration. Curiosity-driven methods define rewards using prediction errors of neural networks. Burda et al. (2018b) introduced Random Network Distillation (RND) to quantify intrinsic reward through prediction differences between networks. In VIME, an intrinsic reward term based on information gains is added to RL algorithms to promote exploration. Burda et al. (2018b) proposed Random Network Distillation (RND) to quantify intrinsic reward by prediction differences between networks. Liu et al. (2019) introduced Competitive Experience Replay (CER) using two actors and a centralized critic to define intrinsic rewards based on state coincidence. The Task-Novelty Bisector (TNB) learning method, introduced by Zhang et al. (2019), aims to optimize the trade-off between external rewards and intrinsic rewards in reinforcement learning. It jointly optimizes extrinsic rewards and intrinsic rewards provided by different heuristics. The Task-Novelty Bisector (TNB) learning method, introduced by Zhang_ Zhang et al. (2019), aims to optimize the trade-off between external rewards and intrinsic rewards in reinforcement learning by jointly optimizing extrinsic and intrinsic rewards. However, the foundation of this joint optimization is not solid, and creating an extra intrinsic reward function and evaluating the novelty of states or policies are also important. The TNB learning method optimizes the trade-off between external and intrinsic rewards by updating the policy in the direction of the angular bisector of the two gradients. However, the foundation of joint optimization is not solid, requiring additional neural networks for evaluating novelty, leading to extra computation expenses. The DPPO method introduced by Heess et al. (2017) enables agents to learn complex locomotion skills in diverse environments. The policy learned impressive and effective skills for traveling terrains and obstacles, showing that rich environments can encourage different locomotion behaviors. The research by Such et al. (2018) demonstrates that different RL algorithms can lead to varying policies for the same task. Policy gradient algorithms tend to converge to a similar local optimum in Pitfall, while off-policy and value-based algorithms may learn differently. Rich environments can promote diverse locomotion behaviors, but designing such environments requires additional manual effort. In contrast to previous research by Such et al. (2018) on RL algorithms converging to different policies, this paper focuses on learning various policies through a single algorithm and avoiding local optima. Kurutach et al. (2018) maintain model uncertainty using data collected from the environment through an ensemble of deep learning algorithms. In this paper, the focus is on learning different policies through a single algorithm and avoiding local optima. Kurutach et al. (2018) maintain model uncertainty with data from the environment using deep neural networks. A metric is defined to measure policy differences, laying the foundation for the proposed algorithm. Learned policies are denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}, where \u03b8 i represents parameters of the i-th policy. In this paper, policies are learned through deep neural networks to encourage behavioral diversity in RL. A metric is defined to measure policy differences, denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}. The metric space is defined as an ordered pair (M, d) where M is a set and d is a metric on M. The Total Variance Divergence is used to measure the distance between policies in a metric space (\u0398, D \u03c1 T V). The Total Variance Divergence (D TV) is utilized to measure the distance between policies in a metric space (\u0398, D \u03c1 TV). It can be extended to continuous state and action spaces by replacing sums with integrals. The factor 1/2 in Schulman et al. (2015) is omitted for conciseness, aiming to motivate RL with social uniqueness. The Total Variance Divergence (D TV) measures the distance between policies in a metric space. It can be extended to continuous state and action spaces by using integrals. To motivate RL with social uniqueness, the factor 1/2 in Schulman et al. (2015) is omitted for conciseness. The method aims to maximize the uniqueness of a new policy by including all existing policies in the reference set \u0398 ref. Monte Carlo estimation is used to calculate D \u03c1 T V (\u03b8 i , \u03b8 j ) by sampling s from \u03c1(s). In practice, D \u03c1 T V (\u03b8 i , \u03b8 j ) is calculated using Monte Carlo estimation by sampling s from \u03c1(s). Challenges arise in continuous state cases due to the difficulty in efficiently obtaining enough samples. The domain of \u03c1(s) is denoted as S and the domain of \u03c1 \u03b8 (s) as S \u03b8 \u2282 S. In finite time horizon problems, \u03c1(s|s \u223c \u03b8) = P (s 0 = s|\u03b8) + P (s 1 = s|\u03b8) + ... + P (s T = s|\u03b8), focusing on reachable regions. In continuous state cases, the challenge of ergodicity arises due to the difficulty in efficiently obtaining enough samples. To improve sample efficiency, it is proposed to approximate \u03c1(s|s \u223c \u03b8) when the domain of possible states is similar between different policies. In continuous state cases, to improve sample efficiency, it is proposed to approximate \u03c1(s|s \u223c \u03b8) when the domain of possible states is similar between different policies. This approximation requires a necessary condition where the domain of possible states are similar between different policies. Adding noise on \u03b8 can ensure this condition, and for more general cases, sampling from S \u03b8 \u222a S \u03b8j is necessary to satisfy the properties. In continuous state cases, to improve sample efficiency, it is proposed to approximate \u03c1(s|s \u223c \u03b8) when the domain of possible states is similar between different policies. Adding noise on \u03b8 can ensure this condition, and sampling from S \u03b8 \u222a S \u03b8j is necessary to satisfy the properties in Definition 1. Plugging Eq.(4) into Eq. (2), the objective function of policy differentiation involves exploring the domain S \u03b8 adequately during training and initialization. The objective function of policy differentiation involves exploring the domain S \u03b8 adequately during training and initialization. The estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased. The last term disappears in training and initialization of \u03b8. The estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased. The objective in traditional RL is to maximize the expectation of cumulative rewards. To improve behavioral diversity, an efficient learning algorithm is needed. In traditional RL, the objective is to maximize cumulative rewards. To enhance behavioral diversity, the learning algorithm should consider both task rewards and policy uniqueness. Previous methods combine primal task rewards with intrinsic rewards for this purpose. To enhance behavioral diversity in reinforcement learning, the learning objective should consider both task rewards and policy uniqueness. Previous approaches combine primal task rewards with intrinsic rewards, but the choice of weight parameter and formulation of intrinsic reward can significantly impact results. The learning objective in reinforcement learning should consider task rewards and policy uniqueness. The weight parameter and formulation of intrinsic reward can impact results significantly. To address these issues, inspiration is drawn from the passive motivation of social uniqueness. To address the impact of weight parameter and intrinsic reward formulation in reinforcement learning, inspiration is drawn from passive social uniqueness motivation. The multi-objective optimization problem is transformed into a constrained optimization problem, with a threshold for minimal permitted uniqueness. Further discussion on the threshold selection will be discussed. The multi-objective optimization problem is transformed into a constrained optimization problem with a threshold for minimal permitted uniqueness. The penalty method replaces the constrained optimization problem with a penalty term and coefficient, challenging the selection of alpha. Zhang et al. (2019) address this challenge with the Task Novel Bisector. In this work, the constrained optimization problem is solved using Interior Point Methods (IPMs) instead of the Task Novel Bisector approach. The difficulty lies in selecting the penalty coefficient alpha. In this work, the constrained optimization problem is solved using Interior Point Methods (IPMs) with a focus on penalty coefficient alpha. The approach differs from the Task Novel Bisector method and involves reformulating the problem to an unconstrained form with a barrier term in the objective. In our proposed RL paradigm, the constrained optimization problem is solved using Interior Point Methods (IPMs) with a focus on penalty coefficient alpha. Directly applying IPMs can be computationally challenging and numerically unstable, especially when alpha is small. However, in our approach where an agent's behavior is influenced by its peers, a more natural way can be used for solving the optimization problem. In the proposed RL paradigm, the constrained optimization problem is addressed using Interior Point Methods (IPMs) with a focus on penalty coefficient alpha. Applying IPMs directly can be computationally challenging and numerically unstable, especially for small alpha values. However, a more natural approach is used where the feasible region is bounded by terminating new agents that step outside it. During the training process, transitions are bounded within the feasible region by terminating new agents that step outside it. This ensures collected samples are unique and less likely to appear in previously trained policies, resulting in a new policy with sufficient uniqueness. This eliminates the need to consider trade-offs. During training, transitions are constrained within a feasible region by terminating agents that step outside it. This ensures unique samples are collected, leading to a new policy with sufficient uniqueness. This eliminates the need to consider trade-offs and results in a more robust learning process without objective inconsistency. The approach is named Interior Policy Differentiation (IPD) method. The proposed Interior Policy Differentiation (IPD) method eliminates the trade-off problem between intrinsic and extrinsic rewards, ensuring a robust learning process without objective inconsistency. Tested on MuJoCo-based environments, including Hopper-v3, Walker2d-v3, and HalfCheetah-v3. The proposed Interior Policy Differentiation (IPD) method is tested on MuJoCo environments such as Hopper-v3, Walker2d-v3, and HalfCheetah-v3, with default parameters. Experiments show that different policies can be generated by selecting different random seeds before training. In experiments on MuJoCo environments, different policies can be generated by selecting different random seeds before training. The proposed method is based on PPO and aims to improve behavior diversity. The proposed method, based on PPO, aims to enhance behavior diversity by leveraging stochasticity in training and random weight initialization. Comparison with TNB and WSR approaches is also conducted to combine task goals with uniqueness motivation. The unbiased approximation of uniqueness under the proposed metric is discussed in detail. The proposed method, based on PPO, aims to enhance behavior diversity by leveraging stochasticity in training and random weight initialization. Comparison with TNB and WSR approaches is conducted to combine task goals with uniqueness motivation. The uniqueness metric is utilized directly in learning new policies without reshaping, and 10 different policies are trained sequentially to be unique. Our method utilizes a uniqueness metric directly in learning new policies without reshaping. 10 different policies are trained sequentially to be unique, with qualitative results shown in Fig.2. The motion of agents is visualized by drawing multiple frames representing their pose at different time steps. The policy is trained sequentially to be unique without any social influence. Fig.2 shows qualitative results of the method, visualizing agent motion with multiple frames representing pose at different time steps. The visualization starts at the beginning of each episode, showing the process of acceleration. The visualization in Fig. 3 displays experimental results on uniqueness and performance of policies in different environments. Our proposed method outperforms others in Hopper and HalfCheetah, while in Walker2d, both WSR and our method show effectiveness. Fig. 3 displays experimental results on uniqueness and performance of policies in different environments. Our proposed method outperforms others in Hopper and HalfCheetah, while in Walker2d, both WSR and our method show effectiveness. Detailed comparison on task-related rewards is provided in Table 1, with performance of each trained policy and reward gaining curve shown in Fig. 5 and Fig. 6 in the Appendix. In Walker2d, both WSR and our method improve policy uniqueness, but none surpass PPO's performance. Detailed comparisons on task-related rewards are in Table 1. Figures in Appendix C show performance and reward curves. Success rate is also used as a metric for comparison. In this work, success rate is used as a metric to compare the performance of different approaches. A policy is considered successful if it outperforms the baseline performance of policies trained without social influences, such as PPO. If a new policy surpasses the baseline during training, it is deemed successful. This metric helps determine if the policy has learned unique strategies. Our method consistently outperforms the baseline performance of policies trained without social influences, ensuring improved performance in experiments. Our method consistently outperforms the baseline performance, ensuring improved performance in experiments by preventing new policies from falling into the same local minimum. Our method prevents new policies from getting stuck in local minima, leading to noticeable performance improvements in the Hopper and HalfCheetah environments. This property enhances traditional RL schemes. Our method prevents policies from being trapped in local minima, encouraging exploration of different action patterns to improve performance. This feature is beneficial for enhancing traditional RL schemes and has led to performance improvements in the HalfCheetah environment. The HalfCheetah environment in RL schemes encourages exploration to prevent being trapped in local minima. The absence of an explicit termination signal leads to random agent actions initially, resulting in repeat samples and high control costs. In the HalfCheetah environment of RL schemes, there is no explicit termination signal, leading to random agent actions initially. The agent can receive termination signals from peers to avoid wasting effort, learning to terminate itself early to reduce control costs. During the learning process in our method, an agent learns to terminate itself early to avoid control costs by imitating previous policies and then explores different behaviors for higher rewards. The increasing number of policies learned with social influence may make finding a unique policy more challenging. Results from our ablation study show how performance changes. The learning process involves an agent adapting its behavior to pursue higher rewards, akin to an implicit curriculum. As the number of policies learned with social influence increases, finding a unique policy becomes more difficult. A study on performance changes under varying levels of social influence shows a more significant decrease in performance in the Hopper environment due to its limited action space. In an ablation study, the performance changes under different scales of social influence are analyzed. The decrease in performance is more pronounced in the Hopper environment due to its limited action space. A new approach is developed to encourage RL to learn diverse strategies inspired by social influence, defining policy uniqueness and treating the problem as a constrained optimization one. In this work, an efficient approach called Interior Policy Differentiation (IPD) is developed to motivate RL to learn diverse strategies inspired by social influence. By defining policy uniqueness and treating the problem as a constrained optimization one, IPD draws insights from Interior Point Methods. Experimental results show that IPD can help agents avoid local minimum and facilitate implicit curriculum learning in certain cases. Our proposed method, Interior Policy Differentiation (IPD), draws insights from Interior Point Methods to learn various well-behaved policies and avoid local minimum. Experimental results show that IPD can be interpreted as implicit curriculum learning in certain cases, with results demonstrating policy uniqueness in different environments. The curr_chunk discusses the triangle inequality and the uniqueness of policies in different environments. Direct optimization of uniqueness reward can sometimes lead to a decrease in task performance, requiring careful hyper-parameter tuning and reward shaping. The implementation details include the calculation of DTV using the deterministic part of policies. The implementation details involve calculating DTV using the deterministic part of policies and using MLP with 2 hidden layers for actor models in PPO. Ablation study on unit numbers in the second layer is detailed in Table 2, Table 3, and Fig. 8. In the calculation of DTV, deterministic part of policies is used, removing Gaussian noise on the action space in PPO. Actor models in PPO utilize MLP with 2 hidden layers, with the first layer fixed at 32 units. Unit numbers in the second layer are varied in an ablation study detailed in Table 2, Table 3, and Fig. 8. Different hidden unit numbers are chosen for tasks based on success rate, performance, and computation expense considerations. Training timesteps are fixed in the experiments. In the experiments, different hidden unit numbers were chosen for tasks based on success rate, performance, and computation expense considerations. Training timesteps were fixed for each task. The proposed method allows for flexible control of policy uniqueness by adjusting the constraint threshold. In the experiments, training timesteps were fixed for each task. The proposed method allows for flexible control of policy uniqueness by adjusting the constraint threshold, which affects the agent's behavior. A larger threshold may lead to more diverse behavior but could result in poorer performance as finding a feasible solution becomes less likely. Constraints in the form of Eq. (7) were not used in the method. A larger threshold in the proposed method may drive the agent to behave more differently, potentially leading to poorer performance as finding a feasible solution becomes less likely. Constraints in the form of Eq. (7) were not used, instead focusing on cumulative uniqueness as constraints. Testing was done with different threshold values to analyze agent performance. In testing the proposed method, different threshold values were used to analyze agent performance. Constraints in the form of Eq. (7) were not utilized, with a focus on cumulative uniqueness as constraints. In testing the proposed method, constraints in the form of Eq. (7) were not used. Instead, cumulative uniqueness was focused on for long-term differences. The WSR, TNB, and IPD methods represent different approaches in constrained optimization problems. The constraints can be applied after the first t timesteps for similar starting sequences. The WSR, TNB, and IPD methods represent different approaches in constrained optimization problems. Constraints can be applied after the first t timesteps for similar starting sequences. The optimization of policy is based on batches of trajectory samples and implemented with stochastic gradient descent. The Penalty Method considers constraints by putting them into a penalty term to solve the unconstrained problem. The optimization of policy in constrained optimization problems is based on batches of trajectory samples and implemented with stochastic gradient descent. The Penalty Method considers constraints by putting them into a penalty term to solve the unconstrained problem. The final solution heavily depends on the selection of a fixed weight term \u03b1. The Penalty Method in constrained optimization problems involves adding constraints into a penalty term and solving the unconstrained problem iteratively. The final solution's accuracy is highly influenced by the selection of a fixed weight term \u03b1. The Feasible Direction Method (FDM) and TNB method are also used to consider constraints in optimization. The Feasible Direction Method (FDM) and TNB method consider constraints in optimization by selecting a direction p that satisfies certain conditions. In TNB, the shape of g is crucial for success, as the final optimization result heavily relies on its selection. In TNB optimization, the shape of g is crucial for success, as the final result heavily relies on its selection. The barrier term of -\u03b1 log g(\u03b8) can be used to introduce minimal influence on the objective, with \u03b1 being a small positive number. The shape of g is important for TNB success. The barrier term of -\u03b1 log g(\u03b8) can be used with \u03b1 as a small positive number. The objective will increase fast as \u03b8 gets closer to the barrier. Choosing a sequence of decreasing \u03b1 values will bring the solution closer to the primal objective. Applying this method directly is computationally challenging, especially with small \u03b1 values. In practice, methods choose a sequence of decreasing \u03b1 values to get closer to the primal objective. Directly applying this method is computationally challenging, especially with small \u03b1 values. A more natural approach involves bounding collected transitions in the feasible region by terminating new agents during training. During training, collected transitions are bounded in the feasible region by terminating new agents, ensuring uniqueness in the new policy obtained at the end. During training, transitions are bounded in the feasible region by terminating new agents, ensuring uniqueness in the new policy obtained at the end. This eliminates the need to consider the trade-off between intrinsic and extrinsic rewards, making the learning process more robust and free from objective inconsistency. The pseudo code of IPD based on PPO is shown in Algorithm.1, with additions highlighted in blue."
}