{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. We propose two ontology-based neural network architectures for sound event classification, demonstrating improved performance by incorporating ontological information. Humans can identify various sounds in their environment, which can be categorized into more abstract classes represented by ontologies. The benefits of including ontological information in sound event classification are demonstrated. Humans can identify a variety of sounds in their environment, which can be categorized into abstract classes represented by ontologies. However, current sound event classification methods do not fully utilize this additional information. Ontologies can provide structure to sound event classification datasets, but this information is rarely utilized. Neural networks are the state of the art for sound event classification, but they are seldom designed with ontologies in mind. Ontologies offer a formal representation of domain knowledge through categories and relationships, enhancing the training data and neural network architecture. An ontology is a formal representation of domain knowledge through categories and relationships, providing structure to training data and neural network architecture. Taxonomies are commonly based on abstraction hierarchies defined by linguistics, where a super category represents its subcategories. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Another taxonomy can be defined by interactions between objects, materials, actions, and descriptors. Taxonomies are defined by nouns or verbs, e.g., animal contains dog and cat. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Hierarchical relations in sound event classifiers offer benefits by allowing the classifier to back-off to more general categories when encountering ambiguity. Hierarchical relations in sound event classifiers offer benefits by allowing the classifier to back-off to more general categories when encountering ambiguity. This can disambiguate acoustically similar classes and penalize misclassifications differently. Additionally, hierarchical relations can serve as domain knowledge to model neural networks. Ontology-based network architectures have shown improvement in sound event classification performance and offer other benefits. Ontological information can be used to disambiguate acoustically similar classes, penalize misclassifications differently, and serve as domain knowledge for neural network modeling. Ontology-based network architectures have been used to improve performance in various fields such as computer vision and music, but have rarely been applied to sound event classification. A study proposed an ontology-based deep restricted Boltzmann machine for textual topic classification, showing improved performance and reduced overfitting in training data. Another approach used a perceptron for each node in the hierarchy to classify images. In response to the use of ontology-based network architectures in different fields, a study introduced a machine for textual topic classification. The model improved performance and reduced overfitting in training data by adding intermediate layers to replicate a tree-like structure. Another approach utilized perceptrons for each node in the hierarchy to classify images, showing enhanced performance through class disambiguation. Inspired by these methods, the proposal introduces ontology-based networks for deep learning models. The proposal introduces ontology-based networks for deep learning models, aiming to improve performance through class disambiguation. The framework includes assumptions about ontologies and a Feed-forward model with defined constraints. The framework presented in this section introduces a Feed-forward model with constraints for dealing with ontological information using deep learning architectures. It includes a Siamese Neural Network for computing ontology-based embeddings to preserve an embedding space consistent with the ontological structure. The framework aims to utilize ontology structure in modeling neural network architectures. The framework introduced utilizes a Siamese Neural Network to compute ontology-based embeddings, preserving an embedding space consistent with the ontological structure. It is designed to model neural network architectures based on ontology structure, considering training data with audio representations associated with labels from a two-level ontology. The framework can be easily generalized to more levels. The framework utilizes a Siamese Neural Network to compute ontology-based embeddings for audio representations associated with labels from a two-level ontology. It can be generalized to more levels, with each class mapped hierarchically to the next level in the ontology. The ontology-based embeddings for audio representations are computed using a Siamese Neural Network, with classes mapped hierarchically to the next level in the ontology. For example, in a two-level ontology, each element in the first level is related to one element in the second level. The Siamese Neural Network computes ontology-based embeddings for audio representations. In this case, k = 2, with classes C1 and C2. Each element in C1 is related to one element in C2. Given a representation x \u2208 X and label y1 in C1, we can infer its label in C2 using a probabilistic formulation. The architecture includes a Feed-forward Network with an Ontological Layer. The architecture includes a Feed-forward Network with an Ontological Layer. Given x \u2208 X and label y1 in C1, we can infer its label in C2 using a probabilistic formulation. By assuming p(y2 | y1, x) = p(y2 | y1), we can estimate p(y2 | x) by computing p(y1 | x) and summing the values corresponding to the children of y2. The architecture involves a Feed-forward Network with an Ontological Layer. By assuming p(y2 | y1, x) = p(y2 | y1), we can estimate p(y2 | x) by computing p(y1 | x) and summing the values corresponding to the children of y2. It is important to consider using knowledge to relate different classes in y1 during training to improve model performance, especially for predicting classes y2. The proposed framework involves designing ontology-based neural network architectures, specifically utilizing an ontological layer. The Feed-forward Network (FFN) with Ontological Layer includes a base network (Net), an intermediate vector z, and two outputs. The proposed framework involves designing ontology-based neural network architectures, specifically utilizing an ontological layer. The Feed-forward Network (FFN) with Ontological Layer consists of a base network (Net), an intermediate vector z, and two outputs for each ontology level. The base network learns weights at every parameter update and generates probability vectors for different classes based on input audio features. The proposed framework includes a Feed-forward Network (FFN) with an ontological layer, utilizing a base network, an intermediate vector z, and two outputs for each ontology level. The base network learns weights at every parameter update and generates probability vectors for different classes based on input audio features. The ontological layer reflects the relation between super classes and sub classes in the ontology. The proposed framework includes a Feed-forward Network (FFN) with an ontological layer that reflects the relation between super classes and sub classes in the ontology. The output of the softmax layer is multiplied by the ontological layer to generate predictions for classes in C1 and C2. The ontological layer's weights are defined by the matrix M, which helps in predicting classes based on input x. The proposed framework includes a Feed-forward Network (FFN) with an ontological layer that reflects the relation between super classes and sub classes in the ontology. The ontological layer's weights, defined by matrix M, are used to generate predictions for classes in C1 and C2. The model is trained using a gradient-based method to minimize the loss function, which is a convex combination of two categorical cross-entropy functions. The ontological layer M defines weights for standard layer connections in the proposed framework. The model is trained using a gradient-based method to minimize the loss function, a convex combination of two categorical cross-entropy functions. The hyperparameter \u03bb \u2208 [0, 1] is tuned to adjust the balance between the two functions. In this section, the ontology-based embeddings were learned using a Siamese neural network (SNN) to preserve the ontological structure. The goal was to create embeddings that enforce samples of the same class to be closer while separating samples of different classes. The hyperparameter \u03bb \u2208 [0, 1] was tuned to adjust the balance between two categorical cross-entropy functions. The ontology-based embeddings were learned using a Siamese neural network (SNN) to preserve the ontological structure. The SNN architecture enforces samples of the same class to be closer while separating samples of different classes based on subclass and superclass relationships. The Siamese neural network (SNN) architecture with the Feed-forward Network with Ontological Layer enforces samples of the same class to be closer while separating samples of different classes based on subclass and superclass relationships. The architecture includes twin networks with shared weights and ontological embeddings to compute a Similarity metric. The Siamese neural network architecture enforces samples of the same class to be closer while separating samples of different classes based on subclass and superclass relationships. The twin networks have shared weights and ontological embeddings to compute a Similarity metric using Euclidean Distance. The distance between embeddings indicates the difference between samples, with specific thresholds for subclasses and superclasses. The output probabilities are calculated for both levels. The study evaluates sound event classification performance using a Siamese neural network with ontological embeddings. Different thresholds are set for subclass and superclass relationships to measure the difference between samples. Output probabilities are calculated for different levels. The study evaluates sound event classification performance using a Siamese neural network with ontological embeddings. Output probabilities are calculated for different levels in the hierarchy to train the Feed-forward Model with Ontological layer. The evaluation includes datasets, baseline and proposed architectures, and classification performance. Making Sense of Sounds Challenge 2 - MSoS dataset is designed for classifying abstract classes at the highest level. The study evaluates sound event classification performance using ontological-based neural network architectures. The dataset for Making Sense of Sounds Challenge 2 (MSoS) aims to classify the most abstract classes at the highest level of its taxonomy. The ontology has two levels with 97 classes at level 1 and 5 classes at level 2, using audio files from various databases. The development dataset consists of 1500 audio files divided into five categories. The ontology used for sound event classification has two levels with 97 classes at level 1 and 5 classes at level 2. Audio files were sourced from Freesound, ESC-50 dataset, and Cambridge-MT Multitrack Download Library. The development dataset includes 1500 audio files divided into five categories, with 300 files each. The evaluation dataset consists of 500 audio files, with 100 files per category, all in a standardized format. The evaluation dataset consists of 500 audio files, 100 files per category, all in a standardized format. The files are partitioned for training, tuning parameters, and testing. The dataset is designed to evaluate the classification of urban sounds with more nodes in the taxonomy than the annotated number of classes. The dataset for the urban sounds classification challenge consists of 500 audio files, with 100 files per category. The files are 5 seconds long and were taken from the Freesound database. The taxonomy was adjusted to avoid redundant levels, resulting in 10 classes at level 1 and 4 classes at level 2. The ontology was adjusted to have two levels, with 10 classes at level 1 and 4 classes at level 2. The dataset contains 8,732 audio files from Freesound, with files in a single-channel 44.1 kHz, 16-bit .wav format. The audio recordings were represented using state-of-the-art Walnet features BID1, with a 128-dimensional logmel-spectrogram vector computed for each audio. The dataset consists of 8,732 audio files in a single-channel 44.1 kHz, 16-bit .wav format. Audio recordings were represented using state-of-the-art Walnet features BID1 and transformed via a convolutional neural network (CNN). The base network architecture used in the experiment is a feed-forward multi-layer perceptron network with 4 layers. The base network architecture used in the experiment is a feed-forward multi-layer perceptron network with 4 layers. It consists of an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. The dense layers utilize Batch Normalization, a dropout rate of 0.5, and the ReLU activation function. Parameters in the network were tuned for optimization. The experiment involved a feed-forward multi-layer perceptron network with 4 layers, including dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. Parameters were tuned for optimization, and baseline models were considered for different datasets. The models did not include ontological information in this case. In the experiment, parameters were tuned in the Net box and for transforming z into p(y 1 |x). Baseline models were considered for different datasets, without ontological information. The baseline models consisted of the Base Network Architecture with an additional output layer for level 1 or level 2. For level 1, this is similar to training the Feed-forward model with Ontological Layer using \u03bb = 1. The loss function for level 2 was not considered with \u03bb = 1. The baseline model for level 2 differed from the Feed-forward model with \u03bb = 0, as it lacked a prediction layer. The baseline models were compared for level 1 and level 2, with different architectures and performance results reported for the MSoS and US8K datasets. The architecture presented in Section 2.2 was validated, and the utility of the models was analyzed. The baseline models for MSoS and US8K datasets were compared in level 1 and level 2. The architecture in Section 2.2 was validated by training models with different values of \u03bb. The effect of \u03bb on performance in both datasets was shown in FIG3, indicating that values other than 0 and 1 improved performance. The ontological layer had an impact on classification in both levels, with the best performance observed in the MSoS dataset. The ontological layer was found to improve performance in both MSoS and US8K datasets when different values of \u03bb were used. In MSoS, the best accuracy was achieved with \u03bb = 0.8, resulting in a 5.4% and 6% improvement in levels 1 and 2 respectively. In US8K, a smaller improvement was observed with the best performance at \u03bb = 0.7. In the case of the MSoS dataset, the best performance was achieved with \u03bb = 0.8, resulting in accuracy of 0.74 and 0.913 for levels 1 and 2 respectively. This led to an absolute improvement of 5.4% and 6% compared to baseline models. On the US8K dataset, a smaller improvement was seen with the best performance at \u03bb = 0.7, yielding accuracy of 0.82 and 0.86 for levels 1 and 2. This represented an improvement of 2.5% and 0.2% only compared to baseline models. The performance using \u03bb = 0.7 resulted in an accuracy of 0.82 and 0.86 for levels 1 and 2, showing a slight improvement of 2.5% and 0.2% compared to baseline models. The best result was achieved with \u03bb = 0.8 in the US8K dataset. The MSoS t-SNE plots show tighter clusters with ontology-based embeddings. The study evaluated the performance of ontology-based embeddings for sound event classification using a specific architecture. The embeddings resulted in tighter and better-defined clusters compared to base network vectors. t-SNE plots were included to show how the embeddings cluster at different levels using Walnet audio features. The study tested the ontology-based embeddings for sound event classification using a specific architecture. t-SNE plots were used to show how the embeddings cluster at different levels. The Siamese neural network was trained with different super and sub class pairs to produce the embeddings, which were then passed to the base network for training. The SNN was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for good performance. The Siamese neural network was trained with different super and sub class pairs to produce ontology-based embeddings. The SNN was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for good performance. The input training data consisted of 100,000 pairs, yielding the best performance. The loss function values were derived from previous experiments. The Siamese neural network was trained with 100,000 pairs for ontology-based embeddings. The loss function values were adjusted, affecting overall performance. Results showed accuracy performance for MSoS and US8K in level 1 and level 2. The Siamese neural network was trained with ontology-based embeddings, adjusting the loss function values. Results in Table 1 showed accuracy performance for MSoS and US8K in level 1 and level 2. The architecture outperformed the baseline but slightly underperformed the method without embeddings. Ontology-based embeddings provided better grouping, as seen in Figure 6. MSoS data was used to create t-SNE plots of classes in level 2. Based on the results, the architecture performed better than the baseline but slightly underperformed without embeddings. Ontology-based embeddings showed improved grouping with tighter clusters. However, the performance on the US8K dataset was limited due to a similar number of subclasses. The FF + Ontology vectors and ontology-based embeddings provided clustered groups of level 2 classes, with tighter and better defined clusters. Performance on the US8K dataset was limited due to a similar number of subclasses compared to superclasses. The contribution of ontology was negligible when the ratio between subclasses and classes was not large. Both approaches were used in the Making Sense of Sounds Challenge. In the Making Sense of Sounds Challenge, the Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89 accuracy for sound event classification. The ratio between subclasses and classes impacted the contribution of ontology, with both approaches outperforming the baseline accuracy of 0.80 for level 2. The paper proposes a framework for designing neural networks for sound event classification using hierarchical ontologies. Two methods were shown to incorporate this structure into deep learning models without adding more parameters. A Feed-forward Network with an ontological layer and a Siamese neural Network were used to relate predictions at different levels in the hierarchy. Incorporating hierarchical ontologies into neural networks for sound event classification was achieved through a Feed-forward Network with an ontological layer and a Siamese neural Network. Results showed improved performance over baselines in datasets and challenges, paving the way for further exploration of ontologies and relations. The Siamese neural Network proposed computes ontology-based embeddings to preserve ontology in an embedding space, showing clusters of super classes with different sub classes. Results in datasets and challenges improved over baselines, paving the way for further exploration of ontologies and relations in sound event classification."
}