{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model for forecasting data streams from geospatial point-cloud sources. It uses a Dynamic Point-cloud Convolution operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. The D-Conv operator in CloudLSTM extracts local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. It resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM architecture is applied to point-cloud stream forecasting for mobile service traffic and air quality indicator prediction. Real-world datasets show CloudLSTM outperforms other neural network models in accurate long-term predictions. Point-cloud stream forecasting predicts future values/locations of data streams from geospatial point-cloud S using historical observations. Data sources include mobile network antennas, air quality sensors, and moving crowds. Unlike traditional spatiotemporal forecasting, this method focuses on individual trajectories rather than grid-structural data like precipitation nowcasting or video frame prediction. Point-cloud stream forecasting operates on scattered sets of points, exploiting spatial correlations. Convolution-based RNN models like ConvLSTM are used for this purpose. Point-cloud stream forecasting requires handling irregular and unordered point sets with complex spatial correlations. While vanilla LSTMs have limited spatial feature exploitation, ConvLSTM and PredRNN++ are not suitable for scattered point-cloud data. Different approaches to geospatial data stream forecasting involve modeling grid-structured data with RNN models like ConvLSTM and PredRNN++, or directly forecasting over point-cloud data streams without pre-processing. The proposed PointCNN leverages spatial-local correlations of point clouds, regardless of input order, enabling forecasting directly over point-cloud data streams using historical information. The PointCNN architecture leverages spatial-local correlations of point clouds, allowing for forecasting over point cloud-streams. The CloudLSTM architecture, incorporating the DConv operator, is introduced to handle temporal dependencies and enable precise forecasting by combining Seq2seq learning and attention mechanisms. The CloudLSTM architecture, utilizing the DConv operator, enables precise forecasting over point-cloud streams by combining Seq2seq learning and attention mechanisms. Point clouds are formally defined as sets of points with value features like mobile traffic measurements and air quality indexes. The CloudLSTM architecture, using the DConv operator, enables accurate forecasting over point-cloud streams by combining Seq2seq learning and attention mechanisms. Point clouds are defined as sets of points with value features such as mobile traffic measurements and air quality indexes. The ideal forecasting model for point-cloud streams should encompass five key mechanisms. An ideal point-cloud stream forecasting model should have five key properties: (i) Order invariance, where the arrangement of points does not affect the forecasting output, (ii) Information intactness, ensuring data integrity. A point-cloud stream forecasting model should have five key properties: order invariance, information intactness, interaction among points, and robustness. The point-cloud stream forecasting model should maintain order invariance, information intactness, interaction among points, robustness to transformations, and adaptability to changing spatial correlations over time. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM, ensuring robustness to transformations and adaptability to changing spatial correlations over time. It generalizes ordinary convolution on grids by computing weighted summations over a small receptive field. The Dynamic Point Cloud Convolution (DConv) operator, introduced as the core module of CloudLSTM, generalizes ordinary convolution on grids by computing weighted summations over a small receptive field on point-clouds. It inherits desirable properties of ordinary convolution and takes U in channels of a point-cloud S, outputting U out channels. The DConv operator performs weighted summations on point-clouds, inheriting properties of ordinary convolution. It takes U in channels of a point-cloud S and outputs U out channels while preserving information intactness. The DConv operator takes U in channels of a point-cloud S and outputs U out channels, ensuring information intactness. Points in S i in are defined as Q K n, including the K nearest points to p n in Euclidean space. Each point p n in S contains H value features and L coordinate features. The DConv operator processes points in S to obtain values and coordinates for a point in S out, using the K nearest points to p n in Euclidean space. Each point in S contains value and coordinate features. The DConv operator in the context of processing points in S to obtain values and coordinates for a point in S out using the K nearest points in Euclidean space. It involves aggregating coordinate features and utilizing learnable weights shared across different anchor points in the input map. The DConv operator involves aggregating coordinate features and utilizing learnable weights shared across different anchor points in the input map to exploit dynamic spatial correlations. The weights are defined as 5D tensors and each element represents a scalar weight for input and output channels, as well as nearest neighbors of each point. Bias terms are also defined for the output map. The DConv operator aggregates coordinate features using shared learnable weights across anchor points in the input map to capture dynamic spatial correlations. Bias terms are defined for the output map, and coordinates of raw point-clouds are normalized before feeding them to the model. The sigmoid function limits predicted coordinates to (0, 1) to prevent outliers. The input/output point set features are normalized to (0, 1) before feeding them to the model for improved transformation robustness. The K nearest points can vary for each channel at each location in the pointcloud dataset. The K nearest points can vary for each channel at each location in the pointcloud dataset, improving transformation robustness. Spatial correlations vary between different measurements due to human mobility, reflecting data consumption patterns. The spatial correlations between mobile apps and air quality indicators vary due to human mobility. CloudLSTM allows each channel to find the best neighbor set for improved forecasting. The CloudLSTM model enables each channel to find the best neighbor set for improved forecasting performance, illustrated by the DConv operator weighting its K nearest neighbors across features. CloudLSTM improves forecasting performance by using the DConv operator to weight its K nearest neighbors across features, ensuring symmetry and consistent output. The DConv operator in CloudLSTM ensures symmetry and consistent output by ranking distances for K nearest neighbors across features. It captures local dependencies and improves robustness to global transformations through normalization over coordinate features. The DConv operator in CloudLSTM captures local dependencies, improves robustness to global transformations, and enables dynamic positioning tailored to each channel and time step by learning the layout and topology of the cloud-point for the next layer. DConv in CloudLSTM learns the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step. This allows for \"location-variance\" and efficient implementation using simple 2D convolution. The DConv operator in CloudLSTM enables dynamic positioning tailored to each channel and time step, utilizing simple 2D convolution for efficient implementation. It builds upon PointCNN and Deformable Convolution neural networks, introducing variations for pointcloud structural analysis. The DConv operator in CloudLSTM builds upon PointCNN and Deformable Convolution neural networks, introducing variations for pointcloud structural analysis. It employs X-transformation over point clouds to learn weight and permutation using multilayer perceptrons, ensuring order invariance but may lead to information loss. The DConv operator in CloudLSTM builds upon PointCNN and Deformable Convolution neural networks, introducing variations for pointcloud structural analysis. It ensures order invariance without extra complexity and loss of information by aligning the weight of distance rankings between points. The DConv operator in CloudLSTM maintains permutation by aligning distance rankings between points, ensuring order invariance without added complexity. It can be seen as a variation of DefCNN over point-clouds, with differences in how filters are deformed and neighboring points are selected for operations. Both models offer transformation modeling flexibility. The DConv operator in CloudLSTM is a variation of DefCNN over point-clouds, with differences in filter deformation and point selection. It offers transformation modeling flexibility and can be integrated into LSTMs for spatial and temporal correlation learning. The DConv operator in CloudLSTM allows adaptive receptive fields on convolution and can be integrated into LSTMs for learning spatial and temporal correlations over point-clouds. CloudLSTM is formulated with input, forget, and output gates, memory cell, and hidden states as point cloud representations, with learnable weight and bias tensors. The DConv operator is used for element-wise product and a simplified version that removes the sigmoid. The CloudLSTM model incorporates input, forget, and output gates, memory cell, and hidden states as point cloud representations. It utilizes learnable weight and bias tensors, along with the DConv operator for element-wise product and a simplified version without the sigmoid function. The CloudLSTM model includes input, forget, and output gates, memory cell, and hidden states as point cloud representations. It utilizes learnable weight and bias tensors, along with a simplified DConv that removes the sigmoid function. The architecture combines CloudLSTM with Seq2seq learning and soft attention mechanism for forecasting, proven effective in spatiotemporal modeling on grid-structural data. The Seq2seq CloudLSTM model incorporates encoder and decoder stacks of CloudLSTMs connected via a soft attention mechanism for forecasting. The encoder encodes historical data into a tensor, while the decoder decodes it into predictions using a context vector. The Seq2seq CloudLSTM model includes an encoder and decoder with CloudLSTMs connected via a soft attention mechanism. Before generating forecasts, the data is processed by Point Cloud Convolutional layers to translate the raw point-cloud into tensors. In this study, the data is processed by Point Cloud Convolutional layers using CloudCNN to translate the raw point-cloud into tensors. A two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is employed, with no significant performance improvement observed by increasing the number of stacks and channels. Additionally, the study explores incorporating DConv into vanilla RNN and Convolutional GRU, resulting in a new Convolutional Point-cloud RNN. In this study, a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is used. The study also investigates integrating DConv into vanilla RNN and Convolutional GRU, creating CloudRNN and CloudGRU. These models share a Seq2seq architecture with CloudLSTM but do not use the attention mechanism. Performance evaluation of these architectures is conducted. The study introduces new Convolutional Point-cloud RNN (CloudRNN) and Convolutional Point-cloud GRU (CloudGRU) models, which share a Seq2seq architecture with CloudLSTM but do not utilize the attention mechanism. Performance evaluation is done using traffic and air quality datasets to forecast future demands. In this study, the CloudLSTM model is used to forecast future mobile service demands and air quality indicators in target regions. A comparison with 12 baseline deep learning models is provided, using TensorFlow and TensorLayer libraries for implementation. Models are trained on a computing cluster with two NVIDIA Tesla K40M GPUs and optimized by minimizing mean square error (MSE). The study compares the CloudLSTM model with 12 baseline deep learning models using TensorFlow and TensorLayer libraries. Models are trained on a computing cluster with two NVIDIA Tesla K40M GPUs and optimized by minimizing mean square error (MSE) using the Adam optimizer. The study conducts experiments on spatiotemporal point-cloud stream forecasting tasks. The study compares the CloudLSTM model with 12 baseline deep learning models using TensorFlow and TensorLayer libraries. Experiments are conducted on spatiotemporal point-cloud stream forecasting tasks by minimizing mean square error (MSE) with the Adam optimizer. Two typical scenarios are used for each use case, omitting coordinate features in the final output. For mobile traffic forecasting, large-scale multi-service datasets from two European metropolitan areas were used, with data collected over 85 consecutive days. The study focused on point-cloud stream forecasting tasks, omitting coordinate features in the final output. The study focused on mobile traffic forecasting using large-scale multi-service datasets from two European metropolitan areas over 85 days. The data included traffic volume from 792 and 260 antennas in the cities, aggregated in 5-minute intervals, resulting in 24,482 traffic snapshots. The antennas were non-uniformly distributed, forming 2D point clouds over space. The study analyzed mobile traffic forecasting in two European cities using data from 792 and 260 antennas. Traffic volume was aggregated in 5-minute intervals, resulting in 24,482 snapshots for 38 mobile services. The antennas were non-uniformly distributed, forming 2D point clouds over space. Further details can be found in Appendix G. The study analyzed mobile traffic forecasting in two European cities using data from 792 and 260 antennas. Traffic volume was aggregated in 5-minute intervals, resulting in 24,482 snapshots for 38 mobile services. Snapshots are gathered independently for each of 38 different mobile services, selected among the most popular apps for video streaming, gaming, messaging, cloud services, and social networking. Air quality forecasting performance is investigated using a public dataset (Zheng et al., 2015) comprising six air quality indicators collected by 437 air quality monitoring stations in China over a span of one year. The dataset (Zheng et al., 2015) includes six air quality indicators collected by 437 monitoring stations in China over one year. The stations are divided into two city clusters, with data measured hourly. Each cluster has 8,760 snapshots. Missing data is filled using linear interpolation. Measurements for mobile services and air quality indicators are transformed into input channels for the models. Before feeding to the models, the measurements associated with each mobile service and air quality indicator are transformed into different input channels of the point-cloud S. All coordinate features are normalized to the (0, 1) range. The point-clouds are transformed into grids using the Hungarian algorithm for baseline models that require grid-structural input. The ratio of training plus validation, and test sets is 8:2. CloudLSTM is compared with baseline models like PointCNN and CloudCNN, which use convolution over point-clouds for classification and segmentation. CloudCNN introduces the DConv operator for feature extraction over multiple layers. The performance of CloudLSTM is compared with baseline models like PointCNN and CloudCNN, as well as with its variations CloudRNN and CloudGRU. PointLSTM is introduced as a benchmark for Seq2seq architectures. The CloudLSTM model is compared with baseline models like PointCNN and CloudCNN, as well as its variations CloudRNN and CloudGRU. PointLSTM serves as a benchmark for Seq2seq architectures. Other baseline models such as MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are also discussed in detail in the appendix. The CloudLSTM model is evaluated against baseline models like PointCNN and CloudCNN, as well as variations CloudRNN and CloudGRU. The accuracy of CloudLSTM is measured using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are used to assess the fidelity of the forecasts. Various other baseline models and their configurations are discussed in detail in the appendix. For mobile traffic prediction, CloudLSTM accuracy is evaluated using MAE and RMSE. PSNR and SSIM are used to measure forecast fidelity and similarity to ground truth. Neural networks forecast city-scale mobile traffic consumption over 30 minutes based on past data. Details on metrics are in Appendix E and F. In the air quality forecasting use case, all models receive 12 measurements as input and forecast. In the air quality forecasting use case, RNN-based models receive 12 measurements as input and forecast indicators for the following 12 hours. The number of prediction steps is then extended to 72 for all models, with a 6-step forecasting performed for 4,888 instances across the test set. In the air quality forecasting use case, RNN-based models receive 12 measurements as input and forecast indicators for the following 12 hours. The number of prediction steps is extended to 72 for all models, with a 6-step forecasting performed for 4,888 instances across the test set. The performance of RNN-based architectures, including CloudLSTM, CloudRNN, and CloudGRU variants, outperforms CNN-based models and MLP. Different numbers of neighboring points and the attention mechanism are also investigated. Among the RNN-based architectures investigated, CloudLSTM, CloudRNN, and CloudGRU variants outperform CNN-based models and MLP in air quality forecasting. The CloudLSTM model performs better than CloudGRU, which in turn outperforms CloudRNN, suggesting that the DConv operator learns features over geospatial point-clouds more effectively than vanilla convolution and PointCNN. Among the RNN-based architectures investigated, CloudLSTM, CloudRNN, and CloudGRU variants outperform all other benchmark architectures in urban scenarios. CloudLSTM performs better than CloudGRU, which in turn outperforms CloudRNN. The forecasting performance of CloudLSTM is fairly insensitive to the number of neighbors (K), suggesting the use of a small K to reduce model complexity. Additionally, the attention mechanism improves forecasting performance by capturing better dependencies between input sequences and vectors. The CloudLSTM outperforms CloudGRU, which in turn outperforms CloudRNN in urban scenarios. The forecasting performance of CloudLSTM is not affected by the number of neighbors (K), suggesting the use of a small K for model simplicity. The attention mechanism improves forecasting by capturing better dependencies between input sequences and vectors. This effect has been confirmed in other NLP tasks. In a service-wise evaluation, the attention mechanism improves forecasting by capturing dependencies in input sequences and vectors. Long-term forecasting performance shows that most models are reliable with a prediction horizon of up to 36 time steps. Low K values may lead to poorer long-term performance in city 2. In city 1, most models show reliable long-term forecasting with stable MAE curves. For city 2, low K values may impact CloudLSTM performance in the long term. The study recommends choosing K based on forecast length. CloudLSTMs outperform other models in 12-step air quality forecasting across six indicators. In city 2, low K values may affect CloudLSTM performance in the long term, providing guidance on choosing K for different forecast lengths. Results show CloudLSTMs outperform state-of-the-art methods in 12-step air quality forecasting, with superior performance in MAE and RMSE compared to ConvLSTM. CloudCNN consistently outperforms PointCNN in this use case. CloudLSTMs outperform ConvLSTM by up to 12.2% and 8.8% in MAE and RMSE. CloudCNN is superior to PointCNN in feature extraction from point-clouds. Overall, CloudLSTM models are effective for spatiotemporal point-cloud stream data modeling. The effectiveness of CloudLSTM models for modeling spatiotemporal point-cloud stream data is demonstrated, with performance evaluations conducted on RNN-based models for long-term forecasting. Experiments are carried out using strict variable-controlling methodology to study the effect of each factor. The experiments on RNN-based models in the study show that D-Conv significantly improves performance compared to other core operators. CloudLSTM outperforms CloudRNN and CloudGRU. CloudLSTM, a dedicated neural model for spatiotemporal forecasting tailored to pointcloud data streams, builds upon the DConv operator. Comparing different models, D-Conv contributes significantly to performance improvements, with CloudLSTM outperforming CloudRNN and CloudGRU. The attention mechanism in Attention CloudLSTM does not show significant effects, suggesting that core operator > RNN structure > attention in terms of contribution to performance. The CloudLSTM model, based on the DConv operator, is designed for spatiotemporal forecasting with pointcloud data streams. It predicts values and coordinates of each point, adapting to changing spatial correlations. The attention mechanism in the model does not have a significant impact on performance. The DConv operator builds upon convolution over point-clouds to learn spatial features while maintaining permutation invariance. It predicts values and coordinates of each point, adapting to changing spatial correlations. DConv can be combined with various RNN models, Seq2seq learning, and attention mechanisms. It can be efficiently implemented using a standard 2D convolution operator. The input and output of DConv are 3D tensors with specific shapes. The DConv operator, which builds upon convolution over point-clouds to learn spatial features while maintaining permutation invariance, can be combined with various RNN models, Seq2seq learning, and attention mechanisms. It can be efficiently implemented using a standard 2D convolution operator. The input and output of DConv are 3D tensors with specific shapes, and an efficient algorithm for DConv implementation using 2D convolution is provided. The DConv operator, which enhances convolution over point-clouds for spatial feature learning, can be integrated with RNN models, Seq2seq learning, and attention mechanisms. An efficient algorithm for DConv implementation using a 2D convolution operator is outlined, transforming input into a 4D tensor for processing. The DConv operator is implemented using a 2D convolution operator, transforming input into a 3D tensor and applying the sigmoid function for optimization in deep learning frameworks. The complexity of DConv is analyzed by separating the operation into finding neighboring sets and performing weighting computations. The DConv operator is implemented as a standard convolution operation in deep learning frameworks. The complexity is analyzed by separating the operation into finding neighboring sets and performing weighting computations. The complexity of each step is discussed separately. The DConv operator is analyzed for complexity, with step (i) involving finding K nearest neighbors with a complexity of O(K \u00b7 L log N) using KD trees. Step (ii) computes one feature of the output point with complexity O((H + L) \u00b7 K). Overall complexity is equivalent to a vanilla convolution operator with (H + L) channels, N elements in the input map, and K elements in the kernel. The DConv operator introduces extra complexity by searching the K nearest neighbors for each point, with a complexity of O(K \u00b7 L log N). This complexity remains stable even with higher dimensional point clouds, as the normalization of coordinate features enables transformation invariance with shifting. The DConv operator introduces complexity by searching for K nearest neighbors for each point, with a stable complexity of O(K \u00b7 L log N) even with higher dimensional point clouds. Normalizing coordinate features enables transformation invariance with shifting and scaling, making the model invariant to these transformations. The proposed CloudLSTM model combines normalization for transformation invariance with shifting and scaling, and integrates an attention mechanism. The context tensor for encoder state i is calculated using a score function e i,j = v. The model's performance was compared against other methods. Our proposed CloudLSTM model with an attention mechanism was compared against baseline models like MLP, CNN, and 3D-CNN for mobile traffic forecasting. The context tensor for encoder state i is calculated using a score function e i,j = v. DefCNN learns convolutional filter shapes similar to the DConv operator in this study. Our proposal was compared against baseline models like MLP, CNN, and 3D-CNN for mobile traffic forecasting. DefCNN learns convolutional filter shapes similar to the DConv operator in this study, while LSTM and ConvLSTM are also commonly used for time series forecasting and spatiotemporal predictive learning. PredRNN++ is considered the state-of-the-art architecture for spatiotemporal forecasting on grid-structural data. The DConv operator proposed in this study is compared to LSTM, ConvLSTM, PredRNN++, CloudRNN, and CloudGRU for spatiotemporal forecasting on grid-structural data. CloudRNN and CloudGRU share a similar Seq2seq architecture with CloudLSTM but do not employ the attention mechanism. The CloudRNN and CloudGRU, along with other models, were compared for spatiotemporal forecasting on grid-structural data. They share a similar architecture with CloudLSTM but do not use the attention mechanism. Different configurations and parameters for each model were detailed in Table 3. Increasing the number of layers did not improve the performance of some models. In this study, various models were compared for spatiotemporal forecasting on grid-structural data. The models included ConvLSTM, PredRNN++, and PointLSTM, each with 2 layers. The use of 3x3 filters was noted for image applications, providing a receptive field of 9, equivalent to K=9 in CloudLSTMs. Additionally, PredRNN++ had a unique structure compared to other Seq2seq models. The study compared various models for spatiotemporal forecasting on grid-structural data, including ConvLSTM, PredRNN++, and PointLSTM with 2 layers. The use of 3x3 filters provided a receptive field of 9, equivalent to K=9 in CloudLSTMs. PredRNN++ had a unique structure compared to other Seq2seq models. Different architectures were optimized using the MSE loss function for mobile traffic volume forecast. The study compared various models for spatiotemporal forecasting on grid-structural data, including Seq2seq CloudLSTM with 36 channels and K=6, 2-stack Seq2seq CloudLSTM with 36 channels and K=9, and Attention CloudLSTM. The models were optimized using the MSE loss function and evaluated using MAE, RMSE, PSNR, and SSIM metrics. The study evaluated models for spatiotemporal forecasting on grid-structural data using MAE, RMSE, PSNR, and SSIM metrics. PSNR is defined as 20 log v max (t) \u2212 10 log 1 where \u00b5 v (t) and v max (t) are the average and maximum traffic recorded for all services/quality indicators. Coefficients c 1 and c 2 stabilize the fraction in the presence of weak denominators. The anonymized locations of the antenna set in both cities are collected via traditional flow-level deep packet inspection at the packet gateway (P-GW) using proprietary traffic classifiers to associate flows to specific services. The dynamic range of float type data is set at 2, with k1 = 0.1 and k2 = 0.3. Anonymized locations of antenna sets in both cities are shown in Figure 5. Data collection is done through traditional flow-level deep packet inspection at the packet gateway using proprietary traffic classifiers to associate flows with specific services. Operator names, target regions, and detailed classifier operations are not disclosed for data protection and confidentiality reasons. Mobile services studied are also not named for similar reasons. The study collected mobile service traffic information using anonymized locations of antenna sets in two cities. Data was collected under supervision of privacy agencies, ensuring compliance with regulations and protecting personal information of subscribers. The dataset used in the study is fully anonymized and does not contain personal information about individual subscribers. Due to a confidentiality agreement, the raw data cannot be made public. The analysis considered 38 different services. The dataset used in the study is fully anonymized and does not contain personal information about individual subscribers. The analysis considered 38 different services, with streaming being the dominant type of traffic. The raw data cannot be made public due to a confidentiality agreement. The study analyzed the total traffic consumed by different services in two cities. Streaming was the most dominant type of traffic, accounting for almost half of the total consumption. Other services like web, cloud, social media, and chat also had significant shares, while gaming only made up 0.5% of the demand. The air quality dataset includes information from 43 cities in China, with 2,891,393 air quality records from 437 monitoring stations over a year. Stations are divided into two clusters based on geographic locations. Cluster A has 274 stations. The dataset contains air quality data from 43 cities in China, with 2,891,393 records from 437 monitoring stations over a year. Stations are grouped into two clusters based on geography, with Cluster A having 274 stations and Cluster B having 163. Missing data was filled using linear interpolation. The dataset can be accessed at https://www.microsoft.com/en-us/research/project/urban-air/. The dataset includes air quality data from 43 cities in China, with 2,891,393 records from 437 monitoring stations. Stations are divided into Cluster A (274 stations) and Cluster B (163 stations). Missing data was filled using linear interpolation. The performance of Attention CloudLSTMs is evaluated for forecasting accuracy on individual mobile services, showing similar results for both cities at the service and category level. The MAE evaluation on forecasting accuracy for individual mobile services shows that services with higher traffic volume tend to have higher prediction errors due to more frequent fluctuations, making them harder to predict. CloudLSTMs achieve similar performance over both cities at the service and category level. Services with higher traffic volume, like streaming and cloud, result in higher prediction errors due to frequent fluctuations, increasing uncertainty in traffic series prediction. MAE for long-term air quality forecasting on city clusters shows error growth over time for all models. CloudLSTM with different K values initially performs similarly but larger K values show significant improvement. The MAE for long-term air quality forecasting on city clusters shows error growth over time for all models. CloudLSTM with different K values initially performs similarly, but larger K values show significant improvement in robustness. Visualizing hidden features of CloudLSTM provides insights into the knowledge learned. At the beginning, larger K values improve the robustness of CloudLSTM for mobile traffic forecasting. Evaluation includes visualizing hidden features to understand the knowledge learned by the model. Scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM are shown in Fig. 10, with input snapshots from City 2. In Fig. 10, scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM are shown with input snapshots from City 2, providing insights into the knowledge learned by the model. Each scatter subplot represents features for encoders and decoders in stack 2. In Fig. 10, scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM from City 2 are shown, providing insights into the knowledge learned by the model. Each scatter subplot represents features for encoders and decoders in stack 2. Additionally, Fig. 11 and 12 display NO2 forecasting examples in City Cluster A and B generated by RNN-based models for air quality prediction, offering a performance comparison from a visual perspective. In Fig. 11 and 12, NO2 forecasting examples in city clusters A and B are shown, generated by RNN-based models. The performance comparison visually demonstrates the better prediction offered by Attention CloudLSTMs, capturing trends in point-cloud streams with high long-term fidelity. The comparison visually shows the superior prediction of Attention CloudLSTMs in capturing trends in point-cloud streams with high long-term fidelity, unlike other architectures that degrade rapidly over time. The NO2 forecasting examples in Cluster B are generated by RNN-based models. The CloudLSTM model uses DConv with Sigmoid functions to refine the positions of input points at each time step, allowing each point to move to its optimal position. This results in stronger representability and better forecasting accuracy compared to other models. The CloudLSTM model utilizes DConv with Sigmoid functions to enhance input point positions, improving forecasting accuracy. By stacking multiple DConv via LSTM, the model refines positions at each time step, enabling points to move to optimal positions. Outliers are identified using DBSCAN in the air quality dataset. The CloudLSTM model uses DConv with Sigmoid functions to improve forecasting accuracy by refining point positions. Outliers are identified using DBSCAN in the air quality dataset, with CloudLSTM showing the lowest prediction error compared to other models. The algorithm identifies 16 outlier points in the point-cloud data. The CloudLSTM model still achieves the lowest prediction error compared to other models. CloudCNN, using the DConv operator, shows the best forecasting performance among CNN-based models. Further experiments are conducted to test the robustness of CloudLSTM to outliers. The CloudCNN model, utilizing the DConv operator, demonstrates superior forecasting performance compared to other CNN-based models. Further experiments test the robustness of CloudLSTM to outliers by creating a toy dataset with randomly selected weather stations, including outliers moved away from the center. In a toy dataset, 10 weather stations are randomly selected as outliers and moved away from the center by different distances on both x and y axes. The positions of the remaining 40 weather stations remain unchanged. The CloudLSTM and PointLSTM models are retrained under the same settings, and their performance is shown in Table 5. The positions of 10 outlier weather stations are moved 5 times the maximum range away from the center, while the remaining 40 stations stay unchanged. CloudLSTM outperforms PointLSTM in forecasting outliers, showing robustness to outlier locations. The proposed CloudLSTM model demonstrates robustness to outliers, outperforming PointLSTM in forecasting. It also performs well regardless of the distance to outliers and shows better performance compared to simple baselines using MLPs and LSTMs with different input forms. Our CloudLSTM model outperforms MLPs and LSTMs that rely on k-nearest neighbors for forecasting. The number of neighbors K impacts the model's receptive field, with smaller K values indicating a more limited scope. The number of neighbors K impacts the receptive field of models like CloudLSTM, with smaller K values indicating limited local spatial dependencies. Larger K values allow for looking around larger location spaces but may lead to overfitting. The results suggest that K does not significantly affect the performance of each baseline. Our proposed CloudLSTM model, which combines local spatial dependencies through DConv kernels and global spatial dependencies through stacks of time steps and layers, outperforms simple baselines. Seasonal information in mobile traffic series can further enhance forecasting performance. The CloudLSTM model outperforms simple baselines by extracting local and global spatial dependencies. Seasonal information in mobile traffic series can enhance forecasting performance, but feeding the model with data spanning multiple days is infeasible due to the difficulty of handling long sequences. To improve forecasting performance, the model concatenates 30-minute sequences with a sub-sampled 7-day window to efficiently capture seasonal information. This approach avoids handling extremely large data points and makes the forecasting model practical for real deployment. By concatenating 30-minute sequences with a sub-sampled 7-day window, seasonal information is efficiently captured in the forecasting model. This approach improves performance without dealing with an impractically large amount of data points. Experiments on a subset of mobile traffic data show that incorporating seasonal information boosts the performance of forecasting models. By incorporating seasonal information in the forecasting model, performance is boosted for most models. The periodic information is learned by the model, reducing prediction errors. However, this increases input length and model complexity. Future work will focus on a more efficient way to fuse seasonal information with minimal complexity increase."
}