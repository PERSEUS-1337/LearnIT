{
    "title": "rygFWAEFwS",
    "content": "Stochastic Weight Averaging in Parallel (SWAP) is a new algorithm designed to speed up DNN training by using large mini-batches to quickly compute an approximate solution. It then refines this solution by averaging the weights of multiple models computed independently and in parallel. The resulting models show good generalization performance on CIFAR10, CIFAR100, and ImageNet datasets, achieving the same level of accuracy as models trained with small mini-batches but in a significantly shorter time. Stochastic Weight Averaging in Parallel (SWAP) speeds up DNN training using large mini-batches to compute an approximate solution quickly. The resulting models demonstrate good generalization performance on CIFAR10, CIFAR100, and ImageNet datasets, achieving similar accuracy levels as models trained with small mini-batches but in a significantly shorter time. Stochastic Weight Averaging in Parallel (SWAP) accelerates DNN training by using large mini-batches to compute precise gradient estimates. Multiple nodes in a distributed setting can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates with one synchronization event per iteration. Larger mini-batches in a distributed setting allow for higher learning rates and more precise gradient estimates. Training with larger mini-batches requires fewer updates and synchronization events, leading to good overall scaling behavior. However, there is a maximum batch size where the model's generalization performance may worsen. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging weights with one synchronization event per iteration. However, there is a maximum batch size where the resulting model tends to have worse generalization performance, limiting the usefulness of large-batch training strategies. Stochastic Weight Averaging (SWA) produces models with good generalization performance by averaging weights from a set of models sampled from the final stages of training. This method is effective as long as the models are in a mostly convex region of population loss. Stochastic Weight Averaging (SWA) generates models with good generalization performance by averaging weights from models sampled at the end of training. By averaging models from multiple independent SGD sequences, similar generalization performance can be achieved. Starting with a model trained with large-batches and using small-batches in independent sequences also leads to comparable generalization performance. By averaging models from multiple independent SGD sequences, Stochastic Weight Averaging in Parallel (SWAP) accelerates DNN training by utilizing compute resources efficiently. The algorithm is simple, fast, and produces good results with minor tuning for image classification tasks in computer vision. SWAP is a strategy to accelerate DNN training by efficiently utilizing compute resources. It achieves comparable generalization performance to small-batch models in a similar time frame to large-batch training runs. SWAP is a method to speed up DNN training by utilizing compute resources efficiently. It achieves similar generalization performance to small-batch models in a timeframe comparable to large-batch training runs. SWAP has been successful in reducing training times for efficient models and outperformed the state of the art for CIFAR10, training in 68% of the time of the winning entry in the DAWNBench competition. The impact of training batch size on generalization performance remains unknown. The impact of training batch size on generalization performance is still unknown. Larger mini-batches may lead to models getting stuck in sharper global minima, as explained in Keskar et al. (2016). Sharp minima are sensitive to data variations due to shifts in the minimizer's location causing significant increases in average loss value. In (Keskar et al., 2016), larger mini-batches can lead to models getting stuck in sharper global minima, which are sensitive to data variations. Counterexamples exist where flat minimizers can be transformed into sharp ones without changing model behavior. In (McCandlish et al., 2018), authors predict that batch size can be increased without drop in accuracy, validated empirically. Accuracy drops for CIFAR10 image classification when batch sizes exceed 1k samples. Large batch sizes make mini-batch gradient close to full gradient. The authors in (et al., 2018) predict that increasing batch size up to a critical point does not affect accuracy. They suggest that a larger batch size implies fewer model updates, impacting weight initialization. The authors argue that increasing batch size does not significantly improve signal to noise ratio. Using larger batches implies fewer model updates, affecting weight initialization and generalization performance. Training with large batches for longer times can improve generalization performance, but takes more time than small batches. The authors demonstrate that increasing batch size affects generalization performance and optimization process. Training with large batches for longer times can improve generalization performance, but at the cost of more time compared to small batches. In the over-parameterized setting, there is a critical batch size below which an iteration with a batch size of M is roughly equivalent to M iterations with a batch size of one. In the over-parameterized setting, batch size affects optimization process. A critical batch size exists where an iteration with size M is equivalent to M iterations with size one. Adaptive batch size methods exist but may require specific dataset design or hyper-parameter tuning. Methods exist for adaptive batch sizes in optimization algorithms, but they often require dataset-specific design or extensive hyper-parameter tuning. Local SGD is a distributed optimization algorithm that balances gradient precision with communication costs by allowing workers to update their models independently before synchronizing. Post-local SGD is a variant of the distributed optimization algorithm Local SGD, which refines models trained with large-batch training. It has been observed to improve generalization and achieve significant speedups. SWAP averages models after multiple epochs, while Post-local SGD lets models diverge for a few steps before synchronizing. SWAP achieves better generalization and significant speedups, similar to Post-local SGD. In comparison to Post-local SGD, SWAP averages models after tens of thousands of updates rather than after at most 32 iterations. This difference suggests that the mechanisms behind the success of SWAP and Post-local SGD are likely different in DNN optimization. SWA involves sampling models from later stages of an SGD training run and averaging their weights. Stochastic weight averaging (SWA) involves sampling models from later stages of an SGD training run and averaging their weights to improve generalization properties. SWA has been effective in various domains such as deep reinforcement learning, semisupervised learning, Bayesian inference, and low-precision training. This method is adapted in this work to accelerate DNN training. Stochastic weight averaging (SWA) involves sampling models from later stages of an SGD training run and averaging their weights to improve generalization properties. SWA has been effective in various domains such as deep reinforcement learning, semisupervised learning, Bayesian inference, and low-precision training. In this work, SWA is adapted to accelerate DNN training through a three-phase algorithm. In this work, SWA is adapted to accelerate DNN training through a three-phase algorithm. The first phase involves all workers training a single model with large mini-batch updates and synchronization. The second phase has each worker refining its model independently with smaller batch sizes and different randomizations. The final phase averages the weights of resulting models to produce the final output. In the final phase of the algorithm, the weights of resulting models are averaged, and new batch-normalization statistics are computed to generate the final output. Phase 1 involves training a single model with large mini-batch updates and synchronization, while Phase 2 has workers refining their models independently with smaller batch sizes and different randomizations. The algorithm stops Phase 1 before reaching zero training loss or 100% training accuracy to prevent optimization from getting stuck and improve generalization performance. During Phase 2, batch size is reduced for small-batch training with different random order sampling by each worker. This allows for independent and simultaneous training to improve generalization performance. The optimal stopping accuracy in Phase 1 is a hyper-parameter that needs tuning to prevent optimization from getting stuck. During Phase 2, small-batch training is conducted with reduced batch size and different random order sampling by each worker. This leads to independent training and the production of different models by each worker. The accuracies and learning-rate schedules for SWAP are plotted in Figure 1, showing the transition from large-batch phase to small-batch phase with shared model and varying generalization performance. After training, each worker produces a different model. Figure 1 shows accuracies and learning-rate schedules for SWAP. In the small-batch phase, models diverge due to stochasticity. The averaged model outperforms individual models consistently. In the small-batch phase of SWAP, workers' testing accuracies differ due to stochasticity causing models to diverge. The averaged model consistently outperforms individual models. Phase Figure 1 illustrates learning rate schedules and CIFAR10 test accuracies in SWAP. The large-batch phase with synchronized models transitions to the small-batch phase with diverging independent models. Test accuracy is computed by averaging independent models. The mechanism behind SWAP is visualized by plotting error on a plane with outputs of different algorithm phases. Orthogonal vectors u, v span the plane containing \u03b8 1 , \u03b8 2 , \u03b8 3 to plot loss values. The mechanism behind SWAP is visualized by plotting error on a plane with outputs of different algorithm phases. Orthogonal vectors u, v span the plane containing \u03b8 1 , \u03b8 2 , \u03b8 3 to plot loss values for the resulting model. Training and testing error for the CIFAR10 dataset are shown in Figure 2. The loss value is plotted at location (\u03b1, \u03b2) using a weight vector \u03b8. Batch-norm statistics are computed, and test/train accuracies are evaluated. Figure 2 shows training/testing error for CIFAR10 dataset. 'LB' represents phase one output, 'SGD' represents a worker's output in phase two, and 'SWAP' is the final output. In Figure 2a, training error level-sets form a convex basin, with 'LB' and 'SGD' outputs on the outer edges. Phase 2 moves the model to a different side. During phase 2, the model moved to a different side of the convex basin, with 'LB' and 'SGD' outputs on the outer edges. The final model ('SWAP') is closer to the center of the basin, showing variations in the topology causing 'LB' and 'SGD' points to fall in regions of higher error. The final model ('SWAP') is closer to the center of the basin, less affected by variations in the topology compared to 'LB' and 'SGD' points. In Figure 3, 'SGD1', 'SGD2', 'SGD3' lie at different sides of the training error basin while 'SWAP' is closer to the center. In Figure 3, the 'SWAP' point is less affected by changes in topology compared to 'SGD' points. The worker points in the basin have higher testing errors than 'SWAP', which remains close to the center. In Figure 3b, the change in topology causes worker points to have higher testing errors than 'SWAP', which remains close to the center. The authors argue that in later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. In later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. The distribution is centered at the local minimum, with a covariance that grows proportionally with the learning rate and inversely proportional to the batch size. The shape depends on the Hessian of the mean loss and covariance of the gradient, with all mass concentrated near the 'shell' of the ellipsoid, making it unlikely for SGD to access the interior. The authors argue that the high-dimensional Gaussian distribution of weight iterates in SGD is concentrated near the ellipsoid's 'shell', making it unlikely for SGD to access the interior. Sampling weights from different SGD runs will choose weights spread out on the ellipsoid's surface, with their average closer to the center. This justifies sampling from different runs during SWAP. The authors argue that sampling weights from different SGD runs will choose weights spread out on the ellipsoid's surface, with their average closer to the center. This justifies sampling from different runs during SWAP. During SWAP, runs starting in the same basin of attraction will converge to the same stationary distribution, allowing for independent sample generation. The cosine similarity between gradient descent direction and SWAP output decreases as training progresses, indicating a large angle between the gradient direction and the center of the basin towards the end of training. In SWAP, the cosine similarity between gradient descent direction and SWAP output decreases as training progresses, indicating a large angle between the gradient direction and the center of the basin towards the end of training. This allows for faster progress towards the center by averaging samples from different sides of the basin. In SWAP, the process moves mostly orthogonally to the basin, and progress slows. Averaging samples from different sides of the basin makes faster progress towards the center. The performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated in this section. Best hyper-parameters were found using grid searches. Training is done using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation is done using cutout, and a custom ResNet 9 model is used for training. For experiments in this subsection, best hyper-parameters were found using grid searches. Training was done with mini-batch SGD using Nesterov momentum and weight decay. Data augmentation was performed with cutout, and a custom ResNet 9 model was used. Experiments were conducted on one machine with 8 NVIDIA Tesla V100 GPUs using Horovod for distributed computation. Statistics were collected over 10 runs. Phase one involved 4096 samples per batch with 8 GPUs. The experiments on the DAWNBench leaderboard were conducted using Horovod on one machine with 8 NVIDIA Tesla V100 GPUs. Different settings were used for phase one and phase two, with varying batch sizes and number of GPUs. The experiments on the DAWNBench leaderboard were conducted using Horovod on one machine with 8 NVIDIA Tesla V100 GPUs. GPUs were used with different batch sizes and number of workers in phase one and phase two. The experiments included large-batch training on 8 GPUs, small-batch training on 2 GPUs, and SWAP training with 8 workers each using one GPU. Test accuracies and training times were compared for the different training methods. The study compared test accuracies and training times for models trained with small-batch only, large-batch only, and SWAP methods. Significant improvement in test accuracies was observed after averaging the results. The study compared test accuracies and training times for models trained with small-batch only, large-batch only, and SWAP methods. Significant improvement in test accuracies was observed after averaging the results. Training with small-batches achieved higher testing accuracy than training with large-batches but took longer. SWAP terminated in comparable time to large-batch runs and achieved accuracies on par with or better than small batch training. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch can achieve state-of-the-art training speeds for CIFAR10, reaching 94% test accuracy in 27 seconds, compared to 37 seconds with 4 Tesla V100 GPUs. The DAWNBench competition leader trains CIFAR10 to 94% accuracy in 27 seconds using SWAP with 8 Tesla V100 GPUs, compared to 37 seconds with 4 GPUs. The DAWNBench competition leader achieved 94% accuracy on CIFAR10 in 27 seconds using SWAP with 8 Tesla V100 GPUs, compared to 37 seconds with 4 GPUs. SWAP was also used to accelerate a fast-to-train ImageNet model with modified learning rates and batch sizes. Small-batch experiments ran for 28 epochs, while large-batch experiments doubled batch size and learning rates for 22 epochs. The large-batch experiments in SWAP phase 1 used doubled batch sizes and learning rates on 16 Tesla V100 GPUs for 22 epochs. In SWAP phase 2, two workers with 8 GPUs each used small-batch settings for 6 epochs. Doubling the batch size reduced test accuracies, but SWAP recovered generalization performance with faster training times. Results are shown in Table 3. In a large-batch experiment, two independent workers each with 8 GPUs used small-batch settings for 6 epochs. Doubling the batch size decreased test accuracies, but SWAP recovered generalization performance with faster training times. Results are compiled in Table 3. In a large-batch experiment, accelerations were achieved by doubling batch size, GPUs, and learning rate with no tuning. SWAP compared with SWA algorithm for accuracy and training time improvements. In a large-batch experiment, accelerations were achieved by doubling batch size, GPUs, and learning rate with no tuning. For SWAP, the schedule switches from modified to original as phases progress. SWAP is compared with SWA algorithm for accuracy and training time improvements on CIFAR100 dataset. Models are sampled and averaged differently for SWA and SWAP. In this section, the weight averaging algorithm is applied to the CIFAR100 dataset. SWA and SWAP are compared by sampling models with the same number of epochs. SWA averages models with 10 epochs in-between, while SWAP uses 8 independent workers for 10 epochs each. The goal is to see if SWA can match the test accuracy of small-batch training on a large-batch training run. Initial training cycles with cyclic learning rates are used to sample 8 models. We investigate if SWA can improve test accuracy in large-batch training runs by averaging models with 10 epochs. However, results show that SWA did not enhance training accuracy. The large-batch training run achieved lower training accuracy, but SWA did not improve it. Executing SWA using small batches after a large-batch training run resulted in reaching test accuracy of a small-batch run, but took more than three times longer to compute the model. SWA can reach test accuracy of a small-batch run but takes more than three times longer to compute the model compared to SWAP. The learning rate schedule for small-batch SWA and SWAP starts from the best model found by solely small-batch training. The peak learning rate is selected using grid-search. Small-batch SWA and SWAP use a fixed cycle length and count, with the peak learning rate selected through grid-search. SWA achieves better accuracy than SWAP with a 6.8x longer training time. Phase two starts from the model generated in phase 1 for the experiment in section 5.1. After selecting the SWA schedule through grid-search, we reuse the peak learning rate settings in SWAP. Starting phase two from the model generated in phase 1, small-batch SWA achieves higher accuracy than SWAP with 6.8x longer training time. To explore SWAP's speed-up over SWA, constraints are relaxed by increasing the phase two schedule and sampling more models, resulting in a test accuracy of 79.11% in 241. SWAP is an algorithm that uses a variant of Stochastic Weight Averaging to improve model generalization with large mini-batches. By relaxing constraints and increasing the phase two schedule, SWAP achieved a test accuracy of 79.11% in 241 seconds, 3.5x faster than SWA. SWAP is a novel algorithm that enhances model generalization by using large mini-batches initially and then averaging weights from models trained with small-batches. This approach resulted in a test accuracy of 79.11% in 241 seconds, 3.5x faster than traditional methods. Using large-batches initially and then refining with small-batches through SWA or SWAP results in a final model with good generalization performance trained in a shorter time. This approach allows models to achieve good performance even when using large-batches in the initial stages of training. Refining models with large-batch runs using SWA or SWAP can lead to good generalization performance, comparable to models trained with small-batches only. This is confirmed in image classification datasets like CIFAR10, CIFAR100, and ImageNet. Visualizations show that averaged weights are closer to the center of a training loss basin compared to models from stochastic gradient descent. The basin where large mini-batch runs converge seems to be the same as where smaller batches converge. Our method for refining models with large-batch runs requires choosing a transition point between large and small batches. The basin where large mini-batch runs converge appears to be the same as where smaller batches converge, suggesting a connection between regions of good and bad generalization performance. The method for refining models with large-batch runs requires choosing a transition point between large and small batches. Future work will focus on a principled method to select this transition point. Additionally, future research will explore the behavior of SWAP with other optimization schemes such as Layer-wise Adaptive Rate Scaling (LARS) and mixed-precision training. In future work, the focus will be on choosing a transition point between large and small batches for refining models. The behavior of SWAP with optimization schemes like LARS, mixed-precision training, post-local SGD, and NovoGrad will also be explored. Parameters used in experiments are provided in Section 5.1. The parameters used in the experiments of Section 5.1 include momentum and weight decay constants kept at 0.9 and 5 \u00d7 10 \u22124 respectively. Tables 5 and 6 list the remaining hyperparameters, with a stopping accuracy of 100% indicating the maximum number of epochs were used."
}