{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A model based on differential equations is used, but inferring parameters is challenging. A proposed method generalizes Bayesian optimization for approximate inference of posterior distributions. In this work, a generalization of the Bayesian optimization framework is proposed for approximate inference. The method learns approximations to the posterior distribution using Stein variational gradient descent on estimates from a Gaussian process model. Preliminary results show the method's effectiveness in likelihood-free inference for reinforcement learning environments. The method proposed in this work involves using Stein variational gradient descent on Gaussian process model estimates for distribution approximation. It focuses on likelihood-free inference for reinforcement learning environments when estimating parameters of a physical system based on observed data. Recent methods address the efficiency of simulations in robotics and reinforcement learning by constructing conditional density estimators from joint data, overcoming resource constraints. Recent methods in robotics and reinforcement learning address the challenge of limited simulations by constructing conditional density estimators or learning approximations to the likelihood function for more efficient use of resources. Recent methods in robotics and reinforcement learning aim to improve simulation efficiency by using joint data {\u03b8 i ,\u0177 i } N i=1 and combining variational inference methods with Bayesian optimization to reduce the number of simulator runs needed. The paper proposes an active learning approach using Bayesian optimization to reduce simulator runs. It combines variational inference methods with Bayesian optimization for data efficiency. Thompson sampling is used to refine variational approximations, and parameters for new simulations are proposed using Stein variational gradient descent over Gaussian process samples. The paper introduces a Thompson sampling strategy to refine variational approximations for a black-box posterior using Stein variational gradient descent over Gaussian process samples. It also includes a method to optimally subsample variational approximations for batch evaluations of simulator models. The approach presented in the paper utilizes a Gaussian process to estimate a distribution that approximates a posterior distribution over simulator parameters given observations from a target system. The goal is to find the optimal distribution by minimizing a discrepancy measure between simulator outputs and observations. The paper presents a Bayesian optimization approach to find the optimal distribution that approximates a posterior distribution over simulator parameters. This approach minimizes the discrepancy between simulator outputs and observations using a kernelized Stein discrepancy. The algorithm does not require gradients of the target distribution or its closed form availability. The algorithm presented in the paper uses a black-box approach to minimize the discrepancy between the target distribution and the optimal distribution q. It includes a GP model, Thompson sampling acquisition function, and kernel herding procedure to select simulator parameters efficiently. The algorithm in the paper uses a black-box approach with a GP model, Thompson sampling, and kernel herding to select simulator parameters efficiently. It bypasses modeling the map from q's parameters to KSD by learning q directly via Stein variational gradient descent (SVGD) to model the synthetic likelihood. The paper uses Stein variational gradient descent (SVGD) to learn q directly and model the synthetic likelihood. A Gaussian Process (GP) is used to model g : \u03b8 \u2192 \u2212\u2206 \u03b8, defining a synthetic likelihood function. The choice of \u2206 \u03b8 for experiments is detailed in Section 3, with background on the Kernelized Stein Discrepancy (KSD) in the appendix. The paper proposes using Stein variational gradient descent (SVGD) to model the synthetic likelihood with a Gaussian Process (GP) for approximating the simulations-observations discrepancy \u2206 \u03b8. The GP provides a cheap and differentiable approximation for running a black-box simulator, enabling the application of SVGD in the Bayesian optimization loop. The paper suggests using Thompson sampling with a Gaussian Process (GP) to select candidate distributions in Bayesian optimization. Thompson sampling accounts for uncertainty by sampling functions. The paper proposes using Thompson sampling with a Gaussian Process to select candidate distributions in Bayesian optimization, accounting for uncertainty by sampling functions from the GP posterior. Thompson sampling with Gaussian Process involves sampling functions from the GP posterior to select candidate distributions in Bayesian optimization. For models like sparse spectrum Gaussian processes (SSGPs), weights are sampled from a multivariate Gaussian to constitute a sample from the posterior. The acquisition function is defined based on the objective in Equation 1, with SVGD representing the variational distribution as a set of particles forming an empirical distribution. The acquisition function in Bayesian optimization is defined based on the objective in Equation 1, with SVGD representing the variational distribution as a set of particles forming an empirical distribution. The particles are optimised via smooth perturbations to guide them to local maxima and encourage diversification. The particles in Bayesian optimization are initialised as i.i.d. samples from the prior and optimised through smooth perturbations using the SSGP kernel. The first term in the definition of \u03b6 guides particles to local maxima of the objective, while the second term promotes diversification. Gradients of logp n are available for SSGP models with differentiable mean functions, and for a uniform prior, \u2207 \u03b8 log p(\u03b8) = 0 almost everywhere. The second term in Bayesian optimization encourages diversification by repelling nearby particles. Gradients of logp n are available for SSGP models with differentiable mean functions. For a uniform prior, \u2207 \u03b8 log p(\u03b8) = 0 almost everywhere. Selecting a distribution q n, evaluations of \u2206 \u03b8 from samples \u03b8 \u223c q n are needed to update the GP model. Representing q by a large number of particles M improves Algorithm 1. The text discusses the selection of a distribution q n and the use of a large number of particles M to improve Algorithm 1 in Bayesian optimization. The large number of particles helps in exploring the approximate posterior surface, allowing SVGD to find distant modes. However, these particles should not be used directly as sample parameters for simulations due to their high cost. Instead, observations are collected by sampling simulator parameters from q n. The text discusses the use of Kernel herding to select a subset of query parameters {\u03b8 n,j } S j=1 \u2282 \u0398 from a large number of particles to optimize Bayesian optimization. This method minimizes error on empirical estimates under distribution q by constructing samples that minimize the maximum mean discrepancy (MMD) between the kernel embedding of q. Kernel herding selects a subset of query parameters {\u03b8 n,j } S j=1 \u2282 \u0398 to optimize Bayesian optimization by minimizing error on empirical estimates under distribution q. The method constructs samples that minimize the maximum mean discrepancy (MMD) between the kernel embedding of q. Kernel herding selects informative samples for Bayesian optimization by minimizing the maximum mean discrepancy (MMD) between the kernel embedding of q and its subsampled version. The procedure uses the GP posterior kernel to select samples that are most informative for the model, providing an embedding for q that accounts for previously observed locations in the GP data. The GP posterior kernel is used by the DBO algorithm to select informative samples for Bayesian optimization. The sampling scheme replaces \u03c8 q with \u03c8 n q, and experimental results compare DBO against mixture density networks in synthetic data scenarios. The DBO algorithm uses \u03c8 n q in its sampling scheme, as summarized in Algorithm 1. Experimental results compare DBO with mixture density networks on synthetic data scenarios, specifically in the OpenAI Gym's 3 cart-pole environment. The method is evaluated using a dataset of parameters sampled from p(\u03b8) and simulator outputs \u0177 \u03b8. The experiment evaluates a method proposed by et al. (2019) on OpenAI Gym's 3 cart-pole environment using a dataset of parameters sampled from the prior p(\u03b8) and simulator outputs \u0177 \u03b8. Summary statistics \u03b3 were calculated as in Ramos et al. (2019). Results show the method can recover the target. Further details on the experimental setup can be found in Appendix B. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, with results showing improved approximations compared to the MDN approach. The method successfully recovered the target system's curve-shaped posterior and outperformed MDN in terms of MMD. An open-source implementation is available online. The paper introduced a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations than the MDN method. It demonstrated the method's potential for reinforcement learning applications, highlighting its efficiency in inferring parameters of a classical reinforcement learning environment. Future work includes further exploration of the method's capabilities. The method demonstrated potential for reinforcement learning applications by efficiently inferring parameters of a classical environment. Future work includes scalability and theoretical analysis. Code is available at the provided link. The method showed promise for reinforcement learning applications by efficiently inferring parameters of a classical environment. Future work involves scalability and theoretical analysis. Code can be found at the provided link for further exploration. The posterior over g is determined by \u00b5 N (\u03b8) := \u00b5 0 (\u03b8) + \u03c6(\u03b8), with GP posterior mean and variance functions denoted by \u00b5 N and \u03c3 2 N. Fast incremental updates are proposed to reduce time complexity in updating the GP posterior when given a new observation pair. This method allows for updating the GP posterior with time complexity O(M^2), which is constant with respect to the number of data points N."
}