{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is investigated. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters while improving performance. The use of branches also provides additional regularization. Tighter coupling of these branches by averaging their log-probabilities enhances the learning of better representations. In this study, a new architecture called \"coupled ensembles\" is introduced, which involves splitting parameters into parallel branches and averaging their log-probabilities to improve representation learning. This approach can be applied to various neural network architectures and has shown promising results with DenseNet-BC, achieving low error rates with a parameter budget of 25M. The study introduces \"coupled ensembles,\" a new architecture that splits parameters into parallel branches to improve representation learning. This approach can be applied to various neural network architectures and has shown promising results with DenseNet-BC, achieving low error rates on CIFAR-10, CIFAR-100, and SVHN tasks. The study introduces \"coupled ensembles,\" a new architecture that splits parameters into parallel branches to improve representation learning. DenseNet-BC achieves low error rates of 2.72%, 15.13%, and 1.42% on CIFAR-10, CIFAR-100, and SVHN tasks with 50M total parameters. The design of early convolutional architectures involved choices of hyper-parameters like filter size and number of filters. The design of early convolutional architectures involved choices of hyper-parameters like filter size and number of filters. Recent architectures like ResNet and DenseNet follow a template with fixed filter size of 3x3, down-sampling by maxpooling or strided convolutions, and doubling feature maps after each down-sampling. These models also use skip-connections between non-contiguous layers. Our work extends the template used by state-of-the-art models like ResNet and DenseNet by introducing \"coupled ensembling\", decomposing the network into branches similar to complete CNNs. This approach achieves comparable performance with fewer parameters. The proposed template introduces \"coupled ensembling\" by decomposing the network into branches similar to complete CNNs. It achieves comparable performance with fewer parameters, showing that splitting parameters among branches is better than having a single branch. Different ways to combine activations are compared, with an arithmetic mean being the most effective. In this paper, the authors demonstrate the benefits of splitting parameters among branches rather than having a single branch in neural networks. They also find that combining activations with an arithmetic mean leads to improved performance on CIFAR and SVHN datasets with reduced parameter count. Further ensembling of coupled ensembles results in additional improvements. The paper discusses the benefits of splitting parameters among branches in neural networks and combining activations with an arithmetic mean to improve performance on CIFAR and SVHN datasets. Further ensembling of coupled ensembles leads to additional improvements. The paper is organized into sections discussing related work, introducing coupled ensembles, evaluating the proposed approach, and comparing it with the state of the art. The paper introduces the concept of coupled ensembles and evaluates the proposed approach by comparing it with existing methods. The network architecture is similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, but differs in training a single model composed of branches. The proposed approach, similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, involves training a single model composed of branches with a fixed parameter budget for improved performance. This differs from training each member or column separately and utilizing multiple models of fixed size. Multi-branch architectures involve training a single model composed of branches with a fixed parameter budget for improved performance. The activations of the branches are combined by their log-probabilities over target categories, and the same input is used for all branches with different preprocessing blocks. This approach has been successful in various vision applications. Multi-branch architectures have been successful in vision applications by combining branch activations using log-probabilities over target categories. Modifications using grouped convolutions and template building blocks have been proposed for improved feature extraction. Proposed modifications in vision applications involve using grouped convolutions and template building blocks at the global model level, rather than just at the building block level of base architectures like ResNet and Inception. This approach specifies an \"element block\" template that is replicated in parallel for improved feature extraction. Our proposed modification involves a generic restructuring of the CNN at the global model level, using an \"element block\" template replicated in parallel branches. Shake-Shake regularization BID5 further enhances performance by introducing a stochastic mixture of branches, achieving good results on CIFAR datasets. However, it requires more epochs for convergence compared to the base model. Our method involves restructuring the CNN at a global level using an \"element block\" template replicated in parallel branches. Shake-Shake regularization BID5 improves performance with a stochastic mixture of branches on CIFAR datasets, but requires more epochs for convergence. BID26 explores connecting layers across paths in a ResNet for information exchange. Our method involves a global restructuring of the CNN using an \"element block\" template replicated in parallel branches. In contrast to BID26's approach of connecting layers across paths in a ResNet, our method is a generic rearrangement of architecture parameters without introducing additional choices. Additionally, we confirm empirically that our configuration efficiently utilizes parameters. Our method involves a global restructuring of the CNN using an \"element block\" template replicated in parallel branches, which does not require modification at a local level of residual blocks. We empirically confirm that this configuration leads to efficient parameter usage. Ensembling neural networks is a reliable technique to improve model performance by combining outputs from multiple trainings of the same architecture, resulting in better overall task performance. Ensembling neural networks is a reliable technique to improve model performance by combining outputs from multiple trainings of the same architecture, resulting in better overall task performance. Our proposed model architecture involves a single model with parallel branches trained jointly, similar to the residual block in ResNet and ResNeXt. The proposed model architecture involves a single model with parallel branches trained jointly, similar to ResNet and ResNeXt. Arranging parameters into parallel branches leads to improved performance. Classical ensembling can still be applied for fusion of independently trained models. The proposed model architecture involves parallel branches trained jointly, similar to ResNet and ResNeXt. Arranging parameters this way improves performance. Ensembling on checkpoints during training, rather than fully converged models, is efficient and leads to higher performance within the same training time budget. The proposed model architecture involves parallel branches trained jointly, similar to ResNet and ResNeXt, leading to significant performance improvement. Ensembling on checkpoints during training is efficient, resulting in higher performance within the same training time budget. The approach aims to maintain model size while improving performance or achieving the same performance with a smaller model size. The proposed model architecture involves parallel branches trained jointly, similar to ResNet and ResNeXt, leading to significant performance improvement. The model aims to maintain model size while improving performance or achieving the same performance with a smaller model size. The model comprises several branches, each producing a score vector for target classes. The proposed model architecture involves parallel branches trained jointly, similar to ResNet and ResNeXt, leading to significant performance improvement. Each branch uses DenseNet-BC or ResNet with pre-activation as element blocks. The branches are combined using the average of their individual log probabilities over target classes in the fuse layer. Different operations for the fuse layer are explored in Section 4.4. The model architecture involves parallel branches using DenseNet-BC and ResNet with pre-activation as element blocks. The fuse layer combines branches by averaging their individual log probabilities over target classes. The classification task involves samples associated with one class from a finite set, applicable to tasks like CIFAR, SVHN, and ILSVRC. Neural network models for classification tasks output a score vector for target classes, followed by a SoftMax layer for probability distribution. Training includes a loss layer. Neural network models output a score vector for target classes, followed by a SoftMax layer for probability distribution. Training includes a loss layer, such as negative log-likelihood. Recent network architectures for image classification have variations before the last fully connected layer. The differences among recent network architectures for image classification lie in the setup before the last fully connected layer. Ensemble models are trained separately and their predictions are fused by averaging them. The \"element block\" takes an image as input and produces a vector of N values, parametrized by a tensor W. Ensemble models are trained separately, and their predictions are fused by averaging them. This fusion process is equivalent to using a \"super-network\" with parallel branches and an averaging layer. Supernetworks are not commonly implemented due to memory constraints, but the AVG layer operation can be implemented separately. In our model setup, parallel branches produce score vectors for target categories, which are fused through a \"fuse layer\". The AVG layer operation can be implemented separately due to memory constraints in supernetworks. The model setup includes parallel branches generating score vectors for target categories, fused through a \"fuse layer\". The AVG layer operation can be implemented separately or placed after the last FC layer and before the SM layer. Three options are explored to combine score vectors during training and inference. The model setup involves parallel branches generating score vectors for target categories, fused through a \"fuse layer\". Three options are explored to combine score vectors during training and inference: Activation average, Probability average, and Log Likelihood average. The model setup includes parallel branches generating score vectors for target categories, fused through a \"fuse layer\". The branches average log-probabilities and loss, leading to improved performance with lower parameters. The parameter vector W of the composite branched model is the concatenation of parameter vectors W e of the e element blocks. The proposed architecture involves multiple branches with score vectors combined by averaging log probabilities of target categories, leading to improved performance with lower parameters. The parameter vector W of the composite branched model is the concatenation of parameter vectors W e of the element blocks. The architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets. The proposed architecture involves multiple branches with score vectors combined by averaging log probabilities of target categories. Parameters are in the \"element blocks\" as the \"fuse layer\" has none. The architecture is tested on CIFAR-10, CIFAR-100, and SVHN datasets with specific image sizes and categories. Hyper parameters are set based on the original descriptions of the \"element block.\" SVHN dataset consists of 73,257 training images and 26,032 testing images, all 32\u00d732 pixels in size and categorized into 10 classes. Hyper parameters are set based on the original descriptions of the \"element block.\" Input images for CIFAR datasets are normalized by subtracting the mean image and dividing by the standard deviation, with standard data augmentation during training. During training on CIFAR datasets, standard data augmentation is used, including random horizontal flips and random crops. For SVHN, no data augmentation is used, but a dropout ratio of 0.2 is applied in the case of DenseNet. Testing is done after normalizing the input in the same way as during training, with error rates given in percentages as an average of the last 10 epochs. In DenseNet training on SVHN, a dropout ratio of 0.2 is applied, and testing is done after input normalization. Error rates are given as an average of the last 10 epochs. DenseNet-BC experiments on CIFAR-100 use a single NVIDIA 1080Ti GPU with optimal micro-batch 2. The DenseNet-BC experiments on CIFAR-100 were conducted using a single NVIDIA 1080Ti GPU with optimal micro-batch 2. The experiments considered a configuration with a single branch as the baseline reference point for comparison with an ensemble of independent models. The DenseNet-BC experiments on CIFAR-100 used a single branch configuration as the baseline. Comparing with an ensemble of independent models, the jointly trained branched configuration had a lower test error rate despite the same number of trainable parameters. The results in Table 1 compare error rates of jointly trained branched configuration and single branch models with similar parameters. The branched configuration outperformed the single branch models in terms of test error. The results in Table 1 show that the error rate of the multi-branch model (17.61) is lower than that of a single branch model (20.01) with similar parameters. This indicates that organizing parameters into parallel branches leads to better performance as the number of parameters increase. The error rate of the multi-branch model in Table 1 is lower than that of a single branch model, showing the efficiency of organizing parameters into parallel branches as the number of parameters increase. The relation between the number of branches and model performance is analyzed in Section 4.5, comparing different choices of the \"fuse layer\" in the proposed branched model. In Section 4.5, the efficiency of organizing parameters into parallel branches is highlighted, showing that a multi-branch model outperforms a single branch model. The performance of a branched model with different \"fuse layer\" combinations is compared in experiments, with a focus on training and prediction fusion after different layers. Table 1 shows Coupled Ensembles of DenseNet-BCs with e = 4 and various fusion combinations. Experiments evaluate the best training and prediction fusion combinations for a branched model with e = 4. Table 1 compares Coupled Ensembles of DenseNet-BCs with different \"fuse layer\" combinations to a single branch model, showing top-1 error rates on the CIFAR-100 test set. In a study comparing Coupled Ensembles of DenseNet-BCs with various \"fuse layer\" combinations to a single branch model, the top-1 error rates on the CIFAR-100 test set were evaluated. The number of branches (e = 4), element block architecture (columns \"L\" and \"k\"), and different \"fuse layer\" choices during training were analyzed. The performance of each branch, as well as the choices for \"fuse layer\" during inference, were also considered. Time taken for training epochs and testing each image was recorded. The performance of models under different \"fuse layer\" operations for inference is shown in Table 1, which includes models with parameters obtained using different training methods. The average and standard deviations are computed for independent trainings comprising 4 models. Time for training epochs and testing each image is also provided. The performance of models under different \"fuse layer\" operations for inference is shown in Table 1. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with significantly more parameters. The average error rate of \"element blocks\" trained jointly in coupled ensembles with LSM fusion is also discussed. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with significantly more parameters. Coupled ensembles with LSM fusion result in lower error rates for \"element blocks,\" indicating improved learning of complementary features and better representations. The coupling of ensembles with LSM fusion leads to lower error rates compared to individual training, indicating improved learning of complementary features and better representations. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal for improved performance. When training with ensemble combinations, all branches update consistently for improved performance. Using 4 branches with a parameter budget of 3.2M reduces the error rate to 17.61, compared to 20.01 for a single branch model. However, training with Avg. FC shows that individual branches do not perform well. Ensemble combinations outperform single branch networks in weight updates. Using 4 branches with a 3.2M parameter budget reduces error rate to 17.61 from 20.01. However, training with Avg. FC shows poor performance of individual branches due to unrelated FC instances. The Avg. FC prediction works better than Avg. SM prediction due to the non-linearity of the SM layer distorting the FC average. Avg. FC training with Avg. FC prediction yields good performance as FC values transmit more information than SM layer probabilities. In this section, the study investigates the optimal number of branches for a given model parameter budget using DenseNet-BC as the base model on CIFAR-100 dataset. The Avg. FC prediction outperforms the Avg. SM prediction due to the FC layer transmitting more information than the SM layer probabilities. The experiments involve using Avg. LSM for the training \"fuse layer\" in the branched models. The study investigates the optimal number of branches for a given model parameter budget using DenseNet-BC on CIFAR-100. Results are shown in table 3 for different configurations of branches, depth, and growth rate. The study explores the optimal number of branches for DenseNet-BC models on CIFAR-100, with results displayed in table 3 for various configurations of branches, depth, and growth rate. DenseNet-BC parameter counts are quantified based on the depth and growth rate values, with the e value in the coupled ensemble version also playing a critical role, especially in moderate size models. Model configurations with parameters just below a certain threshold were selected. The study focuses on determining the optimal number of branches for DenseNet-BC models on CIFAR-100, with parameter counts strongly influenced by depth, growth rate, and the e value in the ensemble version. Model configurations with parameters just below the target threshold were chosen for fair comparisons. The optimal configuration for the considered case is e = 3, L = 70, k = 9, with 800k parameters. In the study on DenseNet-BC models on CIFAR-100, models with parameters just below the target were compared. The optimal configuration had e = 3, L = 70, k = 9, with 800k parameters. Using 2 to 4 branches showed significant performance gains over the single branch case, with error rates decreasing from 22.87 to 21.10. The error rates decreased from 22.87 to 21.10 with 2 to 4 branches in the DenseNet-BC model. Performance gains were significant compared to the single branch case, but using 6 or 8 branches performed worse. Model performance was robust to slight variations in parameters. The DenseNet-BC model showed improved performance with 2 to 4 branches, but using 6 or 8 branches performed worse. Model performance was robust to slight variations in parameters, showing the coupled ensemble approach and DenseNet-BC architecture's robustness. The gain in performance led to increased training and prediction times due to smaller values of k reducing throughput for smaller models. The use of smaller values of k in coupled ensembles leads to increased training and prediction times, impacting throughput for smaller models. Evaluation was done using DenseNet-BC and ResNet BID8 architectures to compare performance with existing models. The current state of the art models were evaluated, including ResNet BID8 with pre-activation as the element block. Coupled ensembles were used and results were presented in a table, showing performance of single models and ensembles. Further ensembling involving multiple models was also considered. In the study, coupled ensembles with ResNet pre-act as element block and different network sizes of DenseNet-BC were evaluated for performance. Coupled ensembles showed significantly better performance compared to single branch models, even with comparable or higher parameters. In section 4.7, coupled ensembles with ResNet pre-act as element block and varying network sizes of DenseNet-BC were tested. Results showed that coupled ensembles outperformed single branch models, even with similar or higher parameter counts. The trade-off between depth L and growth rate k was found to not be critical for a given parameter budget. The trade-off between depth L and growth rate k is not critical for a given parameter budget in DenseNet-BC models. Experimentation with different configurations showed that choosing between the number of branches e, depth L, and growth rate k is not crucial as long as e \u2265 3 (or even e \u2265 2 for small networks). Error rates were higher for single branch DenseNet-BC compared to multi-branch versions. For the largest model, experiments were conducted with different numbers of branches (e = 3, 6, 8) in addition to the single-branch and multi-branch versions. Error rates were higher in the single branch DenseNet-BC compared to the multi-branch versions, possibly due to different initializations and non-deterministic computations. The coupled ensemble of DenseNet-BC models outperforms DenseNet-BC's reported performance, with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The difference in error rates may be attributed to conservative error rate measures, statistical differences in initializations, and non-deterministic computations. The coupled ensemble of DenseNet-BC models outperforms reported performance, with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The larger models perform better than current state-of-the-art implementations. Comparisons with meta-learning scenarios are presented in the supplementary material. The coupled ensemble approach with DenseNet-BC models outperforms current state-of-the-art implementations, achieving lower error rates on CIFAR 10, CIFAR 100, and SVHN datasets. The Shake-Shake S-S-I model BID5 performs slightly better on CIFAR 10. The limitation of the coupled ensemble approach is the network size that can fit into GPU memory and training time constraints. Further improvements were achieved through classical ensembling based on independent trainings. The coupled ensemble approach is limited by GPU memory and training time constraints. Classical ensembling was used for further improvements, as the performance plateaus after a small number of models. For example, SGDR with snapshots BID16 showed significant improvement from 1 to 3 models but not much from 3 to 16 models. The classical approach plateaus after a small number of models, so the coupled ensemble approach was used for further improvements. SGDR with snapshots BID16 showed significant improvement from 1 to 3 models but not much from 3 to 16 models. Ensembling four large coupled models resulted in a significant gain by fusing two models, with minimal improvement from further fusion of three or four models. Ensembling four large coupled models resulted in a significant gain by fusing two models, with minimal improvement from further fusion of three or four models. The ensembles of coupled ensemble networks outperform all state of the art implementations including other ensemble-based ones. The error rates between single and multi-branch models are highlighted in the lower half of table 3. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" resembling standalone CNN models. These ensembles of coupled ensemble networks outperform all state of the art implementations, showing significant improvement in error rates compared to single branch models. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" resembling standalone CNN models. These ensembles of coupled ensemble networks outperform all state of the art implementations, showing significant improvement in error rates compared to single branch models. With 13M parameters, the error rate for CIFAR-100 is 16.24%, surpassing single branch models with double the parameters. The approach utilizes intermediate score vectors from each element block, coupled via a \"fuse layer\", and averages log-probabilities during training for improved performance. The proposed approach involves using multiple \"element blocks\" as standalone CNN models, with intermediate score vectors coupled via a \"fuse layer\". This leads to a significant performance improvement over single branch configurations, although there is a slight increase in training and prediction times. The approach shows the best performance for a given parameter budget, as demonstrated in tables 3 and 4, and in figure 2. The proposed approach involves using multiple \"element blocks\" as standalone CNN models, with intermediate score vectors coupled via a \"fuse layer\". This results in improved performance over single branch configurations, with a slight increase in training and prediction times. The approach demonstrates the best performance for a given parameter budget, as shown in tables 3 and 4, and in figure 2. Additionally, individual \"element block\" performance is better when trained together compared to independently. The increase in processing times is mainly due to sequential processing of branches during forward and backward passes, affecting data parallelism on GPUs. The increase in training and prediction times for multiple \"element blocks\" is mainly due to sequential processing of branches during forward and backward passes, reducing data parallelism on GPUs. To address this issue, data parallelism can be extended to the branches by implementing multiple 2D convolutions in parallel or by using multiple GPUs for spreading the workload. To address the decrease in data parallelism on GPUs when processing multiple branches, two solutions are proposed. Firstly, parallel implementation of multiple 2D convolutions can be used to extend data parallelism within the branches. Alternatively, when using multiple GPUs, spreading the branches over them can also help. Initial experiments on ImageNet show that coupled ensembles have lower errors compared to single branch models with the same parameter budget. Further experiments will be conducted in the future. Preliminary experiments on ImageNet show that coupled ensembles have lower errors with multiple GPUs compared to single branch models. The common structure of test and train networks is illustrated in Figures 3 and 4, showcasing the placement of the averaging layer before the SM layer. Model instances do not require the same architecture. In the test version, the averaging layer can be placed after the last FC layer of element block instances and before the SM layer, which is then \"factorized\". In the train version, the averaging layer can be placed after the last FC layer, after the SM layer, or after the LL layer. Element blocks from other groups can be reused in their original form. In the train version, the averaging layer can be placed after the last FC layer, SM layer, or LL layer. Element blocks from other groups are reused in their original form for efficiency and meaningful comparisons. Each branch is defined by a parameter vector containing the same parameters as the original implementation. When training in coupled mode, a script splits the global parameter vector W into individual parameter vectors W e for each branch. The same global parameter vector W is used for all versions in coupled networks for both training and testing. The global parameter vector W is split into individual parameter vectors W e for each branch in coupled networks. This allows for combining different training and prediction conditions, even if they are not all equally efficient. The overall network architecture is determined by a global hyper-parameter specifying the train versus prediction conditions. The parameter vector W is used with the same split and defining element block functions to combine different training and prediction conditions. The overall network architecture is determined by global hyper-parameters specifying the mode, number of branches, and placement of the AVG layer. The global hyper-parameters specify the train versus test mode, number of branches, and placement of the AVG layer. Larger models may require data batches to be split into micro-batches for training with smaller batch sizes. When training larger models, data batches may need to be split into micro-batches with b/m elements each to accommodate a batch size larger than 64. Gradient accumulation over micro-batches helps approximate the equivalent gradient of processing data as a single batch, with the exception of BatchNorm layer due to the use of micro-batch statistics. BatchNorm layer uses micro-batch statistics for normalization during forward pass, which differs from using whole batch statistics. However, this difference does not significantly impact parameter updates during training. To ensure fair comparison among different models, parameter updates are done using gradient for a batch, while forward passes are done with micro-batches for optimal throughput. Memory usage in single branch cases depends on network depth and batch size. The micro-batch \"trick\" is used to adjust memory usage while keeping the batch size constant. This approach does not affect performance. In the single branch case, memory usage depends on network depth and batch size. The micro-batch \"trick\" adjusts memory usage without affecting performance. The multi-branch version requires more memory only if branch width is reduced. Hyper-parameter search was conducted to optimize parameters. In the multi-branch version, memory usage remains constant if branch width is maintained. Hyper-parameter search experiments showed that reducing both width and depth was the best option. Training with 25M parameters was done within 11GB memory using micro-batch sizes of 16 for single-branch and 8 for multi-branch versions. In practice, for \"full-size\" experiments with 25M parameters, training was done within 11GB memory using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch versions. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but this does not significantly increase speed or improve performance. Training with 25M parameters was done within 11GB memory using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch versions. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but this does not significantly increase speed or improve performance. The test time equivalence between FC average and logsoftmax for branches E1, E2, etc., shows similar results as in table 1 for coupled ensembles with two branches. Using only two branches still provides a significant gain over a single branch. Using only two branches provides a significant gain over a single branch architecture of comparable size, as shown in table 1. An extended version of table 2, TAB8, evaluates the variation of depth L and growth rate k for a fixed parameter count. Performance remains stable against changes in (L, k) compromise. The experiment was also conducted on a validation set with a 40k/10k random split. In an extended version of table 2, TAB8, the performance of different combinations of depth L and growth rate k was evaluated for a fixed parameter count. The experiment showed that the (L = 82, k = 8, e = 3) combination was predicted to be the best on the test set, while the (L = 70, k = 9, e = 3) combination appeared slightly better but not statistically significant. The experiment compared different combinations of depth and growth rate, concluding that (L = 82, k = 8, e = 3) was predicted to be the best on the test set. The (L = 70, k = 9, e = 3) combination showed slightly better performance but not statistically significant. Comparing parameter usage and performance of branched coupled ensembles with meta learning techniques raised issues of reproducibility and statistical significance. In comparing parameter usage and performance of branched coupled ensembles with meta learning techniques, issues of reproducibility and statistical significance arise due to various sources of variation in performance measures. These sources include the underlying framework for implementation (Torch7 and PyTorch), random seed for network initialization, and CuDNN non-determinism during training. The observed variation in performance measures can be attributed to different sources, such as the underlying framework used (Torch7 and PyTorch), random seed for network initialization, CuDNN non-determinism during training, and fluctuations in batch normalization. GPU associative operations are fast but non-deterministic, leading to varying results even with the same tool and seed. Fluctuations in batch normalization can occur regardless of learning rate, momentum, and weight decay settings. The choice between the model obtained after the last epoch or the best performing model also impacts training outcomes. During training, the influence of learning rate, SGD momentum, and weight decay remains consistent even when set to 0. The choice between using the model from the last epoch or the best performing model can impact training outcomes. Different random seeds can lead to variations in evaluation measures due to numerical determinism and Batch Norm moving average. Choosing the best performing model involves looking at test data and considering factors like numerical determinism, Batch Norm moving average, and epoch sampling. Different random seeds can lead to variations in evaluation measures, but properly designed and trained neural networks should have similar performance. The dispersion of evaluation measures due to random initialization is small but not negligible. Different random seeds can lead to different local minima in neural networks. Despite a small dispersion, comparisons between models can be complicated as differences below the dispersion may not be significant. Classical statistical tests may not be helpful in this scenario. The dispersion in neural network models due to different random seeds can complicate comparisons, as differences below the dispersion may not be significant. Classical statistical tests may not be effective in this case, making it challenging to quantify the relative importance of different effects. The dispersion in neural network models due to different random seeds can complicate comparisons. Experiments on a moderate scale model show the importance of seed variation in DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results from different seed configurations were compared using Torch7 and PyTorch. In the case of DenseNet-BC with L = 100, k = 12 on CIFAR 100, different effects were observed. The results were compared using Torch7 and PyTorch with varying seed configurations. Performance measures included error rates at the last epoch, average error rates of the last 10 epochs, and error rates of the model with the lowest error rate. The data was analyzed for minimum, median, maximum, and mean\u00b1standard deviation. In analyzing DenseNet-BC with L = 100, k = 12 on CIFAR 100, performance measures such as error rates at the last epoch, average error rates of the last 10 epochs, and error rates of the model with the lowest error rate were compared using Torch7 and PyTorch with varying seed configurations. The data was analyzed for minimum, median, maximum, and mean\u00b1standard deviation over 10 measures from 10 identical runs. Additionally, the root mean square of the standard deviation of fluctuations on the last 10 epochs was presented. There is no significant difference between Torch7 and PyTorch implementations in analyzing DenseNet-BC with L = 100, k = 12 on CIFAR 100. Using the same seed or different seeds also does not show a significant difference in results. The study found no significant difference between Torch7 and PyTorch implementations when analyzing DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results were consistent whether using the same seed or different seeds. Reproducing exact results was not possible due to observed dispersion. There was also no significant difference between means computed on the single last epoch and the last 10 epochs. The standard deviation of measures over 10 runs was consistently smaller. The study found no significant difference between Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results were consistent with different seeds, but exact reproduction was not possible due to dispersion. There was no significant difference between means computed on the single last epoch and the last 10 epochs. The standard deviation of measures over 10 runs was consistently smaller. The mean of the measures computed on the 10 runs is significantly lower when the measure is taken at the best epoch than when they are computed either on the single last epoch or on the last 10 epochs. The mean of the measures computed on the 10 runs is significantly lower when the measure is taken at the best epoch. This measure involves using the test data for selecting the best model. A method for ensuring reproducibility and fair comparisons is proposed. Proposing a method for ensuring reproducibility and fair comparisons in selecting the best model based on error rate during training. Avoiding bias in absolute performance estimation by not tuning on the test set. The text discusses the importance of avoiding bias in absolute performance estimation by not tuning on the test set when selecting the best model based on error rate during training. It also mentions that using the error rate at the 10 last iteration is preferred when a single experiment is conducted. In experiments, using the error rate at the 10 last iterations is preferred for absolute performance estimation. The standard deviation is smaller for the 10 last iterations compared to the last iteration. Using the average error rate from the last 10 epochs is recommended for robustness and conservatism in model evaluation. In experiments, using the average error rate from the last 10 epochs is recommended for robustness and conservatism in model evaluation. This approach is more robust and conservative compared to using a single value or the error rate from the last iteration. The study recommends using the average error rate from the last 10 epochs for robustness and conservatism in model evaluation, especially in the case of SVHN experiments with a smaller number of bigger epochs. Comparisons between single-branch and multi-branch architectures show a clear advantage for multi-branch networks, despite their longer training time. In this study, comparisons between single-branch and multi-branch architectures have shown a clear advantage for multi-branch networks, despite their longer training time. The training time of multi-branch networks is currently significantly longer than single-branch networks, but there are ways to reduce it. The study aims to investigate if multi-branch architectures can still improve over single-branch ones at a constant training time budget. The study aims to investigate if multi-branch architectures can improve over single-branch ones at a constant training time budget by exploring ways to reduce training time, such as reducing iterations, parameter count, or increasing width while reducing depth. Results are compared to a baseline single branch DenseNet-BC L = 190, k = 40, e = 1 with a training time of about 80 hours. To reduce training time, options include reducing iterations, parameter count, or increasing width while reducing depth for a constant parameter count. Results for CIFAR 10 and 100 are compared to a baseline single branch DenseNet-BC L = 190, k = 40, e = 1 with a training time of about 80 hours. Results are shown for CIFAR 10 and 100 with statistics on 5 runs for each configuration, comparing different options to reduce training time, such as reducing epochs, depth, or matching parameter count and training time with a single-branch baseline. Results are shown for reducing training time by adjusting the number of training epochs, depth, or matching parameter count and training time with a single-branch baseline. Options include reducing epochs to 188, decreasing depth from 106 to 88, and matching parameter count and training time with different configurations. These adjustments still outperform the single-branch baseline. In a comparison of DenseNet-BC models with different parameters, results show that even with reduced parameter budgets, they outperform the single-branch baseline. DenseNet-BC L = 88, k = 20, e = 4 performs better than the single-branch baseline with lower parameter count and training time. This comparison is done in the context of low training data scenarios. In a comparison of DenseNet-BC models with different parameters, DenseNet-BC L = 88, k = 20, e = 4 outperforms the single-branch baseline with reduced parameter count and training time in low training data scenarios. The comparison involves training single branch models and multi-branch coupled ensembles with the same number of parameters on STL-10 and a 10K balanced random subset of CIFAR-100. Results from training a single branch model and multi-branch coupled ensemble on STL-10 and a 10K subset of CIFAR-100 show that coupled ensembles outperform single branch models with the same parameter budget. Preliminary experiments on ILSVRC2012 also support this finding. Results from training a single branch model and multi-branch coupled ensemble on STL-10 and a 10K subset of CIFAR-100 show that coupled ensembles outperform single branch models with the same parameter budget. Preliminary experiments on ILSVRC2012 (Russakovsky et al., 2015) compared a baseline single-branch model DenseNet-169-k32-e1 with a coupled ensemble DenseNet-121-k30-e2, showing significant performance improvements. Experiments were conducted on images of size 256\u00d7256 with data augmentation involving random horizontal flips and random crops of size 224\u00d7224. The experiments were conducted on images of size 256\u00d7256 with data augmentation involving random horizontal flips and random crops of size 224\u00d7224. A baseline single-branch model DenseNet-169-k32-e1 was compared with a coupled ensemble DenseNet-121-k30-e2, showing that the coupled ensemble approach with two branches produced better results. Further experiments with full-sized images and increased data augmentation are planned but will be after the deadline. The experiments showed that the coupled ensemble approach with two branches outperformed the baseline model, even with a constant training time budget. Further experiments with full-sized images and increased data augmentation are planned but will be after the deadline."
}