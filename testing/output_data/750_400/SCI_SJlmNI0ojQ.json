{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. Contextual acoustic word embeddings can be constructed directly from these models using attention distribution, showing competitive performance on standard sentence evaluation tasks. In this paper, methods are described to construct contextual acoustic word embeddings from a supervised sequence-to-sequence acoustic-to-word speech recognition model using attention distribution. These embeddings show competitive performance on standard sentence evaluation tasks and match text-based embeddings in spoken language understanding tasks. The paper describes methods to create contextual acoustic word embeddings from a speech recognition model, showing competitive performance against word2vec. These embeddings are evaluated in spoken language understanding tasks and match text-based embeddings. The task of learning fixed-size representations for variable length data like words or sentences is a focus of current research in natural language processing. Methods like word2vec, GLoVE, CoVe, and ELMo have become popular for their utility in NLP tasks. In speech recognition, research focuses on creating embeddings from short-term audio features, presenting challenges due to variability in speakers, acoustics, and microphones. In speech recognition, research focuses on creating embeddings from short-term audio features, presenting challenges due to variability in speakers, acoustics, or microphones. Prior work involved aligning speech and text by providing word boundaries or chunking input speech into fixed-length segments. These techniques aim to learn acoustic word embeddings. Our work aims to construct individual acoustic word embeddings at the utterance level, capturing contextual dependencies in speech that previous techniques overlook. Our work presents methods for obtaining acoustic word embeddings from utterance-level acoustics, in contrast to techniques that ignore contextual dependencies in speech. The model used is trained for direct Acoustic-to-Word (A2W) speech recognition, jointly learning to segment and classify input speech. In this paper, different methods are presented for obtaining acoustic word embeddings from utterance-level acoustics using an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word (A2W) speech recognition. The model learns to automatically segment and classify input speech into individual words, eliminating the need for pre-defined word boundaries. The A2W model is trained at the utterance level, allowing for the learning of acoustic word embeddings in the context of their containing sentence. The A2W model automatically segments input speech into words and learns acoustic word embeddings in the context of their sentence. Attention is used to align words to acoustic frames and construct Contextual Acoustic Word Embeddings, showing usability in non-transcription tasks. In this paper, the authors demonstrate the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). They show that CAWE constructed from a speech recognition model are competitive with text-based word2vec embeddings on standard sentence evaluation benchmarks. Additionally, they highlight the utility of CAWE in a speech-based downstream task of Spoken Language Understanding. The authors demonstrate the competitiveness of constructing Contextual Acoustic Word Embeddings (CAWE) directly from a speech recognition model compared to text-based word2vec embeddings. They also show the utility of CAWE in a speech-based downstream task of Spoken Language Understanding, highlighting the potential for transfer learning similar to VGG in vision or CoVe in natural language understanding. Pretrained speech models can be used for transfer learning in Spoken Language Understanding tasks, similar to VGG in vision or CoVe in natural language understanding. A2W modeling has been pursued using CTC and S2S models, requiring large amounts of training data with large vocabularies. Recent progress shows the possibility of training these models with smaller data amounts but limited vocabularies. Recent progress in the field has shown the potential to train speech models with smaller amounts of data and restricted vocabularies, using solutions like character or sub-word units for out-of-vocabulary words. This approach deviates from pure-word models but addresses the issue of rare word generation. Recent advancements in speech model training involve restricting vocabulary to words occurring at least 5 or 10 times. Solutions for out-of-vocabulary words include using characters or sub-words. A study presented a S2S model for large vocabulary A2W recognition with a 30,000-word vocabulary. Further work improved training for this task, showing that direct A2W models can learn word boundaries automatically. In a study, a S2S model was presented for large vocabulary A2W recognition with a 30,000-word vocabulary. Further work improved training for this task, showing that direct A2W models can learn word boundaries automatically. The model was expanded to learn acoustic embeddings, exploring ways to obtain these embeddings through unsupervised learning methods. The S2S model can learn word boundaries without supervision and is the best pure-word model. Various methods have been explored to learn acoustic word embeddings, with most using unsupervised learning except for one that uses a supervised CNN-based speech recognition model. This model simplifies training but limits scalability for contextual word learning. BID6 uses a supervised CNN-based speech recognition model with short speech frames as input, while BID4 proposes an unsupervised method using a fixed context of words in the past and future to learn speech embeddings. The drawbacks include the fixed context and the need for forced alignment between speech and words for training. Learning text-based word embeddings is also a rich area of research. The models aim to learn contextual word embeddings from utterance level acoustics. BID4 introduces an unsupervised method for speech embeddings, but it has limitations like fixed context and requiring alignment between speech and words. Research also focuses on text-based word embeddings, including contextualized ones like BID2 and BID3, which are beneficial for various text-based tasks. BID2 learns contextual word embeddings from a trained machine translation model for reuse in downstream tasks. Our work ties a speech recognition model with learning contextual word embeddings from speech. The model structure includes an encoder network, a decoder network, and an attention model. The encoder maps input acoustic feature vectors. Our S2S model, similar to the Listen, Attend and Spell model BID10, consists of an encoder network, a decoder network, and an attention model. The encoder utilizes a pyramidal multi-layer bi-directional LSTM network to map input acoustic features into higher-level features. The decoder, also an LSTM network, models the output distribution conditioned on previous predictions. The encoder in the S2S model is a pyramidal multi-layer bi-directional LSTM network that converts input acoustic features into higher-level features. The decoder, also an LSTM network, predicts the output distribution based on previous predictions using an attention mechanism. The decoder generates targets from the higher-level features using a location-aware attention mechanism to enforce monotonicity in alignments. The decoder in the S2S model generates targets using an attention mechanism that enforces monotonicity in alignments. It utilizes a location-aware attention mechanism to calculate attention for the current time step, leading to a peaky distribution. The model follows the same experimental setup and hyper-parameters as word-based models, with the difference of learning 300 dimensional acoustic features. The model described in Figure 1 obtains acoustic word embeddings from an end-to-end trained speech recognition system. It uses hidden representations from the encoder and attention weights to construct embeddings. The model follows the same setup as word-based models but learns 300 dimensional acoustic feature vectors instead of 320 dimensional ones. Our method for obtaining acoustic word embeddings involves using hidden representations from the encoder and attention weights from the decoder. This approach is similar to CoVe BID2 for text embeddings but addresses the challenge of aligning input speech with output words. The model learns 300 dimensional acoustic feature vectors instead of 320 dimensional ones. Our method involves constructing \"contextual\" acoustic word embeddings using attention weights from the decoder. Unlike CoVe BID2, our approach addresses the alignment issue between input speech and output words by utilizing a location-aware attention mechanism. This mechanism assigns higher probability to certain frames, leading to a peaky attention distribution. We leverage this property in an A2W model to segment continuous speech into words automatically, as demonstrated in our previous work BID12. The location-aware attention mechanism assigns higher probability to specific frames, creating a peaky attention distribution. This property is utilized in an A2W model to segment continuous speech into words automatically, as demonstrated in previous work BID12. The attention weights on acoustic frames reflect their importance in classifying a word, providing a correspondence between the frame and word within a given acoustic context. The process of constructing contextual acoustic word embeddings involves assigning attention weights to acoustic frames' hidden representations based on their importance in classifying a word. This creates a correspondence between the frame and word within a given acoustic context, allowing for the construction of word representations. This is illustrated in Figure 1, where hidden representations and their attention weights are colored according to their correspondence with a specific word. The model assigns attention weights to acoustic frames' hidden representations to construct word representations. The mappings of words to acoustic frames are obtained based on their attention weights. The model assigns attention weights to acoustic frames' hidden representations to construct word representations. The mappings of words to acoustic frames are obtained based on their attention weights. The model then describes three different ways of using attention to obtain acoustic word embeddings for a word. The model uses attention weights to create word representations from acoustic frames. Three methods are described: unweighted Average, Attention weighted Average, and maximum attention for obtaining acoustic word embeddings. The model utilizes attention weights to generate word representations from acoustic frames. Three methods are discussed: unweighted Average, Attention weighted Average, and maximum attention for acquiring acoustic word embeddings, collectively known as Contextual Acoustic Word Embeddings (CAWE). The Contextual Acoustic Word Embeddings (CAWE) are generated using attention weights to calculate the average and maximum attention scores for acoustic frames in speech recognition datasets like Switchboard corpus and How2 BID27 instructional videos. The 300 hour Switchboard corpus (LDC97S62) BID20 consists of telephonic conversations between 500 speakers with 3 million words. The How2 BID27 dataset has instructional videos with 3.5 million words. A2W achieves word error rates of 22.2% on Switchboard, 36.6% on CallHome, and 24.3% on How2 dev5 test sets. The A2W system achieves word error rates of 22.2% on Switchboard, 36.6% on CallHome, and 24.3% on How2 dev5 test sets. The embeddings are evaluated in 16 benchmark sentence evaluation tasks covering Semantic Textual Similarity, classification, sentiment analysis, question type, Subjectivity/Objectivity, opinion polarity, and entailment. The embeddings are evaluated in 16 benchmark sentence evaluation tasks covering Semantic Textual Similarity (STS), classification, sentiment analysis, question type, Subjectivity/Objectivity, opinion polarity, entailment, and semantic relatedness using the SICK dataset. The tasks measure Spearman's coefficient of correlation between embedding-based similarity and human scores, with scores ranging from -1 to 1. The curr_chunk discusses the evaluation of embeddings in various tasks such as sentiment analysis, opinion polarity, entailment, and semantic relatedness using the SICK dataset. The evaluation involves measuring correlation between embedding-based similarity and human scores, with scores ranging from -1 to 1. Training details mention the use of logistic regression for classification tasks. The evaluation of embeddings in classification tasks is done using logistic regression. CAWE-M outperforms U-AVG and CAWE-W on Switchboard and How2 datasets. The concatenation of CAWE and CBOW can be used as features in logistic regression without adding tunable embedding parameters. CAWE-M outperforms U-AVG and CAWE-W by 34% and 13% on Switchboard and How2 datasets in terms of average performance on STS tasks. CAWE-W performs worse than CAWE-M on STS tasks due to noisy estimation of word embeddings. U-AVG also performs worse than CAWE-W on STS and SICK-R tasks. The embeddings in CAWE-M are constructed using the most confident attention score, while U-AVG performs worse due to weighting all encoder hidden representations equally. The datasets for downstream tasks remain the same as described in Section 5.1. The text discusses the comparison between embeddings obtained from a speech recognition model and text-based word embeddings. The embeddings in CAWE-M are based on the most confident attention score, while U-AVG weights all encoder hidden representations equally. The datasets for downstream tasks are consistent with those described in Section 5.1. The CAWE model uses word embeddings trained on all transcripts to ensure a fair comparison with word2vec. A drawback of the A2W speech recognition model is its limited vocabulary coverage. Despite this, CAWE's performance is competitive with word2vec CBOW. The A2W speech recognition model has limited vocabulary coverage, recognizing only a fraction of the total words. However, the CAWE model's performance is competitive with word2vec CBOW. Evaluations show that acoustic embeddings combined with text embeddings outperform word2vec embeddings on multiple tasks. The concatenated embeddings of acoustic and text embeddings outperform word2vec embeddings on multiple tasks, showing that CAWE-M adds more information that improves the CBOW model. The gains are more prominent in Switchboard compared to the How2 dataset due to the characteristics of the speech data. CAWE-M improves CBOW embedding, with more gains in Switchboard than How2 due to speech characteristics. CAWE evaluated on ATIS dataset for Spoken Language Understanding, similar to Switchboard domain. For spoken language understanding evaluation, CAWE is tested on the ATIS dataset, which consists of airline reservation queries with intent and named entities. The model architecture includes an embedding layer, a single layer RNN variant (Simple RNN, GRU), a dense layer, and softmax, trained for 10 epochs. The model architecture for spoken language understanding evaluation on the ATIS dataset includes an embedding layer, a single layer RNN variant (Simple RNN, GRU), a dense layer, and softmax, trained for 10 epochs with RMSProp. The study demonstrates that direct speech-based word embeddings could lead to matching performance. In this study, the model is trained for 10 epochs with RMSProp using different seed values. Direct speech-based word embeddings show comparable performance to text-based embeddings on the ATIS dataset. Test scores are compared using CAWE-M, CAWE-W, and CBOW embeddings, fine-tuned for the task. In this study, the model is trained for 10 epochs with RMSProp using different seed values. Direct speech-based word embeddings show comparable performance to text-based embeddings on the ATIS dataset. Test scores are compared using CAWE-M, CAWE-W, and CBOW embeddings, fine-tuned for the task. The speech-based embeddings demonstrate matching performance in a downstream task, highlighting their utility. Acoustic word embeddings are learned from a sequence-to-sequence acoustic-to-word speech recognition model, with attention playing a key role in their construction. These acoustic embeddings are found to be highly competitive with word2vec (CBOW) text embeddings. The study presents a method to learn contextual acoustic word embeddings from a speech recognition model. The acoustic embeddings are competitive with word2vec text embeddings and outperform simple average methods in semantic tasks. These embeddings match text-based embeddings in spoken language understanding, making them useful for downstream speech tasks. Contextual acoustic word embeddings outperform simple average methods by up to 34% on semantic textual similarity tasks and match text-based embeddings in spoken language understanding. These embeddings can be used as pre-trained models for other speech-based downstream tasks, despite the additional complexity of noisy audio input. Future work will focus on scaling the model to larger corpora and vocabularies and comparing with non-contextual acoustic word embeddings. Contextual audio embeddings are expected to improve downstream tasks similar to text embeddings, despite the challenges of noisy audio input. Future plans include scaling the model to larger datasets and vocabularies and comparing with non-contextual acoustic word embedding methods. This research was supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook."
}