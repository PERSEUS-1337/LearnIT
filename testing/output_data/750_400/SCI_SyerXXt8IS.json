{
    "title": "SyerXXt8IS",
    "content": "Auto-generating stronger input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, particularly in the insect olfactory network. The network utilizes competitive inhibition, sparse connectivity, and Hebbian updates for rapid odor learning. MothNet, a computational model of the moth olfactory network, generates new features for use by standard ML classifiers, resulting in improved performance. MothNet, a computational model of the moth olfactory network, acts as a feature generator for ML classifiers, outperforming traditional methods like PCA and NNs. These \"insect cyborgs\" show significantly better performance on MNIST and Omniglot datasets, reducing test set errors by 20% to 55% and highlighting the potential of BNN-inspired feature generators in machine learning. The MothNet feature generator outperforms traditional methods like PCA and NNs on MNIST and Omniglot datasets, reducing test set errors by 20% to 55%. This highlights the potential of BNN-inspired feature generators in machine learning. The limited-data constraint in machine learning (ML) methods, especially neural nets (NNs), creates bottlenecks to deployment and constrains the types of problems that can be addressed. To improve ML methods' ability to learn from limited data, an architecture is proposed to automatically generate new class-separating features from existing features, inspired by biological neural nets (BNNs) that can learn rapidly even from a few samples. The architecture aims to enhance ML methods' learning from limited data by generating new class-separating features inspired by biological neural nets (BNNs) like the insect olfactory network, which can learn rapidly from just a few samples. The insect olfactory network, specifically the Antennal Lobe (AL) and Mushroom Body (MB), can rapidly learn new odors with just a few exposures. It utilizes competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. The synaptic connections are mostly random. This is demonstrated in the computational model MothNet, which shows rapid learning capabilities. The MothNet model, based on the insect olfactory network, demonstrates rapid learning of vectorized MNIST digits with superior performance using competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. The MothNet model showed rapid learning of vectorized MNIST digits with superior performance using competitive inhibition and sparsity in the MB. The MothNet model utilizes competitive inhibition and sparsity in the MB for rapid learning of vectorized MNIST digits. Weight updates only affect MB\u2192Readout connections, with Hebbian updates based on neural firing rates. The MothNet model utilizes competitive inhibition and sparsity in the MB for rapid learning of vectorized MNIST digits. Weight updates only affect MB\u2192Readout connections, with Hebbian updates based on neural firing rates. 5% to 15% of the total population are allowed to fire through global inhibition. The architecture was tested as a front-end feature generator for an ML classifier by combining MothNet with a downstream ML module. The MothNet model was tested as a front-end feature generator for an ML classifier by combining it with a downstream ML module. The Readouts of the trained AL-MB model were fed into the ML module as additional features for a non-spatial, 85-feature, 10-class task derived from the vMNIST dataset. The MothNet model acted as an automatic feature generator for a non-spatial task using the vMNIST dataset. The trained Mothnet Readouts significantly improved the accuracies of ML methods on the test set. The MothNet model, used for a non-spatial task with the vMNIST dataset, improved ML method accuracies on the test set by generating features that outperformed PCA, PLS, NNs, and transfer learning. The MothNet-generated features contained class-relevant information that enhanced ML accuracy significantly. The MothNet model improved ML method accuracies on the vMNIST dataset by generating features that outperformed PCA, PLS, NNs, and transfer learning. The insect-derived network encoded stronger features that significantly enhanced ML accuracy compared to other methods. vMNIST, with 85 pixels-as-features, provided an advantage as baseline ML methods did not achieve full accuracy at low N. The insect-derived network (MothNet) generated stronger features for ML accuracy on the vMNIST dataset compared to other methods. The vMNIST dataset, with 85 pixels-as-features, provided an advantage as baseline ML methods did not achieve full accuracy at low N. Full network architecture details and Matlab code for the MothNet model can be found in references [11] and [12]. The MothNet model (AL-MB) outperformed baseline ML methods on the vMNIST dataset. Full network architecture details and Matlab code can be found in references [11] and [12]. Experiments involved training ML methods on randomly generated MothNet instances from templates specifying connectivity parameters. MothNet instances were randomly generated from templates for simulations. Two sets of experiments were conducted: Cyborg vs baseline ML methods on vMNIST. Training samples were drawn from vMNIST for ML methods and MothNet. MothNet utilized time-evolved stochastic differential equation simulations and Hebbian updates. ML methods were retrained with Readout Neuron outputs from MothNet as additional features. These were the \"insect cyborgs\". MothNet was trained on vMNIST samples using stochastic differential equation simulations and Hebbian updates. ML methods were then retrained with MothNet's Readout Neuron outputs as additional features, creating \"insect cyborgs\". Trained ML accuracies of baselines and cyborgs were compared to assess gains. The study compared the effectiveness of MothNet features with features generated by conventional ML methods. Different feature generators were used, including PCA, PLS, and a pre-trained NN on vMNIST training samples. Feature generators used in the study included PCA, PLS, and a pre-trained NN on vMNIST training samples. PCA and PLS generated new features by projecting onto the top 10 modes, while the pre-trained NN used the (logs of the) 10 output units as features. CNNs were not used due to the lack of spatial content in vMNIST. For feature generation, PCA, PLS, and a pre-trained NN on vMNIST training samples were utilized. The pre-trained NN used the (logs of the) 10 output units as features and served as a front end to SVM and Nearest Neighbors. CNNs were not employed due to the absence of spatial content in vMNIST. Additionally, a NN with weights initialized by training on an 85-feature vectorized Omniglot dataset was used for transfer learning on the vMNIST data. MothNet features significantly improved accuracy of ML methods on Omniglot data, demonstrating effective capture of new class-relevant features. Non-spatial 10-class task also showed similar gains with MothNet-generated features. MothNet features improved ML accuracy on a 10-class task from the Omniglot dataset. The features outperformed other generators like PCA, PLS, and NN. Test accuracies ranged from 10% to 88% on the vMNIST baseline test set. MothNet features significantly outperformed other generators like PCA, PLS, and NN, increasing accuracy on the vMNIST ML baseline test set by 10% to 88%. The raw gains in accuracy due to MothNet features were evident across all ML models, with a relative reduction in test set error ranging from 20% to 55%. MothNet features improved accuracy across all ML models, with a relative reduction in test set error ranging from 20% to 55%. NN models benefited the most, showing a 40% to 55% reduction in test error. Even when ML baseline accuracy exceeded 75%, MothNet still improved accuracy by providing clustering information. The MothNet front-end improved ML accuracy by 40% to 55%, even when ML baseline accuracy was already above 75%. Gains were significant in almost all cases with N > 3, as shown in Table 1 with p-values. The cyborg framework was used on vMNIST with different feature generators like PCA, PLS, and NN. The MothNet front-end significantly improved ML accuracy, outperforming other methods like PCA, PLS, and NN as feature generators. Results showed a substantial increase in mean accuracy with N > 3, as indicated in Table 2. The MothNet architecture includes a competitive inhibition layer (AL) and a high-dimensional, sparse layer (MB). Testing the effectiveness of the MB alone, experiments were conducted using a pass-through AL layer. Despite this modification, significant improvements in accuracy were still observed. The MothNet architecture includes a competitive inhibition layer (AL) and a high-dimensional, sparse layer (MB). Testing the effectiveness of the MB alone, experiments were conducted using a pass-through AL layer. Cyborgs with a pass-through AL still showed significant improvements in accuracy over baseline ML methods, with gains generally between 60% and 100% of those with normal ALs. This suggests that the high-dimensional, trainable layer (MB) was most important, but the competitive inhibition of the AL layer added value. The gains of cyborgs with pass-through ALs were generally between 60% and 100% of those with normal ALs, indicating the importance of the high-dimensional, trainable layer (MB). Competitive inhibition of the AL layer added up to 40% of the total gain, with NNs benefiting the most from the AL layer. An automated feature generator based on a simple BNN with competitive inhibition and sparse projection into a high-dimensional layer was deployed. The AL layer added value by generating strong features, contributing up to 40% of the total gain. NNs benefitted the most from the AL layer. An automated feature generator based on a simple BNN with competitive inhibition, sparse projection, and Hebbian weight updates significantly improved learning abilities of standard ML methods on vMNIST and vOmniglot datasets. MothNet's pre-processing made class-relevant information accessible in raw feature distributions. The bio-mimetic feature generator MothNet significantly improved learning abilities of standard ML methods on vMNIST and vOmniglot datasets by making class-relevant information accessible in raw feature distributions. MothNet features outperformed standard methods like PCA, PLS, NNs, and pre-training, with a competitive inhibition layer enhancing classification by creating attractor basins focused on class-specific features. The competitive inhibition layer in MothNet enhances classification by creating attractor basins focused on class-specific features, increasing the effective distance between samples of different classes. The sparse connectivity from AL to MB has computational and anti-noise benefits, resembling sparse autoencoders. The insect MB, similar to sparse autoencoders, has sparse connectivity from AL to MB, providing computational and anti-noise benefits. However, MBs differ as they do not aim to match the identity function, have more active neurons than the input dimension, require no pre-training step, and can bake in structure with very few samples. The Mushroom Body (MB) is different from sparse autoencoders (SAs) as it does not aim to match the identity function, has more active neurons than the input dimension, requires no pre-training, and can improve classification with few samples. MB neurons do not have recurrent connections like Reservoir Networks, and the Hebbian update mechanism is distinct from backpropagation. The Mushroom Body (MB) requires very few samples to improve classification and differs from Reservoir Networks as MB neurons have no recurrent connections. The Hebbian update mechanism in MB is distinct from backpropagation, with weight updates occurring on a \"use it or lose it\" basis. The dissimilarity of optimizers (MothNet vs ML) may have increased total encoded information."
}