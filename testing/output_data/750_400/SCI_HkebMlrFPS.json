{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but we introduce a multi-mode phrase representation to capture different semantic facets of phrases or sentences. Our model predicts cluster centers from input text sequences to summarize the distribution of words in a pre-trained embedding space. The codebook embeddings serve as cluster centers summarizing co-occurring words in a pre-trained embedding space. An end-to-end neural model predicts these cluster centers from input text sequences, providing interpretable semantic representations and outperforming baselines on various NLP tasks. The per-phrase/sentence codebook embeddings offer a more interpretable semantic representation and outperform strong baselines on various NLP tasks. These embeddings are used in models that learn from co-occurrence statistics in raw text without supervision. The curr_chunk discusses NLP models like word2vec, GloVe, skip-thoughts, ELMo, and BERT that learn from co-occurrence statistics in raw text without supervision. These models use single embeddings for sentences or phrases, limiting their ability to capture multiple senses or topics. The curr_chunk discusses the limitations of NLP models like ELMo and BERT in capturing multiple senses or topics due to using single embeddings for sentences or phrases. Word sense induction methods and multi-mode word embeddings are proposed to represent each target word with multiple points in a distributional semantic space. The curr_chunk discusses word sense induction methods and multi-mode word embeddings to represent target words with multiple points in a distributional semantic space, addressing the limitations of NLP models like ELMo and BERT in capturing multiple senses. The curr_chunk discusses multi-mode representations of real property, illustrating how it can have different meanings in legal documents and philosophical discussions. Unlike topic modeling, which clusters all words in a corpus, these approaches cluster neighboring words for each target word. The curr_chunk discusses approaches to discovering senses by clustering neighboring words for each target word, unlike topic modeling which clusters all words in a corpus. Extending these representations to phrases or sentences faces efficiency challenges due to the large number of unique sequences. Extending multi-mode representations to phrases or sentences is challenging due to the large number of unique sequences, leading to efficiency issues in estimating and storing parameters for clustering-based approaches. Extending multi-mode representations to phrases or sentences is challenging due to the large number of unique sequences, leading to efficiency issues in estimating and storing parameters for clustering-based approaches. |K| is the number of modes/clusters, and |E| is the number of embedding dimensions. Estimating and storing such a large number of parameters take time and space. Our compositional model learns to predict the embeddings of cluster centers from the sequence of words in the target phrase to reconstruct the co-occurring distribution well. Our compositional model aims to predict cluster center embeddings from word sequences in target phrases to reconstruct co-occurring distributions effectively, addressing the challenge of sparseness in co-occurring statistics. The model uses a neural encoder and decoder to compress redundant parameters in local clustering problems, overcoming the challenge of sparseness in co-occurring statistics. Instead of clustering co-occurring words at test time, it learns to predict cluster center embeddings from word sequences in target phrases. The model utilizes a neural encoder and decoder to compress redundant parameters in local clustering problems by learning to predict cluster center embeddings from word sequences in target phrases. The model uses a neural network to predict cluster centers from target sequences during training, allowing for direct prediction of cluster centers at test time. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. Gradients are back-propagated to update cluster centers. The model utilizes a neural network to generate cluster centers in a specific order by using a coefficient matrix to match predicted cluster centers with observed word embeddings. Gradients are back-propagated to update cluster centers and train the model jointly. Experimental results show that the model outperforms traditional methods in capturing the meanings of words in unsupervised phrase similarity tasks. The proposed model utilizes cluster centers and weights of encoder and decoder to train jointly and capture compositional meanings of words better than traditional methods in unsupervised phrase similarity tasks. It can also measure asymmetric relations like hypernymy without supervision and outperforms single-mode alternatives in sentence representation, as demonstrated in an extractive summarization experiment. The model proposed in the current section utilizes strong baselines widely used in practice and can measure asymmetric relations like hypernymy without supervision. The multimode representation outperforms single-mode alternatives in sentence representation, as shown in an extractive summarization experiment. The training setup, objective function, and architecture of the prediction mode are described in subsequent sections. The training setup, objective function, and architecture of the prediction mode are described in Section 2.1, 2.2, and 2.3, respectively. The model represents sentences using codebook embeddings predicted by a sequence to embeddings model, aiming to reconstruct co-occurring words while avoiding negatively sampled words. The model generates codebook embeddings to reconstruct co-occurring words and avoid negatively sampled words. The sequence of words in the corpus is represented as a target sequence with start and end positions. The model reconstructs co-occurring words in the corpus using codebook embeddings and avoids negatively sampled words. The target sequence is represented with start and end positions, and neighboring words are considered related to the sequence. Training signals differ for sentences and phrases, requiring the reconstruction of neighboring words within a fixed window size. The training signal for reconstructing neighboring words in a fixed window size differs for sentences and phrases. Separate models are needed for each representation if both are desired. The goal is to cluster words that could potentially occur beside a sequence, rather than the actual co-occurring words in the training corpus. To train models for both phrases and sentences, separate models are required due to differences in co-occurring words. The focus is on clustering words that could potentially occur together, rather than the actual words in the training corpus. This allows for learning to predict co-occurring words by observing similar sequences, emphasizing semantics over syntax. The model focuses on learning to predict co-occurring words by observing similar sequences, emphasizing semantics over syntax. It considers word order information in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in a pre-trained word embedding space. The model focuses on predicting co-occurring words based on similar sequences, emphasizing semantics over syntax. It considers word order in the input sequence but disregards the order of co-occurring words. Co-occurring words are modeled in a pre-trained word embedding space using a matrix arrangement. The model predicts co-occurring words based on similar sequences, emphasizing semantics over syntax. It arranges word embeddings into matrices and predicts cluster centers for input sequences using a neural network model with a fixed number of clusters. The impact of different cluster numbers is discussed in experiments. The predicted cluster centers for input sequences are denoted as a matrix in our neural network model. The number of clusters is fixed to simplify the design, with the impact of different cluster numbers discussed in experiments. The reconstruction loss in k-means clustering is calculated in the word embedding space using a permutation matrix to match cluster centers and co-occurring words. In the experimental section, the reconstruction loss of k-means clustering in word embedding space is discussed. A permutation matrix is used to match cluster centers and co-occurring words, allowing neural networks to generate centers in any order. Non-negative sparse coding relaxes constraints by allowing positive coefficients, with models using this loss learning to generate diverse K. In this work, the use of Non-negative sparse coding (NNSC) is preferred over kmeans loss for generating diverse cluster centers in neural networks. NNSC allows positive coefficients, leading to smoother optimization and better capturing of conditional co-occurrence distribution compared to kmeans loss. Using NNSC loss, diverse K cluster centers are generated, while kmeans loss results in collapsed cluster centers that cannot capture the conditional co-occurrence distribution well. The smoother optimization of NNSC makes it easier for neural networks to learn, compared to kmeans loss. The reconstruction error is defined using NNSC, with a hyper-parameter \u03bb controlling sparsity. Coefficients are constrained to avoid learning to predict centers with small magnitudes. The proposed NNSC loss generates diverse cluster centers, unlike kmeans loss which results in collapsed centers. The reconstruction error is defined with a hyper-parameter controlling sparsity, and coefficients are constrained to prevent learning small magnitude centers. The proposed loss efficiently minimizes L2 distance in a pre-trained embedding space. The proposed loss efficiently minimizes L2 distance in a pre-trained embedding space by estimating M Ot using convex optimization. This approach allows for end-to-end training by treating M Ot as a constant and back-propagating gradients through the neural network. In a pre-trained embedding space, M Ot is estimated using convex optimization, allowing for end-to-end training by back-propagating gradients through the neural network. The loss function for each sequence prevents the network from predicting the same global topics regardless of input. Our method extends Word2Vec by encoding compositional meaning and decoding multiple embeddings. The neural network architecture is similar to a seq2seq model, using the same encoder to transform input sequences into contextualized embeddings. Our neural network architecture extends Word2Vec by encoding compositional meaning and decoding multiple embeddings. The encoder transforms input sequences into contextualized embeddings to map sentences with similar word distribution closer together. Unlike typical seq2seq models, our decoder outputs a sequence of embeddings instead of words, eliminating the need for discrete decisions. The encoder in our neural network architecture maps sentences with similar word distribution closer together by generating contextualized embeddings. Unlike traditional seq2seq models, our decoder produces a sequence of embeddings instead of words, removing the need for discrete decisions. The <eos> embedding is used as the sentence representation, and different linear layers are applied to capture various aspects in the codebook embeddings. The decoder in the neural network architecture generates contextualized embeddings to capture different aspects in the codebook embeddings. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation due to the complexity of compressing multiple facets into a single embedding. The attention on contextualized word embeddings from the encoder significantly impacts validation loss for sentence representation, as there are too many facets to compress into a single embedding. However, the attention does not have a significant effect on phrase representation. The encoder and decoder can be replaced with other architectures in this flexible framework. In a flexible framework, the attention on contextualized word embeddings from the encoder has a significant impact on validation loss for sentence representation. However, it does not affect phrase representation much. The encoder and decoder can be replaced with other architectures, such as (bi-)LSTMs, allowing for the incorporation of additional input features like the author of the sentence. The cluster centers predicted by the model are visualized in Table 1. The model's flexibility allows for the replacement of encoder and decoder architectures with options like (bi-)LSTMs. This enables the incorporation of additional input features, such as the author of the sentence. Cluster centers predicted by the model are visualized in Table 1, summarizing target sequences effectively. Codebook embeddings capture semantic facets of phrases or sentences, improving topic evaluation. The model's flexibility allows for the replacement of encoder and decoder architectures with options like (bi-)LSTMs. Cluster centers predicted by the model effectively summarize target sequences. Codebook embeddings capture semantic facets of phrases or sentences, improving topic evaluation using pre-trained GloVe embeddings. Codebook embeddings using pre-trained GloVe embeddings can enhance unsupervised semantic tasks by improving topic evaluation. The model is trained on Wikipedia 2016, removing stop words from co-occurring words. Noun phrases are considered in phrase experiments, with boundaries extracted using regular expression rules on POS tags. Sentence boundaries and POS tags are detected using spaCy. No additional resources like PPDB are required for the models. The model is trained on Wikipedia 2016 with stop words removed from co-occurring words. Noun phrases are considered in experiments, with boundaries extracted using regular expression rules on POS tags. Sentence boundaries and POS tags are detected using spaCy. The models do not require additional resources like PPDB and are compared with baselines using only raw text and sentence/phrase boundaries. This setup is practical for domains with low resources such as scientific literature. Our models do not need additional resources like PPDB and are compared with baselines using only raw text and sentence/phrase boundaries. The number of dimensions in our transformers is set as the GloVe embedding size (300) to control the effect of embedding size. Due to limited computational resources, all models are trained using one modern GPU within a week. However, the relatively small model size causes our models to underfit the data after a week. Comparing with BERT is challenging due to this underfitting issue. Due to limited computational resources, models are trained using one GPU for a week with GloVe embedding size set at 300 dimensions. However, the small model size causes underfitting after a week, making it challenging to compare with BERT, which is trained on a masked language modeling loss. BERT is compared to the models in terms of training parameters, output dimensions, corpus size, and computational resources. BERT uses a word piece model to address out-of-vocabulary issues, while unsupervised performances are provided for reference. BERT uses a word piece model to alleviate the out-of-vocabulary problem and provides unsupervised performances based on cosine similarity. Standard benchmarks for evaluating phrase similarity include Semeval 2013 task 5(a) English and Turney 2012. BiRD and WikiSRS are recent datasets with ground truth phrase similarities derived from human annotations. Semeval 2013 and Turney 2012 are benchmarks for phrase similarity evaluation. BiRD and WikiSRS contain human-annotated phrase similarities. Semeval 2013 distinguishes similar from dissimilar phrase pairs. Turney's task involves identifying the most similar unigram to a query bigram. BiRD and WikiSRS measure phrase relatedness. In Turney (5), the goal is to identify the most similar unigram to a query bigram among 5 candidates. Turney (10) adds negative phrase pairs by pairing the reverse of bigram with unigrams. BiRD and WikiSRS measure phrase relatedness, while our model evaluates phrase similarity using transformer encoder embeddings and cosine similarity. Our method is labeled as Ours Emb. The model evaluates phrase similarity by averaging contextualized word embeddings from a transformer encoder to compute cosine similarity. It also calculates a symmetric distance SC by comparing normalized codebook embeddings of phrases. Negative distance is used to rank similar phrases. Our method, Ours Emb, computes the reconstruction error between normalized codebook embeddings of phrases to determine similarity. Negative distance is used to rank similar phrases. Comparison with 5 baselines, including GloVe Avg and Word2Vec Avg, shows strong performance. Our method, Ours Emb, computes similarity by comparing normalized codebook embeddings of phrases using negative distance. Performance is compared with 5 baselines including GloVe Avg and Word2Vec Avg, with results shown in Table 2. The FCT LM Emb model outperforms baselines in SemEval 2013 and Turney datasets, showing the effectiveness of non-linearly composing word embeddings for prediction. Our models significantly outperform baselines in 4 datasets, showing the effectiveness of non-linearly composing word embeddings. The performance of Ours (K=1) is usually slightly better than Ours (K=10), supporting the finding that multi-mode embeddings may not improve performance in word similarity benchmarks. The performance of Ours (K=10) remains strong compared with baselines, indicating that similarity performance is not sensitive to the number of clusters. This alleviates the problem of selecting K in practice. STS benchmark (Cer et al., 2017) is a widely used sentence similarity task. Each model predicts a. The performances of Ours (K=10) remain strong compared to baselines, showing that similarity performance is not affected by the number of clusters. This alleviates the issue of selecting K in practice. STS benchmark (Cer et al., 2017) is a common sentence similarity task where models predict semantic similarity scores between sentence pairs. In the sentence similarity task, models predict semantic similarity scores between sentence pairs and compare them with average similarity annotations using the Pearson correlation coefficient. Performance is also evaluated on datasets with lower ground truth similarities, known as STSB Low. Additionally, comparisons are made with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). In addition to BERT CLS, BERT Avg, and GloVe Avg, comparisons are made with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). Arora et al. (2017) propose weighting words in sentences based on a constant \u03b1 and word probability p(w). \u03b1 is set to 10^-4 in STS benchmark, followed by post-processing to remove unnecessary elements. The method proposed by Arora et al. (2017) involves weighting words in sentences based on a constant \u03b1 and word probability p(w). The post-processing method, GloVe SIF, removes the first principal component estimated from the training distribution. Another approach, GloVe Prob_avg, does not remove principal components. The performance of the (weighted) average embedding is strong. The GloVe SIF method, based on Arora et al. (2017), removes the first principal component estimated from the training distribution. It is noted that post-processing may not be desired in some applications, as highlighted by Singh et al. (2018). The performance of the (weighted) average embedding suggests considering word embeddings in addition to sentence embeddings for measuring sentence similarity, which is challenging with other sentence representation methods. The (weighted) average embedding performance indicates the importance of considering word embeddings along with sentence embeddings for measuring sentence similarity. Multi-facet embeddings in the same space as word embeddings can be used to estimate word importance in predicting co-occurring words. The multi-facet embeddings in the same space as word embeddings can be used to estimate word importance in predicting co-occurring words. The importance of a word in a sentence is computed by calculating cosine similarity with predicted codebook embeddings and summing all similarities. The importance of words in a sentence is determined by calculating cosine similarity with predicted codebook embeddings and summing all similarities. Our simple importance weighting for words in a query sentence is compared with results from GloVe Avg, GloVe Prob_avg, and GloVe SIF. Our method outperforms WMD and BERT Avg, especially in STSB Low. Our method, utilizing GloVe Avg, GloVe Prob_avg, and GloVe SIF, outperforms WMD and BERT Avg, particularly in STSB Low. The benefits of multi-mode representation are evident in our results, with significant improvements when using attention weighting. The proposed method outperforms WMD and BERT Avg, especially in STSB Low, demonstrating the benefits of multi-mode representation and attention weighting. A variant using bi-LSTM as the encoder and LSTM as the decoder performs worse than the transformer alternative. The variant of the method using a bi-LSTM as the encoder and LSTM as the decoder performs worse than the transformer alternative, with higher validation loss and inferior performance on Table 3. This variant, similar to skip-thoughts, outperforms ST Cos, supporting the approach of ignoring word order in the NNSC loss. The model is applied to HypeNet for hypernymy detection based on co-occurring words. The variant significantly outperforms ST Cos in the NNSC loss approach for hypernymy detection on HypeNet dataset by clustering co-occurring words embeddings. The asymmetric scoring function defined in the study improves hypernym detection by clustering co-occurring words embeddings, outperforming baselines with symmetric similarity measurement. The study introduces an asymmetric scoring function that enhances hypernym detection by clustering co-occurring word embeddings. The method outperforms baselines with symmetric similarity measurement, showing improved accuracy in detecting hypernyms. The study introduces an asymmetric scoring function to improve hypernym detection by clustering word embeddings. Our (K=1) performs similarly to Our (K=10). The objective is to find a summary A with normalized embeddings that reconstruct the word distribution in the document. The importance of a word w is determined by \u03b1 \u03b1+p(w). The summary A consists of T sentences A 1 ... A T, and the embedding set of the summary is the union of the embedding sets of the sentences. The extractive summarization method described in the current chunk optimizes the embedding of sentences in a document by selecting sentences greedily. Multiple codebook embeddings are generated to represent different aspects of each sentence, as shown in Table 1. Our model optimizes sentence embeddings by selecting sentences greedily, generating multiple codebook embeddings to represent different aspects of each sentence. Comparison with alternative methods like average word embeddings and using all words in the sentences as different aspects is also discussed. Our approach optimizes sentence embeddings by generating multiple codebook embeddings to represent different aspects of each sentence. Comparison with alternative methods like average word embeddings and using all words in the sentences as different aspects is also discussed. Additionally, baselines of selecting random sentences (Rnd) and first n sentences (Lead) in the document are tested. The method W Emb optimizes sentence embeddings by generating multiple codebook embeddings to represent different aspects of each sentence. Results on the testing set of CNN/Daily Mail are compared using ROUGE metrics. The results of testing on the CNN/Daily Mail dataset using ROUGE metrics are compared in Table 5. Different methods are evaluated based on ROUGE-1, ROUGE-2, and average summary length. Unsupervised methods like Lead-3 perform similarly to supervised methods like RL, indicating the importance of sentence order information in English news corpora. In evaluating unsupervised sentence embeddings, the focus is on methods that do not rely on sentence position information. Lead-3 and RL are strong baselines, with Lead-3 performing similarly to RL. Increasing cluster numbers in unsupervised methods leads to better results, as shown in Table 5. In evaluating unsupervised sentence embeddings, larger cluster numbers yield better results, with K=100 performing best after selecting 3 sentences. This approach alleviates computational challenges and allows for a relatively large K. Topic modeling has also been extensively studied. Our method allows for a large cluster number K, with K=100 performing best after selecting 3 sentences. Larger K is desired in this application. Topic modeling has been widely studied and applied for its interpretability and flexibility in incorporating different input features. Neural networks have also been used to discover coherent topics. Sparse coding on word embedding space is used to model multiple aspects of a word, and parameterizing word embeddings using neural networks is employed for efficient discovery of different sets of topics/clusters on small word subsets that co-occur with target phrases or sentences. Optimizing a global topic model involves efficiently discovering different sets of topics on small word subsets that co-occur with target phrases or sentences. Sparse coding and neural networks are used to model word aspects and parameterize word embeddings, respectively. Representing words as single or multiple regions in Gaussian embeddings helps capture asymmetric relations. One challenge in extending neural network embeddings is designing a decoder for sets rather than sequences, requiring a matching step and distance loss computation. This approach aims to address the limitations of existing methods in handling longer sequences. The challenges of extending neural network embeddings to longer sequences involve designing a neural decoder for sets, requiring a matching step and distance loss computation. Various loss options, such as Chamfer distance, are used to measure distances between ground truth and predicted sets. The studies focus on measuring distances between ground truth and predicted sets using loss options like Chamfer distance. Different methods for achieving permutation invariants loss for neural networks are also discussed. Our set decoder aims to reconstruct a set using fewer bases, different from other methods that focus on predicting permutations or sequences. Our goal is to efficiently predict clustering centers that can reconstruct observed instances. We use a neural encoder to model the target sequence's meaning and a neural decoder to predict codebook embeddings as the representation of sentences. In this work, the focus is on learning the multi-mode representation for long sequences like phrases or sentences efficiently. A neural encoder is used to model the compositional meaning of the target sequence, while a neural decoder predicts a set of codebook embeddings as the representation of the sentences or phrases. During training, a non-negative sparse coefficient matrix is utilized to match the predicted codebook embeddings to observed co-occurring words, allowing the neural decoder to predict clustering centers with an arbitrary permutation. The proposed models demonstrate the ability to learn interpretable clustering. The proposed models focus on learning multi-mode representation for long sequences efficiently. They use a neural encoder to model the compositional meaning of the target sequence and a neural decoder to predict clustering centers with an arbitrary permutation. The models outperform baselines like BERT and skip-thoughts in unsupervised benchmarks, showing that multi-facet embeddings work best for sequences with many aspects. The study introduces a model that predicts clustering centers for sequences, outperforming BERT and skip-thoughts in unsupervised benchmarks. Multi-facet embeddings are found to be most effective for sequences with multiple aspects. Future work aims to create a single model for generating multi-facet embeddings for both phrases and sentences. In the future, the goal is to develop a single model that can generate multi-facet embeddings for both phrases and sentences. This method will be evaluated as a pre-trained embedding approach for supervised or semi-supervised settings, and applied to other unsupervised learning tasks that rely on co-occurrence statistics. The model is kept simple to achieve nearly converged training loss after 1 or 2 epochs due to computational resource constraints. The model is kept simple to achieve nearly converged training loss after 1 or 2 epochs due to computational resource constraints. The sparsity penalty weights on coefficient matrix \u03bb is set to be 0.4, with a maximal sentence size of 50. The model uses a smaller architecture compared to BERT but shares most hyper-parameters with it. The model uses a smaller architecture compared to BERT but shares most hyper-parameters with it. The sparsity penalty weights on coefficient matrix \u03bb is set to be 0.4, with a maximal sentence size of 50. The maximal number of co-occurring words is set to be 30, and the number of dimensions in transformers is set to be 300. The maximal sentence size is set to 50, and the maximal number of co-occurring words is 30. The number of dimensions in transformers is 300. For sentence representation, the number of transformer layers on the decoder side is 5 with a dropout on attention of 0.1 for K = 10, and 1 for K = 1. For phrase representation, the number of transformer layers on the decoder side is 2 with a dropout on attention of 0.5. The number of transformer layers on the decoder side is 5 with a dropout on attention of 0.1 for K = 10, and 1 for K = 1. For phrase representation, the number of transformer layers on the decoder side is 2 with a dropout on attention of 0.5. The window size d t is set to be 5. Architecture and hyperparameters are determined by validation loss, and the number of codebook embeddings K is chosen based on performance. The window size d t is set to be 5. The architecture and hyperparameters are determined by validation loss, and the number of codebook embeddings K is chosen based on performance. The performances are usually not sensitive to the numbers as long as K is large enough. Models with too large K might experience slight performance drops due to longer training time. In training data, codebook embeddings K are chosen based on performance, with larger K needing longer training time. Skip-thoughts use a hidden embedding of 600 and are retrained for 2 weeks in Wikipedia 2016. The model has fewer parameters than BERT base and requires less computational resources for training. Only the BERT base performance is presented. In comparison to BERT base, BERT large performs better in similarity tasks but worse in hypernym detection. In comparison to BERT base, BERT large generally outperforms in similarity tasks but lags in hypernym detection. Despite the potential benefits of training a larger model, the method presented in the study excels, especially in phrase similarity tasks. The study's method excels, especially in phrase similarity tasks, outperforming BERT large in most cases. The objective function of BERT might not be suitable for short sequences like phrases. BERT is trained by predicting masked words in input sequences, which may not perform well for short sequences like phrases. Comparing methods with the same number of sentences, poor performance in representing sentences with word embeddings may be due to selecting shorter sentences. Performance of unsupervised summarization methods without sentence order information is plotted against sentence length to verify this hypothesis. In a study comparing unsupervised summarization methods, it was found that using word embeddings to represent sentences may lead to poor performance, possibly due to selecting shorter sentences. The performance of different methods was plotted against sentence length, showing that one method significantly outperformed others when summaries had similar lengths. In Figure 3, Ours (K=100) outperforms W Emb (GloVe) and Sent Emb (GloVe) when summaries have similar length. W Emb usually outperforms Sent Emb with similar length summaries, but this comparison may not be fair as W Emb can select more sentences. Preventing many short sentences in extractive summarization may improve fluency. In practice, selecting many short sentences in extractive summarization may not be preferable for fluency. Ours (K=100) is the best choice for summaries less than 50 words, while W Emb (BERT) is better for longer summaries. The figure suggests that Ours (K=100) is the best choice for summaries under 50 words, while W Emb (BERT) is more suitable for longer summaries. Combining our method with BERT could lead to improved performance in this task. The study suggests that combining their method with BERT could improve performance in the task by using contextualized word embeddings from BERT. Predicted embeddings from randomly selected sentences in the validation set are visualized in a GloVe embedding space."
}