{
    "title": "SJlgOjAqYQ",
    "content": "We conducted experiments to test global translation-invariance in deep learning models trained on the MNIST dataset. Both convolutional and capsules neural networks showed poor performance in this aspect, but data augmentation improved their performance. While the capsule network performed better on the MNIST testing dataset, the convolutional neural network generally had better translation-invariance performance. The success of convolutional neural networks in computer vision tasks is attributed to two key features according to the deep learning community. The capsule network is better on the MNIST testing dataset, while the convolutional neural network excels in translation-invariance. CNNs have achieved state-of-the-art performance in computer vision tasks due to reduced computation cost and generalization with local invariance. CNNs need to learn different models for different viewpoints, requiring big data. The success of CNN is attributed to reduced computation cost with weight sharing in convolutional layers and generalization with local invariance in subsampling layers. CNNs need to learn different models for different viewpoints, requiring big data and expensive cost. Capsule networks, on the other hand, are robust in dealing with different viewpoints by using capsules that include pose, color, lighting, and deformation of the visual entity. Capsule networks are robust in handling different viewpoints by using capsules that include pose, color, lighting, and deformation of the visual entity. They aim for 'rate-coded' equivariance, where weights code viewpoint-invariant knowledge, not neural activities. Viewpoint changes in capsule networks have linear effects on pose matrices between different layers, but it is still unclear if they can achieve generalization on a wider range of viewpoints. Capsule networks focus on achieving 'rate-coded' equivariance by encoding viewpoint-invariant knowledge in weights rather than neural activities. Viewpoint changes in capsule networks result in linear effects on pose matrices between different layers. However, it remains uncertain if capsule networks can generalize for global translation invariance. Visualizing and quantifying translation-invariance in deep learning models is crucial for understanding architectural choices and developing generalization models resistant to viewpoint changes. An analysis using translation-sensitivity maps for the MNIST digit dataset has been conducted. In this paper, a simple method is introduced to test global translation-invariance in convolutional and capsule neural network models trained on the MNIST dataset. The study focuses on assessing the performance of deep learning models in achieving global translational invariance. The study introduces a method to test global translation-invariance in CNN and capsule neural network models trained on the MNIST dataset. It evaluates the performance of deep learning models in achieving global translational invariance by using a testing dataset with images generated by shifting the centre of mass of a digit. The GTI testing dataset consists of 2520 images generated by shifting the centre of mass of a digit. The images are 28x28 in size, similar to MNIST images, and cover all possible cases of translational translations. Deep learning models are trained on the MNIST dataset with 60000 samples and tested on both MNIST and GTI testing datasets. The testing dataset includes 2520 images covering all possible translational translations. Deep learning models are trained on the MNIST dataset with 60000 samples and tested on both MNIST and GTI datasets. The approach is robust against random noise and mislabeling compared to other methods. The GTI dataset, with images distributed uniformly on the canvas, is smaller and more robust than the MNIST dataset. It allows for capturing tiny differences in models and quantifying global invariance. The GTI dataset is advantageous for model predictions due to its consistency across different models. The CNN model used in the study has nine layers, including convolutional and max-pooling layers with specific channel sizes and filter dimensions. The GTI dataset is advantageous for capturing tiny model differences and quantifying global invariance in model predictions. The CNN model used in the study has nine layers with specific channel sizes and filter dimensions. The total number of parameters is 361578, much smaller than Capsule networks. Dropout is applied to certain layers, and the last layer uses softmax activation function. The CNN model used in the study has 6 max-pooling layers and 3 fully connected layers with sizes 256, 128, and 10. Dropout with a rate of 0.5 is applied to certain layers. The total number of parameters is 361578, much smaller than Capsule networks. ReLU is used for all layers except the last one, which uses softmax activation. The optimizer is Adam with default parameters, and the objective function is cross entropy loss. The CNN model achieves high accuracy on the MNIST testing set. The CNN model uses ReLU for all layers except the last one, which employs softmax activation. The optimizer is Adam with default parameters, and the objective function is cross entropy loss. Results of the CNN model are shown in FIG2 and TAB0, achieving high accuracy on the MNIST testing set. However, without data augmentation, the model performs poorly on the GTI testing dataset, indicating a struggle with global translational invariance. Images with the digit's center of mass around the canvas center are predicted correctly, while those at the corner are assigned to incorrect classes. The CNN model struggles with global translational invariance, as shown by its low accuracy of 42.16% on the GTI testing dataset. Images with the digit's center of mass around the canvas center are predicted correctly, while those at the corner are assigned to incorrect classes, indicating a 'place-code' equivariant behavior. To improve CNN performance on the GTI dataset, MNIST images were augmented by shifting them from the center in x and y-direction. This increased accuracy on the GTI testing dataset to 98.05%. Training MNIST with data augmentation by shifting images from the center in x and y-direction improved CNN performance on the GTI dataset, increasing accuracy to 98.05%. Data augmentation implies 'place-code' equivariance in CNN, activating neurons at the corner of feature maps when training samples with objects at the edge are seen. The GTI dataset accuracy increased to 98.05% through data augmentation by shifting MNIST training images. CapsNet was tested with the same architecture as CNN, with 8.2M parameters, trained using Adam optimizer with exponential decay of learning rate. Margin loss was used with parameters from BID9. Reconstruction was also added. The CapsNet model with 8.2M parameters was tested on the GTI dataset using Adam optimizer and margin loss from BID9. Despite its robustness in viewpoint invariance, the model's performance on global invariance needs improvement. The model trained on MNIST without data augmentation failed to predict the class correctly. The CapsNet model with 8.2M parameters was tested on the GTI dataset using Adam optimizer and margin loss from BID9. Despite its robustness in viewpoint invariance, the model's performance on global invariance needs improvement. Data augmentation in MNIST training dataset helps the CapsNet achieve better accuracy on the GTI dataset. The CapsNet model fails to predict the class correctly and generates incorrect images near the edge. Data augmentation in the MNIST training dataset improves accuracy on the GTI dataset. CNN outperforms CapsNet on the GTI dataset, even with wider receptive fields in CapsNet's convolutional layers. The CapsNet model struggles with predicting classes accurately and produces incorrect images near the edge. CNN performs better than CapsNet on the GTI dataset, even with wider receptive fields in CapsNet's convolutional layers. There is potential for improvement in CapsNet's handling of translational invariance. The CapsNet struggles with predicting classes accurately and produces incorrect images near the edge. Despite wider receptive fields in CapsNet's convolutional layers, CNN outperforms CapsNet on the GTI dataset. There is room for improvement in CapsNet's ability to handle translational invariance. A GTI testing dataset is introduced to assess CNN and CapsNet's performance in dealing with global translational invariance. CapsNet currently requires data augmentation to handle global translational invariance, but its architecture shows potential advantages over CNN in this aspect. The testing dataset evaluates the performance of CNN and CapsNet on global translational invariance. CapsNet struggles without data augmentation but shows potential advantages over CNN due to its ability to learn viewpoints regardless of information source. The testing method involves random shifting in the MNIST training dataset to quantify accuracy. CapsNet outperforms CNN in dealing with global translational invariance by learning viewpoints from all information sources. The testing method involves quantifying accuracy with random shifting in the MNIST training dataset, making it easy to implement for other computer vision tasks."
}