{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance. The codes capture the coarse structure of the target sentence by predicting simplified part-of-speech tags. Planning ahead enhances translation quality by accommodating different sentence structures. The study introduces a neural network with a discretization bottleneck to learn codes for capturing the structure of target sentences. Planning ahead improves translation performance and allows for obtaining translations with different structures by manipulating the planner codes. Linguists have found evidence that speakers plan ahead in discourse or sentence level to ensure grammatical and logical correctness. The study introduces a neural network with a discretization bottleneck to learn codes for capturing the structure of target sentences. Manipulating planner codes allows for obtaining translations with different structures. While humans plan ahead in speech to ensure correctness, neural machine translation models do not have a planning phase when generating sentences. In contrast to human speech planning, neural machine translation models lack a planning phase when generating sentences. This results in uncertainty in word prediction due to the model being unaware of the overall sentence structure. In this research, the model plans the coarse structure of the output sentence before decoding real words, addressing the uncertainty in word prediction in neural machine translation models. Planner codes are inserted at the beginning of the output sentences to govern the sentence structure. The research focuses on letting the model plan the sentence structure before decoding words in neural machine translation. Planner codes are inserted at the beginning of output sentences to govern the structure, addressing uncertainty in word prediction. The research focuses on using planner codes to govern the sentence structure in neural machine translation, addressing uncertainty in word prediction. The input sentence provides information about the target-side structure, and by learning planner codes, the effectiveness of beam search can be increased. The research focuses on using planner codes to regulate sentence structure in neural machine translation, addressing uncertainty in word prediction. Planner codes are learned to disambiguate uncertain information about sentence structure, improving the effectiveness of beam search without modifying the NMT model. The study utilizes planner codes to regulate sentence structure in neural machine translation, enhancing beam search effectiveness without altering the NMT model. Simplified POS tags are used to annotate the structure, and a code learning model is employed to obtain the planner codes, which can be manipulated to control the output sentence structure. The study uses planner codes to improve translation performance in neural machine translation by controlling the structure of output sentences. Structural annotation is simplified using POS tags, and a code learning model is used to obtain planner codes for sentence structure manipulation. Beam search or the NMT model itself can efficiently handle uncertainty in local structures during decoding. The study utilizes planner codes to enhance translation performance in neural machine translation by controlling sentence structure. Structural annotation is simplified using POS tags, and a code learning model is employed to obtain planner codes for sentence structure manipulation. Beam search or the NMT model can efficiently handle uncertainty in local structures during decoding. In this work, coarse structural annotations are extracted through a two-step process simplifying POS tags of the target sentence. The study utilizes planner codes to control sentence structure in neural machine translation, enhancing translation performance. The study uses planner codes to control sentence structure in neural machine translation, improving translation performance. The code learning model architecture involves computing discrete codes based on simplified POS tags. The code learning model architecture involves computing discrete codes based on simplified POS tags, which are then discretized into approximated one-hot vectors using the Gumbel-Softmax trick. The information from these codes is combined with input data to initialize a decoder LSTM for sequential prediction. The tag sequence is encoded using a backward LSTM and discretized into approximated one-hot vectors using the Gumbel-Softmax trick. The information from these vectors is combined with input data to initialize a decoder LSTM for sequential prediction. The probability of emitting each tag is then predicted. The architecture of the code learning model is shown in Fig. 2. The architecture of the code learning model, depicted in Fig. 2, involves a sequence auto-encoder with an extra context input X to the decoder. The parameters are optimized with crossentropy loss, and the model can generate planner codes C for all target sentences in the training data. The code learning model, depicted in Fig. 2, is a sequence auto-encoder with an extra context input X to the decoder. Parameters are optimized with crossentropy loss to obtain planner codes C for target sentences. The training data is then modified to include (X, C Y ; Y ) pairs, and a regular NMT model is trained using a modified dataset. Beam search is used during sentence decoding to search for planner codes. The machine translation dataset consists of (X, Y) sentence pairs. The training data is modified to include planner codes C Y, resulting in (X, C Y; Y) pairs. A regular NMT model is trained using this modified dataset. Beam search is used during sentence decoding to search for planner codes. Some methods aim to improve syntactic correctness in translations, such as BID19 restricting the search space of the NMT decoder using a lattice from a Statistical Machine Translation system, and BID2 taking a multi-task approach. Recently, methods have been proposed to enhance the syntactic correctness of translations in machine translation. BID19 limits the NMT decoder's search space using a lattice from a Statistical Machine Translation system, while BID2 adopts a multi-task approach. Other works incorporate target-side syntactic structures explicitly, such as interleaving CCG supertags with output words in the target side. Aharoni and Goldberg (2017) train a NMT model to generate words instead of predicting them. BID2 takes a multi-task approach in NMT, combining parsing loss with the original loss. Other methods incorporate syntactic structures in translation, like interleaving CCG supertags with output words. Aharoni and Goldberg (2017) train NMT to generate words. Goldberg (2017) trains a NMT model to generate linearized constituent parse trees. BID20 proposed a model to generate words and parse actions simultaneously, conditioning word prediction and action prediction on each other. Some works, like Shu and Nakayama (2018) and BID7, learn discrete codes for different purposes, compressing word embeddings and breaking down word dependencies with shorter code sequences. The models are evaluated on IWSLT 2014 Germanto-English task BID1 and ASPEC. Some works learn discrete codes for different purposes, compressing word embeddings and breaking down word dependencies with shorter code sequences. Models are evaluated on IWSLT 2014 Germanto-English task BID1 and ASPEC Japanese-to-English task BID13. Tokenization is done using Kytea for Japanese texts and moses toolkit for other languages, with bytepair encoding used in the code learning model. The model is trained using Nesterov's accelerated gradient (NAG) for maximum efficiency. In the code learning model, discrete codes are learned for different purposes, with evaluation done on IWSLT 2014 Germanto-English task BID1 and ASPEC Japanese-to-English task BID13. Tokenization is performed using Kytea for Japanese texts and moses toolkit for other languages, with bytepair encoding utilized. The model is trained with Nesterov's accelerated gradient (NAG) for up to 50 epochs, testing different settings of code length N and number of code types K. The information capacity of the codes is N log K bits, evaluated in TAB1 for accuracy in reconstructing source sentences. In the code learning model, discrete codes are learned for different purposes, with evaluation done on IWSLT 2014 Germanto-English task BID1 and ASPEC Japanese-to-English task BID13. The model is trained with Nesterov's accelerated gradient (NAG) for up to 50 epochs, testing different settings of code length N and number of code types K. The information capacity of the codes is N log K bits, evaluated in TAB1 for accuracy in reconstructing source sentences. There is a trade-off between S Y accuracy and C Y accuracy, with higher code capacity resulting in better S Y recovery but lower C Y accuracy. The setting of N = 2, K = 4 is found to be balanced. The trade-off between source sentence (SY) accuracy and code (CY) accuracy is evident in the model. Increasing code capacity improves SY recovery but lowers CY accuracy. The NMT model uses 2 layers of bidirectional LSTM encoders and decoders, with 256 units for IWSLT De-En task and 1000 units for ASPEC Ja-En task. Key-Value Attention is applied in the first decoder layer, and a residual connection (BID3) is used for combination. The NMT model uses 2 layers of bidirectional LSTM encoders and decoders with 256 units for IWSLT De-En task and 1000 units for ASPEC Ja-En task. Key-Value Attention is applied in the first decoder layer, and a residual connection (BID3) is used for combination. Dropout is applied with a drop rate of 0.2, and the NAG optimizer is used for training with a learning rate of 0.25. The NMT model uses residual connection BID3 to combine hidden states in two decoder layers. Dropout with a rate of 0.2 is applied outside the recurrent function. NAG optimizer is used for training with a learning rate of 0.25, annealed by a factor of 10 after 20K iterations. Conditioning word prediction on generated planner codes improves translation performance. Greedy search on JaEn dataset results in a BLEU score. The loss value is observed in 20K iterations, and the best parameters are chosen on a validation set. Conditioning word prediction on generated planner codes improves translation performance, possibly by regulating the search space. However, applying greedy search on JaEn dataset leads to a lower BLEU score compared to the baseline. Beam search on planner codes followed by greedy search does not significantly change the results. It is hypothesized that exploring multiple candidates with different structures simultaneously is crucial for the Ja-En task. Planning ahead allows for more diverse candidate exploration. The BLEU score is lower with greedy search on JaEn dataset compared to the baseline. Beam search on planner codes followed by greedy search does not change results significantly. It is important to explore multiple candidates with different structures simultaneously for the Ja-En task. Planning ahead allows for more diverse candidate exploration, improving beam search but not greedy search. The performance of beam search depends on candidate diversity. Manual selection of codes instead of letting beam search decide is also an option. By planning ahead, more diverse candidates can be explored, improving beam search but not greedy search. Results align with a recent study showing beam search performance depends on candidate diversity. Manual selection of codes is an alternative to letting beam search decide. Example candidate translations are provided in Table 3. Table 3 shows candidate translations produced by the model based on different planner codes in a Ja-En task. The distribution of assigned planner codes for English sentences is illustrated in Figure 3. The process of AP is described with translation results conditioned on different planner codes in a Ja-En task. Manipulating the codes can result in translations with diverse structures, showing the method's usefulness for sampling paraphrased translations. The distribution of learned codes for English sentences in the ASPEC Ja-En dataset is illustrated. The proposed method allows for diverse translations by manipulating planner codes in the ASPEC Ja-En dataset. The distribution of learned codes for English sentences shows a skewed distribution, indicating room for improvement in code capacity utilization. The English sentences in the ASPEC Ja-En dataset show a skewed distribution of assigned codes. This suggests that there is potential for improving code capacity utilization. Additionally, experimenting with directly predicting structural annotations instead of discrete codes resulted in a performance degradation of around 8 BLEU points on the IWSLT dataset. In this paper, a planning phase is added in neural machine translation to generate planner codes for controlling the output sentence structure. An end-to-end neural network with a discretization bottleneck is designed to predict simplified POS tags of target sentences. Experimental results show that this method improves translation performance. In this paper, a planning phase is incorporated into neural machine translation to generate planner codes for controlling the output sentence structure. An end-to-end neural network with a discretization bottleneck is used to predict simplified POS tags of target sentences. Experimental results demonstrate improved translation performance and the effectiveness of planner codes in generating translations with different structures. The planning phase aids the decoding algorithm by reducing uncertainty in sentence structure. The planning phase in neural machine translation improves translation performance by generating planner codes to control sentence structure. Different planner codes allow for sampling translations with varied structures, reducing uncertainty in decoding algorithms. This framework can be extended to plan other latent factors like sentiment or topic."
}