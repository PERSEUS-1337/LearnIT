{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features from images to improve CNN robustness. This framework uses a generative adversarial network (GAN) to reconstruct images with texture information. EdgeGANRob is a unified framework that enhances CNN robustness by extracting shape/structure features from images and using a GAN to reconstruct images with texture information. A robust edge detection approach called Robust Canny is also proposed to reduce sensitivity to adversarial perturbations. Comparison with EdgeNetRob shows that learning directly on robust edge features can improve model performance. To enhance CNN robustness, a robust edge detection approach called Robust Canny is proposed to reduce sensitivity to adversarial perturbations. EdgeGANRob is compared with EdgeNetRob, showing that EdgeNetRob boosts model robustness but decreases clean model accuracy, while EdgeGANRob improves clean model accuracy without sacrificing robustness benefits. Extensive experiments demonstrate EdgeGANRob's resilience in various learning tasks and settings. Convolutional neural networks (CNNs) have achieved state-of-the-art performance but are vulnerable to adversarial examples and data poisoning attacks. Recent studies show that CNNs tend to be susceptible to manipulation. Recent studies have shown that CNNs are vulnerable to imperceptible perturbations in test data, leading to reduced generalization accuracy through data poisoning or backdoor attacks. CNNs tend to learn surface statistical regularities instead of high-level abstractions, making them fail to generalize to superficial pattern transformations. This problem is referred to as the model's robustness under distribution shifting. Recent studies show that CNNs tend to learn surface statistical regularities instead of high-level abstractions, leading to a lack of generalization to superficial pattern transformations. This issue is known as the model's robustness under distribution shifting. Improving the general robustness of DNNs in these settings remains a challenge. Studies have explored the vulnerability of CNNs, with some attributing the existence of adversarial examples to non-robust but highly-predictive features. Suggestions include training a classifier only on \"robust features.\" To enhance the robustness of DNNs, recent studies investigate the vulnerability of CNNs, linking adversarial examples to non-robust yet predictive features. Training classifiers solely on \"robust features\" is suggested. Additionally, human recognition relies more on global object shapes than local patterns, unlike CNNs. Recent studies suggest training classifiers on \"robust features\" that are insensitive to small perturbations. Human recognition relies on global object shapes, while CNNs are biased towards local patterns. This bias contributes to CNN's vulnerability to adversarial examples. The bias towards local features in CNNs contributes to their vulnerability to adversarial examples and backdoor attacks. Researchers have proposed using robust features like edge detection to improve predictions. CNN's vulnerability to adversarial examples, distribution shifting, and backdoor attacks has led researchers to explore improving robustness by focusing on global shape structure. EdgeNetRob and EdgeGANRob are proposed to enhance predictions using edge detection and texture information. The importance of global shape structure in improving the robustness of CNNs is highlighted. Using edges as a shape representation is proposed due to its effectiveness and ease of capture in images. Various algorithms are available for edge detection. The paper proposes using edges as a shape representation due to its effectiveness and ease of capture in images. A new approach called EdgeGANRob aims to enhance CNNs' robustness to adversarial attacks by leveraging structural information in images. The unified framework, illustrated in Figure 1, includes a two-stage procedure named EdgeNetRob. This paper introduces EdgeGANRob, a new approach to improve CNNs' robustness to adversarial attacks by leveraging structural information in images. The simplified version, EdgeNetRob, extracts structural information by detecting edges and trains the classifier on these edges, forcing CNNs to make predictions based solely on shape information. EdgeGANRob is a two-stage procedure named EdgeNetRob that improves CNNs' robustness by extracting structural information through edge detection. The approach forces CNNs to make predictions based on shape information, eliminating texture bias. However, challenges remain as direct differentiable edge detection algorithms are vulnerable to attacks. To address this, a robust edge detection algorithm called Robust Canny is proposed to enhance EdgeNetRob's performance. The proposed Robust Canny algorithm enhances EdgeNetRob's robustness against attacks, improving EdgeGANRob's performance. However, EdgeNetRob's improvement in CNNs' robustness leads to decreased clean accuracy due to missing texture/color information, motivating the development of EdgeGANRob. The Robust Canny algorithm enhances EdgeNetRob's robustness, leading to improved performance of EdgeGANRob. However, this improvement decreases clean accuracy in CNNs due to missing texture/color information, prompting the development of EdgeGANRob to refill the textures/colors based on edge images before classification. The main contribution is the proposal of a unified framework, EdgeGANRob, to enhance CNNs' robustness. More visualization results can be found on the website: https://sites.google.com/view/edgenetrob. The development of EdgeGANRob involves embedding a generative model to refill texture/colors based on edge images before classification. The main contributions include proposing a unified framework to improve CNNs' robustness against multiple tasks simultaneously by extracting edge/structure information and refilling textural information with GAN. To combat adaptive evasion attacks, a robust edge strategy is also proposed. More visualization results can be found on the website: https://sites.google.com/view/edgenetrob. EdgeGANRob aims to enhance CNNs' robustness by extracting edge/structure information from input images and reconstructing them using GAN. A robust edge detection approach, Robust Canny, is proposed to reduce sensitivity to adversarial perturbations. The effectiveness of the inpainting GAN in EdgeGANRob is demonstrated through the simplified backbone procedure, EdgeNetRob, for learning tasks directly on the extracted robust features. The defense algorithm proposes a robust edge detection approach, Robust Canny, to reduce sensitivity to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Various defense methods against adversarial examples have been proposed. The study evaluates EdgeNetRob and EdgeGANRob in adversarial attacks, distribution shifting, and backdoor attacks, showing significant improvements. Various defense methods against adversarial examples have been proposed, with a focus on adversarial training and the identification of gradient obfuscation as a common pitfall. Adversarial examples are not robust against adaptive attacks. Defense methods should be evaluated against customized white-box attacks and strong adaptive attacks. Distribution shifting is common in real-world applications. CNNs have been studied in this context. Defense methods should be evaluated against customized white-box attacks and strong adaptive attacks. Distribution shifting is common in real-world applications, with CNNs having a tendency to learn superficial statistical cues. Wang et al. (2019a) proposed a method to robustify CNNs by penalizing the predictive power of local representations and mitigating the fitting of superficial statistical cues. Jo and Bengio (2017b) found that CNNs tend to learn superficial statistical cues. Wang et al. (2019a) introduced a method to enhance CNN robustness by penalizing local representations' predictive power. Hendrycks and Dietterich (2019) proposed benchmark datasets for evaluating model robustness. Backdoor attacks, a type of poisoning attack, involve injecting patterns into training data to manipulate model predictions. Recent work has highlighted a connection between recognition and robust visual features, including negcolor, radial kernel, and random kernel. Backdoor attacks involve injecting patterns into training data to manipulate model predictions, with proposed methods for detection and protection. Recent work has shown a connection between recognition robustness and visual features. Tran et al. (2018) proposed a method to detect poisoned training data, while Itazuri et al. (2019) found that adversarially robust models capture global features. Geirhos et al. (2019) and Baker et al. (2018) discovered that CNNs rely more on textures for image recognition, while humans focus more on shape structure. The connection between recognition robustness and visual features has been highlighted in recent work. Geirhos et al. (2019) and Baker et al. (2018) found that CNNs prioritize textures over global shape structure for image recognition, while humans focus more on shape structure. Itazuri et al. (2019) showed that adversarially robust models tend to capture global object structure. Ilyas et al. (2019) argued that non-robust features in natural images are highly predictive but not interpretable by humans, and CNNs can achieve robustness by learning from images with only robust features. However, they did not directly identify these robust features. In response to the highlighted connection between recognition robustness and visual features, Ilyas et al. (2019) proposed using edge features as a robust feature for CNNs. They introduced a new classification pipeline called EdgeGANRob, which extracts edge/structure features from images and reconstructs them by refilling texture information. In response to the connection between recognition robustness and visual features, a new classification pipeline called EdgeGANRob was introduced. It extracts edge/structure features from images and reconstructs them using a generative adversarial network (GAN). The method, EdgeNetRob, serves as a simplified backbone for EdgeGANRob, along with Robust Canny and inpainting GAN. Three settings are described for evaluating robustness. The EdgeNetRob pipeline, a simplified backbone of EdgeGANRob, involves extracting edge maps from images using an edge detection method and training a standard image classifier on these maps. The goal is to address the problem of recognition robustness by utilizing texture information with a generative adversarial network (GAN) and evaluating robustness under three settings. EdgeNetRob, a simplified backbone of EdgeGANRob, involves using an edge detection method to extract edge maps from images. A standard image classifier is then trained on these maps to make decisions solely based on edges, reducing sensitivity to local textures. This approach aims to improve recognition robustness by transforming original images into edge maps, even if a pre-trained classifier on the original data is available. EdgeNetRob forces CNN decisions based on edges, reducing sensitivity to textures. Despite simplicity, it degrades CNN performance on clean data. This led to the development of EdgeGANRob, filling edge maps with texture/colors. The development of EdgeGANRob aims to improve the performance of CNNs by filling edge maps with texture/colors, increasing clean accuracy. The robustness of this classification system relies on the edge detector used, as existing algorithms are vulnerable to attacks, impacting recognition accuracy. This motivates the proposal of a robust edge detection method. The development of EdgeGANRob aims to improve CNN performance by filling edge maps with texture/colors for higher clean accuracy. Existing edge detection algorithms are vulnerable to attacks, leading to low recognition accuracy. A robust edge detection algorithm named Robust Canny is proposed to address this issue. The development of EdgeGANRob aims to improve CNN performance by filling edge maps with texture/colors for higher clean accuracy. A robust edge detection algorithm named Robust Canny is proposed to address the vulnerability of existing edge detection algorithms to attacks. The algorithm improves the robustness of the traditional Canny edge detector by truncating noisy pixels. The Robust Canny edge detector improves traditional Canny's robustness by truncating noisy pixels in its intermediate stages. It includes 6 stages: noise reduction with a Gaussian filter and gradient computation using the Sobel operator. The Robust Canny edge detector, a modified version of the traditional Canny, includes 6 stages: noise reduction with a Gaussian filter, gradient computation using the Sobel operator, noise masking with thresholding, and non-maximum suppression for edge thinning. The Robust Canny edge detector includes stages for noise reduction, gradient computation, noise masking with thresholding, non-maximum suppression for edge thinning, double thresholding, and edge tracking by hysteresis. The Robust Canny edge detector includes stages for noise reduction, gradient computation, noise masking with thresholding, non-maximum suppression for edge thinning, double thresholding, and edge tracking by hysteresis. The algorithm suppresses gradients not at a maximum, applies double thresholding to map pixels to strong, weak, and non-edge levels, and tracks edges by connecting strong and weak pixels. A noise masking stage is added after computing gradients to mitigate perturbation noise. The Robust Canny edge detector includes stages for noise reduction, gradient computation, noise masking with thresholding, non-maximum suppression for edge thinning, double thresholding, and edge tracking by hysteresis. A modified Canny algorithm adds a noise masking stage after computing gradients to reduce perturbation noise. Parameters like standard deviation of gaussian filter and thresholds also impact the algorithm. By adding a truncation operation, the Robust Canny edge detector aims to reduce adversarial noise on the gradient map with small magnitude early on without compromising final edge map quality. Parameters like standard deviation of gaussian filter and thresholds (\u03b8 l , \u03b8 h) play a crucial role in the algorithm's robustness level, with larger values leading to better robustness but potentially sacrificing clean accuracy. The robustness level of the Robust Canny edge detector is influenced by parameters like filter \u03c3 and thresholds \u03b8 l , \u03b8 h. Larger values improve robustness but may reduce clean accuracy. Careful parameter selection is crucial for a robust edge detector. Training a Generative Adversarial Network (GAN) in EdgeGANRob is described in detail in the experiment section. To achieve a robust edge detector, careful parameter selection is essential. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves following the common setup of pix2pix and using an objective function with adversarial loss. Training the inpainting GAN involves two stages: first, a conditional GAN is trained following the pix2pix setup with adversarial and feature matching losses. In the second stage, the GAN is fine-tuned along with a classifier to achieve high accuracy on generated RGB images. In the second stage, the trained GAN is jointly fine-tuned with a classifier to improve accuracy on generated RGB images by minimizing the classification loss. This approach aims to enhance robustness under different conditions. The objective function aims to minimize the classification loss of generated images by inpainting GAN. The method improves robustness under adversarial attack, distribution shifting, and backdoor attack by focusing on generating realistic images first. EdgeGANRob is expected to enhance robustness against adversarial attacks by leveraging the invariance of edges to small perturbations. EdgeGANRob improves robustness under adversarial attack, distribution shifting, and backdoor attack by focusing on shape structure and leveraging edge features. EdgeGANRob focuses on making specific edge pixels appear/disappear by reversing image gradients within a limited adversarial budget. Leveraging edge features can enhance model generalization, especially in cases of distribution shifting. Extracting edges acts as a data sanitization step to improve model robustness against backdoor attacks. The proposed method, EdgeNetRob, enhances model robustness against backdoor attacks by extracting edges as a data sanitization step. It is shown to have unique advantages in certain settings and is considered a robust recognition method. The proposed method, EdgeNetRob, enhances model robustness by removing malicious patterns, making backdoor attacks ineffective. It is evaluated for its robustness against adversarial attacks, distribution shifting, and backdoor attacks on two datasets: Fashion. The proposed method, EdgeNetRob, enhances model robustness by removing malicious patterns to make backdoor attacks ineffective. Evaluation includes robustness against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets for gender classification. The study evaluates the EdgeNetRob method for enhancing model robustness on Fashion MNIST and CelebA datasets for gender classification. The choice of datasets is explained, and the evaluation is done using adversarial perturbation constraints. The same network architecture is used for classification in both the method and the vanilla classifier. The study evaluates the EdgeNetRob method for enhancing model robustness on Fashion MNIST and CelebA datasets for gender classification. The evaluation is done using adversarial perturbation constraints with specific budgets for each dataset. The study evaluates the EdgeNetRob method for enhancing model robustness on Fashion MNIST and CelebA datasets for gender classification, using specific adversarial perturbation constraints. The evaluation includes measuring robustness to white-box attacks by the BPDA attack. The study evaluates the robustness of edge detection methods against white-box attacks using the BPDA attack. Three methods compared are RCF, traditional Canny, and Robust Canny. The study compares the robustness of three edge detection methods: RCF, traditional Canny, and Robust Canny, against adversarial attacks. Results show that RCF's edge maps are not robust, with accuracy dropping near 0 under strong adaptive attacks. The study compares the robustness of edge detection methods RCF, traditional Canny, and Robust Canny against adversarial attacks. Results show RCF's edge maps are not robust, with accuracy dropping near 0 under strong adaptive attacks. The study presents results for Fashion MNIST and CelebA datasets, comparing with the state-of-the-art baseline Adversarial training for strong robustness to white-box attacks. In a study comparing edge detection methods against adversarial attacks, EdgeNetRob and EdgeGANRob show a small drop in clean accuracy compared to the baseline model. However, they outperform adversarial training with = 8 in terms of clean accuracy. EdgeGANRob performs better than EdgeNetRob on the CelebA dataset, highlighting the importance of adding GANs. EdgeNetRob and EdgeGANRob exhibit a slight decrease in clean accuracy compared to the baseline model, but outperform adversarial training with = 8. EdgeGANRob shows higher clean accuracy than EdgeNetRob on the CelebA dataset, emphasizing the importance of incorporating GANs for more complex datasets. Additionally, both EdgeNetRob and EdgeGANRob demonstrate robustness against strong adaptive attacks, maintaining a level of robustness comparable to adversarial training baselines. Incorporating GANs for complex datasets like CelebA is crucial for closing accuracy gaps. EdgeNetRob and EdgeGANRob show robustness against strong adaptive attacks, with EdgeNetRob being time-efficient due to not using adversarial training. Generalization ability is tested under distribution shifting following HEX and PAR experiment settings. EdgeNetRob demonstrates time efficiency by not using adversarial training. Test accuracy plots under different attack iterations are shown. Generalization ability is tested on perturbed Fashion MNIST and CelebA datasets with various transformations. Comparisons are made with state-of-the-art methods. In the study, models were tested under perturbed Fashion MNIST and CelebA datasets with different patterns. The random kernel and radial kernel transformations were introduced to preserve high-level semantics. Comparison was made with the state-of-the-art method PAR, which includes a local patchwise adversarial regularization loss. Results showed that EdgeNetRob and EdgeGANRob significantly improved accuracy on negative color and radial patterns. The study compared EdgeNetRob and EdgeGANRob with the state-of-the-art method PAR, showing significant improvements in accuracy on negative color, radial kernel, and random kernel patterns. Results also demonstrated that edge features aid CNN generalization to test data and can be used as a defense against backdoor attacks. Our method, EdgeNetRob, outperforms PAR on negative color, radial kernel, and random kernel patterns. It maintains high accuracy on greyscale images and utilizes edge features for CNN generalization. Additionally, it serves as a defense against backdoor attacks by embedding invisible watermark patterns. Our method, EdgeNetRob, embeds invisible watermark patterns as a defense against backdoor attacks. Watermark patterns include the letter \"A\" for Fashion MNIST and \"classified\" for CelebA. Qualitative results are shown in figures. Different attack and target pairs are chosen for Fashion MNIST and CelebA datasets. Poisoning ratios are selected accordingly. Comparison with the baseline method Spectral Signature is conducted. Our method, EdgeNetRob, embeds invisible watermark patterns as a defense against backdoor attacks. Different attack and target pairs are chosen for Fashion MNIST and CelebA datasets with specific poisoning ratios. Comparison with the baseline method Spectral Signature shows high poisoning accuracy on both datasets. The study by Tran et al. (2018) presents results on poisoning accuracy using invisible watermark patterns to attack the vanilla Net on CelebA and Fashion MNIST datasets. The Spectral Signature method does not consistently perform well with invisible watermark patterns, while EdgeNetRob and EdgeGANRob show low poisoning accuracy. Qualitative results of backdoor images and reconstructed images are shown in Figure 4. The study compared the performance of Spectral Signature, EdgeNetRob, and EdgeGANRob in handling invisible watermark patterns. Edge detection algorithms were used to remove the effect of the watermark, showing that EdgeGANRob had better clean accuracy than EdgeNetRob. A new method based on robust edge features was introduced to improve model robustness. The study introduced a new method using robust edge features and a generative adversarial network to improve model robustness against invisible watermark patterns. The method achieved competitive results in adversarial robustness, generalization, and backdoor attack resistance, emphasizing the importance of shape information. Our method combines a robust edge feature extractor with a generative adversarial network to improve model robustness against various attacks. We resize images in CelebA to 128 \u00d7 128 and normalize data to [-1, 1]. The approach shows promising results in adversarial robustness, generalization, and backdoor attack resistance. In improving model robustness, shape information is crucial. Images in CelebA are resized to 128 \u00d7 128 and normalized to [-1, 1]. Different models like LeNet-style CNN and ResNet are used for Fashion-MNIST and CelebA datasets respectively. Training involves stochastic gradient descent with momentum, Projected Gradient Descent (PGD), and Carlini & Wagner \u221e attack (CW). For CelebA dataset, a standard ResNet with depth 20 is used. Models are trained using stochastic gradient descent with momentum, Projected Gradient Descent (PGD), and Carlini & Wagner \u221e attack (CW). Different attack steps and distances are evaluated, with Robust Canny used for adversarial robustness evaluation. For the Robust Canny evaluation, hyper-parameters were chosen using the validation set to balance robustness and accuracy. Parameters for Fashion MNIST included \u03c3 = 1, \u03b8 l = 0.1, \u03b8 h = 0.2, \u03b1 = 0.3, while for CelebA, \u03c3 = 2.5, \u03b8 l = 0.2, \u03b8 h = 0.3, \u03b1 = 0.2. The last three steps in the Robust Canny algorithm involve non-differentiable transformations. The hyper-parameters for Robust Canny were chosen using the validation set to balance robustness and accuracy. Parameters for Fashion MNIST were \u03c3 = 1, \u03b8 l = 0.1, \u03b8 h = 0.2, \u03b1 = 0.3, and for CelebA, \u03c3 = 2.5, \u03b8 l = 0.2, \u03b8 h = 0.3, \u03b1 = 0.2. The last three steps in the algorithm involve non-differentiable transformations, which can be replaced with differentiable approximations in white-box attack scenarios. In a white-box attack scenario, backpropagating gradients through the edge detection algorithm is necessary to construct adversarial samples. The attacker can use the Backward Pass Differentiable Approximation (BPDA) technique to replace non-differentiable transformations and create adversarial examples. To enhance the attack, a differentiable approximation of the Robust Canny algorithm is found by approximating the output of the RCanny algorithm based on the pixel intensities in the original image. To strengthen attacks, a differentiable approximation of the Robust Canny algorithm is used, breaking the transformation into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 involves a non-differentiable operation where the output is a masked version of the input. The Robust Canny algorithm is broken down into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 involves a non-differentiable operation where the output is a masked version of the input. To make R-Canny differentiable for BPDA, the mask is assumed to be constant. The Robust Canny algorithm consists of two stages: C1 and C2. C2 involves a non-differentiable operation with a constant mask assumed for differentiability. Test accuracy changes are shown under radial and random mask transformations in Figure A, with additional visualization results in Figure B for CelebA. In Figure A, test accuracy changes are shown under radial and random mask transformations. Figure B shows additional visualization results for CelebA. Figure D displays qualitative results of EdgeGANRob and EdgeNetRob for backdoor attacks on Fashion MNIST."
}