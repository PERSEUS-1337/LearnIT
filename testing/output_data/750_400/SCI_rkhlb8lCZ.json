{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks advance 2D and 3D image classification. Wavelet Pooling reduces feature dimensions and addresses overfitting. Wavelet Pooling is introduced as an alternative to traditional neighborhood pooling, decomposing features into a second level decomposition to reduce dimensions and address overfitting. Experimental results show it outperforms or performs comparably with other pooling methods on benchmark datasets. Our proposed method for feature pooling outperforms traditional methods like max, mean, mixed, and stochastic pooling on benchmark datasets. Convolutional Neural Networks (CNNs) are the standard in image and object classification due to their high accuracy rates. Researchers are constantly improving foundational concepts to enhance growth and progress in the field. Convolutional Neural Networks (CNNs) are superior in image and object classification, consistently outperforming vector-based deep learning techniques. Researchers continuously enhance CNN components like the convolutional and pooling layers to improve accuracy and efficiency. Pooling, derived from predecessors like Neocognitron and Cresceptron, undergoes modifications to elevate CNN performance. Pooling in Convolutional Neural Networks (CNNs) is a crucial operation that reduces spatial dimensions, increases computational efficiency, and prevents overfitting. It has evolved from predecessors like Neocognitron and Cresceptron, with the most popular form being max pooling. Modifications and innovations in the convolutional and pooling layers aim to elevate CNN accuracy and efficiency beyond previous benchmarks. The first max pooling operation in deep learning reduces spatial dimensions, increases computational efficiency, and prevents overfitting. Max pooling and average pooling are popular methods, but have weaknesses. Other pooling operations like mixed pooling and stochastic pooling use probabilistic approaches to address these issues. Pooling operations in deep learning, such as max pooling and average pooling, are popular for their deterministic and efficient nature. However, they have weaknesses that hinder optimal network learning. Other methods like mixed pooling and stochastic pooling use probabilistic approaches to address these issues. Despite their differences, all pooling operations utilize a neighborhood approach to subsampling, similar to nearest neighbor interpolation in image processing, which can introduce artifacts like edge halos, blurring, and aliasing. The proposed wavelet pooling algorithm utilizes second-level wavelet decomposition to subsample features, avoiding artifacts introduced by nearest neighbor interpolation methods. This approach aims to minimize discontinuities in data for network regularization and improved classification accuracy. The proposed wavelet pooling algorithm utilizes second-level wavelet decomposition to subsample features, aiming to minimize discontinuities in data for network regularization and improved classification accuracy. It forgoes nearest neighbor interpolation in favor of a subband method for more accurate representation of feature contents with fewer artifacts. The algorithm is compared to other pooling methods to validate its effectiveness in producing comparable or superior results on image classification datasets. The proposed wavelet pooling algorithm utilizes second-level wavelet decomposition to subsample features, aiming to minimize discontinuities in data for network regularization and improved classification accuracy. It favors an organic, subband method that accurately represents feature contents with fewer artifacts. The algorithm is compared to other pooling methods like max, mean, mixed, and stochastic pooling on benchmark image classification datasets to verify its validity and superior results. The simulations are conducted in MATLAB R2016b. The paper discusses the proposed wavelet pooling algorithm for subsampling features in image classification. The simulations are conducted in MATLAB R2016b, and the paper is organized into sections covering background, methods, experimental results, and conclusion. Pooling is explained as a form of subsampling in convolutional layers to condense output dimensions. Pooling in convolutional layers is a method of subsampling to condense output dimensions. It involves summarizing regions into one neuron value, with popular forms being max pooling (selecting the maximum value) and average pooling (selecting the average value). Pooling in convolutional layers involves summarizing regions into one neuron value, with popular forms being max pooling (selecting the maximum value) and average pooling (selecting the average value). Max pooling takes the maximum value of a region for the condensed feature map, while average pooling calculates the average value. The formulas for max pooling and average pooling are shown, with an illustration of both methods provided. The max pooling function selects the maximum value for the condensed feature map, while average pooling calculates the average value. Max pooling can erase details from an image if main details have less intensity than insignificant details and commonly overfits training data. Researchers have created probabilistic pooling methods like mixed pooling to combat the shortcomings of max and average pooling, which can erase or dilute details from an image. Researchers have developed probabilistic pooling methods like mixed pooling to address the limitations of average pooling, which can overlook important details in an image. Mixed pooling combines max and average pooling by randomly selecting one method during training, applied in various ways within a layer. Mixed pooling combines max and average pooling by randomly selecting one method during training, applied in various ways within a layer. This method is applied arbitrarily in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. The equation for mixed pooling includes a random value \u03bb (0 or 1) indicating max or average pooling for a particular region/feature/layer. Another probabilistic pooling method, called stochastic pooling, improves upon max pooling by randomly sampling from neighborhood regions based on probability values. Mixed pooling combines max and average pooling by randomly selecting one method during training for different features within a layer. Stochastic pooling improves upon max pooling by randomly sampling from neighborhood regions based on probability values, calculated by normalizing the activations within the region. The pooled activation is sampled from a multinomial distribution to pick a location within the region. Stochastic pooling selects activations within regions based on probability values, avoiding the limitations of max and average pooling. The process involves normalizing activations, sampling from a multinomial distribution, and choosing activations with the highest probabilities. In this method, the midrange activation is selected with a 13% probability. The proposed pooling method uses wavelets for subsampling, different from traditional methods like max and average pooling. It selects activations based on probability values, avoiding limitations of other pooling methods. Our proposed pooling method utilizes wavelets to reduce feature map dimensions, aiming to minimize artifacts from neighborhood reduction. By discarding first-order subbands, we aim to capture data compression more organically, reducing jagged edges and other imperfections. The proposed wavelet pooling method reduces feature map dimensions using wavelets to minimize artifacts. By discarding first-order subbands, it captures data compression organically, reducing jagged edges and other imperfections. The scheme performs a 2nd order decomposition in the wavelet domain according to the fast wavelet transform (FWT). The proposed wavelet pooling scheme reduces artifacts in image classification by performing a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT). It involves approximation and detail functions with coefficients for efficient implementation. The discrete wavelet transform (DWT) involves approximation and detail functions with coefficients for efficient implementation. When using the FWT on images, it is applied twice to obtain detail subbands (LH, HL, HH) at each decomposition level and an approximation subband (LL) for the highest decomposition level. Reconstruction of image features is done after the 2nd order decomposition. The FWT is applied twice on images to obtain detail subbands (LH, HL, HH) at each decomposition level and an approximation subband (LL) for the highest level. Reconstruction of image features is done using the 2nd order wavelet subbands. The wavelet pooling algorithm performs backpropagation for feature pooling. The proposed wavelet pooling algorithm performs backpropagation by reversing the process of its forward propagation, utilizing 2nd order wavelet subbands for image feature pooling. The wavelet pooling algorithm performs backpropagation by reversing the forward propagation process. Image features are decomposed using 1st order wavelet decomposition, upsampled, and reconstructed using 2nd order wavelet decomposition for further backpropagation. The algorithm utilizes Haar wavelet basis for this process. The backpropagation algorithm of wavelet pooling uses 2nd order wavelet decomposition with Haar wavelet basis. Experiments are conducted on a 64-bit operating system with an Intel Core i7-6800k CPU and two GeForce Titan X Pascal GPUs for training. The Haar wavelet basis is used for wavelet pooling experiments on a 64-bit operating system with an Intel Core i7-6800k CPU and two GeForce Titan X Pascal GPUs for training. CNN structures are based on Zeilers network BID31, with variations like Dropout BID22 and Batch Normalization BID7 for different datasets. The experiments involve testing different regularization techniques like Dropout BID22 and Batch Normalization BID7 on CIFAR-10 and SHVN datasets using Zeilers network BID31 architecture. Pooling methods are compared by solely using each method for all pooling layers in the network with a 2x2 window. The network structure is based on the MNIST example from MatConvNet with batch normalization added. The proposed method outperforms all other pooling methods in the network architecture based on the MNIST example from MatConvNet with batch normalization. The input data comes from the MNIST database of handwritten digits, with the full training and testing sets used. Max pooling is the only method that starts to overfit the data during training, while mixed and stochastic pooling show a rocky performance. The proposed method outperforms other pooling methods in the network architecture based on MNIST data. Different pooling methods show varying performance during training, with max pooling starting to overfit. Mixed and stochastic pooling have a rocky trajectory, while average and wavelet pooling show smoother learning curves. Two sets of experiments were conducted with the pooling methods, one without dropout layers to observe their performance. In experiments with different pooling methods, mixed and stochastic pooling show a rocky trajectory but do not overfit, while average and wavelet pooling exhibit smoother learning curves. Two sets of experiments were conducted, one without dropout layers and the other with dropout and batch normalization, to observe the effects of these changes. The CIFAR-10 dataset was used for training and testing. The CIFAR-10 experiments compared different pooling methods with and without regularization. The proposed method showed the second highest accuracy in both cases. Max pooling led to overfitting quickly, while wavelet pooling resisted overfitting. Adjusting the learning rate prevented overfitting and resulted in slower learning. In experiments comparing pooling methods with and without regularization on CIFAR-10 dataset, the proposed method showed the second highest accuracy. Max pooling led to overfitting quickly, while wavelet pooling resisted overfitting. Adjusting the learning rate prevented overfitting and resulted in slower learning. Dropout was used in the second set of experiments to observe its effects on the pooling methods. The study compares different pooling methods in neural networks using the SHVN dataset. Two sets of experiments were conducted, one without dropout layers and the other with dropout. The network structure for the experiments is shown in FIG0. The training data consists of 55,000 images for the no dropout case and 73,257 images for the dropout case, with a validation set of 30,000 images extracted from the training set. The study compares different pooling methods in neural networks using the SHVN dataset. The network structure for the experiments is shown in FIG0. For the case with no dropout, 55,000 images are used from the training set, while for the case with dropout, 73,257 images are used along with a validation set of 30,000 images. Our proposed method has the second lowest accuracy, following the path of max pooling but slightly underperforming. The study compares different pooling methods in neural networks using the SHVN dataset. The proposed method, with and without dropout, shows slightly better stability than max pooling. Other pooling methods like mixed, stochastic, and average show a slow progression of learning with similar validation trends. Energy per epoch is visualized in FIG0. Experiments with dropout are also conducted on the network structure for the KDEF dataset. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. The network structure for the KDEF experiments is shown in FIG0. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. The dataset had errors like missing or corrupted images, which were fixed by mirroring counterparts in MATLAB and adding them back to the dataset. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. Errors like missing or corrupted images were fixed by mirroring counterparts in MATLAB and adding them back to the dataset. The missing images at specific angles were manually cropped to match the dimensions set by the creators. The dataset was shuffled, with 3,900 images used for training and 1,000 for testing. Images were resized to 128x128 due to memory and time constraints, and dropout layers were used to regulate the network and maintain stability. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. Errors were fixed by mirroring counterparts in MATLAB. Images were manually cropped to match dimensions set by creators. Data was shuffled, with 3,900 images for training and 1,000 for testing. Images were resized to 128x128 due to constraints. Dropout layers regulated the network for stability. Proposed method in TAB5 showed second highest accuracy. Max pooling overfits, while wavelet pooling resists overfitting. Stochastic pooling maintains consistent learning progression. Our proposed method in TAB5 has the second highest accuracy among pooling methods. Wavelet pooling resists overfitting and follows a smoother learning progression. However, its computational complexity is not efficient, presented as a proof-of-concept to showcase its potential and validity. Wavelet pooling shows a smoother learning progression and high accuracy, but lacks computational efficiency. The method is presented as a proof-of-concept with potential for improvement. The code for the method is a proof-of-concept, showing potential for improvement in computational efficiency. The focus is on optimizing mathematical operations for better algorithm performance. The focus is on optimizing mathematical operations for better algorithm performance by calculating efficiency in terms of mathematical operations for different pooling methods. The study focuses on optimizing mathematical operations for various pooling methods, including average, max, mixed, stochastic, and wavelet pooling. Average pooling requires the least number of computations, followed by mixed pooling. Wavelet pooling involves calculating operations for each subband in decomposition and reconstruction. Average pooling is the most computationally efficient method, followed by mixed pooling and max pooling. Stochastic pooling is the least efficient among neighborhood-based methods, using about 3x more operations than average pooling. Wavelet pooling is the least efficient method, using 54 to 213x more operations. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. However, by implementing good coding practices, GPUs, and an improved FTW algorithm, this method can be optimized. Wavelet pooling is an efficient method that can be optimized through good coding practices, GPUs, and an improved FTW algorithm. There are improvements to the FTW algorithm that utilize multidimensional wavelets, lifting, parallelization, and other methods to improve efficiency in speed and memory. Wavelet pooling has the potential to equal or surpass some existing methods. Our proposed method utilizing wavelet pooling outperforms others in the MNIST dataset, CIFAR-10, and KDEF datasets, and performs competitively in the SHVN dataset. The addition of dropout and batch normalization enhances the efficiency of our method. Our proposed method, utilizing wavelet pooling, outperforms all others in the MNIST dataset and performs competitively in the CIFAR-10 and KDEF datasets. The addition of dropout and batch normalization enhances network regularization. Our proposed method, utilizing wavelet pooling, outperforms all but one in both the CIFAR-10 & KDEF datasets, and performs within respectable ranges of the pooling methods that outdo it in the SHVN dataset. Results confirm no one pooling method is superior, but some perform better depending on the dataset and network structure. Future work could explore varying wavelet basis for optimal performance. Future work and improvements in this area could involve varying the wavelet basis to determine the best performing basis for pooling. Adjusting the upsampling and downsampling factors in decomposition and reconstruction may enhance image feature reduction beyond the 2x2 scale. Retaining discarded subbands for backpropagation could improve accuracy and reduce errors. Enhancing the computational efficiency of the FTW method used could greatly benefit the process. Additionally, analyzing the structural similarity could provide further insights for optimization. Altering upsampling and downsampling factors in decomposition and reconstruction can improve image feature reduction beyond 2x2 scale. Retaining discarded subbands for backpropagation may increase accuracy. Improving FTW method could enhance computational efficiency. Analyzing SSIM of wavelet pooling versus other methods could validate our approach."
}