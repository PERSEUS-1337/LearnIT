{
    "title": "SkgpBJrtvS",
    "content": "Knowledge distillation is a common method for transferring representational knowledge between neural networks. However, it overlooks important structural knowledge of the teacher network. An alternative approach called contrastive learning aims to train a student network to capture more information from the teacher's data representation. The objective of contrastive learning is to train a student network to capture more information from the teacher's data representation, outperforming knowledge distillation in various knowledge transfer tasks. Our new contrastive learning objective outperforms knowledge distillation in various knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Combining our method with knowledge distillation sets a state of the art in many transfer tasks, sometimes even surpassing the teacher network."
}