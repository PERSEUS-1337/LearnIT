{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. However, as the number of samples (K) is increased, the inference-network gradients break down, requiring the removal of high-variance score-function terms to circumvent this issue. In IWAE, increasing the number of samples leads to breakdown in inference-network gradients, requiring removal of high-variance score-function terms. This can be circumvented by heuristically ignoring them or through a different approach. Directly optimizing the proposal distribution in importance sampling is preferred over other methods. In this work, the authors argue that optimizing the proposal distribution in importance sampling is preferable to optimizing IWAE-type multi-sample objectives. They introduce an adaptive-importance sampling framework called AISLE, which generalizes the RWS algorithm. AISLE allows for the avoidance of breakdown in IWAE gradients. In this work, the authors introduce an adaptive-importance sampling framework called AISLE, which generalizes the RWS algorithm. AISLE allows for the avoidance of breakdown in IWAE gradients and analyzes algorithms for variational inference. The text discusses generative models and variational inference algorithms, aiming to learn the generative model and construct a tractable variational approximation. The text discusses algorithms for learning generative models and constructing variational approximations, focusing on a single latent representation-observation pair. No shared parameters between generative models are assumed. The text discusses algorithms for learning generative models and constructing variational approximations, focusing on a single latent representation-observation pair. It mentions the absence of shared parameters between the generative model and the variational approximation, and the potential for amortised inference. The text discusses stochastic gradient-ascent algorithms for optimizing parameters in generative models and variational approximations. Two main classes of algorithms using Monte Carlo samples have been proposed, with one method inducing bias for the IWAE objective and another method unbiasedly removing bias. Algorithms for optimizing parameters in generative models and variational approximations using Monte Carlo samples have been proposed. One method induces bias for the IWAE objective, while another method unbiasedly removes bias. The reweighted wake-sleep (RWS) algorithm optimizes two separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling with K particles. RWS is an adaptive importance-sampling approach that improves its proposal distribution while optimizing \u03b8 via stochastic approximation. Unlike IWAE, RWS \u03c6-gradients do not degenerate as K \u2192 \u221e. The RWS algorithm is an adaptive importance-sampling approach that optimizes \u03b8 via stochastic approximation. Unlike IWAE, RWS \u03c6-gradients do not degenerate as K \u2192 \u221e. IWAE has superior empirical performance to RWS, despite lacking a joint objective optimization for \u03b8 and \u03c6. In this work, it is shown that directly optimizing the proposal distribution, as done by RWS, is preferable to optimizing the IWAE multi-sample objective. The IWAE-STL gradient performed well despite lacking a firm theoretical footing, but IWAE exhibited inferior empirical performance compared to RWS. The debate continues on whether the multi-sample objective approach of IWAE or the adaptive importance-sampling approach of RWS is more effective. In this work, it is shown that directly optimizing the proposal distribution, as done by RWS, is preferable to optimizing the IWAE multi-sample objective. The multi-sample objective typically relies on reparametrisations and can lead to the \u03c6-gradient breakdown, while modifications of the IWAE \u03c6-gradient can be justified in a more principled manner by taking an RWS-type adaptive importance-sampling view. This conclusion aligns with previous findings by Le et al. (2019). The multi-sample objective relies on reparametrisations and can lead to the \u03c6-gradient breakdown. Modifications of the IWAE \u03c6-gradient (IWAE-STL and IWAE-DREG) can be justified by an adaptive importance-sampling view. Le et al. (2019) showed that reparametrisations can make IWAE inferior to RWS, especially for discrete latent variables. This work formalizes this argument by generalizing the RWS algorithm to create a generic adaptive importance-sampling framework for variational inference. Our work complements Le et al. (2019) by formalizing the argument that reparametrizations can make IWAE inferior to RWS, especially for discrete latent variables. We introduce the AISLE framework, a generic adaptive importance-sampling framework for variational inference, which includes RWS, IWAE-DREG, and IWAE-STL gradients as special cases. The AISLE framework is introduced as a generic adaptive importance-sampling method for variational inference, encompassing RWS, IWAE-DREG, and IWAE-STL gradients. It is shown that AISLE can derive various gradient estimators in a principled manner without degeneration as K approaches infinity. Specifically, the IWAE-STL gradient can be obtained as a special case of AISLE through a novel application of the 'double-reparametrisation' identity. The derived gradient estimators, including IWAE-STL, are guaranteed not to degenerate as K \u2192 \u221e. IWAE-STL gradient can be recovered as a special case of AISLE through a novel application of the 'double-reparametrisation' identity. This work provides a theoretical foundation for IWAE-STL. The breakdown of RWS observed in Tucker et al. (2019) may not be due to its lack of a joint objective as previously conjectured. Our work provides a theoretical foundation for IWAE-STL and proves that AISLE also admits the IWAE-DREG gradient as a special case. The learning rate should be scaled as O(K) for the IWAE \u03c6-gradient unless gradients are normalized as done by popular optimizers like ADAM (Kingma & Ba, 2015). AISLE, a new gradient estimator, generalizes \u03b1-divergences and does not require scaling the learning rate with K. It also admits the IWAE-DREG gradient as a special case and provides insights into the impact of self-normalization bias. The focus of the work is not to derive new algorithms or determine the best AISLE special case. AISLE, a gradient estimator, generalizes \u03b1-divergences without needing learning rate scaling with K. It includes insights on self-normalization bias and compares algorithms empirically. The focus is not on deriving new algorithms or determining the best AISLE special case. The work includes insights on self-normalization bias and compares algorithms empirically. The work compares algorithms empirically on Gaussian models and refers readers to other studies for further comparisons. The notation is simplified for concise representation. The expectation of a test function can be estimated by independent and identically distributed samples. Importance weights and self-normalised versions are used for approximations. The notation simplifies the dependence on parameters and variables. The self-normalised importance sampling estimator approximates the type \u03c0 \u03b8 (f). The estimator depends on \u03c6 and z, with importance weights w \u03c8 (z k) and its self-normalised version s w k \u03c8. The bias of the self-normalised estimate typically vanishes at rate O(K \u22121), with the standard deviation vanishing at Monte-Carlo rate. The importance weighted autoencoder (IWAE) aims to maximize a lower bound L on the generative-model parameters \u03b8. The self-normalised estimate \u03c0 \u03b8 \u03c6, z (f) is biased but its bias vanishes at rate O(K \u22121). The importance weighted autoencoder (IWAE) aims to maximize a lower bound L K \u03c8 on the log-marginal likelihood by optimizing the inference-network parameters \u03c6. Burda et al. (2016) prove that for any \u03c6, L K \u03c8 increases log Z \u03b8. The IWAE maximizes a lower bound on the log-marginal likelihood by optimizing the inference-network parameters \u03c6 and the number of samples, K \u2265 1. For any finite K, optimizing \u03c6 tightens the evidence bound. As K \u2192 \u221e, L K \u03c8 approaches log Z \u03b8. The IWAE extends the VAE to an auxiliary-variable construction. The IWAE maximizes a lower bound on the log-marginal likelihood by optimizing the inference-network parameters \u03c6 and the number of samples, K \u2265 1. For any finite K, optimizing \u03c6 tightens the evidence bound. As K \u2192 \u221e, L K \u03c8 approaches log Z \u03b8. The IWAE extends the VAE to an auxiliary-variable construction. The gradient of the IWAE objective can be approximated unbiasedly via a vanilla Monte Carlo approach using a single sample, but this approximation typically has a large variance. The IWAE objective in Andrieu & Roberts (2009); Andrieu et al. (2010); Lee (2011) can be approximated using a single sample, but this often results in high variance. The reparametrisation trick is commonly used to address this issue, requiring the assumption of a distribution q and differentiable mappings for gradient computation. The reparametrisation trick is used to reduce high variance in the IWAE objective. It involves assuming a distribution q and differentiable mappings for gradient computation. The IWAE uses a vanilla Monte Carlo estimate and a lemma from Tucker et al. (2019) is stated to generalize a well-known identity. The IWAE utilizes a reparametrisation trick to reduce variance in its objective. A lemma from Tucker et al. (2019) generalizes a known identity, and the IWAE gradient has drawbacks related to 'score-function' terms. The IWAE gradient has drawbacks related to 'score-function' terms, including reliance on reparametrisations and vanishing signal-to-noise ratio. A reparametrisation \u00e0 la R1 is needed to eliminate the high-variance term G \u03c8 (z). Control-variate approaches or continuous relaxations have been proposed for discrete models that violate R1, but they come with additional costs and may not reduce variance. The \u03c6-gradient breaks down due to vanishing signal-to-noise ratio. The inability to achieve zero variance is also highlighted. The \u03c6-gradient breaks down with a vanishing signal-to-noise ratio, leading to an inability to achieve zero variance. Modifications have been proposed to address these issues, such as the IWAE-STL gradient. Two modifications of \u2207 iwae \u03c6 \u03b8, z have been proposed to achieve zero variance in the IWAE gradient. IWAE-STL ignores score function terms, while IWAE-DREG removes them through Lemma 1. The IWAE-DREG gradient removes score function terms through Lemma 1, while the RWS algorithm approximates intractable quantities using self-normalised importance sampling. The RWS algorithm approximates intractable quantities using self-normalised importance sampling, allowing for simultaneous optimization of both \u03b8 and \u03c6 gradients. The bias relative to importance sampling is discussed in Appendix A. The bias of RWS algorithm relative to importance sampling is of order O(1/K) and discussed in Appendix A. The optimization of both \u03b8 and \u03c6 gradients is carried out simultaneously, with a lack of a joint objective seen as a drawback. The algorithm derives rws \u03c6 \u03b8, z in expectation using Lemma 1 and self-normalised importance weights. The lack of a joint objective in RWS is a drawback. The algorithm optimizes both \u03b8 and \u03c6 gradients simultaneously. The self-normalised importance weights are used to derive rws \u03c6 \u03b8, z in expectation. The function F(w) := w(1 \u2212 w) transforms the importance weights. In high-dimensional settings, the weights are mainly supported on the two particles with the largest weights. Re-using Monte Carlo samples for both gradients reduces error. The ordered self-normalised importance weights are mainly supported on the two particles with the largest weights. Re-using Monte Carlo samples for both \u03b8 and \u03c6 gradients reduces error. Adapting the proposal distribution in importance-sampling schemes does not always require minimizing the KL-divergence. Other techniques from the literature may sometimes be preferred. Based on insights from the literature, the proposal distribution in importance-sampling schemes may not always need to minimize the KL-divergence. Alternative techniques, such as minimizing the \u03c7 2 -divergence, have theoretical support. This leads to a slight generalization of the RWS-objective, where the goal is to maximize log Z \u03b8 and minimize the \u0192-divergence D\u0192(\u03c0\u03b8 q \u03c6). Another popular approach with strong theoretical support is based on minimizing the \u03c7 2 -divergence. The RWS-objective is slightly generalized as \u03b8 := arg max \u03b8 log Z \u03b8 , \u03c6 := arg min \u03c6 D\u0192(\u03c0\u03b8 q \u03c6). Alternative approaches for optimizing \u03c6 could be used, but for concreteness, we state the resulting algorithm as adaptive importance sampling for learning (AISLE). This unified framework allows for a straightforward and principled approach. The unified framework of adaptive importance sampling for learning (AISLE) allows for a straightforward derivation of robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e. Optimization is performed via stochastic gradient ascent, with the \u03b8-gradient remaining the same for all algorithms discussed in this work. The \u03b8-gradient is the same for all algorithms discussed in this work, viewed differently by IWAE and AISLE/RWS. Derivations involve integrals of the form \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ), which can be expressed as Z. The curr_chunk discusses the bias and approximation in variational inference methods, specifically in relation to the gradient \u2207 \u03b8 log Z \u03b8. It highlights the bias and standard deviation of the approximation using the vanilla Monte Carlo method. The \u0192-divergences used in variational inference are also mentioned. The curr_chunk discusses the optimization of \u0192-divergence in variational inference without relying on the normalizing constant Z \u03b8. It shows that the approximation has a bias of order O(K \u22121) and a standard deviation of order O(K \u22121/2). The optimization of \u0192-divergence in variational inference can be done without knowing the normalizing constant Z \u03b8. The approximation involves self-importance sampling and reparametrised estimators, with several particular cases described. The optimization of \u0192-divergence in variational inference involves self-importance sampling and reparametrised estimators, with several particular cases described, such as AISLE-KL-NOREP/RWS and AISLE-KL. The optimization of \u0192-divergence in variational inference involves self-importance sampling and reparametrised estimators. Specifically, AISLE-KL-NOREP/RWS and AISLE-KL are cases discussed. Without reparametrisation, Equation (12) gives the gradient equal to \u2207 rws \u03c6 \u03b8, z. With reparametrisation, Equation (13) yields a different gradient. Proposition 1 demonstrates that IWAE-STL can be derived from AISLE in a principled manner, avoiding breakdowns highlighted in previous work. Proposition 1 shows that IWAE-STL can be derived from AISLE without the need for a multi-sample objective, providing a theoretical basis for IWAE-STL. This approach avoids breakdowns highlighted in previous work and has shown good empirical performance even when other methods fail. Proposition 1 provides a theoretical basis for IWAE-STL, derived from AISLE without a multi-sample objective. IWAE-STL has shown good empirical performance, even when other methods fail, suggesting that breakdowns may not be due to a lack of optimizing a joint objective as previously thought. The breakdown in RWS may not be due to a lack of optimizing a joint objective as previously thought. Variance reduction is achieved by replacing the exact \u03c6-gradient with a self-normalized importance-sampling approximation. AISLE-KL reduces bias and variance by approximating the exact \u03c6-gradient. The \u03b1-divergence between distributions p and q is given by Z. AISLE-KL is derived by applying Lemma 1 to the exact \u03c6-gradient and then approximating the expression, potentially reducing bias and variance. The \u03b1-divergence between distributions p and q can be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz, with \u03ba = \u2212\u03b1 and f (y) = y \u03b1. This is equivalent to a standard \u03c7 2 -divergence when \u03b1 = 2. The \u03b1-divergence between distributions p and q can be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz, with \u03ba = \u2212\u03b1 and f (y) = y \u03b1. Minimising this divergence is important in importance sampling as it relates to the variance of the importance weights. AISLE-\u03b1-NOREP provides a special case without reparametrisation, which is proportional to the 'score gradient' from Dieng et al. (2017, Appendix G). The \u03b1-divergence between distributions p and q can be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz, with \u03ba = \u2212\u03b1 and f (y) = y \u03b1. Minimising this divergence is important in importance sampling as it relates to the variance of the importance weights. AISLE-\u03b1-NOREP provides a special case without reparametrisation, which is proportional to the 'score gradient' from Dieng et al. (2017, Appendix G). AISLE-\u03b1, using reparametrisation, can be derived in a principled manner from AISLE without the need for a multi-sample objective. This demonstrates that IWAE-DREG can be derived in a principled manner from AISLE. Using reparametrisation, Equation (12) becomes with the special case \u2207 This demonstrates that IWAE-DREG can be derived in a principled manner from AISLE, i.e. without the need for a multi-sample objective. If the implementation normalises the gradients, the constant factor cancels out and AISLE-\u03c7 2 becomes equivalent to IWAE-DREG. This shows that the learning rate needs to be scaled as O(K) for the IWAE or IWAE-DREG \u03c6-gradients. For the 'exclusive' KL-divergence, we have KL(q \u03c6 \u03c0 \u03b8 ) = f (w \u03c8 (z))q \u03c6 (z) dz + C(\u03b8) with f (y). The learning rate needs to be scaled as O(K) for the IWAE or IWAE-DREG \u03c6-gradients. For the 'exclusive' KL-divergence, KL(q \u03c6 \u03c0 \u03b8 ) = f (w \u03c8 (z))q \u03c6 (z) dz + C(\u03b8). This approximation is a simple average over K independent replicates of the 'sticking-the-landing' estimator for VAEs proposed in Roeder et al. (2017, Equation 8). The 'exclusive' KL-divergence approximation involves a simple average over K independent replicates of the 'sticking-the-landing' estimator for VAEs, leading to faster convergence of \u03c6 compared to the 'inclusive' KL-divergence. However, minimizing the exclusive divergence may negatively impact learning of \u03b8. Optimizing the 'exclusive' KL-divergence in VAEs can lead to faster convergence of \u03c6 compared to the 'inclusive' KL-divergence. However, minimizing the exclusive divergence may negatively impact learning of \u03b8. The adaptive-importance sampling paradigm of reweighted wake-sleep (RWS) is preferred over the multi-sample objective paradigm of importance weighted autoencoders (IWAEs). The adaptive-importance sampling paradigm of reweighted wake-sleep (RWS) is preferable to the multi-sample objective paradigm of importance weighted autoencoders (IWAEs) due to achieving the same goals while avoiding drawbacks. The self-normalised importance-sampling approximation in RWS interpolates between accurate approximations as the number of particles, K, varies. The self-normalised importance-sampling approximation in RWS interpolates between accurate approximations as the number of particles, K, varies. As K increases, the estimators become more accurate. The self-normalised importance-sampling approximation in RWS becomes increasingly accurate as the number of particles, K, increases. For K = 1, the estimators reduce to vanilla Monte Carlo approximations. This is similar to the standard IWAE \u03c6-gradient, which also represents a vanilla Monte Carlo approximation if K = 1. The self-normalised importance-sampling approximation in RWS becomes more accurate with increasing particles, K. For K = 1, estimators reduce to vanilla Monte Carlo approximations, similar to the standard IWAE \u03c6-gradient. Characterising the small-K self-normalisation bias of reparametrisation-free AISLE \u03c6 gradients is challenging, as they also constitute vanilla Monte Carlo approximations if K = 1. The bias of reparametrisation-free AISLE \u03c6 gradients is challenging to characterize, as they resemble vanilla Monte Carlo approximations with K = 1. Le et al. (2019) suggest that the small-K self-normalisation bias may favor minimizing the exclusive KL-divergence. The use of IWAEs over VAEs aims to reduce bias in \u03b8-gradient by using self-normalised importance-sampling with K > 1 particles. The main motivation for using IWAEs instead of VAEs is to reduce bias in the \u03b8-gradient by using self-normalised importance-sampling with K > 1 particles. The error of such approximations can be controlled by ensuring q \u03c6 is close to \u03c0 \u03b8 in areas where \u03c0 \u03b8 has positive probability mass, with a focus on minimizing the exclusive KL-divergence. The error in IWAEs can be minimized by ensuring q \u03c6 is close to \u03c0 \u03b8 in areas with positive probability mass. The 'inclusive' KL-divergence is crucial for well-behaved importance weights, while the 'exclusive' KL-divergence is not sufficient. The family of proposal distributions Q can be expressive or not. The family of proposal distributions Q can be flexible enough to contain a distribution q \u03c6 close to \u03c0 \u03b8. Minimizing the exclusive KL-divergence can yield well-behaved importance weights in this scenario. In cases where the family of proposal distributions Q is not flexible enough to contain a distribution q \u03c6 close to \u03c0 \u03b8, minimizing the exclusive KL-divergence could lead to poorly-behaved importance weights. In cases where the family of proposal distributions Q is not flexible enough, minimizing the exclusive KL-divergence could lead to poorly-behaved importance weights. For a sufficiently flexible Q, using a gradient-descent algorithm to minimize the exclusive divergence can sometimes be preferable. In Scenario 1, a gradient-descent algorithm minimizing exclusive divergence may be preferable for flexible Q, leading to faster convergence in some applications. Smaller particle numbers, K, could be better for some \u03c6-gradients due to self-normalization bias outweighing standard deviation. In some scenarios, a smaller number of particles, K, may be preferable for certain \u03c6-gradients due to self-normalization bias outweighing standard deviation, leading to faster convergence. Simply setting K = 1 for approximating \u03c6-gradients is not always optimal, as increasing K can help reduce variance. Increasing the number of particles, K, can help reduce the variance of gradient approximations, even in scenarios where self-normalization bias favors faster convergence. Simply setting K = 1 for \u03c6-gradients may not be optimal, as using all K particles and weights can lead to better approximations. Increasing K is desirable to reduce gradient approximation variance. Using all K particles and weights for \u03c6-gradients can lead to better approximations. If K = 1, \u03c6-gradients are simply vanilla Monte Carlo estimates. The text discusses different \u03c6-gradient estimators, including AISLE-KL-NOREP, which is based on the KL-divergence without reparametrisation. This estimator does not require R1 but does not achieve optimal performance. The text compares different gradient estimators for AISLE, including AISLE-KL-NOREP, AISLE-KL, and AISLE-\u03c72-NOREP based on different divergences. The text discusses gradient estimators for AISLE, such as AISLE-KL, AISLE-\u03c72-NOREP, and AISLE-\u03c72, based on different divergences. These gradients do not require R1 but do not achieve zero variance even if q \u03c6 = \u03c0 \u03b8. The gradient for AISLE based on the \u03c72-divergence after reparametrising and exploiting the identity from Lemma 1 is proportional to IWAE-DREG from Tucker et al. (2019). When normalising the gradients, both these gradient approximations lead to the same algorithm as IWAE employing the reparametrisation trick from Kingma & Welling (2014). The gradient approximations for IWAE, IWAE-DREG, and RWS-DREG are computationally equivalent when normalizing the gradients. IWAE-DREG is proportional to AISLE-\u03c72 and was proposed in Tucker et al. (2019). The text discusses gradient approximations for IWAE, IWAE-DREG, and RWS-DREG, with IWAE-DREG being proportional to AISLE-\u03c72 and proposed in Tucker et al. (2019). The joint law of observations and latent variables is parametrized by \u03b8, and each latent variable-observation pair is modeled. The text discusses gradient approximations for IWAE, IWAE-DREG, and RWS-DREG, with RWS-DREG being proportional to AISLE-\u03c72. The joint law of observations and latent variables is parametrized by \u03b8, and each latent variable-observation pair is modeled as a fully factored Gaussian. The parameters to optimize are denoted as column vectors. The text discusses gradient approximations for IWAE, IWAE-DREG, and RWS-DREG, with RWS-DREG being proportional to AISLE-\u03c72. The joint law of observations and latent variables is parametrized by \u03b8. The model assumes a known variable-observation pair (z, x) in a fully factored Gaussian form. The optimization parameters are represented as column vectors. The proposal distribution is a fully factored Gaussian with mean matching the posterior if certain conditions are met. The model assumes a known variable-observation pair in a fully factored Gaussian form. The proposal distribution is a fully factored Gaussian with mean matching the posterior under certain conditions. This model is similar to benchmarks in previous studies, with isotropic Gaussians used for both the generative model and variational approximation. In a more realistic scenario, the latent vectors z can be correlated in the generative model, while the variational approximation remains fully factored, potentially leading to incomplete uncertainty capture. In a more realistic scenario, the latent vectors z can be correlated in the generative model. The variational approximation, however, remains fully factored, which may result in incomplete uncertainty capture. The variance of the A-and b-gradient portion of certain models approaches zero as C \u2192 1 2 I, indicating minimal variance in the 'score-function free' \u03c6-gradients. In this model, the 'score-function free' \u03c6-gradients achieve near-zero variance for the proposal mean parameters when the variance-parameters are close to their optimal values. The variance of the C-gradient portion also approaches zero as the parameters (A, b, C) converge to their optimal values. The 'score-function free' \u03c6-gradients have near-zero variance for proposal mean parameters when variance-parameters are optimal. The variance of the C-gradient portion also approaches zero as parameters (A, b, C) converge to their optimal values. Further analysis on reparametrisation-trick gradients in Gaussian settings is discussed in Xu et al. (2019). Empirical comparison of algorithms is done for different numbers of particles and model dimensions. In Xu et al. (2019), an analysis of the benefits of reparametrisation-trick gradients in Gaussian settings is conducted. The algorithms from Subsection B.1 are empirically compared for various numbers of particles and model dimensions. Each configuration is repeated 100 times with new synthetic data sets. The focus is on optimizing \u03c6 while fixing \u03b8 throughout. The study compares algorithms in Gaussian settings, focusing on optimizing \u03c6 while fixing \u03b8. Different model settings are explored, including scenarios with specific generative models. The study compares algorithms in Gaussian settings, focusing on optimizing \u03c6 while fixing \u03b8. Different model settings are explored, including scenarios with specific generative models. Results show that the fully-factored variational approximation may not fully mimic the dependence structure of the latent variables under the generative model. The fully-factored variational approximation may not fully mimic the dependence structure of latent variables under the generative model. Gradient-ascent algorithm is initialized with values drawn from a standard normal distribution. Stochastic gradient-ascent and ADAM optimization methods are used with a total of 10,000 iterations. Learning-rate parameters decrease with each step. The gradient-ascent algorithm is initialized with values drawn from a standard normal distribution. Stochastic gradient-ascent and ADAM optimization methods are used with a total of 10,000 iterations, with learning-rate parameters decreasing with each step. The covariance matrix is not a diagonal matrix, with a logarithmic scaling on the second axis."
}