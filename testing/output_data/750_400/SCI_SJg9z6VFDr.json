{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) has been introduced to extend the idea of continuous-depth models to graph data. The derivative of hidden node states is parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. Two end-to-end methods for efficient training of GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. The derivative of hidden node states in the graph ordinary differential equation (GODE) is parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. Two efficient training methods for GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver, with the latter showing superior performance in experiments. Additionally, a family of bijective blocks is introduced to reduce memory consumption, enabling easy adaptation to different graph neural networks and improving accuracy. The GODE model, utilizing direct backpropagation through the ODE solver, outperforms the adjoint method. Bijective blocks reduce memory consumption and enhance accuracy. GODE is easily adaptable to various graph neural networks, improving accuracy in tasks like node classification and graph classification. GODE model excels in semi-supervised node and graph classification tasks with continuous time modeling, memory efficiency, accurate gradient estimation, and adaptability to different graph networks. Unlike CNNs limited to grid data like images and text, GODE's performance extends to irregularly structured datasets. Graph data structures represent objects as nodes and relations as edges, widely used for irregularly structured data like social networks, protein interactions, and knowledge graphs. Traditional methods are limited by Euclidean domains, hindering their application in such datasets. Graph data structures represent objects as nodes and relations as edges, widely used for irregularly structured data like social networks, protein interactions, and knowledge graphs. Traditional methods like random walk, independent component analysis, and graph embedding have limited expressive capacity. Recently, graph neural networks (GNN) have been proposed as a new class of models to improve performance in modeling graphs. Graph neural networks (GNN) have been proposed as a new class of models to improve performance in modeling graphs. These models generalize convolution operations to graphs to capture local information, with two main methods for performing convolution: spectral and non-spectral methods. Spectral methods compute the graph Laplacian and perform filtering in the spectral domain. Researchers have proposed methods for performing convolution on graphs, including spectral methods that compute the graph Laplacian for filtering in the spectral domain, and non-spectral methods that directly perform convolution in the graph domain by aggregating information from neighboring nodes. Graph convolution methods can be categorized into spectral methods, which compute the graph Laplacian for filtering in the spectral domain, and non-spectral methods that perform convolution directly in the graph domain by aggregating information from neighboring nodes. Existing GNN models have discrete layers, making it challenging to model continuous diffusion processes in graphs. The GraphSAGE model learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it difficult to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) views a neural network as an ordinary differential equation, and we extend this concept to graphs with graph ordinary differential equations (GODE). Graph ordinary differential equations (GODE) extend the concept of neural ordinary differential equations (NODE) to model message propagation on graphs. NODEs offer adaptive evaluation, accuracy-speed control, and continuous invertible models, but their performance in benchmark image classification tasks is not well-known. In this work, the inferior performance of NODEs compared to discrete-layer models in image classification tasks is attributed to errors in gradient estimation during training. A memory-efficient framework for accurate gradient estimation is proposed to address this issue. In this work, the inferior performance of NODEs in image classification tasks is due to errors in gradient estimation during training. A memory-efficient framework for accurate gradient estimation is proposed to address this issue, leading to high accuracy for both NODE and GODE in benchmark tasks. Our framework for free-form ODEs generalizes to various model structures and achieves high accuracy for both NODE and GODE in benchmark tasks. It significantly improves performance on benchmark classification tasks, reduces test error from 19% to 5% on CIFAR10, and is memory-efficient for free-form ODEs. Additionally, we propose GODE models for graph data and demonstrate improved performance. Our method improves benchmark classification performance, reducing test error from 19% to 5% on CIFAR10. It is memory-efficient for free-form ODEs and achieves constant memory usage with restricted-form invertible blocks. We generalize ODE to graph data with GODE models, demonstrating improved performance on various datasets. Previous work has explored neural networks as differential equations, proposing new architectures based on numerical methods in ODE solvers. In previous work, neural networks have been viewed as differential equations, with new architectures proposed based on numerical methods in ODE solvers. NODE treats the neural network as a continuous ODE and has been used in continuous normalizing flow for generative models. Studies have focused on training NODE using the adjoint method widely used in optimal control. In 2017, a stable architecture based on ODE analysis was proposed for neural networks. NODE treats neural networks as continuous ODEs and has been used in generative models. Various studies have focused on training NODE using the adjoint method. Dupont et al. (2019) proposed augmented neural ODEs to enhance expressive capacity. However, none of the methods discussed the issue of inaccurate gradient estimation or empirical performances in benchmarks. Spectral and non-spectral GNNs are two categories used in geophysical problems and optimal control. Non-spectral GNNs do not require information of the whole graph to determine the graph Laplacian. The empirical performances of NODE in benchmark classification tasks are notably worse than state-of-the-art discrete-layer models. GNNs are categorized into spectral methods and non-spectral methods. Spectral GNNs filter in the Fourier domain of a graph, requiring information of the entire graph to determine the graph Laplacian. On the other hand, non-spectral GNNs only consider message aggregation around neighbor nodes, making them localized and computationally lighter. Several spectral methods, including graph convolution based on the graph Laplacian, have been introduced. Spectral methods for GNNs involve filtering in the Fourier domain of a graph using the Laplacian, leading to heavy computation. Non-spectral methods, on the other hand, focus on message aggregation around neighbor nodes, making them more localized and computationally lighter. Various approaches have been proposed to address the challenges of spectral methods, such as parameterizing spectral filters into a localized version with smooth coefficients. The computation burden is heavy due to non-localized filters. Different approaches have been proposed to address this issue, such as incorporating a graph estimation procedure in spectral networks and using Chebyshev expansion to approximate filters without computing the graph Laplacian. Kipf & Welling (2016) proposed a localized first-order approximation of graph convolution for superior performance in semi-supervised tasks. Defferrard et al. (2016) also introduced fast localized spectral filtering on graphs. The Laplacian and its eigenvectors accelerated running speed. Kipf & Welling (2016) proposed localized first-order graph convolution for superior performance in semi-supervised tasks. Defferrard et al. (2016) introduced fast localized spectral filtering on graphs. MoNet (Monti, 2017) uses a mixture of CNNs for graph convolution. GraphSAGE (Hamilton et al., 2017) samples fixed neighbors for fast inference. Graph attention networks (Veli\u010dkovi\u0107 et al., 2017) learn different weights. In non-spectral methods, convolution operations on graphs are defined by considering neighbors of a node. MoNet uses a mixture of CNNs for graph convolution, GraphSAGE samples fixed neighbors for fast inference, and Graph attention networks learn different weights for different neighbors. GIN has a structure as expressive as the Weisfeiler-Lehman graph isomorphism test. Invertible blocks are neural network blocks with a bijective mapping function. The GIN (Graph Isomorphism Network) learns different weights for neighbors of a node, with a structure as expressive as the Weisfeiler-Lehman test. Invertible blocks are neural network blocks with a bijective mapping function, used in normalizing flow models for calculating log-density of data distribution. Bijective blocks were later used by Jacobsen et al. (2018) for building. Invertible blocks are utilized in normalizing flow models to accurately reconstruct data outputs. They are also used for back propagation without storing activation, leading to a memory-efficient network structure. Jacobsen et al. (2018) and Gomez et al. (2017) utilized invertible blocks in network structures for memory efficiency and back propagation without storing activation. Invertible blocks allow for discarding activation of middle layers as each layer's activation can be reconstructed from the next layer. Invertible blocks in network structures allow for memory efficiency and backpropagation without storing activation. Adding more layers with shared weights leads to a neural ordinary differential equation (NODE) representation. The difference equation transforms into a continuous case using z(t) and a discrete case using x k to represent hidden states. The derivative function f (\u00b7) is parameterized by a network, with a key difference in the form of f between the continuous and discrete cases. In Eq. 1, the difference equation transforms into a neural ordinary differential equation (NODE) using z(t) for continuous case and x k for discrete case to represent hidden states. The forward pass of a model with discrete layers involves different functions f k for each layer, while in the continuous case, f is shared across all time t. The output layer is applied on x K in the discrete case, and the forward pass of a NODE starts with z(0) as input and integrates over time T. In the discrete case, different layers have their own function f k, while in the continuous case, f is shared across all time t. The forward pass of a model with discrete layers involves applying an output layer on x K. For a NODE, the forward pass starts with z(0) as input and integrates over time T, with the transformation of states z modeled as the solution to the NODE. Integration in the forward pass can be done using various ODE solvers. The transformation of states z is modeled as the solution to the NODE, with integration in the forward pass using various ODE solvers. The adjoint method, denoting model parameters as \u03b8, is widely used in optimal process control and functional analysis. The adjoint method, used in optimal process control and functional analysis, involves denoting model parameters as \u03b8. It compares two methods for back-propagation on NODE, showing the discretization of the ODE solver during the forward pass and the hidden state solved in reverse-time. During back-propagation on NODE, the ODE solver is discretized at points {t0, t1, ..., tN } in the forward pass. The adjoint method compares the forward-time hidden state (z(t)) with the reverse-time hidden state (h(t)), aiming for z(t) = h(t) alignment to avoid gradient errors. Direct back-propagation saves evaluation time points during the forward pass and rebuilds the computation graph during the backward pass. The reverse-time solution in back-propagation through ODE solver can be numerically unstable, causing errors in gradient calculation. Direct back-propagation involves saving evaluation time points during forward pass and reconstructing the computation graph during backward pass to accurately evaluate gradients. This allows for optimization of \u03b8 to minimize the loss function L. Detailed proof from an optimization perspective is provided in the appendix. In back-propagation through ODE solver, direct evaluation at the same time points ensures accurate gradient calculation for optimization of \u03b8 to minimize loss function L. The reverse-time integration in Eq. 6 can be solved using any ODE solver, with z(t) determined by solving Eq. 2 in reverse-time. Storing z(t) during forward pass is memory-intensive due to the equivalent infinite-layer model. The reverse-time integration in Eq. 6 is solved using any ODE solver to optimize \u03b8 and minimize loss function L. Storing z(t) during the forward pass is memory-intensive due to the equivalent infinite-layer model. In the backward pass, Eq. 6 requires determining f(z(t), t, \u03b8) and z(t) by solving Eq. 2 in reverse-time, causing inaccurate gradients in adjoint methods. In the backward pass, Eq. 6 requires determining f(z(t), t, \u03b8) and z(t) by solving Eq. 2 in reverse-time, causing inaccurate gradients in adjoint methods due to the instability of reverse-time ODE solver. In the backward pass, inaccurate gradients in adjoint methods are caused by solving Eq. 2 in reverse-time, leading to a mismatch between hidden states solved forward-time and reverse-time. If the ODE is stable in both directions, the Jacobian eigenvalues must have a real part of zero. Proposition 1 states that for an ODE to be stable in both forward and reverse time, the real part of the Jacobian eigenvalues must be zero. If the eigenvalues have a non-zero real part, either the forward or reverse ODE is unstable. This instability can lead to inaccuracies in the solution of the ODE. When the Jacobian of the original system has eigenvalues with non-zero real parts, either the forward or reverse ODE becomes unstable. This instability can affect the accuracy of the solution and computed gradient. To address this, direct back-propagation through the ODE solver is proposed. The ODE solver uses discretization for numerical integration at various time points. The proposed solution involves directly back-propagating through the ODE solver to address numerical errors and instability in the computed gradient. This method ensures accurate hidden states by either saving activation values or reconstructing them at evaluated time points. The proposed solution involves direct back-propagation through the ODE solver to ensure accurate hidden states by saving activation values or reconstructing them at evaluated time points. This method addresses numerical errors and instability in the computed gradient. The proposed solution involves direct back-propagation through the ODE solver to ensure accurate hidden states by saving activation values or reconstructing them at evaluated time points. This method addresses numerical errors and instability in the computed gradient. The model is evaluated at the same time points in forward-time, ensuring accurate back-propagation regardless of stability. The adjoint for discrete forward-time ODE solution is defined, with Eq. 7 as a numerical discretization of Eq. 6 derived from an optimization perspective. Detailed derivations are in appendix E and F. Algorithm 1 outlines accurate gradient estimation in ODE solver for free-form functions. The proposed solution involves direct back-propagation through the ODE solver for accurate hidden states. Eq. 7 is a numerical discretization of Eq. 6 derived from an optimization perspective. Algorithm 1 outlines gradient estimation in ODE solver for free-form functions. Detailed derivations are in appendix E and F. The proposed solution involves direct back-propagation through the ODE solver for accurate hidden states. Algorithm 1 outlines gradient estimation in ODE solver for free-form functions. During the forward pass, the method selects an initial step size and iterates until reaching the final time point T with a given tolerance. During the forward pass, the solver performs numerical integration with adaptive step size based on error estimation, outputting integrated value and time points. Memory is saved by deleting middle activations. During the backward pass, the computation graph is rebuilt directly. The algorithm involves adaptive step size numerical integration during the forward pass and rebuilding the computation graph directly during the backward pass. It supports free-form continuous dynamics and has no constraints on the form of f, making it a generic method. Memory consumption is analyzed based on the function f having N. The algorithm involves rebuilding the computation graph directly during the backward pass, supporting free-form continuous dynamics without constraints on the form of f. Memory consumption analysis shows reduced complexity compared to a naive solver. Memory consumption analysis reveals that our method consumes less memory compared to a naive solver, thanks to deleting middle activations during the forward pass and not needing to search for an optimal stepsize in the backward pass. Additionally, implementing a step-wise checkpoint method can further reduce memory consumption to O(Nf + Nt). The solver can handle free-form functions and invertible blocks efficiently. Implementing a step-wise checkpoint method can reduce memory consumption to O(Nf + Nt) by only storing z(ti) for each ti and computing gradients one at a time. For invertible blocks, memory consumption can be further reduced to O(Nf) by splitting input x into two equal parts. Memory consumption can be reduced to O(Nf) by using invertible blocks, where input x is split into two equal parts. The forward and inverse of a bijective block are denoted as functions F and G, with output (y1, y2) having the same size as (x1, x2).\u03c8(\u03b1, \u03b2) is used in this process. Theorem 1 states that if \u03c8(\u03b1, \u03b2) is a bijective function w.r.t \u03b1 when \u03b2 is given, then the block defined by Eq. 8 is a bijective mapping. Differentiable neural networks F and G with the same input-output shape can be used with different \u03c8 functions for various tasks. Theorem 1 proves that when \u03c8(\u03b1, \u03b2) is bijective w.r.t \u03b1 given \u03b2, the block defined by Eq. 8 is also bijective. This allows for the use of differentiable neural networks with various \u03c8 functions for different tasks, leading to memory-efficient back-propagation without storing activations. The text also introduces graph neural networks with discrete layers and extends to graph ordinary differential equations (GODE). Graph neural networks can apply different \u03c8 functions for various tasks, allowing for memory-efficient back-propagation without storing activations. The text introduces GNNs with discrete layers and extends to graph ordinary differential equations (GODE). GNNs are typically represented in a message passing scheme, where nodes and edges are used to visualize graphs. Graph neural networks (GNNs) are represented with nodes and edges in a graph. Each node is assigned a unique color for visualization. GNNs can be viewed as a 3-stage model with differentiable functions parameterized by neural networks. Graph neural networks (GNNs) operate in a 3-stage model with differentiable functions parameterized by neural networks. In the graph at kth layer, nodes u and v are connected by edge e. N(u) represents the neighbor nodes of node u. The GNN process involves message passing from neighbors to node u, followed by message aggregation by node u. The 3-stage model of Graph Neural Networks involves message passing from neighbor nodes to a central node, message aggregation by the central node, and node state updates based on the aggregated messages. The model can be converted from discrete-time to continuous-time by replacing a function in the equation. The aggregation function \u03b6 is typically permutation invariant operations such as mean and sum for graphs. Nodes update their states based on original states and message aggregation. A discrete-time GNN can be converted to a continuous-time GNN using a graph ordinary differential equation (GODE), which captures highly non-linear functions. GODE's asymptotic stability is demonstrated. By replacing f in Eq. 2 with the message passing process defined in Eq. 9 to 11, GODE is introduced as an ODE that can capture highly non-linear functions. The asymptotic stability of GODE is related to over-smoothing phenomena. Graph convolution is shown to be a special case of Laplacian smoothing. The asymptotic stability of GODE is related to over-smoothing phenomena. Graph convolution is a special case of Laplacian smoothing, represented by Y = (I \u2212 \u03b3D \u22121/2LD\u22121/2 )X. The continuous smoothing process involves real and non-positive eigenvalues of the symmetrically normalized Laplacian. The continuous smoothing process involves the symmetrically normalized Laplacian, with all eigenvalues being real and non-negative. The ODE is asymptotically stable when all eigenvalues of the normalized Laplacian are non-zero, leading to trajectories converging as time grows large. Integration time T affects the similarity of nodes from different classes. In experiments with a CNN-NODE on image classification tasks (CIFAR10 and CIFAR100), it was found that if the integration time T is large enough, all nodes will have similar features, leading to a drop in classification accuracy. The method was also evaluated on bioinformatic graph datasets like MUTAG. The method was evaluated on various benchmark image and graph classification tasks, including CIFAR10, CIFAR100, MUTAG, PROTEINS, IMDB-BINARY, REDDIT-BINARY, Cora, CiteSeer, and PubMed datasets. The experiments involved using a CNN-NODE without pre-processing the raw dataset. The study evaluated the method on different benchmark datasets including CIFAR10, CIFAR100, MUTAG, PROTEINS, IMDB-BINARY, REDDIT-BINARY, Cora, CiteSeer, and PubMed. The raw datasets were inputted into the models without pre-processing for graph classification tasks. For node classification tasks, transductive inference was performed following the train-validation-test split by Kipf & Welling (2016), using less than 6% of nodes as training examples. Details of datasets are in appendix A. For image classification tasks, ResNet18 was directly modified into its corresponding NODE model with a specific function structure. GODE can be applied to any graph neural network by replacing the function in the equation with corresponding structures. GODE can be easily applied to various graph neural network architectures by replacing the function with corresponding structures. It has been demonstrated with different GNN models such as GCN and GAT. To generalize GODE to different GNN architectures, various structures like GCN, GAT, ChebNet, and GIN were used with different depths of layers for training and comparison. Various graph network architectures such as GCN, GAT, ChebNet, and GIN were trained with different depths of layers for comparison. The models used the same hyper-parameters, such as channel number, for fair comparison. For graph classification tasks, the channel number of hidden layers was set as 32 for all models, and for ChebNet, the number of hops was set as 16. The models compared in the study used the same hyper-parameters for fair comparison, with different numbers of hidden layers tested. The performance was evaluated based on accuracy over 10 runs, comparing the adjoint method and direct back-propagation. For classification tasks, different GNN structures were tested with varying parameters such as channel number, number of hops, and number of hidden layers. Direct back-propagation was found to yield higher accuracy compared to the adjoint method. Modifications were made to ResNet18 for CNN-NODE and a GODE model with a GCN was trained for graph networks. Results were reported in Tables 1 and 2 respectively. Direct back-propagation outperformed the adjoint method on classification tasks for both CNN-NODE and graph networks. The training method reduced error rates significantly, validating the instability of the adjoint method. Empirical performance shows that direct back-propagation consistently outperformed the adjoint method on image classification tasks and benchmark graph datasets. The training method reduced error rates significantly and demonstrated robustness to ODE solvers. The error rate of NODE18 decreased significantly on CIFAR10 and CIFAR100 datasets compared to ResNet18. NODE18 also outperformed deeper networks like ResNet101. The method showed robustness to ODE solvers of different orders, improving performance without re-training the network. Our method implemented adaptive ODE solvers of different orders, such as HeunEuler, RK23, and RK45. Using different solvers during inference is equivalent to changing model depth without re-training the network. This approach is robust to different orders of ODE solvers and supports NODE and GODE models with free-form functions. Our method supports adaptive ODE solvers of different orders, demonstrating robustness with only a 1% increase in error rate for continuous models. Bijective blocks can be easily generalized with general neural networks, showcasing differentiable bijective mappings. Results for different \u03c8 are reported in Table 3. The bijective blocks defined with general neural networks can be easily generalized with differentiable bijective mappings. Results for various \u03c8 functions are shown in Table 3, with GODE models outperforming discrete-layer models significantly. Continuous-time models are emphasized for node classification tasks. Results from the study showed that most GODE models outperformed their discrete-layer counterparts significantly, validating the effectiveness of GODE. Different \u03c8 functions behaved similarly on node classification tasks, highlighting the importance of the continuous-time model over the coupling function \u03c8. Lower memory cost was also validated, with detailed information provided in the appendix. Various models were tested on graph classification tasks, including GCN, ChebNet, and GIN, with corresponding GODE models showing promising results. The continuous-time model is more important than the coupling function \u03c8. Lower memory cost is validated in the appendix. Results for different models on graph classification tasks are summarized in Table 4, including GCN, ChebNet, and GIN structures. GODE models performed significantly better in experiments, indicating the importance of the continuous process model for graph models. Integration influence was tested during inference for NODE and GODE models. In experiments comparing free-form and invertible block models, GODE outperformed its discrete-layer counterparts, highlighting the importance of continuous process models for graph models. Integration time was tested during inference, showing that short integration times lead to insufficient information gathering, while long integration times result in oversmoothing issues and decreased accuracy. GODE allows for continuous diffusion modeling. The influence of integration time on network performance was tested, showing that short times lead to insufficient information gathering and long times result in oversmoothing issues. GODE enables continuous diffusion modeling and a memory-efficient back-propagation method for NODEs, demonstrating superior performance in image classification and graph data tasks. The over-smoothing of GNN is related to the asymptotic stability of ODEs. Our paper introduces a memory-efficient back-propagation method for NODEs to accurately determine gradients, improving performance on image classification and graph data tasks. We also address the over-smoothing of GNN by relating it to the asymptotic stability of ODEs. Experiments were conducted on various datasets, including citation networks like Cora, CiteSeer, and PubMed. Our paper improves gradient estimation for NODE, achieving accuracy comparable to state-of-the-art discrete layer models on benchmark tasks. Experiments were conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained and experiments are conducted. The structure of invertible blocks is explained with two important modifications: generalizing to a family of bijective blocks with different functions and proposing a parameter state checkpoint method. The structure of invertible blocks is explained with two important modifications: generalizing to a family of bijective blocks with different functions and proposing a parameter state checkpoint method. This includes using a parameter state checkpoint method to enable bijective blocks to be called multiple times while still accurately generating inversion. The algorithm is summarized in Algo. 2, with pseudo code for forward and backward functions written in PyTorch. Memory consumption is reduced by only keeping outputs y1, y2 in the forward function. The algorithm for the bijective block is outlined in Algo. 2, with pseudo code for forward and backward functions in PyTorch. Memory efficiency is demonstrated by keeping only outputs y1, y2 in the forward function and using inversion to reconstruct input from output in the backward function. The bijective block algorithm outlined in Algo. 2 demonstrates memory efficiency by keeping only outputs y1, y2 in the forward function and using inversion to reconstruct input from output in the backward function. Memory consumption comparison between memory-efficient and memory-inefficient methods on a GODE model with bijective blocks showed significant reductions in memory usage. Memory consumption was compared between memory-efficient and memory-inefficient methods using bijective blocks in a GODE model. Results showed a significant reduction in memory usage with the memory-efficient method, as shown in Table 2. Our memory-efficient method in the GODE model significantly reduces memory consumption compared to conventional methods. The memory usage only slightly increases with depth, from 2.2G to 2.6G, due to caching states of F and G, which minimally impacts memory compared to input. Our memory-efficient bijective block in the GODE model minimizes memory consumption by caching states of F and G, resulting in a slight increase from 2.2G to 2.6G. The block only requires O(1) memory as it stores outputs in cache and deletes activations of middle layers. Algorithm 2 efficiently computes bijective blocks in the GODE model, minimizing memory usage. The stability of an ODE in both forward and reverse time is crucial, with implications for eigenvalues. The stability of an ODE in both forward and reverse time is crucial for bijective blocks in the GODE model. The eigenvalues of J need to have a non-positive real part for stability. Theorem 1 states that a bijective block with defined forward and reverse mappings is a bijective mapping. The eigenvalues of J must have a non-positive real part for stability in both forward and reverse-time ODE. Theorem 1 proves that a bijective block with defined mappings is a bijective mapping by showing it is injective and surjective. To prove the forward mapping is bijective, we need to show it is both injective and surjective. Injective means if Forward(x1, x2) = Forward(x3, x4), then x1 = x3 and x2 = x4. By using the bijective property of \u03c8, we can prove injectivity. Given y1, y2, we construct z1 = \u03c8(x1, G(y2)) = y1, showing the mapping is injective. The mapping is shown to be bijective by proving injectivity and surjectivity. A computation graph is used to derive the gradient for the loss function. The gradient of parameters in a neural-ODE model is derived using a computation graph. The mapping is proven to be bijective through injectivity and surjectivity. In this section, the gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. Notations include z(t) for hidden states, \u03b8 for parameters, x for input, y for target, \u0177 for predicted output, and J(\u0177, y) for loss. The continuous model follows an ODE dz(t)/dt = f(z(t), t, \u03b8), with forward pass defined as \u0177. The continuous model is defined by an ODE with hidden states z at time t. The forward pass is denoted as \u0177, and the loss function is J(\u0177, y). The training process is formulated as an optimization problem, considering one ODE block. The analysis extends from continuous to discrete cases, deriving the gradient of parameters in a neural-ODE model. The forward pass \u0177 and loss function J(\u0177, y) are defined in an optimization problem for training with ODE blocks. The Lagrangian Multiplier Method is used to solve the problem, extending to multiple examples with Karush-Kuhn-Tucker conditions for optimality. When dLoss dz(T) is given, the analysis can be applied to a chain of ODE blocks using the Lagrangian Multiplier Method to solve the problem. Starting from the KKT condition, we derive results by considering the derivative w.r.t. \u03bb at the optimal point. \u03bb is a function of t, and we derive the derivative using calculus of variation. By perturbing \u03bb(t) and a scalar, L becomes a function of the scalar. At optimal point, the derivative w.r.t. \u03bb is derived using calculus of variation. Conditions for Leibniz integral rule are checked, leading to dz(t)/dt - f(z(t), t, \u03b8) = 0 for all continuous differentiable \u03bb(t). To extend results to discrete cases, integrations are replaced. In discrete cases, ODE conditions are replaced with finite sums, leading to the discrete version of the analysis in Eq. 10 and 11."
}