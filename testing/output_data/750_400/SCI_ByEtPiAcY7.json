{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complex nature of deep networks makes it impossible to explain their hidden features in a way that humans can understand. However, this paper proposes a knowledge extraction method using \\textit{M-of-N} rules to address this issue. The paper proposes a knowledge extraction method using \\textit{M-of-N} rules to map the complexity/accuracy landscape of hidden features in a Convolutional Neural Network (CNN). Experiments show an optimal trade-off between comprehensibility and accuracy, indicating each latent variable has an optimal \\textit{M-of-N} rule to describe its behavior. The study uses M-of-N rules to analyze the complexity/accuracy landscape of hidden features in a CNN. Results show an optimal trade-off between comprehensibility and accuracy, with each latent variable having an optimal rule. Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on rule extraction from deep networks and the value of decompositional knowledge extraction. The study analyzes the complexity/accuracy landscape of hidden features in a CNN using M-of-N rules. Optimal trade-offs between comprehensibility and accuracy are found, with rules in the first and final layers being highly explainable, while those in the second and third layers are less so. This highlights the feasibility of rule extraction from deep networks and the value of decompositional knowledge extraction for explainability in AI. Recently, there has been a growing interest in explainable Artificial Intelligence (AI) due to the lack of explainability in neural network models. These models, especially deep networks, rely on distributed representations which make them less explainable compared to symbolic AI or Machine Learning approaches. The use of very large networks, specifically deep networks, relies on distributed representations to accurately model data. Unlike symbolic AI or symbolic Machine Learning, distributed representations do not necessarily correlate with easily identifiable features of the data. Knowledge extraction aims to increase the explainability of neural networks by uncovering the knowledge they possess. Knowledge extraction aims to increase the explainability of neural networks by uncovering the implicit knowledge learned in their weights. This can be done by translating trained neural networks into symbolic rules or decision trees, similar to techniques used in symbolic AI and logic programming. Rule extraction algorithms have been developed for decades to achieve this goal. Rule extraction techniques aim to uncover the implicit knowledge learned in neural networks' weights by translating them into symbolic rules or decision trees. These techniques have been developed for decades, using either decompositional or pedagogical approaches to generate rules from network parameters or behavior. Rule extraction algorithms have been developed over the years, using decompositional or pedagogical approaches to generate rules from neural network parameters or behavior. The complexity of extracted rules remains a major issue, as large rule sets derived from CNNs may not be more comprehensible than the original network. The complexity of extracted rules from neural networks remains a major issue, as large rule sets may not be more comprehensible than the original network due to distributed representations found in neural networks. The distributed nature of neural networks, with patterns of activity over many neurons, makes knowledge extraction challenging. This complexity has led to the conclusion that explaining latent features through symbolic knowledge extraction is not effective. The distributed nature of neural networks and the importance of distributed representations in connectionism have led to the conclusion that explaining latent features through symbolic knowledge extraction is ineffective. Distillation is proposed as an alternative method, but its efficacy is questioned. Other practical approaches focus on guarantees of network behavior rather than opening the black box. In this paper, a method is developed to examine the explainability of latent variables in neural networks by using rule extraction to describe latent variables. The paper develops a method for empirically examining the explainability of latent variables in neural networks by using rule extraction to describe the variables. The method involves searching through a space of rules to measure error and complexity, allowing for a trade-off between complexity and accuracy in capturing network behavior. When applied to a 4-layer CNN trained on fashion MNIST, the method reveals insights into the relationship between rule complexity and behavior capture. The method involves exploring a rule extraction landscape to describe latent variables in neural networks. It reveals that some layers have accurate rules while others do not, even with complex rules. A 'critical point' on the landscape indicates an ideal M-of-N rule for each variable, with rule accuracy depending on the variable being described. In exploring the rule extraction landscape of neural networks, it was found that some layers have accurate rules while others do not, even with complex rules. A 'critical point' on the landscape indicates an ideal M-of-N rule for each variable, with rule accuracy depending on the variable being described. The accuracy of rules varies greatly between layers and architectures, with convolutional layers extracting more complex rules compared to fully connected layers. The explainability trends differ greatly between layers and architectures in neural networks. Convolutional layers extract more complex rules compared to fully connected layers, with rules in the first and final layers having near 0% error. Rules from the second and third layers have around 15% error. The text discusses the extraction of rules with varying complexities in different layers of neural networks. Convolutional layers have higher complexities, while fully connected layers have lower complexities. Rules in the first and final layers have near 0% error, while rules in the second and third layers have around 15% error. The text also outlines previous algorithms used for knowledge extraction and presents experimental results before concluding. The text discusses the extraction of rules with varying complexities in different layers of neural networks, with convolutional layers having higher complexities and fully connected layers having lower complexities. Previous algorithms for knowledge extraction are outlined, and experimental results for rule extraction are presented. The KBANN algorithm used a decompositional approach to extract symbolic rules from hidden variables in feedforward networks. The KBANN algorithm used a decompositional approach to extract symbolic rules from hidden variables in feedforward networks. More recent algorithms generate binary trees representing M-of-N rules, which can be reduced to propositional logic sentences. These algorithms are pedagogical in selecting rules using input units. Neuron activation in M-of-N rules BID20 is followed by algorithms generating binary trees representing M-of-N rules BID12 BID4. These algorithms select rules based on input units for maximum information gain with respect to the output, treating the model as a black box for rule extraction. The extraction methods for M-of-N rules focus on maximum information gain with respect to the output, treating the model as a black box for rule extraction. Various methods exist, including eclectic approaches combining pedagogical and decompositional methods, as well as visually oriented techniques. Many extraction methods, including eclectic and visually oriented techniques, have been developed to solve the 'black-box' problem of neural networks. Most decompositional rule extraction techniques focus on shallow networks and input/output relationships rather than explaining the latent features of deep networks with multiple hidden layers. The problem of neural networks is addressed in BID8. Decompositional rule extraction techniques are typically applied to shallow networks, with a focus on input/output relationships rather than explaining the latent features of deep networks with multiple hidden layers. Extracted rules can become too complex for human understanding due to the hierarchy of rules in deep networks. The use of decompositional techniques to explain the features of deep networks seems impractical due to the complexity of extracted rules in hidden layers. However, some layers may have explainable rules that can clarify the network's behavior in terms of certain features. The experiments in this paper show that certain layers of a deep network may have explainable rules that can clarify the network's behavior in terms of specific features. This suggests that rule extraction could be used to modularly explain network models and provide insights into the similarity and disentanglement of latent features. The possibility of rule extraction as a tool for modular explanation of network models and insight into latent feature similarity and disentanglement is defined formally in logic programming. A logical rule is an implication A \u2190 B, where A is the head and B is the body of the rule. Disjunctions in the body can be represented as multiple rules with the same head. Logic programs often include negation by failure. In logic programming, a logical rule is an implication of the form A \u2190 B, where A is the head and B is the body of the rule. The rules are used to explain the states of neurons in a neural network, with literals representing the neuron states. In neural networks, literals represent neuron states with binary or continuous values. For binary values, a literal X is defined as True if x = 1 and False if x = 0. For continuous values, X is True if x > a and False otherwise. Latent variables in neural networks are not well described by a single conjunctive rule due to various input configurations activating a neuron. In neural networks, a latent variable is poorly described by a single conjunctive rule. M-of-N rules soften the constraint by requiring only M variables to be true in the rule body. M-of-N rules in neural networks soften the conjunctive constraint on logical rules by requiring only M variables to be true in the rule body, offering a more compact and general representation. M-of-N rules provide a compact and general representation in neural networks by requiring only M variables to be true in the rule body, offering a more general representation than a conjunction. M-of-N rules offer a more general representation in neural networks than a conjunction, reflecting input/output dependencies of neurons. They are a subset of propositional formulas and share structural similarity with neural networks, resembling weightless perceptrons. M-of-N rules can be viewed as 'weightless perceptrons' in neural networks, with the output neuron representing the head and visible neurons representing the body of the rule. By setting the bias of the output neuron to M and the weights of input neurons to 1 or -1 for positive or negative literals, M-of-N rules can be encoded in neural networks. This representation has been used in early knowledge extraction but has been overlooked until now. This paper highlights the importance of M-of-N rules in neural networks for explainability. It discusses setting biases and weights for output and input neurons, respectively, and the use of splitting values for continuous activation values. When our network has continuous activation values, we choose splitting values for rule extraction based on information gain. The splitting values are selected to maximize the decrease in entropy of the network outputs on test examples. When explaining a target neuron, a literal is generated by selecting a split that maximizes information gain with respect to the output labels. Input literals are then generated by choosing splits for each input that maximize information gain with respect to the target literal. Each target literal in a layer will have its own set of input literals. The input literals are generated from the inputs to the target neuron by choosing splits that maximize information gain. Each target literal in a layer will have its own set of input literals, corresponding to the same set of input neurons but with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. Only the neuron with the optimal split that maximizes information gain with respect to the network output is tested. In rule extraction, the focus is on the accuracy and comprehensibility of the extracted rules. Accuracy is defined in terms of the expected difference between the rules' predictions and the network output. This approach aims to generate a single rule for each feature map, based on the optimal split that maximizes information gain with respect to the network output. The rule extraction process focuses on accuracy and comprehensibility. Accuracy is measured by the expected difference between the rules' predictions and the network output. Each feature map is represented by a single rule based on the optimal split for maximum information gain. The rule extraction process in a neural network focuses on accuracy and comprehensibility. The network computes the state of a neuron based on input neurons, determining the truth of a literal. Rules relating variables can be used to determine values, and discrepancies between rule and network outputs can be measured. When relating variables H and X i, the state of input x determines X i values, and R determines H value (R(x)). Discrepancy between rule and network outputs can be measured using input configurations I. Comprehensibility is subjective, based on rule complexity analogous to Kolmogorov complexity. In summary, the average error of rules predicting network output is measured over a test set. Comprehensibility is subjective and based on rule complexity, determined by the length of its body in disjunctive normal form. Complexity of an M-of-N rule is calculated as M N M, normalized relative to maximum complexity in experiments. The complexity of a rule is determined by the length of its body in disjunctive normal form. For an M-of-N rule, complexity is calculated as M N M, normalized relative to the maximum complexity in experiments. The complexity of a rule is determined by the length of its body in disjunctive normal form, normalized w.r.t. a maximum complexity. An example with a simple perceptron is provided, showing how the normalized complexity measure is calculated. The complexity of a rule is determined by the length of its body in disjunctive normal form, normalized w.r.t. a maximum complexity. An example with a simple perceptron is provided, showing how the normalized complexity measure is calculated using a weighted sum with a parameter \u03b2 to trade-off between soundness and complexity. The complexity of a rule is determined by its length in disjunctive normal form, normalized with a maximum complexity. A loss function is defined for a rule using a weighted sum with a parameter \u03b2 to balance soundness and complexity. By using a brute force search with different \u03b2 values, the relationship between rule complexity and accuracy is explicitly determined. For \u03b2 = 0, the rule with minimum loss is the one with minimum error regardless of complexity, while for large \u03b2 values, the rule with minimum loss has 0 complexity. The relationship between rule complexity and accuracy is explicitly determined by balancing soundness and complexity using a loss function with a parameter \u03b2. For \u03b2 = 0, the rule with minimum loss is the one with minimum error regardless of complexity, while for large \u03b2 values, the rule with minimum loss has 0 complexity. Neurons are split and literals are generated to represent them, with negation for neurons with negative weight. Neurons are split and literals are generated to represent them, with negation for neurons with negative weight. M-of-N rules are generated based on the magnitude of the weight connecting input neurons to the neuron in consideration. Neurons are split and literals are generated to represent them, with negation for neurons with negative weight. M-of-N rules are generated based on the magnitude of the weight connecting input neurons to the neuron in consideration. The search procedure relies on the ordering of variables X i to minimize L(R) in O(n^2) rules. Information gain is maximized to define the literal H for each neuron x. The search procedure generates splits and literals for neurons based on information gain. Neurons with n inputs have O(2^n) possible M-of-N rules, making exhaustive search intractable. The procedure relies on maximizing information gain to define literals for each neuron. The search procedure generates splits and literals for neurons based on information gain. Neurons with n inputs have O(2^n) possible M-of-N rules, making exhaustive search intractable. The most accurate M-of-N rules are assumed to use literals corresponding to neurons with the strongest weights. This assumption is justified by the conditional independence of hidden units in all layers except the final one. The conditional independence of output neurons is maintained when using a softmax function. The most accurate M-of-N rules are assumed to use literals corresponding to neurons with the strongest weights, justified by the conditional independence of hidden units. The conditional independence of output neurons is maintained with a softmax function. Ordering literals by information gains instead of weights may not be necessary based on high accuracy in experimental results. The conditional independence of output neurons is no longer valid. Ordering literals by information gains instead of weights may not be necessary based on high accuracy in experimental results. Implementing the algorithm in Spark on IBM cloud services helped reduce the computational difficulty in rule extraction. To speed up rule extraction, the algorithm was implemented in Spark on IBM cloud services. Accuracy was measured using examples from the training set, not the test set, to better represent the network's learned behavior. Running the search in parallel helped map the order of literals efficiently. To measure the accuracy of extracted rules, examples from the training set were used instead of the test set. Running the search in parallel allowed for mapping the accuracy/complexity graph for 50 hidden neurons in a few hours. The number of examples used in the accuracy calculation was limited to 1000 to avoid increased time taken. The extraction process for the first hidden feature was demonstrated. By running the search in parallel, the accuracy/complexity graph for 50 hidden neurons can be mapped in a few hours using 1000 examples. The extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset involves selecting 1000 random input examples to compute neuron activations and predicted labels. With input images of 28x28 pixels, there are 784 neurons per feature. The extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset involves selecting 1000 random input examples to compute neuron activations and predicted labels. Each neuron corresponds to a different 5x5 patch of the input, with 784 neurons per feature in the first layer. The optimal splitting value of each neuron is found by computing the information gain with respect to the network's predicted labels. In the CNN trained on the fashion MNIST dataset, the first hidden feature involves selecting 1000 random input examples to compute neuron activations. Each neuron corresponds to a 5x5 patch of the input, with 784 neurons per feature in the first layer. The optimal splitting value of each neuron is determined by computing the information gain with respect to the network's predicted labels. The neuron with the maximum information gain for the first feature is neuron 96, with an information gain of 0.015 when split on the value 0.0004, corresponding to the image patch centered at (3, 12). Neuron 96 has the maximum information gain of 0.015 when split on the value 0.0004, corresponding to the image patch centered at (3, 12). Input splits are defined based on this neuron, and M-of-N rules are used to explain H for different error/complexity tradeoffs. Neuron 96 has the highest information gain when split on value 0.0004, with input splits based on this neuron. M-of-N rules are used to explain H for various error/complexity tradeoffs, resulting in three different rules visualized in Figure 1. Neuron 96 has the highest information gain when split on value 0.0004. Increasing the complexity penalty results in three different rules extracted, visualized in Figure 1. The most complex rule is a 5-of-13 rule with a 0.025 error, agreeing with the network 97.5% of the time. The neuron's input features are represented by grey, white, and black colors in the M-of-N rule extraction process. A 5-of-13 rule with a 0.025 error agrees with the network 97.5% of the time. Penalizing complexity leads to simpler rules with higher errors. Applying penalty to complexity in rule extraction process results in simpler rules with higher errors. Demonstrated on DNA promoter dataset, the relationship between complexity and error is exponential, showing an ideal tradeoff. Training a feed forward network with a single hidden layer of 100 nodes on the DNA promoter dataset shows an exponential relationship between complexity and error, suggesting an ideal tradeoff. The output layer rule 1 \u2212 of \u2212 {H 39 , H 80 } gives 100% fidelity to the network, with splits for the hidden layer defined by information gain. Each literal in the rule can be described with an M-of-N rule extracted from the input layer. The output layer rule 1 \u2212 of \u2212 {H 39 , H 80 } gives 100% fidelity to the network. Each literal in the rule can be described with an M-of-N rule extracted from the input layer, with specific rules for variables H 39 and H 80. The rules for variables H39 and H80 in the network have different error rates, with errors propagating through layers due to imperfect rule approximation. Different splits for input neurons can lead to discrepancies in the same layer. The output of a network can be predicted by a rule, with errors propagating through layers when rules don't perfectly approximate each layer. To replace a network with hierarchical rules, a single set of splits for each layer must be decided by selecting input splits based on information gain against all configurations of the new output layer. When replacing a network with hierarchical rules, a single set of splits for each layer must be decided by selecting input splits based on information gain against all configurations of the new output layer. Conducting experiments layer by layer independently can provide an idealized complexity/error curve for rule extraction with M-of-N rules. This approach helps identify when M-of-N rule extraction might be useful. The experiments were conducted layer by layer independently to create a complexity/error curve for rule extraction with M-of-N rules. This method helps identify when M-of-N rule extraction might be useful and provides a baseline for evaluating other extraction algorithms. The rule extraction landscape of a neural network trained on fashion MNIST in tensorflow was examined using layerwise rule extraction search on a basic CNN with a standard architecture. In a study on rule extraction, a neural network trained on fashion MNIST in tensorflow was tested using layerwise rule extraction search on a basic CNN with a standard architecture. The CNN had specific layers and features, and 1000 random inputs from the fashion MNIST training data were used for testing. The study tested a neural network trained on fashion MNIST using layerwise rule extraction search on a basic CNN with specific layers and features. 1000 random inputs from the fashion MNIST training data were used for testing, with a 32 feature convolutional layer, a 64 feature convolutional layer, and a final hidden fully connected layer of 1024 units. In testing the neural network trained on fashion MNIST, rules were extracted for convolutional layers based on maximum information gain. The search for rules in the third layer was limited to 50 features with 1000 literals or less. The final layer output was tested with 10 one-hot neurons, each undergoing rule searching. The search procedure was repeated for 5 different values of \u03b2, resulting in 5 sets of extracted rules. The final layer output was tested with 10 one-hot neurons, each undergoing rule searching for 5 different values of \u03b2. This produced 5 sets of extracted rules with different error/complexity trade-offs, allowing for a graph mapping out the error/complexity landscape for rules extracted from each layer. The final layer output was tested with 10 one-hot neurons, each undergoing rule searching for 5 different values of \u03b2, resulting in 5 sets of extracted rules with varying error/complexity trade-offs. A graph was created to map out the error/complexity landscape for rules extracted from each layer, showing different trade-offs for each layer. The first and final layers had accurate rules with near 0 error, while the second and third layers had a similar accuracy/complexity trade-off, with the second layer showing a slight improvement in accuracy but with more complex rules. The Complexity/Error trade-off varies for rules extracted from different layers. The first and final layers produced accurate rules with minimal error, while the second and third layers had a similar trade-off. The second layer showed a slight accuracy improvement with more complex rules, while the third layer's minimum error could not be improved with increased complexity. Trivial rules performed worse on the second layer compared to the third. The optimal accuracy/complexity trade-off is not solely determined by the number of input nodes, as the third layer performed similarly to the second. The third layer's minimum error cannot be improved with more complex rules, unlike the second layer. Despite having more input nodes, the third layer performs similarly to the second layer. The final layer provides accurate rules with less complexity compared to the first layer. The results show a consistent pattern in the rule extraction landscape across different layers. The third layer performs similarly to the second layer despite having more input nodes. The final layer provides more accurate rules with less complexity compared to the first layer. There is a critical point where error increases rapidly as complexity penalty increases, suggesting a natural set of rules for explaining latent features. The critical point in the rule extraction landscape shows a rapid increase in error as complexity penalty increases, indicating a natural set of rules for explaining latent features. Existing algorithms do not consider complexity optimization explicitly. The paper introduces a novel approach to rule extraction algorithms by incorporating rule complexity as a key factor in optimization. This is a significant departure from existing methods that do not explicitly consider complexity. Empirical evaluation of popular extraction algorithms is crucial for validation. This paper introduces a novel approach to rule extraction algorithms by incorporating rule complexity as a key factor in optimization. The analysis includes empirical evaluation of popular extraction algorithms, outlining their limitations and potential. In some cases, complex rules have an error rate of 15%, while in others, there are no simple explanations for features. The limitations and potential of rule extraction algorithms are outlined in the results. In some cases, there are no simple explanations for features, with complex rules having a 15% error rate. However, in other cases, output neuron behavior can be accurately captured by relatively simple rules with near 0% error. This suggests that decompositional rule extraction may not be a general method of explainability. The final layer in the CNN can accurately capture output neuron behavior with simple rules, even achieving near 0% error for rules with complexity under 0.05. This suggests selective use of decompositional algorithms depending on the layer we wish to explain. The black box problem of neural networks remains a challenge for their deployment into society. The black box problem of neural networks hinders their deployment into society, but selective use of decompositional algorithms can help explain different layers. Knowledge extraction has been challenging, with large neural networks remaining difficult to interpret and explain. The need for explainability in neural networks has grown as they become more integrated into society. Knowledge extraction has been challenging, with most large neural networks remaining difficult to interpret. Critics argue that the distributed nature of neural networks makes rule extraction unfeasible. A novel search method for M-of-N rules is applied to explain the latent features of a CNN. Critics argue that the distributed nature of neural networks makes rule extraction unfeasible. A novel search method for M-of-N rules is applied to explain the latent features of a CNN, finding that generally latent features can be described by an 'optimal' rule representing an ideal error/complexity trade-off. The search for extracted rules in explaining the latent features of a CNN reveals a large discrepancy in the trade-off between neurons in different layers, suggesting that rule extraction may not adequately describe all latent variables. However, simplifying explanations without reducing accuracy is possible in many cases. The search for extracted rules in explaining the latent features of a CNN reveals a large discrepancy in the trade-off between neurons in different layers, suggesting that rule extraction may not adequately describe all latent variables. However, simplifying explanations without reducing accuracy is possible in many cases, indicating that decompositional rule extraction may still be an important tool for understanding network behavior. Rule extraction can be a useful tool for understanding networks with easily understandable features. Further research is needed to explore the impact of different factors on accuracy and interpretability."
}