{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. Adaptivity is implemented across manifolds in the cartesian product. Amsgrad has not been generalized to Riemannian manifolds. Algorithms and convergence proofs are provided for geodesically convex objectives in the case of a product of Riemannian manifolds, with adaptivity implemented across manifolds in the cartesian product. The generalization is tight, yielding the same algorithms and regret bounds as those known for standard algorithms when choosing the Euclidean space as the Riemannian manifold. Experimentally, Riemannian adaptive methods show faster convergence and lower train loss values compared to their baselines. Developing powerful stochastic gradient-based optimization algorithms is crucial for various applications. Riemannian adaptive methods show faster convergence and lower train loss values compared to their baselines, particularly when embedding the WordNet taxonomy in the Poincare ball. This generalization is tight, with the same algorithms and regret bounds as those for standard algorithms when using the Euclidean space as the Riemannian manifold. The development of powerful stochastic gradient-based optimization algorithms is essential for various applications, including embedding the WordNet taxonomy in the Poincare ball. First order methods like ADAGRAD, ADADELTA, and ADAM have been successful in optimizing deep neural networks and learning embeddings over large vocabularies. The optimization of parameters in a Euclidean space has been addressed by algorithms like ADAGRAD, ADADELTA, and ADAM. However, recent work focuses on optimizing parameters on a Riemannian manifold, a more general setting. Recent work has focused on optimizing parameters on a Riemannian manifold, a more general setting than the Euclidean space used by algorithms like ADAGRAD, ADADELTA, and ADAM. This family of algorithms has various applications, such as solving Lyapunov equations, matrix factorization, geometric programming, dictionary learning, and hyperbolic taxonomy embedding. On a Riemannian manifold, algorithms like Riemannian stochastic gradient descent (RSGD) have been developed for non-Euclidean geometries. These methods have applications in solving Lyapunov equations, matrix factorization, geometric programming, dictionary learning, and hyperbolic taxonomy embedding. The adaptivity of algorithms on a Riemannian manifold involves assigning a learning rate per coordinate, but the lack of an intrinsic coordinate system makes notions like sparsity or coordinate-wise update meaningless. New methods for convergence analysis in the geodesically convex case are needed to find Riemannian counterparts for empirically successful adaptive methods. In this work, the authors discuss the challenges of generalizing adaptive algorithms to Riemannian manifolds and propose new methods for convergence analysis in the geodesically convex case. They highlight the difficulty of assigning learning rates per coordinate in a coordinate-less system and suggest generalizations of the algorithms for a product of manifolds. The authors propose generalizations of adaptive algorithms for Riemannian manifolds, specifically in the case of a product of manifolds representing coordinates. They empirically support their claims with hyperbolic taxonomy embedding tasks. The authors developed Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces, such as the GloVe algorithm. The absence of Riemannian adaptive algorithms could be significant for optimizing semantic/syntactic relationships in word embeddings. The absence of Riemannian adaptive algorithms could hinder the development of competitive optimization-based Riemannian embedding methods, especially in hyperbolic spaces. The recent rise of embedding methods in hyperbolic spaces could benefit from the development of Riemannian adaptive algorithms. A manifold is a space that can be locally approximated by a Euclidean space and is a generalization of the notion of surface to higher dimensions. A manifold of dimension n is a space that can be approximated locally by a Euclidean space, generalizing the notion of surface to higher dimensions. The tangent space at each point is an n-dimensional vector space, while a Riemannian metric is a collection of inner-products at each point on the manifold. A Riemannian manifold is an (n \u2212 1)-dimensional manifold embedded in R n with zero curvature. It has a tangent space at each point, defined by a Riemannian metric \u03c1 that varies smoothly. This metric induces a global distance function on the manifold. A Riemannian manifold is defined by a collection of inner-products varying smoothly with x on the tangent space of M. This induces a global distance function on the manifold, allowing for the calculation of geodesics. In a Riemannian manifold (M, \u03c1), Riemannian SGD is defined by updating with the gradient of the objective function and a step-size. This method matches with Euclidean space when the manifold is the Euclidean space. In a Riemannian manifold (M, \u03c1), BID1 defines Riemannian SGD with the update formula using the Riemannian gradient of f t at x t. The exponential map allows updates along the shortest path in the relevant direction while staying in the manifold. When the exponential map is unknown, a retraction map like R x (v) = x + v is commonly used as a first-order approximation. In practice, when the exponential map exp x (v) is not known, it is common to use a retraction map R x (v) = x + v as a first-order approximation. This approach enables updates along the shortest path in the relevant direction while staying in the manifold. Additionally, algorithms like ADAGRAD rescale updates coordinate-wise based on past gradients, which can be beneficial in scenarios with sparse gradients or deep networks. The ADAGRAD algorithm, introduced by BID5, rescales updates coordinate-wise based on past gradients, which can be beneficial in scenarios with sparse gradients or deep networks. On the other hand, the ADAM algorithm, proposed by BID9, includes momentum and adaptivity terms in its update rule. The ADAM update rule, proposed by BID9, includes momentum and adaptivity terms in its update rule. When \u03b21 = 0, it essentially recovers the RMSPROP method, with the only difference being an exponential moving average instead of a sum for past gradients in the adaptivity term. This helps avoid the issue of learning stopping too early in ADAGRAD. AMSGRAD was proposed as a modification to the ADAM algorithm to address a mistake in its convergence proof. It introduces an increasing schedule for the \u03b22 parameter to improve performance. The AMSGRAD modification was proposed to fix a mistake in the ADAM algorithm's convergence proof. It introduces an increasing schedule for the \u03b22 parameter to improve performance. BID18 identified the mistake and proposed AMSGRAD or ADAMNC as solutions. The ADAMNC modification introduces a time-dependent schedule for \u03b22, with mt and vt defined similarly to ADAM. It works with local coordinate systems on a Riemannian manifold. The ADAMNC modification introduces a time-dependent schedule for \u03b22, with mt and vt defined similarly to ADAM. It works with local coordinate systems on a Riemannian manifold, allowing for intrinsic definitions of quantities. Intrinsic quantities on a Riemannian manifold are defined using coordinate systems, with some being intrinsically defined while others are not. The RSGD update is intrinsic as it involves objects intrinsic to the manifold, but it is unclear if Eqs. (3,4,5) can be expressed in a coordinate-free or intrinsic manner. The RSGD update on a Riemannian manifold is intrinsic, involving exp and grad which are intrinsic objects. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free or intrinsic way. The RSGD update on a Riemannian manifold involves parallel-transporting a canonical coordinate system along the optimization trajectory. In a general Riemannian manifold, parallel transport depends on the chosen path and curvature. In a general Riemannian manifold, parallel transport between two points x and y depends on the chosen path and curvature, introducing a rotational component that breaks the sparsity of gradients and the benefit of adaptivity. The interpretation of adaptivity as optimizing different features at different speeds is lost in this context. In a general Riemannian manifold, parallel transport depends on the chosen path and curvature, introducing a rotational component that breaks the sparsity of gradients and adaptivity benefits. The interpretation of adaptivity as optimizing different features at different speeds is lost in this context. The techniques used to prove the theorems may not apply to updates defined differently. Additional structure is assumed on (M, \u03c1) as the cartesian product of n Riemannian manifolds. The coordinate system for representing gradients depends on the optimization path. Theorems may not apply to updates defined differently. Additional structure is assumed on (M, \u03c1) as the cartesian product of n Riemannian manifolds. The induced distance function on M is given by the induced product metric \u03c1. The exponential, log map, and parallel transport in M are concatenations of those in each component M_i. Designing adaptive schemes in a general Riemannian manifold is challenging due to the absence of intrinsic coordinates. Each component x_i in x can be seen as a \"coordinate\", leading to a simple adaptation of Eq. (3) on the adaptivity term. The adaptivity term involves squared Riemannian norms g. In a general Riemannian manifold, designing adaptive schemes is challenging due to the absence of intrinsic coordinates. Each component x_i in x can be viewed as a \"coordinate\", allowing for a simple adaptation of Eq. (3) on the adaptivity term. The adaptivity term involves squared Riemannian norms g. Additionally, ADAM, ADAGRAD, and AMSGRAD were briefly discussed in section 2. In the Euclidean setting, norms g are used in the adaptivity term for rescaling the gradient. ADAM is a combination of ADAGRAD with momentum, while AMSGRAD corrects the convergence proof of ADAM. ADAMNC is ADAM with a non-constant schedule for parameters \u03b21 and \u03b22. ADAMNC is a modification of ADAM with a non-constant schedule for \u03b21 and \u03b22. It was initially proposed with a schedule for \u03b22 that allows v t to recover the sum of squared-gradients of ADAGRAD. ADAMNC without momentum yields ADAGRAD on geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0. ADAMNC is a modification of ADAM with a non-constant schedule for \u03b21 and \u03b22. The schedule proposed for \u03b22 allows v t to recover the sum of squared-gradients of ADAGRAD. On geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0, ADAMNC without momentum yields ADAGRAD. The manifold (M, \u03c1) is the product of (M i , \u03c1 i )'s, with X i being a compact, geodesically convex set for each i. The set of feasible parameters is X := X 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X n, with \u03a0 Xi : M i \u2192 X i as the projection operator. On geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0, a family of differentiable, geodesically convex functions is considered. The functions are defined on a set of feasible parameters X, with each component having a bounded diameter. The functions satisfy a gradient condition for all parameters within X. On geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0, a family of differentiable, geodesically convex functions is considered. The functions are defined on a set of feasible parameters X, with each component having a bounded diameter. The functions satisfy a gradient condition for all parameters within X. Denote by P i , exp i and log i the parallel transport, exponential and log maps in DISPLAYFORM0 and by g i \u2208 T x i M i the corresponding components of x and g. Let (f t ) be a family of differentiable, geodesically convex functions from M to R. Assume each X i \u2282 M i has a diameter bounded by D \u221e and for all 1 \u2264 i \u2264 n, t \u2208 [T ] and x \u2208 X , (gradf t (x)) i xi \u2264 G \u221e . Our convergence guarantees will bound the regret after T rounds as DISPLAYFORM1. Riemannian AMSGRAD is presented in FIG1, alongside the standard AMSGRAD algorithm. On geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0, a family of differentiable, geodesically convex functions is considered. The functions are defined on a set of feasible parameters X, with each component having a bounded diameter. The functions satisfy a gradient condition for all parameters within X. Denote by P i , exp i and log i the parallel transport, exponential and log maps in DISPLAYFORM0 and by g i \u2208 T x i M i the corresponding components of x and g. Let (f t ) be a family of differentiable, geodesically convex functions from M to R. Assume each X i \u2282 M i has a diameter bounded by D \u221e and for all 1 \u2264 i \u2264 n, t \u2208 [T ] and x \u2208 X , (gradf t (x)) i xi \u2264 G \u221e . Our convergence guarantees will bound the regret after T rounds as DISPLAYFORM1. Riemannian AMSGRAD is presented in FIG1, alongside the standard AMSGRAD algorithm. The convergence guarantee for RAMSGRAD is presented in Theorem 1, where the quantity \u03b6 is defined. On geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0, a family of differentiable, geodesically convex functions is considered. The convergence guarantee for RAMSGRAD is presented in Theorem 1, where the quantity \u03b6 is defined. RADAM and ADAM are obtained by removing max operations. Convergence guarantees between RAMSGRAD and AMSGRAD coincide when (M i , \u03c1 i ) = R for all i. The convergence guarantees for RAMSGRAD, AMSGRAD, and RADAMNC on Riemannian manifolds with lower bounded sectional curvature are discussed. Convergence between RAMSGRAD and AMSGRAD coincides when certain conditions are met. The regret bound worsens at a speed determined by the curvature. RADAGRAD's convergence proof is obtained with a specific parameter setting. The regret bound worsens with small non-zero curvature at a speed determined by a multiplicative factor. Convergence guarantees for RAMSGRAD, AMSGRAD, and RADAMNC on Riemannian manifolds with lower bounded sectional curvature are discussed. The convergence proof for RADAGRAD is obtained with a specific parameter setting. The convergence of RAMSGRAD, AMSGRAD, and RADAMNC on Riemannian manifolds with lower bounded sectional curvature is discussed. The role of convexity in the convergence guarantees is highlighted, with a comparison between differentiable functions and geodesic convexity. The role of convexity in the convergence guarantees of RAMSGRAD, AMSGRAD, and RADAMNC on Riemannian manifolds with lower bounded sectional curvature is emphasized. The notion of convexity in Theorem 5 is replaced by geodesic convexity in Theorem 1, with a comparison between differentiable functions and geodesic convexity. Regret bounds for convex objectives are obtained by bounding each g t , x t \u2212 x * in the Riemannian case. Regret bounds for convex objectives are usually obtained by bounding the difference between the current point and the optimal point using specific formulas. In the Riemannian case, this involves the term \u03c1 xt (g t , \u2212 log xt (x * )). Using a cosine law, the bound on this term simplifies, leading to convergence guarantees for optimization algorithms on Riemannian manifolds. In the Riemannian case, the bound on the difference between the current point and the optimal point simplifies using the cosine law. This leads to convergence guarantees for optimization algorithms on Riemannian manifolds, with a well-chosen decreasing schedule for the second term. In Riemannian manifolds, the second term requires a decreasing schedule for \u03b1, with a lemma that generalizes the step in all Alexandrov spaces. The curvature dependent quantity \u03b6 from the lemma helps bound \u03c1. Adaptivity benefits from these bounds, especially for sparse gradients. The curvature dependent quantity \u03b6 from the lemma helps bound \u03c1, improving bounds for sparse gradients. Algorithms embedding words in a manifold may benefit from adaptivity, with potential improvements in regret bounds by exploiting momentum/acceleration. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the n-dimensional Poincar\u00e9 model of hyperbolic geometry for better graph embedding. The algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically evaluated against the non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the n-dimensional Poincar\u00e9 model of hyperbolic geometry for improved graph embedding. The Poincar\u00e9 model of hyperbolic geometry is used to embed the WordNet noun hierarchy in an n-dimensional space with constant curvature. Optimization tools are explored for algorithms proposed in previous studies. Riemannian gradients are rescaled Euclidean gradients, and various mathematical operations are defined in this space. The transitive closure of the WordNet noun hierarchy consists of 82,115 nouns and 743,241 hypernymy Is-A relations. These words are embedded in an n-dimensional space with constant curvature. Various mathematical operations are defined in this space, including Riemannian gradients being rescaled Euclidean gradients. The transitive closure of the WordNet noun hierarchy includes 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in an n-dimensional space with constant curvature, and mathematical operations are defined in this space. The formula for parallel transport along a geodesic is derived from previous work, and a loss function similar to log-likelihood is minimized using negative word pairs. The text discusses embedding words in an n-dimensional space with constant curvature to minimize a loss function similar to log-likelihood using negative word pairs. The loss function does not consider the direction of the edges in the graph and includes metrics such as mean average precision (MAP) for ranking distances among ground truth negative examples. The loss function for embedding words in an n-dimensional space with constant curvature does not consider edge direction in the graph. It includes metrics like mean average precision (MAP) for ranking distances among ground truth negative examples. Training focuses on 5-dimensional hyperbolic spaces with a \"burn-in phase\" for 20 epochs. For link prediction in 5-dimensional hyperbolic spaces, a validation set of 2% edges is sampled from transitive closure edges. During the \"burn-in phase\" for 20 epochs, negative words are sampled based on graph degree raised at power 0.75, improving all metrics. After this phase, negatives are sampled uniformly. During the \"burn-in phase\" described in BID15, negative words are sampled based on graph degree raised at power 0.75 for 20 epochs with a fixed learning rate of 0.03 using RSGD with retraction. After this phase, optimization methods start, and negatives are sampled uniformly. RADAM showed slightly better results than RAMS-GRAD, and using the first-order approximation of the true exponential map led to convergence to lower loss values in both RSGD and adaptive methods. We use n = 5, following BID15. Experimentally, RADAM showed better results than RAMS-GRAD. Replacing the true exponential map with its first-order approximation led to lower loss values in both RSGD and adaptive methods. Retraction-based methods are reported separately as they require fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn. Results show that \"retraction\"-based methods are reported separately as they require fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn. The study includes results for \"exponential\" based and \"retraction\" based methods using different learning rates. Results show that \"exponential\" and \"retraction\" based methods were compared using different learning rates. The best settings for RADAM and RAMSGRAD were shown, while RADAGRAD was consistently worse. RADAM consistently achieved the lowest training loss. RADAM consistently achieves the lowest training loss and outperforms all other methods on the MAP metric for both reconstruction and link prediction settings in the full Riemannian setting. In the \"retraction\" setting, RADAM achieves the lowest training loss and is comparable to RSGD on the MAP evaluation for reconstruction and link prediction. RAMSGRAD converges faster for link prediction, indicating better generalization capability. Various first-order Riemannian methods have emerged after the introduction of Riemannian SGD by BID1. RAMSGRAD converges faster for link prediction, indicating better generalization capability compared to other Riemannian methods introduced after Riemannian SGD by BID1. The text discusses new methods for optimization such as Riemannian Stein variational gradient descent, accelerated gradient descent, and stochastic gradient Langevin dynamics. It also mentions the proposal of Riemannian counterparts of SGD with momentum and RMSprop, highlighting the idea of transporting the momentum term using parallel translation. However, no convergence guarantee is provided for these methods. The text discusses new optimization methods like Riemannian Stein variational gradient descent, accelerated gradient descent, and stochastic gradient Langevin dynamics. BID20 proposed Riemannian versions of SGD with momentum and RMSprop, suggesting transporting the momentum term using parallel translation. However, their algorithm lacks convergence guarantees and compromises convergence possibilities by performing coordinate-wise adaptive operations in the tangent space. Another version of Riemannian ADAM for the Grassmann manifold G(1, n) was introduced by BID3, which also transports the momentum term using parallel translation but removes the adaptive component. The text discusses the limitations of previous Riemannian optimization methods, such as the lack of convergence guarantees and removal of adaptivity in Riemannian ADAM for the Grassmann manifold G(1, n). The proposed approach aims to generalize popular adaptive optimization tools for non-Euclidean embeddings. The text proposes generalizing adaptive optimization tools for Cartesian products of Riemannian manifolds to address limitations in previous methods. It introduces convergence rates similar to Euclidean models and demonstrates superior performance in hyperbolic word taxonomy embedding tasks compared to non-adaptive methods like RSGD. The text introduces generalizing adaptive optimization tools for Cartesian products of Riemannian manifolds, showing convergence rates similar to Euclidean models. It demonstrates superior performance in hyperbolic word taxonomy embedding tasks compared to non-adaptive methods like RSGD. The text discusses the application of Cauchy-Schwarz' and Young's inequalities in geodesic convexity for optimization on Riemannian manifolds, demonstrating convergence rates in hyperbolic word embedding tasks. The text discusses the application of Cauchy-Schwarz' and Young's inequalities in geodesic convexity for optimization on Riemannian manifolds, demonstrating convergence rates in hyperbolic word embedding tasks. For 1 \u2264 t \u2264 T, the first term is analyzed using \u03b2 1t \u2264 \u03b2 1. A lemma is introduced to prove convergence of gradient-based optimization algorithms in Alexandrov spaces. The text introduces a lemma for proving convergence of optimization algorithms in Alexandrov spaces, based on geodesic convexity. It also presents an analogue of Cauchy-Schwarz inequality for real numbers. The lemma discusses geodesic triangles in Alexandrov spaces with curvature bounded by \u03ba, and introduces an analogue of Cauchy-Schwarz inequality for real numbers. This lemma is used in the convergence proof for the optimization algorithm ADAMNC."
}