{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"core\" features that remain consistent across domains and \"style\" features that can vary across domains. Style features may include attributes like position, rotation, image quality, brightness, hair color, or posture in images of persons. The distribution P(X^ci | Y) remains consistent across domains, while \"style\" features like position, rotation, image quality, and more can vary. To prevent adversarial domain shifts, it is ideal to use \"conditionally invariant\" features for classification. The domain itself is a latent variable, making it impossible to directly observe distributional changes across different domains. In data augmentation, images can be generated from an original image with an identifier variable. This method only requires a small fraction of images to have an ID variable. In data augmentation, images can be generated with an identifier variable, requiring only a small fraction of images to have an ID variable. A causal framework is provided by adding the ID variable to the model, treating the domain as a latent variable. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network using a grouping-by-ID approach improves performance in settings where domains change in terms of image quality, brightness, and color changes. Regularizing the network using a grouping-by-ID approach improves performance in settings where domains change in terms of image quality, brightness, color changes, movement, and posture. This approach penalizes the network with an appropriate graph Laplacian to provide consistent output across samples with the same ID. The method shows promise in improving interpretability, fairness, and transfer learning in deep neural networks. Deep neural networks have achieved outstanding performance on prediction tasks like visual object and speech recognition. Issues can arise when learned representations rely on dependencies that vanish in test distributions, leading to domain shifts that degrade predictive performance. Domain shifts can degrade predictive performance in machine learning systems when learned representations rely on dependencies that vanish in test distributions. For example, the \"Russian tank legend\" highlights how sampling biases in training data can lead to high accuracy in distinguishing between Russian and American tanks, but only because all images of Russian tanks were of poor quality. The \"Russian tank legend\" illustrates how sampling biases in training data can lead to high accuracy in distinguishing between Russian and American tanks, due to differences in image quality. This can result in indirect associations and impact the performance of machine learning systems. The system learned to discriminate between images of different qualities, such as Russian tanks with bad quality images and American tanks with good quality photos. Hidden confounding factors, like the association between image quality and tank origin, can affect machine learning performance. Deep learning requires large sample sizes to mitigate the impact of these confounding factors and achieve invariance to known factors like translation. Deep learning requires large sample sizes to mitigate confounding factors and achieve invariance to known factors like translation, point of view, and rotation through data augmentation. Adversarial examples, imperceptibly perturbed inputs misclassified by ML models, highlight the divergence between human and artificial cognition. Deep learning relies on large sample sizes to address confounding factors and achieve invariance to known factors like translation, point of view, and rotation through data augmentation. Adversarial examples, which are intentionally perturbed inputs misclassified by ML models, underscore the differences between human and artificial cognition. The question arises whether we can mimic human ability to learn desired invariances from a few instances of the same object and align the features DNNs use with human cognition. The text discusses the challenge of mimicking human ability to learn invariances from a few instances of an object and aligning deep neural networks with human cognition. It also mentions the importance of controlling biases in input data to prevent them from impacting decisions made by machine learning algorithms. Existing biases in datasets used for training ML algorithms tend to be replicated in the estimated models, leading to issues of fairness and discrimination in decision-making processes. For example, Google's photo app mistakenly tagged two non-white people as \"gorillas\" due to biased training data that mainly consisted of photos of white persons. This highlights the importance of controlling input data to ensure unbiased decision-making in machine learning algorithms. CORE is proposed as a solution to address biases in ML algorithms by controlling latent features extracted from input data, categorizing them into 'conditionally invariant' and 'orthogonal' features. CORE, a proposed solution to address biases in ML algorithms, introduces counterfactual regularization (CORE) to control latent features extracted from input data. It categorizes features into 'conditionally invariant' (core) and 'orthogonal' (style) categories, aiming for classifiers to focus only on core features related to the target of interest. This approach makes the estimator robust against adversarial domain shifts. CORE introduces counterfactual regularization to control latent features, categorizing them into 'conditionally invariant' (core) and 'orthogonal' (style) categories. The classifier focuses on core features related to the target of interest, making it robust against adversarial domain shifts. CORE exploits knowledge about grouping instances related to the same object. CORE relies on counterfactuals to address domain shifts by grouping instances related to the same object. It reduces the need for data augmentation and improves predictive performance in small sample size settings. The manuscript also introduces counterfactual regularization and the CORE estimator. The manuscript introduces counterfactual regularization and the CORE estimator, showcasing how CORE can enhance predictive performance in small sample size settings. It also evaluates CORE's performance in various experiments using the CelebA dataset, specifically for classifying whether a person wears glasses based on face images of celebrities. In \u00a75, the performance of CORE is evaluated in experiments using the CelebA dataset for classifying whether a person wears glasses based on face images of celebrities. The task involves grouping images of the same person and constraining the classification to yield the same prediction for all images belonging to the same person and sharing the same class label. Additional instances of the same person are considered as counterfactual observations. The text discusses grouping information in classification tasks, where images of the same person are constrained to have the same prediction. Additional instances of the same person are considered as counterfactual observations. The training set includes 10 identities with approximately 30 images each. Examples from the CelebA dataset and augmented MNIST are shown. The training set includes 10 identities with around 30 images each. Grouping-by-ID is used in the CelebA dataset and augmented MNIST. The comparison involves training the same network architecture without using grouping information but with a standard ridge penalty. The comparison involves training the same network architecture without using grouping information but with a standard ridge penalty. Exploiting the group structure reduces the average test error from 24.76% to 16.89%, i.e. by approx. 32%, compared to the estimator which just pools all images and uses a standard ridge. Exploiting grouping information reduces test error by 32% compared to pooling all samples. Test error on rotated digits is reduced by 50%. CORE makes data augmentation more efficient by generating additional samples through modifications like rotation, translation, or flipping images. CORE uses a standard ridge penalty for coefficients and makes data augmentation more efficient by generating additional samples through modifications like rotation, translation, or flipping images. This results in invariance of the estimator with respect to style features. Using augmented data sets for training in CORE enforces invariance with respect to style features, such as rotation on MNIST BID25. This approach includes 100 augmented training examples for 10,000 original samples, resulting in a total sample size of 10,100. Rotation degrees are randomly sampled. The use of augmented data sets in CORE enforces invariance with style features like rotation on MNIST BID25. Including 100 augmented training examples for 10,000 original samples, resulting in a total sample size of 10,100. Rotation degrees are randomly sampled, reducing average test error on rotated examples from 32.86% to 16.33%. The average test error on rotated examples is reduced from 32.86% to 16.33% by using CORE, which samples uniformly from [35, 70]. This approach requires grouped observations, unlike other works like BID14 and Domain-Adversarial Neural Networks (DANN). The main idea of BID13 is to learn a representation without discriminative information about the input's origin through adversarial training. The approach in BID13 is motivated by BID5 and involves learning a representation without discriminative information about the input's origin through adversarial training. Unlike BID14 and DANN, it requires grouped observations and relies on unlabeled data from the target task. The data generating process assumed in BID14 is similar to the model introduced. The approach in BID14 involves adversarial training to maximize domain classification loss and minimize target prediction task loss simultaneously. Unlike BID14, they do not assume data from different domains but different realizations of the same object under different interventions. BID14 identifies conditionally independent features by adjusting transformations to minimize MMD distance between distributions in different domains. The fundamental difference is the use of a different data basis. In BID14, conditionally independent features are identified by adjusting transformations to minimize MMD distance between distributions in different domains. The key difference from our approach is the explicit observability of the domain identifier in BID14, while it is latent in our method. We utilize an identifier variable to penalize the classifier for using latent features outside the set of conditionally independent features. Our approach differs from BID14 as we have a latent domain identifier. We use an identifier variable to penalize the classifier for utilizing latent features. Causal modeling in transfer learning aims to guard against domain shifts and interventions on predictor variables. Causal models in transfer learning aim to address adversarial domain shifts in image classification by ensuring valid predictions under interventions on predictor variables. The challenges lie in the anti-causal nature of the classification task and the need to protect against domain changes. The challenges in transferring causal models to adversarial domain changes in image classification include the anti-causal nature of the task and the need to guard against style feature shifts. Various approaches leveraging causal motivations in deep learning have been proposed to address these challenges. The challenges in transferring causal models to adversarial domain changes in image classification include guarding against style feature shifts. Various approaches leveraging causal motivations in deep learning have been proposed to address these challenges, focusing on cause-effect inference. Various approaches in deep learning focus on cause-effect inference, aiming to find causal relations between random variables X and Y. The Neural Causation Coefficient (NCC) is proposed to estimate the probability of X causing Y, distinguishing between object features and their contexts. The Neural Causation Coefficient (NCC) is used to estimate the causal relation between random variables X and Y by distinguishing object features from their contexts. Generative neural networks are employed for cause-effect inference and to orient the edges of a given graph skeleton. Generative neural networks are used for cause-effect inference and to orient graph edges. Bahadori et al. (2017) devise a regularizer combining penalty with weights based on estimated causal probabilities. Besserve et al. (2017) connect GANs with causal contexts. Bahadori et al. (2017) propose a regularizer combining penalty with weights based on estimated causal probabilities for graph edge orientation. Besserve et al. (2017) link GANs with causal generative models using a group theoretic framework. Kocaoglu et al. (2017) suggest causal implicit generative models for sampling from conditional and interventional distributions using a Conditional GAN architecture. The generator structure must align with the neural connections from the causal graph. BID29 propose using deep latent variable models and proxy variables to estimate individual treatment effects, while BID21 use causal reasoning to address fairness in machine learning by deriving causal nondiscrimination criteria. The causal graph structure must be known for BID29 to use deep latent variable models and proxy variables to estimate individual treatment effects. BID21 exploit causal reasoning to address fairness in machine learning by deriving causal nondiscrimination criteria, requiring classifiers to be constant as a function of proxy variables in the causal graph. This approach bears structural similarity to disentangling core and style features. The algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, similar to disentangling core and style features. Estimating disentangled factors of variation has garnered interest in generative modeling. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce the dependence of the latent representation on a specified transform of the object in the original image. Estimating disentangled factors of variation in generative modeling has gained interest. Matsuo et al. (2017) introduced a \"Transform Invariant Autoencoder\" to reduce dependence on specific object transforms in the latent representation. The goal is to learn a representation that excludes certain features, such as location, image quality, posture, brightness, background, and contextual information. The goal of Matsuo et al. (2017) is to learn a latent representation that excludes specific features like location, image quality, posture, brightness, background, and contextual information. The approach involves a confounding situation with unobserved domains and the use of an ID variable for grouping. In Matsuo et al. (2017), the approach involves a confounding situation with unobserved domains and the use of an ID variable for grouping to separate style and content in a variational autoencoder framework. In a confounding situation, the distribution of style features varies based on class, similar to the approach in Bouchacourt et al. (2017) where grouped observations are exploited in a variational autoencoder framework to separate style and content. The goal is to solve a classification task directly without explicitly estimating latent factors, unlike in a generative framework. A causal graph is developed to compare the setting. In a classification task, samples within a group share a common unknown value for a factor of variation. The focus is on solving the task directly without estimating latent factors explicitly. A causal graph is used to compare different scenarios like adversarial domain shifts, transfer learning, domain adaptation, and adversarial examples. The target of interest is denoted as Y, typically representing regression or classification with K classes. The predictor X is described as the p pixels of an image, with the prediction y being of the form f \u03b8 (x). The causal graph compares adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. Y represents the target of interest, while X is the predictor (e.g., pixels of an image). The prediction y is generated by a function f \u03b8 with parameters \u03b8, corresponding to weights in a DNN. The goal is to minimize the suitable loss between y and \u0177 = f \u03b8 (x). The parameters \u03b8 correspond to the weights in a DNN for regression or classification tasks. The goal is to minimize the expected loss by choosing the weights that minimize the empirical loss. The standard approach to parameter estimation in deep neural networks involves penalized empirical risk minimization to minimize expected loss. The penalty can be a ridge penalty or exploit underlying geometries like Laplacian regularized least squares. The full structural model for all variables is illustrated in a diagram. The domain variable D is latent, while the ID variable is added for identification purposes. The structural model includes a latent domain variable D and an added ID variable for grouping observations. The prediction is anti-causal, using non-ancestral predictors X for \u0176. The ID variable is added for grouping observations in the structural model. The prediction is anti-causal, using non-ancestral predictors X for \u0176. The causal effect from class label Y on image X is mediated via core features X ci and orthogonal style features X \u22a5. The class label Y is causal for the image X, with the causal effect mediated by core features X ci and style features X \u22a5. Interventions are possible on style features but not on core features, and the distribution of style features can change across domains due to confounding with latent domain D. The core features X ci and style features X \u22a5 of the image X are influenced by the causal class label Y. External interventions are possible on style features but not on core features. The distribution of style features can vary across domains due to confounding with latent domain D. The style variable includes factors like point of view, image quality, resolution, rotations, color changes, body posture, and movement, which are context-dependent. The style variable in images includes factors like point of view, image quality, resolution, rotations, color changes, body posture, and movement, which are context-dependent. The style intervention variable influences both the latent style and the image. The style intervention variable influences both the latent style and the image, with a focus on guarding against adversarial domain shifts using causal graph and potential outcome notation. The text discusses using a causal graph to explain domain adaptation, transfer learning, and guarding against adversarial examples. It focuses on the intervention variable's impact on latent style and image, with an emphasis on guarding against adversarial domain shifts. The intervention magnitude is assumed to be within a certain norm around the origin. The text discusses using a causal graph to explain domain adaptation, transfer learning, and guarding against adversarial examples. It focuses on minimizing adversarial loss by devising a classification that considers imperceptible changes in input dimensions leading to misclassification. The goal is to devise a classification that minimizes the adversarial loss. The text discusses minimizing adversarial loss by devising a classification that considers imperceptible changes in input dimensions leading to misclassification. It focuses on adversarial domain shifts with arbitrarily strong interventions on style features. The text discusses adversarial domain shifts with strong interventions on style features, aiming to minimize adversarial loss by considering imperceptible changes in input dimensions. The interventions can be powerful, but certain aspects of the image remain unchanged. Loss under large style interventions is minimized by considering imperceptible changes in input dimensions. Adversarial interventions on style features aim to protect against shifts in test data distribution, distinguishing between core and style features. The motivation behind BID13 is similar to adversarial interventions on style features in domain adversarial neural networks, aiming to protect against shifts in test data distribution by distinguishing between core and style features. Causal inference faces the challenge of never being able to observe a counterfactual, such as observing both health outcomes simultaneously when changing treatment. The classical problem of causal inference is the inability to observe a counterfactual, where we can only see the health outcome under treatment or no treatment but not both simultaneously. This makes observing counterfactuals impossible in general. Observing counterfactuals is generally impossible in causal inference, as we can only observe the outcome under treatment or no treatment but not both simultaneously. Counterfactuals involve keeping class label Y and ID constant while allowing the style intervention \u2206 to change, which could be a do-intervention or a noise-intervention. The treatment effect can be determined by comparing the health outcome under different treatments. In causal inference, counterfactuals involve keeping class label Y and ID constant while allowing the style intervention \u2206 to change. This intervention plays a similar role to treatment T in medical examples, allowing for the observation of the same object under different conditions. For example, in image analysis, we can see the same object (Y, ID) under different 'treatments' \u2206, such as wearing glasses or not. The style intervention \u2206 in image analysis allows for observing the same object under different conditions, similar to a treatment in medical examples. Counterfactuals are conceivable, where \u2206 represents variables determining different images of the same person, including background, posture, and image quality. Unlike in the medical setting, the focus is not on the 'treatment effect' of the style intervention \u2206. The style intervention \u2206 in image analysis involves observing the same object under different conditions, similar to a treatment in medical examples. It represents variables determining different images of the same person, including background, posture, and image quality. Unlike in the medical setting, the focus is not on the 'treatment effect' of \u2206, but rather on penalizing any change in classification under different style interventions. The text discusses the use of style intervention \u2206 in image analysis to rule out parts of the feature space for classification. It aims to penalize any change in classification under different style interventions while keeping class and identity constant. The text discusses classification under different style interventions while maintaining constant class and identity. It introduces the standard approach of pooling over all observations and using a pooled estimator with a ridge penalty. The standard approach is to pool over all observations and use a pooled estimator with a ridge penalty, treating all examples identically. The pooled estimator is always the ridge estimator with a cross-validated penalty parameter choice, which works well in terms of adversarial loss. The pooled estimator with a ridge penalty is always chosen with a cross-validated penalty parameter. The adversarial loss may be infinite, but the estimator performs well in terms of adversarial loss. Conditions (i) and (ii) ensure that X ci and X \u22a5 are not deterministic in relation to Y, preventing X \u22a5 from replacing X ci. The pooled estimator learns to extract X ci from the image X, with no further information explaining Y. The relations between Y, X ci, and X \u22a5 are non-deterministic, ensuring X \u22a5 cannot replace X ci. To minimize adversarial loss, f \u03b8 (x(\u2206)) must be constant for all x \u2208 R p. The pooled estimator works well in minimizing adversarial loss if certain edges are absent. To achieve this, f \u03b8 (x(\u2206)) must be constant for all x \u2208 R p in the invariant parameter space I. The invariant parameter space I is defined as a function of core features x ci in R p. The adversarial loss under interventions is the same as without interventions for all \u03b8 \u2208 I. The optimal predictor in I is based on core features X ci, which are not directly observable. Inference of I is done through empirical risk minimization to approximate the optimal invariant parameter vector. The optimal predictor in the invariant space I is determined by core features X ci, which are not directly observable. Inference of the unknown invariant parameters space I is approximated through empirical risk minimization. The unknown invariant parameters space I is approximated by an empirically invariant space I n through empirical risk minimization, with a regularization constant \u03c4 \u2265 0. The true invariant space I is a subset of the empirically invariant subspace I n. The regularization constant \u03c4 \u2265 0 is used to approximate the unknown invariant parameters space I with an empirically invariant space I n. The true invariant space I is a subset of I n, and under certain assumptions, I n converges to I for n \u2192 \u221e. The Lagrangian form of constrained optimization can be used with a penalty parameter \u03bb instead of \u03c4. The matrix L ID is a graph Laplacian BID4, with n connectivity components. Samples with the same ID are connected, forming fully connected components. Graph Laplacian regularization penalizes the sum of variances \u03c3 2 i (\u03b8). The graph is induced by the identifier variable ID in the sample space. The graph Laplacian regularization penalizes the sum of variances \u03c32i(\u03b8) in the sample space induced by the identifier variable ID. The outcome is not strongly dependent on the penalty value \u03bb, and defining the graph in terms of the identifier is crucial. The outcome is not strongly dependent on the penalty value \u03bb, and defining the graph in terms of the identifier variable ID is crucial for guarding against adversarial domain shifts. Other regularizations do not perform as well in this scenario. The graph is defined in terms of the identifier variable ID to guard against adversarial domain shifts. The adversarial loss is analyzed for the pooled and CORE estimator in a one-layer network for binary classification. The structural equation for the image X is linear in the style features X\u22a5, interventions are additive, and logistic regression is used to predict a class label Y. The pooled estimator has infinite adversarial loss under suitable assumptions. In experiments, the CORE estimator shows superior performance in handling confounded training data and changing style features in test distributions, with controlled levels of confounding. The CORE estimator demonstrates superior performance in handling confounded training data and changing style features in test distributions, with controlled levels of confounding. Additional experiments include classifying elephants and horses based on color, gender, wearing glasses, and brightness. In \u00a75.3, elephants and horses are classified based on color. Additional experiments in \u00a7B involve gender, wearing glasses, and brightness. Experimental results for the settings in \u00a72 can be found in \u00a7C.2 and \u00a7C.3. TensorFlow BID0 implementation of CORE will be available, along with code for reproducing experiments. The value of tuning parameter \u03c4 or penalty \u03bb in Lagrangian form is an open question. Performance is discussed in \u00a7C.1. Architecture details are in \u00a7C.7. In addition to the details provided, an open question remains on setting the tuning parameter \u03c4 or penalty \u03bb in Lagrangian form. Performance is not very sensitive to the choice of \u03bb as shown in \u00a7C.1. The example involves synthetically generated stickmen images with the target of interest being Y \u2208 {adult, child} and X ci \u2261 height, which is considered a core feature for differentiation. In this example, synthetically generated stickmen images are used to differentiate between adults and children based on height, which is a robust predictor. There is a dependence between age and movement in the dataset due to a hidden common cause, the place of observation. A robust predictor for differentiating between children and adults is illustrated in the data generating process. The training dataset shows a dependence between age and movement, which arises from a hidden common cause. The model may fail when presented with images that do not follow the learned patterns. Examples from the training set show that large movements are associated with children and small movements with adults. Test set 1 follows the same distribution. The training dataset shows a dependence between age and movement, with large movements associated with children and small movements with adults. Test set 1 follows the same distribution. Test sets 2 and 3 intervene on the dependence between Y and X, leading to misclassification in predicting age based on movement. In test sets 2 and 3, X \u22a5 is intervened on, removing the edge from D to X \u22a5 and causing the dependence between Y and X \u22a5 to vanish. Large movements are associated with both children and adults in test sets 2 and 3, with heavier movements in test set 3. Misclassification rates for CORE and the pooled estimator for c = 50 with a total sample size of m = 20000 are shown in FIG0. CORE achieves good predictive performance on test sets 2 and 3 with as few as 50 counterfactual observations, where the pooled estimator fails. The pooled estimator fails to achieve good predictive performance on test sets 2 and 3, with misclassification rates exceeding 40%. CORE, on the other hand, succeeds with as few as 50 counterfactual observations, indicating a difference in predictive features used by the two methods. Including more counterfactual examples would not improve the performance of the pooled estimator due to bias issues. The pooled estimator uses movement as a predictor for age, while CORE does not due to counterfactual regularization. Including more counterfactual examples would not improve the pooled estimator's performance. The CelebA dataset is used to classify whether a person in an image is wearing eyeglasses. The CelebA dataset is used to classify whether a person in an image is wearing eyeglasses. The image quality differs based on whether the person is wearing glasses, mimicking confounding similar to the Russian tank legend. The strength of the image quality intervention is controlled by sampling new image quality from a Gaussian distribution. Images of people without glasses remain unchanged. The image quality intervention in the study is controlled by sampling new image quality from a Gaussian distribution. Counterfactual observations are only available for images of people wearing glasses, with the same image used but with a newly sampled image quality value. This is referred to as \"CF setting 1\". Counterfactual observations for images without glasses are not available. The study uses the same image with a newly sampled image quality value for people wearing glasses, known as \"CF setting 1\". Misclassification rates for CORE and the pooled estimator on different test sets are shown in FIG0. Test set 1 follows the same distribution as the training set, while test set 2 reverses the class of the quality intervention. In the study, counterfactual observations are discussed for images without glasses. Misclassification rates for CORE and the pooled estimator on different test sets are compared in FIG0. Test sets vary in quality interventions, with the pooled estimator performing better on test set 1. The quality intervention affects image quality, with images of people without glasses showing lower quality. The pooled estimator outperforms CORE on test set 1 by utilizing predictive information from image quality. However, the pooled estimator struggles on test sets 2-4, as it uses image quality as a predictor, unlike CORE which is less affected by changing image quality distributions. The pooled estimator performs well on test set 1 by using image quality as a predictor, but struggles on test sets 2-4. In contrast, CORE is less affected by changing image quality distributions. Experimental details and results for quality interventions are provided in \u00a7C.5. In the context of assessing invariance with respect to color, the study aims to determine if CORE can exclude color from its learned representation by including counterfactuals of different colors. The experiment is conducted using the \"Animals with attributes 2\" (AwA2) dataset. The study aims to assess if CORE can exclude color from its learned representation by including counterfactuals of different colors. Using the \"Animals with attributes 2\" (AwA2) dataset, grayscale images are added as counterfactual examples for elephants to test this concept. The study uses the AwA2 dataset to classify images of horses and elephants. Counterfactual examples are included by adding grayscale images for elephants. The total sample size is 1850, with misclassification rates compared for different test sets. Test set 1 contains original colored images, while test set 2 has grayscale horse images and modified colorspace for elephant images. The study uses the AwA2 dataset with a total sample size of 1850. Test sets include original colored images, grayscale horse images, modified colorspace for elephant images, and grayscale images only. The pooled estimator performs poorly on test sets 2 and 3. The colorspace of elephant images is altered, changing gray to red-brown in test set 4. The pooled estimator struggles on test sets 2 and 3 due to exploiting the predictive nature of \"gray,\" while CORE's performance remains stable. Adding grayscale elephant examples helps in recognizing colored elephants. The predictive accuracy of the pooled estimator is influenced by the color \"gray\" in the training set, while the CORE estimator remains stable regardless of color distributions. Adding grayscale elephant examples aids in recognizing colored elephants, showcasing the importance of color invariance in predictions. The CORE estimator emphasizes invariance of predictions for instances of the same elephant, allowing for color invariance with added grayscale images. This approach could satisfy fairness by not including \"color\" as a learned representation, unlike the pooled estimator which is influenced by color distributions in the training set. The proposed counterfactual regularization (CORE) aims to achieve robustness by distinguishing core and style features in images, ensuring fairness by not including \"color\" as a learned representation. The CORE estimator exploits the invariance of predictions for the same instance, unlike the pooled estimator influenced by color distributions. The CORE estimator achieves robustness by distinguishing core and style features in images, ensuring invariance to interventions on style features like color. It exploits the invariance of predictions for instances of the same object in training data. The CORE estimator achieves invariance by focusing on core features and exploiting the consistency of predictions for instances of the same object in the training data. This approach ensures robustness to adversarial interventions on style features like image quality, fashion type, color, or body posture, even in the presence of sampling biases. It can achieve similar classification performance as standard data augmentation methods with fewer instances that do not need to be carefully balanced in the training data. The CORE estimator focuses on core features and exploits consistency in predictions for the same object, ensuring robustness to style features like image quality, fashion type, color, or body posture. It can achieve similar classification performance as standard data augmentation methods with fewer instances, even in the presence of sampling biases. Regularization of CORE penalizes features that vary strongly between instances of the same object, automatically avoiding the usage of unknown style features. Further research could explore the use of larger models such as Inception or large ResNet architectures. The CORE estimator focuses on core features and ensures robustness to style features by penalizing strong variations between instances of the same object. Further research could explore the benefits of using larger models like Inception or ResNet architectures. The CORE estimator focuses on core features and penalizes variations in style features. Research aims to assess benefits of using Inception-style models for training, potentially leveraging video data for grouping and regularization. Using video data for grouping and regularization in training Inception-style models could improve sample efficiency and generalization performance. Future work could explore leveraging temporal information for grouping objects and debiasing word embeddings. The text discusses using counterfactual regularization to debias word embeddings and predict class labels using logistic regression on image style features. It introduces interventions acting additively on style features and linear relationships between style features and images. The core features are conditionally invariant, with potential implications for various error terms. The text introduces core features X ci that are conditionally invariant and assumes logistic regression for predicting Y from image data X. The variables Y, X, and ID are observed, while D, X ci, \u2206, X \u22a5, and noise variables are latent. The text discusses logistic regression for predicting Y from image data X, with some variables observed and others latent. It includes estimating \u03b8 with training data and using logistic loss for training and testing, with expected losses on test data. The text discusses logistic regression for predicting Y from image data X, estimating \u03b8 with training data, and using logistic loss for training and testing. Expected losses on test data include standard logistic loss and loss under adversarial interventions on style or domain variables. The formulation of Theorem 1 relies on specific assumptions. The second loss in logistic regression involves adversarial interventions on style or domain variables, with specific assumptions required for the formulation of Theorem 1. These assumptions include conditions on the training data distribution, the rank of the matrix W, and the number of counterfactual examples in the samples. Assuming certain conditions hold, such as the distribution of \u2206 for training data, the full rank of matrix W, and the number of counterfactual examples in the samples, the sampling process involves collecting independent samples and selecting a subset for further analysis. The sampling process involves collecting independent samples and selecting a subset for further analysis, leading to a total of m samples with n distinct values. The sampling process involves collecting independent samples to create m samples with n distinct values. Theorem 1 states that under Assumption 1, the pooled estimator has infinite adversarial loss with probability 1. An equivalent result holds for misclassification loss. Under Assumption 1, the pooled estimator has infinite adversarial loss with probability 1. An equivalent result holds for misclassification loss. The proof involves showing that W t\u03b8pool = 0 with probability 1 by assuming \u03b8 * is orthogonal to the column space of W. The proof involves showing that W t\u03b8pool = 0 with probability 1 by assuming \u03b8 * is orthogonal to the column space of W. This is sufficient as it leads to a contradiction, implying that taking the directional derivative of the training loss with respect to any \u03b4 \u2208 R p in the column space of W should vanish at the solution \u03b8 * . Assuming W t\u03b8pool = 0, the constraint W t \u03b8 = 0 becomes non-active, implying \u03b8 pool = \u03b8 *. The directional derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8 *. The counterfactual training data x i,j (0) is unaffected by interventions (\u2206 i,j = 0). The derivative of the training loss in direction of \u03b4 is proportional to x i,j \u2208 R p. The counterfactual training data x i,j (0) is unaffected by interventions. The oracle estimator \u03b8 * remains the same under true training data and counterfactual training data x(0). The derivative g(\u03b4) can be written as the difference between two formulas. The interventions only affect the column space of W in X, the oracle estimator \u03b8* remains the same under true and counterfactual training data. The derivative g(\u03b4) can be expressed as a difference between two formulas, and can also be written as \u03b4 = W u. The interventions do not affect the oracle estimator \u03b8* under true or counterfactual training data. The eigenvalues of W t W are all positive, and the interventions \u2206 i,j are drawn from a continuous distribution. The left hand side of the equation has a continuous distribution. The interventions \u2206 i,j do not affect the oracle estimator \u03b8* under true or counterfactual training data. The left hand side of the equation has a continuous distribution, and the probability of it not being identically 0 is 1. With probability 1, \u03b8 core = \u03b8* as defined in (6). The invariant space for this model is the linear subspace I = {\u03b8 : W t \u03b8 = 0}. The proof of the first part is completed by contradiction, and with probability 1, \u03b8 core = \u03b8* as defined in (6). The linear subspace I = {\u03b8 : W t \u03b8 = 0} is the invariant space for this model. By (A2) and (A3), with probability 1, I n = {\u03b8 : W t \u03b8 = 0}. Hence, \u03b8 core = \u03b8* with probability 1. The linear subspace I = {\u03b8 : W t \u03b8 = 0} is the invariant space for the model. With probability 1, \u03b8 core = \u03b8* and the estimator remains unchanged using data without interventions as training data. The population-optimal vector can be written as \u03b8* = \u03b8 core. The linear subspace I = {\u03b8 : W t \u03b8 = 0} is the invariant space for the model. The estimator remains unchanged using data without interventions as training data. The population-optimal vector can be written as \u03b8* = \u03b8 core. Comparing formulas by uniform convergence, we have c = m - n samples redrawn from the empirical sample at random. In the CelebA dataset, a confounding problem arises when classifying gender based on images, with a bias towards men wearing glasses. The proof involves uniform convergence of samples and the definition of I and \u03b8. In the CelebA dataset, a confounding problem arises when classifying gender based on images, with a bias towards men wearing glasses. The proof involves uniform convergence of samples and the definition of I and \u03b8. Using counterfactuals, images of the same person without glasses for males and with glasses for females are used in \"CF setting 2\". Test set examples are shown in FIG0.2. In the CelebA dataset, a bias exists towards men wearing glasses when classifying gender based on images. Counterfactuals are used, with images of the same person without glasses for males and with glasses for females in \"CF setting 2\". Test set examples are provided in FIG0.2, with different distributions between gender and glasses in test set 2. The comparison is made between training a four-layer CNN end-to-end and using Inception V3 features. In test set 2, the association between gender and glasses is flipped. The comparison is between training a four-layer CNN end-to-end and using Inception V3 features. Results show similar trends with increasing c values. In test set 2, the association between gender and glasses is flipped. Results show similar trends with increasing c values when training a four-layer CNN end-to-end and using Inception V3 features. The performance difference between CORE and the pooled estimator becomes smaller as c increases. The pooled estimator performs worse on test set 2 as m becomes larger. In a confounded setting with the CelebA dataset, the pooled estimator performs worse on test set 2 as m increases, exploiting the independence of X \u22a5. The scenario involves classifying eyeglasses wearing based on a hidden common cause D indicating indoor or outdoor image settings. The analysis focuses on classifying eyeglasses wearing in images based on a hidden common cause of indoor or outdoor settings affecting brightness. Test set 1 matches the training set distribution, while test set 2 shows differences in brightness intervention. The analysis compares eyeglasses wearing in images based on indoor or outdoor settings affecting brightness. Test sets vary in brightness interventions, with the pooled estimator outperforming CORE on test set 1. The pooled estimator outperforms CORE on test set 1 by utilizing brightness information in images, but struggles on test sets 2 and 4 where brightness is manipulated. The pooled estimator utilizes brightness information in images to outperform CORE on test set 1, but struggles on test sets 2 and 4 where brightness is manipulated. The predictive performance of CORE is hardly affected by changing brightness distributions. Results for different parameters can be found in FIG0.5. The predictive performance of CORE is hardly affected by changing brightness distributions. Different image interventions for counterfactual observations are evaluated, with examples shown in FIG0.4. Counterfactual settings are evaluated by sampling different brightness interventions. \"CF setting 2\" uses a different image of the same person, while \"CF setting 3\" uses an image of a different person as a baseline. Results for all settings can be found in FIG0.5, with setting 1 working best as it controls only brightness variation between examples. In counterfactual settings, different images of the same person can vary in many factors, making it challenging to isolate brightness as the invariant factor. Grouping images of different persons can still improve predictive performance to some extent. An open question remains on how to set the tuning parameter \u03c4 or penalty \u03bb. FIG0.6 provides further insights. In counterfactual settings, images of the same person can vary in many factors, making it challenging to isolate brightness as the invariant factor. Grouping images of different persons can still improve predictive performance. An open question remains on setting the tuning parameter \u03c4 or penalty \u03bb. Performance is not very sensitive to the choice of \u03bb, as shown in the experiment results. In the experiment, the misclassification rates of CORE on the AwA2 dataset are shown to be not very sensitive to the penalty \u03bb. The training data set includes varying numbers of identities, resulting in different sample sizes and average number of counterfactual observations per person. CORE improves predictive performance compared to other estimators on the test set. The study involved using different data set sizes (n = 10, 20, 40, 80, 160) with varying sample sizes (ranging from 321 to 4386) and counterfactual observations per person. CORE showed improved predictive performance compared to pooling all images, especially with small sample sizes. As sample sizes increased, the performance of CORE and the pooled estimator became comparable. The study demonstrated that CORE outperformed the pooled estimator, particularly with small sample sizes. As sample sizes increased, the performance of CORE and the pooled estimator became similar. Further experiments varied the number of augmented training examples and rotations, showing misclassification rates on the test set. In further experiments, the number of augmented training examples and rotations were varied, showing misclassification rates on test sets with rotated digits and the usual MNIST test set. CORE consistently had lower misclassification rates on the rotated test set, indicating more efficient data augmentation. It even improved performance on the usual test set for n = 1000. The misclassification rates of CORE are lower on test set 1, indicating more efficient data augmentation. Even for n = 1000, it improves performance on test set 2. Results for different numbers of counterfactual examples show that CORE's performance is not sensitive to the number of examples. Results for the experiment in \u00a75.1 show that the CORE estimator's performance is not sensitive to the number of counterfactual examples, as long as there are enough in the training set. The pooled estimator struggles with predictive performance on test sets 2 and 3. Experiments were conducted for counterfactual settings 1-3 and for c = 5000. The pooled estimator's predictive performance on test sets 2 and 3 is poor, using \"movement\" as a predictor for \"age\". Experiments were done for counterfactual settings 1-3 and for c = 5000, showing differences in predictive performance. There is a significant performance gap between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator. The misclassification rates for different counterfactual settings were compared on test sets, showing setting 1 as the best. There were small differences between settings 2 and 3, with a notable performance gap between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator. Image color rotations were applied to all images, and models were implemented in TensorFlow. The image quality with \u00b5 = 50 may not be predictive for the target. The colors of all images were rotated cyclically, and in test set 3, images were changed to grayscale. Models were implemented in TensorFlow, with detailed architectures in TAB1.1. CORE and the pooled estimator used the same network architecture and training procedure, differing only in the loss function. Adam optimizer was used in all experiments, with results based on training each model five times to assess variance. The model architectures detailed in TAB1.1 CORE and the pooled estimator use the same network architecture and training procedure, with differences in the loss function. Adam optimizer is used in all experiments, with results based on training each model five times to assess variance. Training data is shuffled in each epoch, with mini batch size set to 120. In each training epoch, the data is shuffled to ensure mini batches contain counterfactual observations, with a mini batch size of 120. This setup makes optimization more challenging, especially for small c values."
}