{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions efficiently. However, NPs suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data they condition on. This issue is addressed by incorporating attention. Neural Processes (NPs) can learn a wide range of conditional distributions but suffer from underfitting, resulting in inaccurate predictions at observed data inputs. To address this, attention is incorporated into NPs, allowing for improved accuracy, faster training, and expanded function modeling capabilities. Incorporating attention into Neural Processes (NPs) improves prediction accuracy, speeds up training, and expands function modeling capabilities. Regression tasks involve modeling the distribution of output given input, typically using a neural network. This approach trains the model on input-output pairs, with predictions being independent of each other given the inputs. Another regression approach involves using training data to compute. Incorporating attention into Neural Processes (NPs) enhances prediction accuracy, accelerates training, and broadens function modeling capabilities. Regression tasks entail modeling the distribution of output given input, often utilizing a neural network. This method trains the model on input-output pairs, with predictions being independent of each other given the inputs. Another regression approach involves computing a distribution over functions using training data. Bayesian machine learning involves using training data to compute a distribution over functions for making predictions on test inputs. Gaussian Processes and Neural Processes offer efficient methods for modeling regression functions. Neural Processes can predict the distribution of target outputs based on a set of context. Neural Processes (NPs) offer an efficient method for modeling a distribution over regression functions, with prediction complexity linear in the context set size. They can predict the distribution of an arbitrary target output based on a set of context input-output pairs of any size, allowing them to model data generated from a stochastic process. NPs have different training regimes compared to Gaussian Processes. Neural Processes (NPs) can model data generated from a stochastic process by predicting an arbitrary target output based on context input-output pairs. Unlike Gaussian Processes, NPs are trained on samples from multiple realizations of a stochastic process, leading to different training regimes. However, NPs tend to underfit the context set, which is a significant weakness. Neural Processes (NPs) are trained on multiple realizations of a stochastic process, unlike Gaussian Processes. However, NPs tend to underfit the context set, leading to inaccurate predictions and overestimated variances. Neural Processes (NPs) underfit the context set, resulting in inaccurate predictions and overestimated variances. The encoder aggregates the context set to a fixed-length latent summary, leading to underfitting behavior due to a bottleneck in the mean-aggregation step. The encoder in Neural Processes aggregates context sets to a fixed-length latent summary, causing underfitting due to a bottleneck in the mean-aggregation step. Increasing dimensionality may not effectively address this issue. The encoder in Neural Processes acts as a bottleneck by giving equal weight to all context points, making it hard for the decoder to learn relevant information. To address this, inspiration is drawn from GPs, where the kernel shows which context points are relevant for a given query. In Neural Processes, the encoder gives equal weight to all context points, making it difficult for the decoder to learn relevant information. To address this, inspiration is taken from Gaussian Processes (GPs), where the kernel identifies relevant context points for a given query. This mechanism is implemented in NPs using differentiable attention to attend to relevant contexts while maintaining permutation invariance. In Attentive Neural Processes (ANPs), differentiable attention is used to attend to relevant contexts for a given target, preserving permutation invariance. ANPs outperform Neural Processes in 1D and 2D regression tasks, showing improved context reconstruction and faster training. Enhanced expressiveness is also demonstrated compared to NPs. Attentive Neural Processes (ANPs) improve upon Neural Processes in regression tasks by enhancing expressiveness, context reconstruction, and training speed. ANPs use differentiable attention to model a wider range of functions by conditioning on observed contexts. The Neural Process (NP) is a model for regression functions that can map inputs to outputs. It defines a family of conditional distributions that can model multiple targets and contexts. The deterministic NP models these distributions using a function r. The Neural Process (NP) uses (x i , y i ) i\u2208C to model targets (x T , y T ) := (x i , y i ) i\u2208T with permutation invariance. The model defines conditional distributions using a deterministic function r that aggregates context pairs (x C , y C ) into a finite dimensional representation. The likelihood p(y T |x T , r C ) is modelled by a Gaussian factorised across the targets. The Neural Process (NP) uses a deterministic function to aggregate context pairs into a finite dimensional representation with permutation invariance. The likelihood of the model is modelled by a Gaussian factorised across the targets, with mean and variance determined by passing inputs through an MLP. The NP model includes a global latent variable to address uncertainty in predictions. The Neural Process (NP) model incorporates a global latent variable z to account for uncertainty in predictions. This latent variable is modelled by a factorised Gaussian parametrised by s, with q(z|s \u2205 ) := p(z) as the prior. The likelihood is referred to as the decoder in the model. The Neural Process model includes a global latent variable z, modelled by a factorised Gaussian parametrised by s. The latent path complements the deterministic path in the model, with q(z|s \u2205 ) := p(z) as the prior. The decoder is the likelihood, and q, r, s form the encoder. The global latent is used to model different realisations of the data generating stochastic process. The decoder, q, r, s form the encoder in the Neural Process model. A global latent variable is used to model different realisations of the data generating stochastic process. The parameters of the encoder and decoder are learned by maximizing the ELBO for a random subset of contexts and targets. In this work, both deterministic and latent paths are explored in the Neural Process model. The encoder and decoder parameters are learned by maximizing the ELBO for a subset of contexts and targets, encouraging the summaries to be close. The Neural Process model learns to reconstruct targets from contexts, regularized by a KL term. The number of contexts and targets are randomly chosen at each iteration for scalability. Neural Processes (NPs) learn from data-generating processes, with random selection of contexts and targets for scalability. NPs offer scalability, flexibility, and permutation invariance in predicting targets from contexts. Neural Processes (NPs) offer scalability, flexibility, and permutation invariance in predicting targets from contexts. However, they do not satisfy consistency in the contexts, as the distribution of targets may vary depending on the order in which they are generated. Neural Processes (NPs) provide scalability, flexibility, and permutation invariance in predicting targets from contexts. However, they lack consistency in the contexts, as the distribution of targets may vary based on the order of generation. Maximum-likelihood learning aims to minimize the KL divergence between the conditional distributions of the data-generating process and the NP, approximating the conditionals of the consistent data-generating process. Maximum-likelihood learning minimizes KL divergence between conditional distributions of data-generating process and Neural Processes (NPs), approximating consistent data-generating process. Attention mechanism computes weights for key-value pairs to form query values, with permutation invariance. The attention mechanism computes weights for key-value pairs to form query values, with permutation invariance. This differentiable addressing mechanism has been successfully applied in various areas of Deep Learning. The attention mechanism computes weights for key-value pairs to form query values, with permutation invariance. Differentiable addressing mechanisms have been successfully applied in Deep Learning, including handwriting generation, neural machine translation, natural language processing, and image modeling. Examples of attention mechanisms used in the paper include matrices K, V, and Q for key-value pairs and queries. In the paper, attention mechanisms are used to compute weights for key-value pairs to form query values. Differentiable addressing mechanisms have been applied in various Deep Learning tasks such as handwriting generation, neural machine translation, natural language processing, and image modeling. Examples of attention mechanisms include matrices K, V, and Q for key-value pairs and queries. Attention mechanisms use matrices K, V, and Q for key-value pairs and queries. Different forms of attention, such as the Laplace kernel and dot-product attention, weight keys based on distance or similarity to the query. Multihead attention extends this concept by parametrising keys, values, and queries for each head. The multihead attention mechanism extends the concept of attention by linearly transforming keys, values, and queries for each head. Dot-product attention is applied to give head-specific values, which are then concatenated and linearly transformed to produce the final values. This architecture allows the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. The multihead attention mechanism extends the concept of attention by linearly transforming keys, values, and queries for each head. Dot-product attention is applied to give head-specific values, which are then concatenated and linearly transformed to produce the final values. This architecture allows the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. Self-attention is applied to compute representations of each (x, y) pair, and cross-attention is used for the target input to predict the target output. Attentive neural processes use self-attention to compute representations of context points and cross-attention for predicting target outputs. Self-attention models interactions between context points by giving high weight to relevant points, resulting in smoother query-values compared to dot-product attention. The self-attention mechanism in attentive neural processes models interactions between context points to obtain richer representations that encode relations between the points. This helps in producing smoother query-values compared to dot-product attention. Higher order interactions are modeled by stacking self-attention, as seen in Vaswani et al. (2017). The self-attention mechanism in attentive neural processes enhances representations of context points to capture relations between them. This is achieved by stacking self-attention to model higher order interactions, allowing each query to focus more closely on relevant context points for prediction. The self-attention mechanism in attentive neural processes enhances context representations by allowing each query to attend closely to relevant context points for prediction. In contrast, a cross-attention mechanism is used to replace context representations in producing query-specific representations, preserving global latent dependencies for target predictions. The latent path, represented by z, captures correlations in the marginal distribution of target predictions, modeling the global structure of the stochastic process. The latent path, represented by z, captures correlations in the marginal distribution of target predictions, modeling the global structure of the stochastic process. The decoder remains the same, except we replace the shared context representation r C with the query-specific representation r * . Permutation invariance in the contexts is preserved with the attention mechanism. Uniform attention is used for all contexts. The NP with attention introduces query-specific representation r* and maintains permutation invariance in contexts. Training is done using the same loss as the original NP, with added computational complexity. The NP with attention introduces query-specific representation and maintains permutation invariance in contexts. Training is done using the same loss as the original NP, with added computational complexity due to self-attention across contexts and target points. The computational complexity of the (A)NP is increased to O(n(n + m)) due to self-attention across contexts and target points. Despite this, training time for ANPs remains comparable to NPs, with ANPs learning significantly faster in terms of training iterations and wall-clock time. The (A)NP learns a stochastic process and should be trained on multiple functions. The (A)NP learns a stochastic process and should be trained on multiple functions. Training time for ANPs remains comparable to NPs, with ANPs learning significantly faster in terms of training iterations and wall-clock time. The (A)NP learns stochastic processes by drawing batches of realisations from the data generating process and selecting random points as targets and contexts to optimize loss. The model uses the same decoder architecture with 8 heads for multihead. The experiments include 1D function regression on synthetic GP data generated from a squared-exponential kernel with small likelihood noise. The (A)NPs can be trained on data not necessarily from a GP or known stochastic process. See Appendix A for architectural details. The (A)NPs are trained on synthetic GP data with fixed or randomly varying kernel hyperparameters. The number of contexts and targets are randomly chosen for each iteration. Each x-value is drawn from a uniform distribution. In an illustrative example, two settings are explored using synthetic GP data: fixed hyperparameters and randomly varying hyperparameters. The number of contexts and targets are randomly chosen at each iteration, with x-values drawn uniformly at random. No self-attention is used, only cross-attention in the deterministic path. The encoder/decoder architecture is the same for NP and ANP, except for cross-attention. See Appendix B for experimental details. For this simple 1D data, cross-attention is explored in the deterministic path without self-attention. ANP shows faster decrease in reconstruction error and lower values at convergence compared to NP, especially for dot product and multihead attention. The ANP model demonstrates faster learning and lower reconstruction error compared to the NP model, especially with dot product and multihead attention mechanisms. Despite the increased computational cost, ANP shows rapid convergence in both training iteration and wall clock time. The Laplace and dot-product ANP have similar computation times as NP, while multihead ANP takes around twice the time. The bottleneck size (d) in the deterministic and latent paths is also analyzed. The size of the bottleneck (d) in the deterministic and latent paths of the NP affects underfitting behavior. Increasing d improves reconstructions, but there is a limit to the improvement. Beyond a certain value, learning becomes slow, and reconstruction error remains high at convergence. Raising the bottleneck size in NPs affects underfitting behavior, but there is a limit to improvement. Using ANPs has significant benefits over simply increasing d in NPs. Visualizing the learned conditional distribution shows a qualitative comparison of attention mechanisms. In a comparison of attention mechanisms, ANPs outperform multihead ANP with 10% of the time. The learned conditional distribution in FIG2 shows that dotproduct attention accurately predicts context points, while Laplace attention underfits the context. The predictive mean of the NP underfits the context, while dotproduct attention accurately predicts almost all context points. Laplace attention is parameter-free, using x-coordinates as keys and queries, while dot-product attention uses parameterised representations of x-values for keys and queries. For dot-product attention, keys and queries are parameterised representations of x-values, computed in a learned space. Laplace attention computes similarities based on L1 distance in x-coordinate domain. Dot-product attention outperforms Laplace attention but displays non-smooth predictions. Multiple heads in multihead attention help smooth predictions. The similarities in dot-product attention are computed based on L1 distance in the x-coordinate domain, leading to better performance than Laplace attention. However, dot-product attention shows non-smooth predictions, especially in the predictive standard deviations. Multiple heads in multihead attention help to smooth out interpolations, improving reconstruction of contexts and prediction of targets while maintaining increased predictive uncertainty away from contexts like a GP. The results for (A)NP trained on fixed GP kernel hyperparameters are similar, with the NP underfitting to a lesser degree due to reduced constraints. The multihead attention improves interpolations, reconstructs contexts, and predicts targets effectively while maintaining predictive uncertainty like a GP. The (A)NP is more expressive than the NP, learning a wider range of functions. Trained (A)NPs are used for a toy Bayesian Optimization problem to find the minimum. The ANP outperforms the NP due to its ability to learn a wider range of functions. Trained (A)NPs are applied to a Bayesian Optimization problem to find the minimum of test functions. This experiment demonstrates the capability of sampling entire functions and accurate context reconstructions. See Appendix C for detailed results analysis. The experiment demonstrates the ANP's ability to sample entire functions from a GP prior and accurately reconstruct context. Image data regression is also discussed, mapping pixel locations to intensities for greyscale or RGB images. Each image represents a realization of the process on a 2D grid. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Three different models are compared on both datasets. The ANP is trained on MNIST and CelebA datasets with self-attentional layers in the encoder. Three models are compared on both datasets, including NP, Multihead ANP, and Stacked Multihead ANP. Three different models are compared on MNIST and CelebA datasets: NP, Multihead ANP, and Stacked Multihead ANP. Predictions are generated with varying numbers of random context pixels, showing reasonable predictions and diversity for fewer contexts with the NP model. The NP model provides diverse predictions with fewer context pixels but inaccurate reconstructions of the whole image compared to the Stacked Multihead ANP model. Attention helps achieve crisper inpaintings, enhancing the ANP's ability to model less smooth 2D functions. The reconstructions of the whole image are not accurate with the NP model compared to the Stacked Multihead ANP model. Attention helps achieve crisper inpaintings, enhancing the ANP's ability to model less smooth 2D functions. The diversity in faces and digits obtained with different values of z is apparent in the samples, providing evidence for the claim that z can model the global structure of the image. The diversity in faces and digits obtained with different values of z is apparent in the samples, providing evidence for the claim that z can model the global structure of the image. The model generalizes well even with limited context points, showing improved performance compared to the NP model. In the latter task, the model generalizes well with limited context points, showing improved performance compared to the NP model. Multihead ANP gives improved context reconstruction error and NLL for target points, with noticeable gains in crispness and global coherence when using stacked self-attention. Visualizing each head of Multihead ANP for CelebA in FIG7. In the latter task, the model generalizes well with limited context points, showing improved performance compared to the NP model. Multihead ANP gives improved context reconstruction error and NLL for target points, with noticeable gains in crispness and global coherence when using stacked self-attention. Visualizing each head of Multihead ANP for CelebA in FIG7, showing different roles for each head. Visualizing each head of Multihead ANP for CelebA reveals distinct roles for different heads, such as focusing solely on the target pixel, nearby pixels, larger regions, specific columns, or exploiting symmetry in faces. Consistent behavior is observed across different target pixels. The Multihead ANP for CelebA shows distinct roles for different heads, focusing on target pixels, nearby regions, specific columns, or exploiting symmetry in faces. Consistent behavior is observed across different target pixels. Additionally, the model can map images from one resolution to another using continuous space modeling of pixel locations. One application of (A)NPs trained on images is mapping images from one resolution to another by predicting pixel intensities in a continuous space. This can be done by using one grid as context and a finer grid as the target, allowing the model to map a given resolution to a higher resolution. The model can predict pixel intensities in a continuous space, mapping a given resolution to a higher resolution. ANPs may provide accurate reconstructions for reliable mappings between different resolutions, as shown in FIG6. The ANP model can accurately map low resolutions to higher resolutions, providing diverse outputs for different contexts. This performance is expected since the model has been trained on data with the target resolution. The ANP model can map low resolutions to higher resolutions with diverse outputs. The model, trained on 32 \u00d7 32 images, can generate realistic 32 \u00d7 32 target outputs and even higher resolutions up to 256 \u00d7 256. The high-resolution images produced by the model have sharper edges compared to baseline interpolation. The ANP model can generate high-resolution images up to 256 \u00d7 256 from 32 \u00d7 32 resolution inputs. It produces realistic images with sharper edges and can even fill in details like eyes, showing evidence of learning internal representations of faces. The ANP model can generate high-resolution images with sharper edges and internal representations of faces, filling in details like eyes. It learns from random context and target pixels during optimization. The ANP model is not a replacement for state-of-the-art image inpainting or super-resolution algorithms. It showcases flexibility in modeling various conditional distributions. The ANP model is not a replacement for state-of-the-art image inpainting or super-resolution algorithms. It highlights flexibility in modeling conditional distributions, with a focus on works relevant to ANPs such as Gaussian Processes. The domain of Gaussian Processes, Meta-Learning, conditional latent variable models, and Bayesian Learning have been extensively discussed in original works. Attention in NPs shows a parallel with GP kernels in measuring similarity between points. The use of attention in an embedding space is related to Deep Kernel Learning, where a GP is applied to learned data representations. Learning is still done in a GP framework by maximizing marginal likelihood. The use of attention in an embedding space is related to Deep Kernel Learning, where a GP is applied to learned data representations. Learning is still done in a GP framework by maximizing marginal likelihood. Comparing the training regimes of GPs and NPs is challenging due to their differences. One possibility is to update the GP kernel hyperparameters at each iteration via one gradient step of the marginal likelihood on the mini-batch of data. The predictive uncertainties of GPs heavily depend on the kernel choice, while NPs learn uncertainties directly from data. GPs are consistent stochastic processes, but comparing their training regimes with NPs is challenging. One approach is to update GP kernel hyperparameters using one gradient step of the marginal likelihood on mini-batch data. The predictive uncertainties of Gaussian Processes (GPs) depend on kernel choice, while Neural Processes (NPs) learn uncertainties from data directly. GPs are consistent stochastic processes with exact closed-form expressions for covariance and marginal variance, a feature lacking in current NP formulations. Variational Implicit Processes (VIP) are related to NPs, defining a stochastic process using a similar decoder setup. Neural Processes (NPs) lack exact closed-form expressions for covariance and marginal variance, unlike Gaussian Processes (GPs). Variational Implicit Processes (VIP) approximate the process and its posterior with a GP using a finite dimensional z. Meta-Learning (A)NPs focus on few-shot learning with input-output pairs from a new function. The process and its posterior are approximated by a GP in Meta-Learning (A)NPs using a generalization of the Wake-Sleep algorithm. Few-shot learning is not the main focus, but input-output pairs from a new function are used to reason about the function's predictive distribution. Works like Vinyals et al. (2016) and Snell et al. (2017) use attention to locate relevant images/prototypes in few-shot classification. Few-shot classification and density estimation have been extensively explored using attention mechanisms in various works such as Vinyals et al. (2016), Snell et al. (2017), Rezende et al. (2016), and Reed et al. (2017). Attention has also been applied in tasks like Meta-RL for continuous control and visual navigation. Noteworthy approaches include the Neural Statistician and the Variational Homoencoder. Few-shot density estimation using attention has been extensively explored in various works. The Neural Statistician and the Variational Homoencoder have similar permutation invariant encoders but use local latents on top of a global latent. For ANPs, regression in a less-explored setting is considered. The authors explore regression on a toy 1D domain using a setup similar to Neural Statistician and Variational Homoencoder, optimizing an approximation to the entropy of the latent function. Generative Query Networks are models for spatial prediction that render a scene frame given a viewpoint, corresponding to a special case of Neural Processes. The authors explore regression on a toy 1D domain without attention mechanisms. They propose ANPs, which augment multitask learning in the GP literature. Generative Query Networks are models for spatial prediction, with a special case of NPs where x is viewpoints and y is frames of a scene. Rosenbaum et al. (2018) apply GQN to 3D localization with attention on context frames. In their work, targets attend to contexts via x. In their work, the authors propose ANPs to address underfitting issues in spatial prediction tasks. By augmenting NPs with attention, they improve prediction accuracy, training speed, and the range of functions that can be modeled. Future work for ANPs includes exploring different model architectures. The authors propose Attention Augmented Neural Processes (ANPs) to address underfitting in spatial prediction tasks. ANPs improve prediction accuracy, training speed, and expand the range of functions that can be modeled. Future work includes exploring different model architectures, such as incorporating cross-attention and training ANPs on text data. One way to improve ANPs is by incorporating cross-attention and modeling dependencies across local latents with a global latent. ANPs could be trained on text data to fill in blanks in a stochastic manner. The Image Transformer (ImT) BID21 has connections with ANPs in how it predicts consecutive pixel blocks, similar to how ANPs attend to context pixels to predict target pixels. By incorporating self-attention in the decoder, ANPs can resemble an Image Transformer model that operates on arbitrary pixel orderings, unlike the original ImT which assumes a fixed order. This modification aims to enhance the expressiveness of ANPs in filling in blanks in text data. By replacing the MLP in the decoder of the ANP with self-attention across target pixels, a model similar to an Image Transformer on arbitrary pixel orderings is created. This modification aims to extend the expressiveness of ANPs. In this setup, the targets will affect each other's predictions, so the ordering and grouping of the targets become important. The architectural details of the NP and Multihead ANP models used for regression experiments are shown in Figure 8. The models have relu non-linearities except for the final layer. The latent path outputs parameterize q(z|s C) and the decoder outputs parameterize p(y i). The NP and Multihead ANP models used for regression experiments have specific architectural details. The latent path outputs parameterize q(z|s C) and the decoder outputs parameterize p(y i). The 1D regression experiments use multihead cross-attention, while the 2D regression experiments use a form of multihead cross-attention from the Image Transformer BID21. The 1D regression experiments utilize multihead cross-attention with a specific parameterization for p(y i). In contrast, the 2D regression experiments employ a different form of multihead cross-attention without dropout. The self-attention module can be stacked due to having the same number of inputs and outputs. The Image Transformer BID21 uses self-attention without dropout to limit stochasticity to latent z. Stacked Multihead ANP in 2D Image regression experiments includes 2 layers of self-attention. Stacking more layers did not significantly improve results. GP data generating kernel parameters are set to l = 0.6 and \u03c32f = 1. In the 2D Image regression experiments, Stacked Multihead ANP uses 2 layers of self-attention. Different kernel hyperparameters are tested, with a batch size of 16. For the fixed kernel hyperparameter experiments, a kernel scale of \u03c3 2 f = 1 is used. In the random kernel hyperparameter case, hyperparameters are sampled from specific ranges. A batch size of 16 is utilized, and Adam Optimiser BID14 with a fixed learning rate of 5e-5 is employed. One sample of q(z|s C ) is used to estimate the loss. In the experiments, 16 random hyperparameter values are sampled, and GPs are used to draw curves. The Adam Optimiser BID14 with a fixed learning rate of 5e-5 is employed. A MC estimate of the loss is formed using one sample of q(z|s C). The Multihead ANP model is closer to the oracle GP in predictions but underestimates predictive variance, possibly due to variational inference. The Multihead ANP model, compared to the NP model, is closer to the oracle GP in predictions but still underestimates predictive variance. Variational inference used in learning the ANP may be the reason for this underestimation. Investigating how to address this issue would be interesting. The conditional distributions for fixed kernel hyperparameters show non-smooth behavior for dot-product attention, similar to the random kernel hyperparameter case. The Multihead ANP model underestimates predictive variance compared to the NP model. Investigating how to address this issue would be interesting. The conditional distributions for fixed kernel hyperparameters exhibit non-smooth behavior for dot-product attention, similar to the random kernel hyperparameter case. The behavior of dot-product attention with fixed kernel hyperparameters is non-smooth, similar to the random kernel hyperparameter case. The KL term in NP loss differs between training on fixed and random kernel hyperparameter GP data. The KL term in NP loss varies between training on GP data with fixed and random kernel hyperparameters. In the fixed hyperparameter case, the KL quickly goes to 0, indicating the model finds the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, there is added variation in the data, leading to a non-zero KL as the model uses latents to model uncertainty in the stochastic process. In the random hyperparameter case, the attention gives a non-zero KL and uses latents to model uncertainty in the stochastic process given context points. The model believes there are multiple realisations of the process that can explain the contexts well, hence uses latents to model this variation. The model uses latents to model uncertainty in the stochastic process given context points, allowing for multiple realisations to explain the contexts well. ANPs trained on 1D GP data are used for Bayesian optimization, comparing different attention mechanisms to an oracle GP with true kernel hyperparameters. This approach considers all previous function evaluations as context points to inform the surrogate of the target function. The study compares ANPs with different attention mechanisms to an oracle GP for Bayesian optimization. Results show that ANPs with multihead attention have the smallest simple regret, approaching the true minimum consistently. Thompson sampling is used to draw a simple function from the surrogate model, with results showing that ANPs with multihead attention have the smallest simple regret, approaching the oracle GP. The cumulative regret decreases most rapidly for multihead ANPs, indicating efficient use of previous function evaluations for predicting the function minimum. For a NP with multihead attention, the cumulative regret decreases rapidly, indicating efficient use of previous function evaluations for predicting the function minimum. The lower initial cumulative regret compared to the oracle GP is due to under-exploration, with uncertainties of ANP smaller than the oracle GP. Random pixels of an image are taken as targets during training, with a subset chosen as contexts. The regret being lower than the oracle GP is due to under-exploration, with uncertainties of ANP smaller than the oracle GP. Random pixels of an image are taken as targets during training, with a subset chosen as contexts. MNIST and CelebA datasets are used with specific batch sizes and learning rates. The x and y values are rescaled for MNIST and CelebA datasets. A batch size of 16 is used for both datasets, with specific learning rates. The self-attention architecture is similar to the Image Transformer BID21, without Dropout or positional embeddings. Little tuning has been done for both datasets. The stacked self-attention architecture used for both Mnist and CelebA datasets does not include Dropout or positional embeddings. Little tuning has been done regarding the architectural hyperparameters. The NP overestimates predictive variance, as shown in the plot of the standard deviation. The NP overestimates predictive variance, especially around the edges of the reconstruction. The Stacked Multihead ANP improves results significantly, giving sharper images with better global coherence. The uncertainty in reconstruction is reduced with more contexts in NP with attention. Stacked Multihead ANP improves results significantly, providing sharper images with better global coherence, even when the face isn't axis-aligned. Different roles of heads are shown in contexts where the target is disjoint from the context. In the NP with attention, adding more contexts reduces uncertainty in reconstruction and improves global coherence. Different roles of heads are shown in contexts where the target is disjoint from the context, as illustrated in the visualizations."
}