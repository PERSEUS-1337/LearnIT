{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, including humans, despite individual incentives conflicting with the common good. Understanding cooperative behavior in self-interested individuals is essential in multi-agent reinforcement learning and evolutionary theory. This study focuses on intertemporal social dilemmas, where the conflict between individual and group interests is pronounced. Through a combination of MARL and natural selection, the research demonstrates the emergence of cooperative behavior. In the study of intertemporal social dilemmas (ISDs), the conflict between individual and group interests is examined. By integrating multi-agent reinforcement learning with natural selection, cooperative behavior can be learned in a model-free manner. An innovative modular architecture for deep reinforcement learning agents is introduced to support multi-level selection, with results interpreted in the context of cultural and ecological evolution. Individual inductive biases for cooperation can be learned in a model-free way using an innovative modular architecture for deep reinforcement learning agents that supports multi-level selection. Results in challenging environments are interpreted in the context of cultural and ecological evolution, showcasing cooperation in nature despite natural selection favoring selfish interests. Nature demonstrates cooperation at various levels, from microscopic interactions to species-wide societies, despite natural selection favoring selfish interests. Altruism can be favored by selection when individuals cooperate with others, leading to rewards without exploitation. Various mechanisms such as kin selection, reciprocity, and group selection also play a role in promoting cooperation. In multi-agent deep reinforcement learning, cooperation among self-interested agents is a key topic. The problem domain is formalized as an intertemporal social dilemma, extending matrix game social dilemmas to Markov settings. Social dilemmas involve a trade-off between collective welfare and individual interests. The emergence of cooperation among self-interested agents in multi-agent deep reinforcement learning is a key topic. The problem domain is formalized as an intertemporal social dilemma, where social dilemmas involve a trade-off between collective welfare and individual interests. Self-interested reinforcement-learning agents typically converge to defecting strategies instead of achieving the collectively optimal outcome. The goal is to find multi-agent training regimes where cooperation emerges. The emergence of cooperation among self-interested agents in multi-agent deep reinforcement learning is a key topic. Previous work has identified three broad categories for resolving social dilemmas: opponent modeling, long-term planning with perfect knowledge, and intrinsic motivation functions from behavioral economics. These hand-crafted approaches contrast with more recent end-to-end model-free learning methods. Evolution can be applied to remove hand-crafted intrinsic motivation in multi-agent deep reinforcement learning, similar to other applications in deep learning. This approach contrasts with recent model-free learning algorithms, showing greater generalization ability. Evolution can be used to optimize various aspects of deep learning, such as hyperparameters, neuroarchitectures, regularization, loss functions, and reward functions. These principles are typically applied in single-agent or competitive multi-agent tasks, but success is not guaranteed in the ISD. Evolution can optimize aspects of deep learning like hyperparameters, neuroarchitectures, regularization, loss functions, and reward functions. These principles are applied in single-agent or competitive multi-agent tasks, but success is not guaranteed in the ISD. The system proposed distinguishes between optimization processes. The system proposed distinguishes between optimization processes in the ISD setting, with a fast time-scale of learning and a slow time-scale of evolution. Individual agents participate in a social dilemma with fixed intrinsic motivation, while that motivation is subject to natural selection in a population. The system distinguishes between optimization processes in the ISD setting, with fast learning and slow evolution. Individual agents engage in a social dilemma with intrinsic motivation, subject to natural selection. The intrinsic motivation is modeled as an additional term in the reward, implemented as a neural network for evolution. Evolution bridges the gap between the fast and slow time-scales. Evolutionary theory predicts that evolving individual intrinsic reward weights across a population does not lead to altruistic behavior. To achieve this goal, the evolutionary dynamics must be structured, starting with a \"Greenbeard\" strategy implementation. To bridge timescales and achieve altruistic behavior, evolutionary dynamics must be structured. A \"Greenbeard\" strategy is implemented for assortative matchmaking, but it is not a general method for multi-agent reinforcement learning. Assortative matchmaking involves agents choosing partners based on signals of cooperativeness, but it has limitations in explaining cooperation across all taxa. To address this, a modular training scheme inspired by multi-level selection theory is introduced as an alternative approach. In response to the limitations of assortative matchmaking in explaining cooperation, a new modular training scheme called shared reward network evolution is introduced for multi-agent reinforcement learning. This approach involves agents composed of policy and reward network modules, where the policy network is trained using modified rewards specified by the reward network on fast timescales, and both modules evolve on slow timescales. In shared reward network evolution, agents have policy and reward network modules that evolve separately. The policy network is trained using modified rewards specified by the reward network on fast timescales, while the reward network's fitness is based on the collective return for the group of co-players. The policy network and reward network modules evolve separately in shared reward network evolution. Each agent has a distinct policy network but shares the same reward network. The fitness for the policy network is the individual's reward, while the fitness for the reward network is the collective return for the group of co-players. This evolutionary paradigm prevents overfitting and points to a potential mechanism for resolving difficult issues without handcrafting. In multi-level selection theory, policy networks are lower level units of evolution, while reward networks are higher level units. Evolving these modules separately prevents overfitting and suggests a potential mechanism for resolving difficult issues without handcrafting. Various parameters were explored, including environments, reward network features, matchmaking, and reward network evolution options. In a study on intertemporal social dilemmas within a MARL setting, different parameters were varied and explored, such as environments, reward network features, matchmaking, and reward network evolution options. This research not only avoids handcrafting but also suggests a potential mechanism for the evolutionary origin of social inductive biases. In this paper, intertemporal social dilemmas in Markov games within a MARL setting are studied. These dilemmas involve conflicts between individual selfish actions and group impacts over different timescales, resembling social dilemmas like the Prisoner's Dilemma. Two dilemmas are considered, implemented as partially observable Markov games on a 2D grid. The intertemporal nature of social dilemmas in Markov games involves conflicts between individual and group rationality over time. Two dilemmas are studied on a 2D grid, such as the Cleanup game where agents collect apples with spawning rates linked to aquifer cleanliness. In the Cleanup game, agents collect apples with spawning rates linked to aquifer cleanliness. The aquifer fills up with waste over time, reducing apple respawn until no apples can spawn. Agents must leave the apple field to clean, which has no reward. In the Harvest game, agents collect rewarding apples with a spawn rate at a particular point. In the Harvest game, agents face a dilemma between quickly harvesting all apples for short-term gain or preserving them for long-term group yield. The apple spawn rate depends on nearby apples, falling to zero when none are present, leading to a lower total yield if depleted too rapidly. The apple spawn rate in the Harvest game depends on nearby apples, decreasing to zero when none are present. There is a dilemma between quickly harvesting all apples for short-term gain or preserving them for long-term group yield. In the model, rewards consist of total, extrinsic, and intrinsic rewards for player i. In the model, rewards consist of total, extrinsic, and intrinsic rewards for player i, with the total reward being the sum of extrinsic and intrinsic rewards. The extrinsic reward is the environment reward obtained by player i when taking action a from state s, while the intrinsic reward is an aggregate social preference across features. The total reward for player i is the sum of the extrinsic reward and an intrinsic reward. The extrinsic reward is the environment reward obtained by player i when taking action a from state s, while the intrinsic reward is an aggregate social preference across features calculated using a 2-layer neural network with evolved parameters. The intrinsic reward for each player is determined by a 2-layer neural network with evolved parameters, where the elements of the parameter vector correspond to coefficients related to advantagenous and disadvantagenous inequity aversion. Each player has access to the same set of features to transform into intrinsic reward via their reward network. The feature vector f i is player-specific and can be transformed into intrinsic reward via a neural network. Each agent has access to the same set of features, with their own feature being demarcated specially. Features are based on recently received or expected future rewards, and social preferences should not be overly influenced by temporal alignment in Markov games. In Markov games, rewards for different players may not align in time. Social preferences should depend on comparing temporally averaged reward estimates between players, rather than instantaneous values. Two ways of aggregating rewards were considered. In Markov games, rewards for different players may not align in time. Social preferences should depend on comparing temporally averaged reward estimates between players. Two ways of aggregating rewards were considered, with Agent A adjusting policy using off-policy importance weighted actor-critic. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The off-policy importance weighted actor-critic (V-Trace) BID10 involves sampling from a queue with trajectories recorded from 500 actors. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. Intrinsic reward is derived from whether agents judge if others have been rewarded recently or are expecting rewards in the future. The prospective variant of the reward system in the V-Trace architecture derives intrinsic reward from agents' expectations of future rewards. The reward values for agents are updated at each timestep, and the value estimates are used for calculation. Gradients are stopped before the reward network module to prevent flow back into other agents during training. The training framework used is the same as in BID27 for distributed asynchronous training in multi-agent environments. The reward system in the V-Trace architecture updates agents' reward values at each timestep using value estimates. Gradients are stopped before the reward network module to prevent flow back into other agents during training. A population of 50 agents with policies {\u03c0 i } was trained using distributed asynchronous training in multi-agent environments. In a population-based training approach, a population of 50 agents with policies {\u03c0 i } was trained in multi-agent environments. 5 players were sampled from this population to populate 500 arenas running in parallel. Agents were sampled using matchmaking processes, and episode trajectories lasted 1000 steps. Weights were updated using V-Trace, including learning rate, entropy cost weight, and reward network weights \u03b8. The agents in the population-based training approach were sampled using matchmaking processes. Episode trajectories lasted 1000 steps, and weights were updated using V-Trace. The set of weights evolved included learning rate, entropy cost weight, and reward network weights. Agents were allowed to observe their last actions and rewards as input to the LSTM in the agent's neural network. The policy network parameters were inherited in a Lamarckian fashion, agents observed their last actions and rewards as input to the LSTM, and the objective function comprised the value function gradient, policy gradient, and entropy regularization. Evolution was based on a fitness measure calculated as a moving average of total episode return. The function presented in BID10 included three components: value function gradient, policy gradient, and entropy regularization, weighted by hyperparameters baseline cost and entropy cost. Evolution was based on a fitness measure calculated as a moving average of total episode return, determined by random or assortative matchmaking methods. The study involved a sum of apples collected minus penalties due to tagging, with matches determined by random or assortative matchmaking methods. Random matchmaking selected agents uniformly, while cooperative matchmaking grouped agents based on recent cooperativeness for more effective gameplay. The study compared random matchmaking with cooperative matchmaking in a game where agents were grouped based on recent cooperativeness. Highly cooperative agents played with others like them, while defecting agents played with other defectors. Cooperativeness was calculated differently for Cleanup and Harvest games. In the study, cooperative matchmaking ensured that agents played with others of similar behavior. Cooperativeness was measured differently for Cleanup and Harvest games, using metrics like steps taken and return compared to the mean. Matchmaking was based on individual or no reward networks, not used in the multi-level selection model. Cooperative metric-based matchmaking was used to rank players based on their return compared to the average. This approach was not applied in the multi-level selection model, where the reward network was evolved separately within its own population. This allowed for independent exploration of hyperparameters and credit assignment of fitness. Building on previous work that evolved either the intrinsic reward or the entire loss function, the reward network was separately evolved within its own population. This allowed for independent exploration of hyperparameters and credit assignment of fitness, leading to a wider exploration of the hyperparameter landscape compared to using only a single pool. Additionally, reward networks could be randomly assigned to any policy network, forcing them to generalize to a variety of policies. In each episode, 5 separate policy networks were paired with the same reward network. The credit assignment of fitness was explored by evolving the reward network separately, allowing for wider exploration of hyperparameters. 5 policy networks were paired with the same reward network in each episode, promoting generalization to various policies. Fitness was determined by individual agent return for policy network weights, while total episode return guided the evolution of reward network parameters. The reward network parameters were evolved based on total episode return across co-players, distinct from previous work on evolving intrinsic rewards. This evolution is motivated by addressing the tension in ISDs rather than just providing denser rewards. The reward network parameters were evolved based on total episode return across co-players, distinct from previous work on evolving intrinsic rewards. This evolution is motivated by addressing the tension in ISDs rather than just providing denser rewards. The approach focuses on evolving social features for cooperation rather than remapping environmental events, emphasizing communication for social cooperation in a sparse-reward environment. Multiple agents can share components in a social setting, highlighting the importance of shared reward networks. Shared reward networks provide a biologically principled method that mixes group fitness on a long timescale and individual reward on a short timescale, critical in a social setting for cooperation. This contrasts with hand-crafted aggregation methods and is shown to be essential for performance in games. Shared reward networks provide a biologically principled method that mixes group fitness on a long timescale and individual reward on a short timescale. In contrast to hand-crafted aggregation methods, using intrinsic reward networks is essential for performance in games. PBT without intrinsic reward networks performs poorly on both Cleanup and Harvest games. Random matchmaking shows no significant improvement over PBT on Cleanup, while assortative matchmaking with reward networks using retrospective social features performs better. In Cleanup and Harvest games, random matchmaking does not show significant improvement over PBT. Assortative matchmaking with reward networks using retrospective social features performs better, especially when individual reward networks are used. In Cleanup and Harvest games, assortative matchmaking with reward networks using retrospective social features performs better, especially when individual reward networks are used. Adding reward networks over social features is not beneficial if players have separate networks evolved selfishly. Agents with shared reward networks perform as well as assortative matchmaking with individual reward networks. Agents with shared reward networks perform as well as assortative matchmaking with individual reward networks, indicating that immediate access to honest signals of other agents' cooperativeness is not necessary to resolve the dilemma. The study found that agents with shared reward networks performed as well as assortative matchmaking with individual reward networks, suggesting that immediate access to honest signals of other agents' cooperativeness is not essential. The retrospective and prospective variants of reward network evolution were compared, with the prospective variant showing worse performance and more instability due to agents needing to learn good value estimates before the reward networks become useful. The prospective variant of reward network evolution, compared to the retrospective variant, generally results in worse performance and more instability. This is because the prospective variant depends on agents learning good value estimates before the reward networks become useful, while the retrospective variant only relies on environmentally provided rewards. Various social outcome metrics are plotted to capture agent behavior complexities. The prospective variant of reward network evolution relies on agents learning value estimates before the networks become useful, leading to worse performance and instability. In contrast, the retrospective variant depends on environmentally provided rewards and does not suffer from this issue. Social outcome metrics are plotted to capture agent behavior complexities, showing that having no reward network results in quick apple collection, while using reward networks leads to more sustainable behavior. Equality is measured by the Gini coefficient over individual returns. The average time step for agents to receive positive rewards is shown in Figure 5(a), indicating faster apple collection without a reward network. Equality is calculated using the Gini coefficient. Figure 4(b) shows lower equality with prospective reward networks and higher equality with retrospective ones. Tagging frequency is higher with prospective reward networks. The prospective reward networks lead to lower equality, while retrospective ones have higher equality. Tagging frequency is higher with prospective reward networks. The final weights of the retrospective shared reward networks evolved to resolve the ISDs. The performance differences between prospective and individual reward networks compared to the retrospective shared reward network are explained in FIG3. The final weights of the retrospective shared reward networks evolved differently for each game, suggesting different social preferences are needed. Cleanup required a less complex reward network. The final layer weights in the second layer evolved differently for each game, indicating different social preferences are needed. In Cleanup, simpler reward networks sufficed, while Harvest required a more complex reward function to prevent over-exploitation of resources. The first layer weights tended to take on arbitrary positive values due to randomness. In Harvest, a more complex reward function was needed to prevent over-exploitation of resources. The first layer weights tended to have arbitrary positive values due to random matchmaking, leading to little specialization. Real environments do not provide scalar reward signals, so organisms have developed internal drives based on primary or secondary goals. In Harvest, complex reward functions were needed to prevent resource over-exploitation. Random matchmaking led to little specialization in weights. Real environments lack scalar reward signals, so organisms develop internal drives based on goals. Examining intrinsic rewards from other agents, natural selection via genetic algorithms did not promote cooperation. Assortative matchmaking generated cooperative behavior when honest signals were present. Intrinsic rewards from other agents did not lead to cooperation with genetic algorithms. Assortative matchmaking promoted cooperation with honest signals. A new evolutionary paradigm based on shared reward networks fosters cooperation in various situations. Evolving intrinsic social preferences helps solve the intertemporal choice problem by aligning short-term and long-term fitness goals. Evolutionary paradigm based on shared reward networks promotes cooperation by aligning short-term and long-term fitness goals, improving credit assignment between selfish acts and negative group outcomes, and exposing social signals correlating with selfishness levels. The shared reward network evolution model aligns short-term and long-term fitness goals, improves credit assignment between selfish acts and negative group outcomes, and exposes social signals correlating with selfishness levels to promote cooperation. The shared reward network evolution model promotes cooperation by aligning short-term and long-term fitness goals, improving credit assignment between selfish acts and negative group outcomes, and exposing social signals correlating with selfishness levels. This model is inspired by multi-level selection but differs in that lower level units constantly swap with higher level units. The shared reward network evolution model, inspired by multi-level selection, involves policy networks constantly swapping with a reward network. This form of modularity can be seen in nature, such as microorganisms forming multi-cellular structures or prokaryotes incorporating plasmids as functional parts of their genome. In nature, modularity can be observed in various forms, such as microorganisms forming multi-cellular structures or prokaryotes incorporating plasmids as functional parts of their genome to achieve cooperation. In humans, a reward network may represent a shared \"cultural norm\" based on cultural information accumulated from groups, allowing the spread of norms independently of individual success. In humans, a reward network may represent a shared \"cultural norm\" based on cultural information accumulated from groups, allowing the spread of norms independently of individual success. Future work could explore alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity, to understand the evolutionary origins of social biases. For future work, investigating alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity, could provide insights into the evolutionary origins of social biases. Additionally, studying an emergent version of the assortative matchmaking model suggested by BID22 could add further generality and power to the setup. Combining an evolutionary approach with multi-agent communication may lead to the emergence of cooperative behaviors. To further enhance the setup, exploring how an evolutionary approach can be integrated with multi-agent communication to generate cooperative behaviors like cheap talk in games with specific observable and action space constraints. The games Cleanup and Harvest involve cooperative behaviors like cheap talk in a partially observable environment with specific action constraints. Training is done through joint optimization of network parameters using SGD. The Cleanup game involves actions like tagging and cleaning waste, with a reward cost of 1 for tagging and a penalty of 50 reward points for the tagged player. Training is done through joint optimization of network parameters using SGD and hyperparameters/reward network parameters via evolution in the standard PBT setup. The optimization process involved using SGD for network parameters and evolution for hyperparameters/reward network parameters in the PBT setup. Gradient updates were applied for trajectories up to 100 steps with a batch size of 32. RMSProp was used for optimization with specific parameters, and learning rates were allowed to evolve. PBT utilized genetic algorithms to search over hyperparameter space instead of manual tuning. The entropy cost and learning rates were evolved using PBT with a mutation rate of 0.1. Hyperparameters were adjusted through genetic algorithms, resulting in an adaptive schedule for joint optimization with network parameters learned through gradient descent. The hyperparameters were evolved using genetic algorithms with a mutation rate of 0.1, implementing perturbations for entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 10^6 agent steps was used before evolution to ensure accurate fitness assessment."
}