{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, specifically Choco-SGD, enables data privacy, on-device learning, and efficient scaling to large compute clusters. The approach achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. Practical performance is demonstrated in training scenarios over decentralized user devices and in datacenters. Choco-SGD achieves linear speedup in the number of workers for high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training scenarios over decentralized user devices and in datacenters. Distributed machine learning enables successful applications in research and industry by leveraging computational scalability and data-locality. Recent theoretical results suggest decentralized schemes can be as efficient as centralized approaches in terms of convergence of training loss vs. iterations. Decentralized schemes in distributed machine learning can be as efficient as centralized approaches in terms of convergence of training loss vs. iterations. Gradient compression techniques are used to reduce the amount of data sent over communication links in the network. Gradient compression techniques have been proposed for decentralized training of deep neural networks to reduce data sent over communication links. Tang et al. (2018) introduced two algorithms (DCD, ECD) for communication compression, but they are restrictive in compression operators used. CHOCO-SGD is a new algorithm introduced for decentralized training of deep neural networks, overcoming restrictions on compression operators and supporting high compression ratios. It has been evaluated for generalization performance on standard machine learning benchmarks. CHOCO-SGD is a new algorithm designed for decentralized training of deep neural networks, focusing on generalization performance on standard machine learning benchmarks. It overcomes constraints on compression operators and supports high compression ratios. The evaluation of the algorithm emphasizes test-set performance rather than just training performance. In previous work, various studies focused on training performance. Two scenarios are studied: training in a challenging peer-to-peer setting and training in a datacenter setting with decentralized communication patterns for better scalability. Speed-ups for CHOCO-SGD over decentralized baselines are shown with less communication overhead. In a datacenter setting, decentralized communication patterns allow better scalability than centralized approaches. CHOCO-SGD can improve time-to-accuracy on large tasks like ImageNet training. However, decentralized algorithms encounter difficulties when scaling to a larger number of nodes and often do not reach the same performance as centralized approaches. Communication efficient CHOCO-SGD can improve time-to-accuracy on large tasks like ImageNet training. Decentralized algorithms face challenges when scaling to a larger number of nodes and often do not perform as well as centralized approaches. These findings highlight deficiencies in current decentralized training schemes and call for further research in this area. The study highlights deficiencies in current decentralized training schemes and calls for further research in this area. CHOCO-SGD converges at a rate on non-convex smooth functions, matching centralized baselines with exact communication for linear speedup. CHOCO-SGD converges at a rate O 1 / \u221a nT + n /(\u03c1 4 \u03b4 2 T ) on non-convex smooth functions, matching centralized baselines with exact communication for linear speedup. A version with momentum is analyzed for practical performance on on-device training over a peer-to-peer social network, reducing bandwidth requirements. CHOCO-SGD with momentum is analyzed for practical performance in on-device training over a social network and in a datacenter setting for training deep learning models. Decentralized schemes are investigated for scalability to larger nodes with shared difficulties noted. In a datacenter setting, decentralized schemes are explored for training deep learning models with reduced bandwidth requirements. The performance of these schemes when scaling to a larger number of nodes is systematically investigated, highlighting shared difficulties faced by current decentralized learning approaches. Various methods, such as gradient compression, have been proposed for training in communication restricted settings. Various methods have been proposed for training in communication restricted settings, including decentralized schemes, gradient compression, asynchronous methods, and performing multiple local SGD steps before averaging. These approaches aim to address the challenges faced by current decentralized learning approaches. In this paper, the focus is on combining decentralized SGD schemes with gradient compression, particularly emphasizing approaches based on gossip averaging. This approach addresses challenges in decentralized learning over decentralized data, as studied in federated learning literature for centralized algorithms. This paper advocates for combining decentralized SGD schemes with gradient compression, focusing on gossip averaging approaches. Convergence rates depend on the spectral gap of the mixing matrix. Lian et al. (2017) combine SGD with gossip averaging, showing convergence at a specific rate. Communication compression with quantization has been popularized in the deep learning community by reported successes in et al., 2003; Xiao & Boyd, 2004; Boyd et al., 2006) whose convergence rate typically depends on the spectral gap \u03c1 \u2265 0 of the mixing matrix. Lian et al. (2017) combine SGD with gossip averaging and show convergence at the rate O 1 / \u221a nT + n /(\u03c1 2 T ). The leading term in the rate, O 1 / \u221a nT, is consistent with the convergence of the centralized mini-batch SGD (Dekel et al., 2012) and the spectral gap only affects the asymptotically smaller terms. Similar results have been observed very recently for related schemes (Scaman et al., 2017; Koloskova et al., 2019; Yu et al., 2019). Communication compression with quantization has gained popularity in deep learning, with successes reported in various studies. Theoretical guarantees have been established for schemes with unbiased compression, and extended to biased compression as well. Schemes with error correction have shown to work best in practice and provide the best theoretical results. Recently, decentralized optimization with quantization has been studied in combination with proximal updates and variance reduction. Gossip averaging has been observed to potentially diverge in this context. Decentralized optimization with quantization has been studied with proximal updates and variance reduction. Gossip averaging may diverge due to quantization noise, but adaptive schemes have been proposed to address this issue. Decentralized optimization with quantization can diverge in the presence of noise. Reisizadeh et al. (2018) proposed an algorithm that converges slower. Adaptive schemes with higher communication cost have been suggested to improve convergence. Tang et al. (2018) introduced DCD and ECD algorithms for deep learning applications. Adaptive schemes with increasing compression accuracy have been proposed to improve convergence, albeit at a higher communication cost. Tang et al. (2018) introduced DCD and ECD algorithms for deep learning applications, while the CHOCO-SGD algorithm can handle high compression rates. Koloskova et al. (2019) analyzed the algorithm for convex functions, showing a convergence rate for non-convex functions as well. The CHOCO-SGD algorithm introduced by Koloskova et al. (2019) can handle high compression rates and has been analyzed for convex functions. For non-convex functions, a convergence rate is shown with a compression quality parameter \u03b4 > 0. In comparison, Tang et al. (2019a) introduced DeepSqueeze, another method for arbitrary compression ratio convergence. In experiments, CHOCO-SGD achieves higher test accuracy under the same tuning conditions. In experiments, CHOCO-SGD achieves higher test accuracy compared to DeepSqueeze, an alternative method for arbitrary compression ratio convergence introduced by Tang et al. (2019a). The decentralized optimization problem and the gossip-based stochastic optimization algorithm CHOCO-SGD are formally introduced in this section. In this section, the decentralized optimization problem, compression operators, and the gossip-based stochastic optimization algorithm CHOCO-SGD are formally introduced for distributed optimization across n nodes with local data distributions and communication limited to local neighbors in a weighted graph. The decentralized optimization problem involves sampling data across n nodes with local distributions and communication limited to local neighbors in a weighted graph. Communication is restricted to local neighbors defined by a weighted graph, with positive weights assigned to edges based on local node degrees. This ensures positive weights and allows for easy local computation. Compression is achieved by transmitting compressed messages using compression operators, with weights set based on local node degrees to ensure positive weights and easy local computation. The weights for compression operators can be computed locally on each node, aiming to transmit compressed messages. These operators do not need to be unbiased, allowing for a wider range of compression methods. Examples of such operators can be found in literature. Compression operators, unlike quantization operators, do not need to be unbiased, allowing for a broader range of compression methods. CHOCO-SGD algorithm involves each worker updating its private variable using stochastic gradient and gossip averaging steps to preserve iterates' averages. The CHOCO-SGD algorithm involves workers updating their private variables using stochastic gradient and gossip averaging steps to preserve iterates' averages, even in the presence of quantization noise. Communication between nodes is done using compressed updates. The CHOCO-SGD algorithm involves nodes communicating with neighbors to update variables using compressed updates. The publicly available copies of private variables are used for communication, allowing for parallel execution of communication and gradient computation. The CHOCO-SGD algorithm allows nodes to update variables using compressed updates by communicating with neighbors. Publicly available copies of private variables are used for communication, enabling parallel execution of communication and gradient computation. Additionally, a momentum-version of CHOCO-SGD is proposed in Algorithm 2. The CHOCO-SGD algorithm enables nodes to update variables using compressed updates by communicating with neighbors. A momentum-version of CHOCO-SGD is proposed in Algorithm 2, extending the analysis to non-convex problems with technical assumptions on bounded variance of stochastic gradients. The CHOCO-SGD algorithm extends analysis to non-convex problems with technical assumptions on bounded variance of stochastic gradients, showing asymptotic convergence. Linear speed-up compared to SGD on a single node is observed, with compression and graph topology affecting higher order terms. The CHOCO-SGD algorithm converges asymptotically with a consensus averaging scheme. It shows linear speed-up compared to SGD on a single node, with compression and graph topology affecting higher order terms. Experimental comparisons with relevant baselines are conducted, leveraging momentum in all algorithms. The CHOCO-SGD algorithm is compared to relevant baselines using compression operators. Momentum is leveraged in all algorithms, with a newly developed momentum version of CHOCO-SGD provided in Algorithm 2. Algorithm 2 CHOCO-SGD with Momentum input: weight decay factor \u03bb, momentum factor \u03b2, local momentum memory v i := 0 \u2200i \u2208 [n]. Setup: ring topology with n = 8 nodes, training ResNet20 on Cifar10 dataset. Training data split between workers and shuffled after every epoch. For the first set of experiments, a ring topology with 8 nodes is used to train ResNet20 on the Cifar10 dataset. Training data is split between workers and shuffled after every epoch. Various optimization algorithms with momentum are implemented, including DCD, ECD, DeepSqueeze, and CHOCO-SGD. In the experiments, various optimization algorithms with momentum are implemented, including DCD, ECD, DeepSqueeze, and CHOCO-SGD. The momentum factor is set to 0.9 without dampening, and the learning rate is fine-tuned and warmed up gradually. Learning rate decay and training stoppage are also implemented, with hyper-parameter tuning for CHOCO-SGD and DeepSqueeze. Compression schemes are applied to every layer of ResNet20 separately, with the top-1 test accuracy evaluated on every node over the dataset. Two unbiased compression schemes are implemented, including gsgd b quantization for rounding weights to b-bit. Hyper-parameter tuning details can be found in Appendix F. Compression schemes are applied to every layer of ResNet20 separately, with top-1 test accuracy evaluated on every node over the dataset. Two unbiased compression schemes are implemented: gsgd b quantization for rounding weights to b-bit, and random a sparsification for preserving a fraction of weights. Additionally, two biased compression schemes are used: top a selects weights with the largest magnitude and sets others to zero. Compression schemes for ResNet20 layers include quantization that rounds weights to b-bit representations, random sparsification preserving a fraction of weights, top a selecting weights with the largest magnitude, and sign compression scaling weights by their sign. These schemes have been analyzed for unbiased quantization, with DCD and ECD only evaluated in combination with unbiased schemes. The compression schemes for ResNet20 layers involve quantization, random sparsification, top-k selection, and sign compression. These schemes have been analyzed for unbiased quantization, with DCD and ECD only evaluated in combination with unbiased schemes. The combination with biased schemes is not supported by theory, but unbiased compression schemes can be scaled down to meet specifications. Results from the experiments show that unbiased compression schemes ECD and DCD perform well at small compression ratios but may diverge at high ratios, consistent with previous theoretical and experimental findings. Results from experiments show that ECD and DCD perform well at small compression ratios but may diverge at high ratios, consistent with previous findings. DCD with biased top sparsification outperforms unbiased random counterpart. CHOCO-SGD generalizes well with minimal accuracy drop. Sign compression achieves state-of-the-art accuracy. In challenging real-world scenarios, decentralized methods are necessary as centralized approaches fail due to local training data on each device. In challenging real-world scenarios, decentralized methods are necessary as centralized approaches fail due to local training data on each device. State-of-the-art accuracy is achieved with approximately 32\u00d7 less bits per weight than the full precision baseline. Focus is now on intrinsically decentralized scenarios like sensor networks, mobile devices, or hospitals where each device has access only to locally stored data, limited communication bandwidth, and unknown global network topology. In decentralized scenarios like sensor networks or mobile devices, each device has limited access to local data, communication bandwidth, and unknown network topology. Privacy is maintained by keeping training data private on each device. Training data is permanently split between nodes to simulate this setting. In decentralized scenarios, each device has limited access to local data and unknown network topology. Privacy is maintained by keeping training data private on each device. The training data is permanently split between nodes to simulate this setting, enabling privacy and data decentralization. In decentralized scenarios, each device has limited access to local data and unknown network topology. Privacy is maintained by keeping training data private on each device. The data is never shuffled between workers during training, and every node has a distinct part of the dataset. The study focuses on decentralized deep learning with sign compression on the Cifar10 dataset. Centralized gathering methods like all-reduce are not efficiently implementable in this setting, so the comparison is made with a centralized baseline where all nodes route their updates to a central coordinator for aggregation. The study compares CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Scaling properties of CHOCO-SGD are analyzed by training on 4, 16, 36, and 64 nodes. Centralized gathering methods like all-reduce are not efficient in decentralized settings. For comparison, CHOCO-SGD with sign compression is considered along with decentralized SGD and centralized SGD without compression. The scaling properties of CHOCO-SGD are studied on 4, 16, 36, and 64 nodes using different network topologies. Learning rates are adjusted separately for each method, with consensus learning rate tuned for CHOCO-SGD. The study compares decentralized algorithms on ring and torus topologies with different spectral gaps. Parameters are listed in table 2. Learning rates are kept constant and tuned separately for each method. Results show CentralizedSGD performs well for all node numbers, while CHOCO-SGD slows down due to graph topology influence. The study compares decentralized algorithms on ring and torus topologies with different spectral gaps. CentralizedSGD performs well for all node numbers, while CHOCO-SGD slows down due to graph topology and communication compression. The performance degradation is due to slower convergence, not a generalization issue. In a comparison of decentralized algorithms on ring and torus topologies with different spectral gaps, it was found that increasing the number of epochs improves performance but does not fully close the gap between centralized and decentralized algorithms. The performance degradation is attributed to slower convergence, not a generalization issue. In the real decentralized scenario, the goal is to reduce communication to save on mobile data costs. Fixing the transmitted bits to 1000 MB, CHOCO-SGD showed the best testing accuracy despite slight degradation with more nodes. Torus topology is recommended for large node numbers due to its good mixing. CHOCO-SGD outperforms in testing accuracy with a fixed 1000 MB transmission, showing slight degradation with more nodes. Torus topology is recommended for large networks due to good mixing properties, while both Decentralized and Centralized SGD require more bits for reasonable accuracy. Experiments on a Real Social Network Graph show that both Decentralized and Centralized SGD require significantly more bits for reasonable accuracy. The study involves training models on user devices connected by a real social network using the Davis Southern women social network with 32 nodes. Experiments on a Real Social Network Graph involved training ResNet20 and LSTM models on user devices connected by the Davis Southern women social network. The models were trained on the Cifar10 dataset for image classification and WikiText-2 for language modeling. The study involved training ResNet20 and LSTM models on a real social network graph for image classification and language modeling tasks. The decentralized algorithm outperformed the centralized and quantized decentralized algorithms in terms of training accuracy, with the highest test accuracy achieved. The study compared the performance of decentralized, centralized, and quantized decentralized algorithms for image classification. Decentralized algorithm had the best training accuracy, while centralized had the highest test accuracy. CHOCO-SGD outperformed exact decentralized scheme in test accuracy with less transmitted data. CHOCO-SGD outperforms exact decentralized scheme in test accuracy with less transmitted data, but does not reach the same level of accuracy as baselines after the same number of epochs. CHOCO-SGD also outperforms centralized SGD in test perplexity for language modeling task. CHOCO-SGD outperforms centralized SGD in test perplexity for language modeling task and performs best in terms of perplexity for a fixed data volume. The benefits of CHOCO-SGD are further pronounced when scaling to more nodes in large-scale training scenarios. In large-scale training scenarios with 8 nodes, CHOCO-SGD using \"Sign+Norm\" quantization scheme outperforms both decentralized and centralized algorithms. Decentralized optimization methods can address scaling issues even in well-connected datacenters with fast network connections. Decentralized optimization methods can address scaling issues in well-connected datacenters with fast network connections. Lian et al. (2017) and Assran et al. (2019) have shown that decentralized schemes can outperform centralized ones, with impressive speedups for training on multiple GPUs. Their algorithms differ from CHOCO-SGD with asynchronous gossip updates and exact communication. Assran et al. (2019) achieved significant speedups in training on 256 GPUs with their algorithm, which includes asynchronous gossip updates and time-varying communication topology. Their approach differs from CHOCO-SGD and shows promise for future combination. The experiments were conducted on 8 machines using ImageNet-1k with Resnet-50. In our experiments, we trained ImageNet-1k with Resnet-50 on 8 machines with 4 Tesla P100 GPUs each. Communication within a machine is fast, using all-reduce, while decentralized communication with compressed communication is used between machines in a ring topology. The mini-batch size on each GPU is 128. In the experiments, ImageNet-1k was trained with Resnet-50 on 8 machines with 4 Tesla P100 GPUs each. Communication within a machine is fast using all-reduce, while decentralized communication with compressed communication is used between machines in a ring topology. The mini-batch size on each GPU is 128, following the general SGD training scheme. The results show that CHOCO-SGD benefits from its decentralized and parallel structure. CHOCO-SGD, based on the SGD training scheme in (Goyal et al., 2017), shows decentralized and parallel benefits with less time and a slight accuracy loss compared to All-reduce. The consensus stepsize for CHOCO-SGD was not heavily tuned due to computational limitations. Speedup in time per epoch does not match that of (Assran et al., 2019) due to different hardware used. CHOCO-SGD offers decentralized and parallel benefits with a slight 1.5% accuracy loss compared to All-reduce. It demonstrates a 20% time-wise gain over the common all-reduce baseline on commodity hardware. Integration with other schemes like (Assran et al., 2019) could further improve training efficiency. The CHOCO-SGD algorithm enables decentralized deep learning training in bandwidth-constrained environments, offering a 20% time-wise gain over common all-reduce methods on commodity hardware. It shows theoretical convergence guarantees in non-convex settings and linear speedup with the number of nodes. Empirical studies validate its performance on image classification tasks. The CHOCO-SGD algorithm enables decentralized deep learning training in bandwidth-constrained environments, offering theoretical convergence guarantees and linear speedup with the number of nodes. Empirical studies validate its performance on image classification tasks like ImageNet-1k, Cifar10, and language modeling tasks like WikiText-2. Our main contribution is enabling training in strongly communication-restricted environments while respecting the constraint of locality of training data. Decentralized schemes show performance for high communication compression and data-locality, expanding the reach of decentralized deep learning applications. In this section, the proof of Theorem 4.1 is presented, demonstrating the performance of decentralized schemes for high communication compression and data-locality in training environments. The proof follows a structure similar to Koloskova et al. (2019), showing that Algorithm 1 is a special case of a more general class of algorithms. The proof of Theorem 4.1 demonstrates the convergence of decentralized algorithms with high communication compression and data-locality. It follows a structure similar to Koloskova et al. (2019), showing Algorithm 1 as a special case of a more general class of algorithms. Decentralized algorithms like Algorithm 1 involve stochastic gradient updates and averaging among nodes, with convergence shown for this type of algorithms. Linear convergence of the averaging scheme used in CHOCO-SGD has been demonstrated, and matrix notation is used for convenience in this subsection. Decentralized algorithms involve stochastic gradient updates and averaging among nodes, with demonstrated linear convergence of the averaging scheme used in CHOCO-SGD. The specific averaging scheme's convergence rate is estimated based on matrix notation, as shown in Algorithm 3 for decentralized SGD with arbitrary averaging. Decentralized algorithms involve stochastic gradient updates and averaging among nodes, with linear convergence of the averaging scheme. Algorithm 3 presents DECENTRALIZED SGD WITH ARBITRARY AVERAGING SCHEME, including assumptions and an example of Exact Averaging. The Exact Averaging algorithm converges at rate \u03c1, defined by the eigengap of mixing matrix W. The D-PSGD algorithm is recovered by substituting the exact averaging algorithm into Algorithm 3. To recover CHOCO-SGD, CHOCO-GOSSIP is chosen as the consensus averaging scheme. The Algorithm 3 recovers D-PSGD algorithm and CHOCO-SGD by choosing CHOCO-GOSSIP as the consensus averaging scheme. The order of communication and gradient computation parts can be exchanged without affecting convergence rate. The order of communication and gradient computation parts in Algorithm 3 can be exchanged without affecting convergence rate. This change does not impact the convergence rate but allows for independent and parallel execution. The proof of Theorem 4.1 shows that the iterates of Algorithm 3 with constant stepsize satisfy certain conditions. The proof of Lemma A.1 shows that the iterates of Algorithm 3 with constant stepsize \u03b7 satisfy certain conditions. These calculations are similar to those in the proof of Lemma 21 from Koloskova et al. (2019). The recursion verifying that rt \u2264 \u03b7^2/4Ac^2 completes the proof. Under Assumptions 1-3 with constant stepsize \u03b7 = nT+1 for T \u2265 64nL^2, the averaged iterates of Algorithm 3 satisfy certain conditions related to convergence rate. The convergence rate for exact averaging with W gives a rate of O(1/\u221anT + n/(T\u03c1^2)), showing a linear speedup compared to SGD on one node. The averaged iterates of Algorithm 3 with constant stepsize \u03b7 satisfy certain conditions related to convergence rate. The convergence rate for exact averaging with W gives a rate of O(1/\u221anT + n/(T\u03c1^2)), showing a linear speedup compared to SGD on one node. CHOCO-SGD with the CHOCO-GOSSIP averaging scheme converges at a rate dependent on the eigengap of the mixing matrix W. The convergence rate of CHOCO-SGD with the CHOCO-GOSSIP averaging scheme is analyzed, showing a dependency on the eigengap of the mixing matrix W. The rate is worse than in the exact case, possibly due to proof techniques or support for high compression. The proof involves estimating terms using L-smoothness and bounding the third term with Lemma A.1. The theorem provides guarantees for the averaged vector of parameters in a decentralized setting, where it may be costly or impossible to average all parameters across multiple machines. This is due to the large number of machines and model size. The theorem gives guarantees for the averaged vector of parameters in a decentralized setting, where it may be costly or impossible to average all parameters across multiple machines. Similar guarantees on individual iterates can be obtained as shown in previous research. Theorem A.4 provides convergence guarantees for Algorithm 3 in a decentralized setting with constant step sizes. The convergence rate is determined by the underlying averaging scheme, and it holds for any number of iterations T. Theorem A.4 guarantees convergence of Algorithm 3 in a decentralized setting with constant step sizes. The convergence rate is determined by the underlying averaging scheme and holds for any number of iterations T. Theorem A.4 guarantees convergence of Algorithm 3 in a decentralized setting with constant step sizes, holding for any number of iterations T. The rate of convergence is determined by the averaging scheme, with the first term being worse than in Theorem A.2 due to \u03c3 2 being smaller than G 2. Corollary A.5 states the convergence of local weights x (t) i under certain assumptions, with Algorithm 1 converging at a speed of c 2 (T + 1) when \u03b7 = n T +1. Lemma B.1 and B.3 provide inequalities for arbitrary sets of vectors and matrices A, B \u2208 R n\u00d7d. Algorithm 1 with \u03b7 = n T +1 converges at the speed c 2 (T + 1) under certain assumptions. Lemma B.1 provides inequalities for arbitrary sets of vectors, while Algorithm 4 CHOCO-SGD involves stochastic gradient updates. Algorithm 4 CHOCO-SGD involves stochastic gradient updates for the sum of two matrices A, B \u2208 R n\u00d7d in Frobenius norm. It can be interpreted as an error feedback algorithm and combined with weight decay and momentum as shown in Algorithm 2. Nesterov momentum can also be adapted for decentralized settings. CHOCO-SGD, a stochastic gradient update algorithm for matrices A, B \u2208 R n\u00d7d, can be combined with weight decay and momentum. Nesterov momentum can be adapted for decentralized settings. CHOCO-SGD works as an error feedback algorithm, saving quantization errors into internal memory for the next iteration. The value transmitted is the difference x (t) i \u2212 x (t\u22121) i, representing the evolution of local variable x i at step t. Algorithm 4 describes error feedback algorithms like CHOCO-SGD, where quantization errors are saved in internal memory and added to the compressed value in the next iteration. The value transmitted is the difference x (t) i \u2212 x (t\u22121) i, representing the evolution of local variable x i at step t. Internal memory is updated to correct errors before compressing the value. Model training and hyper-parameter tuning procedures are detailed in this section. In this section, the model training and hyper-parameter tuning procedures are detailed. The comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM. The comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM for a language modeling task on WikiText-2. The study involved training a three-layer LSTM architecture for a language modeling task on WikiText-2, following the experimental setup of Merity et al. (2017). Key details include using a three-layer LSTM with a hidden dimension size of 650, setting BPTT length to 30, fine-tuning gradient clipping to 0.4, applying dropout of 0.4 only on the output of LSTM, and training for 300 epochs. The experimental setup involves using a three-layer LSTM with a hidden dimension size of 650, setting BPTT length to 30, fine-tuning gradient clipping to 0.4, applying dropout of 0.4 only on the output of LSTM, training for 300 epochs, and using a per node mini-batch size of 32 for both datasets. The learning rate of CHOCO-SGD follows a linear scaling rule proportional to the node degree, with momentum applied only on ResNet20 training. The learning rate of CHOCO-SGD is scaled linearly with the node degree, with momentum applied only during ResNet20 training. The initial learning rate is gradually warmed up from 0.1 to the fine-tuned value for the first 5 epochs, then decayed by a factor of 10 at 50% and 75% of total training epochs. The optimal learning rate per sample is determined during training. During training, the learning rate is adjusted from a small value to the fine-tuned initial rate for the first 5 epochs. It is then decayed by a factor of 10 at 50% and 75% of total epochs. The optimal learning rate per sample is found using a linear scaling rule based on node degree and mini-batch size. The search for the optimal learning rate is done in a predefined grid to ensure best performance. The learning rate per sample is determined by a linear scaling rule based on node degree and mini-batch size. The optimal learning rate is searched in a predefined grid to ensure best performance. Fine-tuned hyperparameters for training ResNet-20 on Cifar10 and a social network topology are presented in tables. The fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20 on Cifar10 and a social network topology are presented in tables, including the per node mini-batch size and maximum degree of the node. The training data is split between nodes with a fixed partition and no shuffling during training. The hyperparameters of CHOCO-SGD for training ResNet-20 on a social network topology with 32 nodes are presented. The training data is split between nodes with a fixed partition and no shuffling during training. The per node mini-batch size is 32, and the maximum degree of the node is 14. The training data is split between nodes with a fixed partition and no shuffling during training. The per node mini-batch size is 32, and the maximum degree of the node is 14. Additionally, plots for training accuracy and test accuracy are provided in Figures 8 and 9. The local mini-batch size is 32, and plots for training and test accuracy are provided in Figures 8 and 9. The local models reach consensus towards the end of optimization, with individual test performances matching the averaged model. The models diverge before decreasing stepsize at epoch 225, only converging when the stepsize decreases. Before decreasing the stepsize at epoch 225, local models diverge from the averaged model, only converging when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}