{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework is proposed in this paper to automatically standardize non-systematic names to systematic names using spelling error correction, byte pair encoding tokenization, and neural networks. The framework proposed in this paper aims to automatically standardize non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and neural sequence to sequence model. The standardization accuracy on the test dataset is 54.04%, a significant improvement compared to previous results, with over 100 million named chemical substances worldwide. Our framework utilizes byte pair encoding tokenization and a neural sequence to sequence model to standardize chemical names. The accuracy on the test dataset is 54.04%, a significant improvement from previous results. There are over 100 million named chemical substances globally, each with systematic names assigned based on their structures by IUPAC. The International Union of Pure and Applied Chemistry (IUPAC) defines rules for assigning systematic names to chemical substances. However, besides systematic names, chemicals also have common or trivial names for simplicity. For example, sucrose is commonly known as sugar. In addition to systematic names assigned by IUPAC, chemical substances may have common or trivial names for simplicity and differentiation in the industry. For example, sucrose is commonly known as sugar. Chemical substances can have multiple names, including systematic, common, and proprietary names. Proprietary names are often used in the pharmaceutical industry to distinguish products. Chemical information extraction involves converting chemical knowledge in text into a database. Chemical substances can have various names, including proprietary names used in the pharmaceutical industry. Chemical information extraction converts chemical knowledge into databases like PubChem and SciFinder, which store information such as chemical names, structures, and formulas. Chemical databases like PubChem and SciFinder rely on standard chemical names to store information such as chemical names, structures, and formulas. Extracting chemical information from papers is ongoing work to update these databases. Using systematic names makes it easier to generate other representations like SMILES and International Chemical Identifier. The ongoing work involves extracting chemical information from papers to update databases like PubChem and SciFinder. Using systematic names makes it easier to generate representations like SMILES and International Chemical Identifier (InCHI). OPSIN is a system developed for converting systematic names to SMILES with high precision. The text discusses the importance of using systematic names in chemical information extraction to generate SMILES and InCHI representations. OPSIN is a system known for converting systematic names to SMILES accurately. Error types in non-systematic names include spelling and ordering errors. Error types in non-systematic names include spelling, ordering, common name, and synonym errors when compared to systematic names. Error types in non-systematic names include incorrect order of groups, common name errors, and synonym errors where words differ but share the same root. Multiple error types can occur simultaneously. The most common error type in non-systematic names is the synonym error, where words share the same root. Multiple error types can occur simultaneously, making the task challenging. A framework is proposed to automatically convert non-systematic names to systematic names, starting with spelling error correction. The framework proposed aims to automatically convert non-systematic names to systematic names by correcting spelling errors, tokenizing names, and using a sequence to sequence model to fix ordering, common name, and synonym errors. This task is challenging due to the mixed types of errors that can occur simultaneously. The text discusses the challenges of chemical name standardization and mentions the limited work done in this area. It highlights the BID2 system as the only work deserving citation for developing an online system called ChemHits. The system relies heavily on chemical knowledge, which limits its application potential and effectiveness. Our work adopts a sequence to sequence model for chemical name standardization, different from the BID2 system which relies on chemical knowledge. The sequence to sequence model is similar to machine translation, with source and target languages corresponding to non-systematic and systematic names. Our approach utilizes a sequence to sequence model for chemical name standardization, similar to machine translation. The model is trained end-to-end without external chemical knowledge, achieving an accuracy of 54.04%. Our framework for chemical name standardization is fully data-driven and achieves an accuracy of 54.04% on the test data set. The corpus used contains chemical names from high-impact Chemical Journals, including both non-systematic and systematic names. Our work achieves 54.04% accuracy on a corpus of chemical names from high-impact Chemical Journals. The corpus includes non-systematic and systematic names, totaling 384,816 data pairs. The distribution of Levenshtein distance between the names is shown in FIG1. In the experiment, 80%, 19%, and 1% of the data are used as training, test, and development sets respectively to correct spelling errors in chemical substance names by separating them into elemental words. In the experiment, different datasets were used for training, testing, and development sets to correct spelling errors in chemical substance names by separating them into elemental words. The process involved creating vocabularies for systematic and non-systematic elemental words. The experiment involved creating vocabularies for systematic and non-systematic elemental words to correct spelling errors in chemical substance names. The non-systematic names were used to build an elemental vocabulary, keeping only the words that appeared frequently. The experiment involved creating vocabularies for systematic and non-systematic elemental words to correct spelling errors in chemical substance names. The non-systematic names were used to build an elemental vocabulary, keeping only the words that appeared frequently. The final elemental vocabulary was structured using BK-Tree for efficient correction searches. The experiment involved creating vocabularies for systematic and non-systematic elemental words to correct spelling errors in chemical substance names. The final elemental vocabulary was structured using BK-Tree for efficient correction searches, based on Levenshtein distance. BK-Tree is defined with a root node and subtrees based on Levenshtein distance. It quickly corrects spelling errors in non-systematic names and allows easy insertion of new data, making it scalable. BK-Tree corrects spelling errors in non-systematic names by finding vocabulary items with the smallest Levenshtein distance. It allows easy insertion of new data, making it scalable. BK-Tree corrects spelling errors in non-systematic names by finding vocabulary items with the smallest Levenshtein distance. It allows easy insertion of new data, making it scalable. Given a chemical substance name, it is separated into elemental words and inputted into the BK-Tree for correction. This process helps in training the sequence to sequence model by reducing noise in elemental words. BK-Tree corrects spelling errors in non-systematic names by finding vocabulary items with the smallest Levenshtein distance. It helps in training the sequence to sequence model by reducing noise in elemental words. Example of BK-Tree built from dataset shown in Figure 4. The Levenshtein distance between nodes in a subtree is the same. Chemical names are tokenized using Byte Pair Encoding (BPE) BID11 for sequence-to-sequence model application. Symbol set is initialized with single characters for tokenization. The sequence-to-sequence model requires tokenization of chemical names using Byte Pair Encoding (BPE) BID11. The symbol set is initialized with single characters and iteratively merged to create new symbols. The final symbol set size is determined by the initial character size and merge operations. Trained symbol vocabulary set is then used for tokenization. The Byte Pair Encoding (BPE) method is used for tokenization of chemical names in a sequence-to-sequence model. It starts with single characters, counts symbol pairs, merges the most frequent pairs, and adds them to the symbol set. The final symbol set size depends on the initial character size and merge operations. BPE helps with out-of-vocabulary problems and separates names into meaningful subwords by finding frequently appearing small molecules in the corpus. The trained symbol vocabulary set is used for tokenization, with BPE chosen for its ability to handle out-of-vocabulary issues and separate names into meaningful subwords. Examples of applying BPE to chemical names are provided in TAB1. The tokenized pairs are then used to train a sequence-to-sequence model (BID12) for machine translation. After tokenizing chemical names using BPE, split pairs are used to train a sequence-to-sequence model, which consists of an encoder and decoder neural network for machine translation. This model is adapted from OpenNMT with some modifications. In this work, an existing implementation of OpenNMT BID5 was adapted with modifications to create a sequence-to-sequence model using two recurrent neural networks: an encoder and a decoder. The encoder utilizes a multilayer bidirectional LSTM (BiLSTM) to process the source sequences and generate a context vector H, while the decoder uses this vector to generate the target sequences. The encoder uses a multilayer bidirectional LSTM to generate a context vector H, while the decoder utilizes this vector to generate target sequences. The encoder uses a multilayer bidirectional LSTM to generate a context vector H. At the spelling error correction stage, the only parameter is the threshold of the BK-Tree, with values tested at 1, 2, and 3. The BPE stage's only parameter is the number of merge operations, with values tested at 2500, 5000, 10000, and 20000. For the sequence to sequence model, parameters include word embeddings and hidden states dimensions of 500, vocabulary size based on basic characters and BPE merge operations, and 2 layers each in encoder and decoder. Spelling error correction is done before training the model. The sequence to sequence model parameters include word embeddings and hidden states dimensions of 500, vocabulary size based on basic characters and BPE merge operations, and 2 layers each in encoder and decoder. Spelling error correction is done before training the model. During training, all parameters are trained jointly using stochastic gradient descent with a cross-entropy loss function computed over minibatches of size 64. During training, the sequence to sequence model parameters are trained jointly using stochastic gradient descent with a cross-entropy loss function. The loss is computed over minibatches of size 64 and normalized. The model is trained for 15 epochs with a dropout rate of 0.3 and a beam size of 5. The weights are initialized with a random uniform distribution. The initial learning rate is 1.0 and decays by a factor of 0.5 every epoch after epoch 8 or when the perplexity does not decrease on the validation set. The model is trained for 15 epochs with a dropout rate of 0.3. The beam size for decoding is set to 5. Another experiment replaces the sequence to sequence model with a Statistical Machine Translation (SMT) model using Moses system BID6 and a 3-grams language model with KenLM BID4. Training sequences are limited to a length of 80. In the experiment, the model is trained for 15 epochs with a beam size of 5 for decoding. A comparison is made with a Statistical Machine Translation (SMT) model using Moses system BID6 and a 3-grams language model with KenLM BID4. Training sequences are limited to a length of 80, and data augmentation is used to handle noisy data. The language model uses KenLM BID4 with BPE tokenization and 5000 merge operations. Data augmentation is employed to handle noisy data, specifically spelling errors. Four types of error insertion methods are used with a probability of 0.025 for non-systematic names. In data augmentation, errors are inserted into non-systematic names with a 0.025 probability using four types of insertion methods. These methods include randomly adding, deleting, exchanging, or replacing characters. Standardization quality is measured using accuracy and BLEU score BID10, where accuracy represents the successful standardization of non-systematic names. The experiment measures standardization quality using accuracy and BLEU score BID10. Results show that the combination of spelling error correction, BPE tokenization, and sequence to sequence model performs the best on the test dataset. The framework shows significant improvement. The experiment results on the test dataset show that the combination of spelling error correction, BPE tokenization, and sequence to sequence model achieves the best performance. The framework has a significant improvement compared to other models. The results also demonstrate the usefulness of BPE with 5000 being the optimal value for this parameter. The experiment results show that the framework, combining spelling error correction, BPE tokenization, and sequence to sequence model, outperforms other models. BPE with 5000 merge operations is optimal. Spelling error correction and data augmentation are beneficial, with the former being more effective. The results demonstrate the effectiveness of BPE tokenization, spelling error correction, and data augmentation in improving the framework's performance. Overcorrection due to large thresholds can reduce standardization quality. Examples in Table 6 showcase the capabilities of the sequence to sequence model in standardizing non-systematic names. The sequence to sequence model can successfully standardize non-systematic names by correcting spelling errors, synonym errors, and ordering errors. Examples in Table 6 demonstrate the model's capabilities in fixing these errors. The sequence to sequence model can fix non-alphabet spelling errors, synonym errors, and ordering errors in chemical names. Examples show successful standardization, including correcting names like 1-propanetriol to propane-1-thiol and P-anise alcohol to its systematic name. Visualization of attentions in an example is provided in FIG2. The sequence to sequence model corrects ordering errors and synonym errors in chemical names. An example of standardization is P-anise alcohol, which is successfully standardized despite looking different from its systematic name. The visualization of attentions in an example is shown in FIG2, where the non-systematic name adenine,9-methyl-(7ci,8ci) is corrected to its systematic name 9-methyl-9H-purin-6-amine. This demonstrates a mixture of common name error and ordering error. The seq2seq model can effectively find the relation between chemical names. The seq2seq model corrects ordering errors and synonym errors in chemical names. An example is the non-systematic name adenine,9-methyl-(7ci,8ci) corrected to its systematic name 9-methyl-9H-purin-6-amine. The model can effectively find the relation between chemical names, as shown in FIG2. The seq2seq model successfully standardizes non-systematic chemical names, such as 4-benzoyl-3-methyl-1-phenyl-4,5-dihydro-1H-pyrazol-5-one. However, there are still failed standardization attempts, with synonym errors being the most common type. In this section, failed standardization attempts of the system are analyzed by randomly selecting 100 samples and labeling their error types. Synonym errors are the most common, while spelling errors are handled well. However, the system struggles with common errors due to the difficulty in finding a rule between unseen common names and systematic names. Out of the 100 samples, 10 are nearly correct and 7 are completely incorrect. Our system performs well at handling spelling errors but struggles with common errors, particularly in finding a rule between unseen common names and systematic names. Out of 100 samples, 10 were nearly correct, 7 were completely incorrect, and the rest were partially correct. Nearly half of the non-systematic names were not successfully standardized. The accuracy of our framework in standardizing systematic names varies based on name length. It performs best for names between 20 and 40 characters but poorly for names over 60 characters, which make up 37% of the test dataset. Chemical rules are not considered in our model, leading to limitations in standardizing some names. Our framework achieves the best performances for systematic names of length between 20 and 40 characters but performs poorly for names over 60 characters. The model does not consider chemical rules, leading to some names being standardized incorrectly. Some names generated by our model disobey chemical rules and subwords generated by BPE are inexplicable. Examples of failed attempts include names like 5-bromo-2-(chlorosulfanyl)toluene and choline dicarbonate. The model's prediction includes various chemical names, with some being nearly correct, partially correct, or totally incorrect. The framework proposed aims to convert non-systematic names to systematic names using spelling error correction, byte pair encoding tokenization, and a sequence to sequence model, achieving an accuracy of 54.04% on the dataset. In this work, a framework is proposed to automatically convert non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. The framework achieves an accuracy of 54.04% on the dataset, significantly outperforming previous rule-based systems. This approach enables the extraction of related chemical information into practical use, starting a new research line in the field. The framework proposed in this work achieves an accuracy of 54.04% on the dataset, outperforming previous rule-based systems by nine times. It is trained end to end, fully data-driven, and independent of external chemical knowledge, enabling practical use of related chemical information extraction. This work initiates a new research line in this field."
}