{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks by preventing overfitting. This paper discusses three novel observations about dropout in DNNs with ReLU activations: 1) it encourages local linear models to be trained on nearby data points, 2) different layers may have varying neural-deactivation rates with a constant dropout rate, and 3) the rescaling factor of dropout can cause inconsistencies between training and testing conditions. The paper discusses three key observations about dropout in DNNs with ReLU activations: 1) it promotes training local linear models on nearby data points, 2) different layers may have varying neural-deactivation rates with a constant dropout rate, and 3) inconsistencies can arise between training and testing conditions due to the rescaling factor of dropout. These findings lead to the proposed method \"Jumpout,\" which samples the dropout rate using a monotone decreasing distribution for improved training. Jumpout is a method that improves dropout in DNNs with ReLU activations by sampling the dropout rate using a monotone decreasing distribution. It also adaptively normalizes the dropout rate at each layer and training sample, ensuring consistent dropout rates for activated neurons. Jumpout improves dropout in DNNs by adaptively normalizing the dropout rate at each layer and training sample, ensuring consistent rates for activated neurons. It also rescales outputs for a better trade-off between variance and mean of neurons, leading to improved performance on CIFAR10, CIFAR100, and Fashion-MNIST datasets. Jumpout improves dropout in DNNs by normalizing the dropout rate at each layer and training sample, ensuring consistent rates for activated neurons. It also rescales outputs for a better trade-off between variance and mean of neurons, leading to improved performance on various datasets. Deep learning has achieved success on various machine learning tasks, but overfitting can weaken generalization performance. Dropout is a technique that randomly sets hidden neuron activations to 0, reducing co-adaptation without significant computational overhead. Dropout is a technique in deep learning that randomly sets hidden neuron activations to 0, reducing co-adaptation without significant computational overhead. However, it has drawbacks such as the need to tune dropout rates for optimal performance, which can slow convergence or yield no improvements on generalization performance. Dropout in deep learning involves randomly setting hidden neuron activations to 0 to reduce co-adaptation without significant computational overhead. However, tuning dropout rates for optimal performance can be challenging, as too high a rate can slow convergence and too low a rate may not improve generalization performance. Ideally, dropout rates should be tuned separately for each layer and during various training stages, but in practice, a single dropout rate is often used for all layers throughout training. In practice, a single dropout rate is often used for all layers throughout training to reduce computation. This fixed rate may hinder generalization by ruling out samples with less perturbation that could potentially improve performance. The fixed dropout rate used to generalize DNN to noisy samples may hinder generalization by excluding samples with less perturbation that could potentially improve performance. Varying fractions of activated neurons in layers and samples can lead to too much or too little perturbation. The fixed dropout rate may hinder generalization by excluding samples with less perturbation. Varying fractions of activated neurons can lead to too much or too little perturbation, affecting the effectiveness of dropout and its compatibility with batch normalization. Rescaling undropped neurons to match the original activation gain breaks the consistency of normalization. Dropout's deficiency lies in its incompatibility with batch normalization, as rescaling undropped neurons to match the original activation gain breaks normalization consistency. This conflict often leads to dropout being excluded in favor of batch normalization in modern DNN architectures. The proposed \"jumpout\" modification aims to address the incompatibility of dropout with batch normalization in DNN architectures. Dropout is often excluded in favor of batch normalization due to normalization parameter inconsistencies. \"Jumpout\" offers three simple modifications to improve dropout and enhance generalization performance. The proposed \"jumpout\" modification aims to improve dropout in DNNs with ReLU activations. It addresses incompatibility with batch normalization and offers three simple modifications to enhance generalization performance. The \"jumpout\" modification improves dropout in DNNs with ReLU activations by changing activation patterns, enhancing generalization performance by training linear models to work for data points in nearby polyhedra. Applying dropout to a training sample changes its ReLU activation patterns and underlying polyhedral structure, improving generalization performance by training linear models to work for data points in nearby polyhedra. The number of units dropped out is typically np on a layer with n units and a fixed dropout rate of p. Dropout improves generalization performance by training linear models to work for data points in nearby polyhedra. The typical number of units dropped out is np on a layer with n units and a fixed dropout rate of p. Each linear model is smoothed to work on data points at a typical distance away, but the probability of smoothing over closer distances is smaller, not achieving local smoothness. In jumpout, this issue is addressed. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, ensuring a higher probability of smaller dropout rates. This approach aims to achieve local smoothness in training linear models. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, ensuring a higher probability of smaller dropout rates. This approach aims to achieve local smoothness in training linear models by decreasing the probability of smoothing polyhedra as points move farther away. Additionally, dropout results in varying fractions of activated neurons in different layers, samples, and training stages. In jumpout, the effective neural-deactivation rate is consistent by adaptively normalizing the dropout rate for each layer and training sample. The fraction of activated neurons can vary significantly in dropout due to dropping out already quiescent neurons. In jumpout, the effective neural-deactivation rate is kept consistent by adaptively normalizing the dropout rate for each layer and training sample. The incompatibility problem between dropout and BN is addressed by rescaling the outputs of jumpout to maintain variance, allowing BN layers learned in training to be directly applied in the test phase. In jumpout, the outputs are rescaled to maintain variance, allowing BN layers learned in training to be directly applied in the test phase without inconsistency. Jumpout randomly generates a 0/1 mask over hidden neurons for deactivation, similar to dropout, without requiring extra training. Jumpout, similar to dropout, randomly generates a 0/1 mask over hidden neurons for deactivation without requiring extra training. It can be easily implemented and incorporated into existing architectures with only a minor modification to dropout code. In experiments on various benchmark datasets, jumpout shows similar memory and computation costs as dropout but consistently outperforms it on a variety of tasks. Jumpout, like dropout, randomly generates a 0/1 mask over hidden neurons for deactivation with minimal modification to existing architectures. In experiments on benchmark datasets, jumpout has similar memory and computation costs as dropout but consistently outperforms it on various tasks. Previous approaches have also addressed the fixed dropout rate issue, such as BID1's \"standout\" method for adaptive dropout rates. Recent work has proposed different methods to generate adaptive dropout rates, such as BID1's \"standout\" method and BID24's extension for learning adaptive dropout rates for different neurons or groups. BID23 also showed a relationship between Rademacher complexity of a DNN and dropout rates, suggesting adaptive changes based on this metric. Jumpout adjusts dropout rates based on ReLU activation patterns, without relying on additional trained models. It introduces minimal computation and memory overhead and can easily be integrated into existing models. BID19 introduced Gaussian dropout as a Gaussian approximation of dropout, optimizing it directly for faster convergence. Variational methods were also applied to extend and study Gaussian dropout. BID9 proposed variational dropout as a generalization of Gaussian dropout. Gaussian dropout, introduced in BID19, is a Gaussian approximation of dropout optimized for faster convergence. Variational methods were used to extend and study Gaussian dropout. BID9 proposed variational dropout as a generalization, connecting global uncertainty with adaptive dropout rates for each neuron. BID11 further extended variational dropout to reduce gradient estimator variance and achieve sparse dropout rates. Other recent dropout variants include Swapout BID16, combining dropout with random skipping connections. In this paper, the focus is on modifications to the original dropout method that do not require additional adjustments. Variants of dropout such as Swapout and Fraternal Dropout have been developed to enhance neural network architectures and bridge the gap between training and testing phases. Jumpout is a modification of the original dropout method that aims to generalize to different neural network architectures without extra training costs or additional parameters. It can be applied alongside other dropout variants and targets various issues of dropout. Jumpout is a modification of the original dropout method that can be applied to various neural network architectures without extra training costs or additional parameters. It targets different issues of dropout and can be used alongside other dropout variants. The deep neural networks described in the paper use a formalization that can represent various architectures, including fully-connected networks with bias terms at each layer. The hidden nodes are denoted as hj and represent the nodes after applying the activation function \u03c8. The DNN formalization presented in the paper can represent various architectures, including fully-connected networks with bias terms at each layer. The hidden nodes, denoted as hj, are the result of applying the activation function \u03c8 to the nodes. The convolution operator is essentially a matrix multiplication, where each row corresponds to applying a convolutional filter on a specific part of the input. The DNNs cover bias terms at each layer and use dummy dimensions on input data. The convolution operator is a sparse weight matrix with tied parameters. Average-pooling is a linear operator, max-pooling is an activation function, and residual networks can be represented as blocks. The weight matrix in DNNs is sparse with tied parameters, and can be represented as a matrix multiplication for average-pooling. Max-pooling functions as an activation function. Residual network blocks can be represented by appending an identity matrix to retain input values. DNNs with shortcut connections can be written in a specific form. In DNNs, residual network blocks can be represented by appending an identity matrix to retain input values. DNNs with shortcut connections can be written in a specific form for piecewise linear activation functions like ReLU. The DNN in Eqn. FORMULA0 can be represented as a piecewise linear function using ReLU activation. The activation pattern for each layer determines the modified weight matrix, resulting in a linear model around a given data point x. The DNN in Eqn. FORMULA0 is transformed into a linear model by eliminating ReLU functions and modifying weight matrices based on activation patterns. The resulting linear model is associated with the activation patterns on all layers for a given data input x. The DNN is transformed into a linear model by eliminating ReLU functions and modifying weight matrices based on activation patterns. The resulting linear model is associated with activation patterns on all layers for a given data input x. This analysis focuses on DNNs with ReLU activations and their piecewise linear property. The analysis focuses on DNNs with ReLU activations and their piecewise linear property. ReLU units are computationally efficient and widely applicable, with dropout improving generalization performance by considering local linear models and nearby convex polyhedra. Dropout improves the generalization performance of DNNs by considering local linear models and nearby convex polyhedra. Three modifications to dropout lead to Jumpout, inspired by the analysis of ReLU units' piecewise linear property. The modifications to dropout lead to Jumpout, inspired by local linear models and convex polyhedra. Dropout prevents neuron co-adaptation and encourages diversity, training smaller networks for ensemble predictions during test. Dropout improves DNN performance by preventing neuron co-adaptation, promoting diversity, and training smaller networks for ensemble predictions during test. It also smooths each local linear model, leading to enhanced generalization performance. Network prediction benefits from treating it as an ensemble of outputs from smaller networks, reducing variance. Dropout improves generalization by smoothing local linear models in deep neural networks with ReLUs, dividing the input space into convex polyhedra. For DNNs with ReLUs, the input space is divided into convex polyhedra, creating local linear models for each training data point. Large DNNs with many neurons per layer can result in exponentially many polyhedra, leading to dispersed training samples and distinct linear models for each data point. Nearby polyhedra may correspond to vastly different linear models. The input space for DNNs with ReLUs is divided into convex polyhedra, resulting in dispersed training samples and distinct linear models for each data point. Nearby polyhedra may correspond to different linear models due to the multiplication of weight matrices. The input space for DNNs with ReLUs is divided into convex polyhedra, resulting in dispersed training samples and distinct linear models for each data point. The linear models are the results of consecutively multiplying weight matrices, where differences in activation patterns can lead to significant variations in the resulting models. This lack of smoothness can make DNNs fragile and unstable on new data, weakening their generalization ability. The linear models of DNNs with ReLUs can vary significantly due to differences in activation patterns, resulting in a lack of smoothness. To address the fragility and instability of DNNs on new data, a dropout rate is proposed to be sampled from a truncated half-normal distribution. To address the fragility and instability of DNNs on new data, a dropout rate is proposed to be sampled from a truncated half-normal distribution with specified limits to ensure effectiveness without compromising performance. The dropout rate is sampled from a Gaussian distribution and truncated to ensure effectiveness. The standard deviation controls generalization enforcement. The Gaussian-based dropout rate distribution encourages smoothness in generalization performance by sampling smaller dropout rates with higher probabilities. This helps training samples contribute more to linear models of closer polyhedra. The Gaussian-based dropout rate distribution promotes smooth generalization performance by sampling smaller dropout rates with higher probabilities. This encourages training samples to contribute more to linear models of closer polyhedra, enhancing performance on points within the polyhedron while diminishing effectiveness for points farther away. The dropout rate for each layer is a hyper-parameter that controls smoothness among nearby local linear models. Tuning dropout rates separately for different layers is ideal. The dropout rate for each layer is a hyper-parameter that controls smoothness among nearby local linear models. Tuning dropout rates separately for different layers is ideal, but it is computationally expensive. One approach is to set a global dropout rate for all layers, which is suboptimal. Setting a single global dropout rate for all layers is a common approach due to the computational expense of tuning multiple hyper-parameters separately. However, this method is suboptimal as the proportion of active neurons in each layer can vary significantly, leading to varying effective dropout rates for active neurons. The proportion of active neurons in each layer can vary significantly, leading to varying effective dropout rates for active neurons. The effective dropout rate of every layer is determined by the fraction of active neurons in that layer and the dropout rate applied. This allows for better control of dropout behavior across different layers and training stages. To better control dropout behavior across different layers and training stages, the dropout rate is normalized by the fraction of active neurons in each layer. This normalization helps achieve a consistent activation pattern and allows for precise tuning of the dropout rate as a single hyper-parameter. In order to achieve consistent activation patterns and precise tuning of dropout rates, the dropout rate is normalized by the fraction of active neurons in each layer. This approach helps in achieving a desirable level of smoothing encouragement by tuning the dropout rate as a single hyper-parameter. In standard dropout, the neurons are scaled by 1/p during training to keep the mean consistent between training and test phases. However, the variance can differ significantly, leading to unpredictable behavior when combined with batch normalization (BN). This highlights the challenge of adapting BN layers to changes in variance from training to test conditions. In combining dropout and batch normalization (BN) in DNNs, the challenge arises from the difference in variance between training and test phases. One approach is to use a linear computational layer followed by a BN layer, ReLU activation layer, and dropout layer. This helps address the issue of adapting BN layers to variance changes during training and testing. In combining dropout and batch normalization in DNNs, a linear computational layer is followed by a BN layer, ReLU activation layer, and dropout layer. Neuron values after ReLU are treated as random variables, contributing to the next layer. Applying dropout in DNNs changes the scales of mean and variance of neurons during training, affecting the values fed into the next layer after ReLU activation. Applying dropout in DNNs changes the scales of mean and variance of neurons during training, affecting the values fed into the next layer after ReLU activation. The dropout layer alters the mean and variance of neurons, leading to inconsistencies in Batch Normalization during testing. Rescaling the output can address this issue. During training, applying dropout in DNNs changes neuron scales, affecting values fed into the next layer. This inconsistency with Batch Normalization during testing can be fixed by rescaling the output to counteract dropout's effects on mean and variance scales. Rescaling factors differ for mean and variance recovery, with consideration for the value of E[w j ]. During training, applying dropout in DNNs changes neuron scales, affecting values fed into the next layer. To recover the original scale of the mean, rescale dropped neurons by (1 \u2212 p j ) \u22121, and for the variance, use (1 \u2212 p j ) \u22120.5 if E(y j ) is small. Consider the value of E[w j ] for scaling undropped neurons by DISPLAYFORM1. Rescaling factors differ for mean and variance recovery, with additional computation and memory cost for w j. During training, applying dropout in DNNs changes neuron scales, affecting values fed into the next layer. To recover the original scale of the mean, rescale dropped neurons by (1 \u2212 p j ) \u22121, and for the variance, use (1 \u2212 p j ) \u22120.5 if E(y j ) is small. Consider the value of E[w j ] for scaling undropped neurons by DISPLAYFORM1. However, computing information about w j requires additional computation and memory cost. Different rescaling factors are needed for mean and variance recovery. During training, dropout in DNNs changes neuron scales. To recover the original scale, rescale dropped neurons by (1 \u2212 p j ) \u22121 for the mean and (1 \u2212 p j ) \u22120.5 for the variance. Different rescaling factors are needed for mean and variance recovery. The rescaling factor (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling. The rescaling factor (1 \u2212 p) \u22120.75 provides a good balance between mean and variance rescaling in DNNs. It ensures that the mean and variance ratios are close to 1, maintaining the original scale of the neurons during training. The rescaling factor (1 \u2212 p) \u22120.75 strikes a balance between mean and variance adjustments in DNNs, ensuring consistency in neuron scale during training. This factor is proposed as a trade-off point between (1\u2212p) \u22121 and (1\u2212p) \u22120.5, making both mean and variance sufficiently consistent. In practice, it is not efficient to compute E(y j) during training, so a rescaling factor of (1 \u2212 p j) \u22120.75 is proposed as a trade-off point between (1\u2212p j) \u22121 and (1\u2212p j) \u22120.5. This factor ensures consistency in neuron scale and makes both mean and variance sufficiently consistent for dropout and non-dropout cases. Comparing the performance of original dropout with dropout using the rescaling factor (1\u2212p j) \u22120.75 is shown in FIG3 and FIG4. In practice, a rescaling factor of (1 \u2212 p j) \u22120.75 is proposed to ensure consistency in neuron scale for dropout and non-dropout cases. Comparing the performance of original dropout with the rescaled dropout is shown in FIG3. The study compares the performance of using dropout with and without Batch Normalization in convolutional networks. It suggests that using dropout with Batch Normalization can enhance performance, especially with larger dropout rates. However, using the original dropout with Batch Normalization leads to a significant decrease in accuracy when the dropout rate exceeds 0.15. In contrast, using a rescaling factor of (1 \u2212 p j) \u22120.75 with dropout shows consistent improvement in performance even with increasing dropout rates. The study compares the performance of using dropout with and without Batch Normalization in convolutional networks. It suggests that using dropout with Batch Normalization can enhance performance, especially with larger dropout rates. However, using the original dropout with Batch Normalization leads to a significant decrease in accuracy when the dropout rate exceeds 0.15. In contrast, using a rescaling factor of (1 \u2212 p j) \u22120.75 with dropout shows consistent improvement in performance even with increasing dropout rates. The proposed improved dropout, called \"Jumpout,\" combines three modifications to overcome the drawbacks of the original dropout and generates a 0/1 mask for input neurons. The proposed improved dropout, \"Jumpout,\" combines three modifications to overcome the drawbacks of the original dropout. Jumpout samples from a decreasing distribution for a random dropout rate and normalizes it adaptively based on active neurons. Jumpout introduces a novel approach to dropout by sampling from a decreasing distribution for a random dropout rate. It also adapts the dropout rate based on active neurons, enforces consistent regularization and generalization effects, and scales the outputs differently during training to synergize with batchnorm operations. Jumpout introduces a novel approach to dropout by sampling from a decreasing distribution for a random dropout rate. It adapts the dropout rate based on active neurons, enforces regularization and generalization effects, and scales the outputs differently during training to synergize with batchnorm operations. Jumpout requires a main hyper-parameter \u03c3 to control the standard deviation of the distribution, and two auxiliary truncation hyperparameters (p min , p max ) to bound the samples. Jumpout introduces a novel dropout approach by sampling from a decreasing distribution for a random dropout rate. It requires a main hyper-parameter \u03c3 to control the standard deviation and two auxiliary truncation hyperparameters (p min , p max ) to bound the samples. The hyperparameters can be tuned, but setting p min = 0.01 and p max = 0.6 has shown consistent performance across datasets and models. Jumpout has three hyperparameters, with \u03c3 being the main one that was tuned for good performance. The input h j is considered as the features of layer j for a data point. For a mini-batch, q + j can be estimated separately for each data point or averaged over all data points. The latter option is preferred for its comparable performance with less computation and memory usage. Jumpout has similar memory cost to original dropout. In practice, the average q + j over data points is used as the estimate for the mini-batch, providing comparable performance with less computation and memory usage. Jumpout has similar memory cost to original dropout, with minimal additional computation required. Jumpout has similar memory cost to original dropout, with minimal additional computation required. In this section, dropout and jumpout are applied to different popular DNN architectures and compared on six benchmark datasets at different scales. These architectures include a small CNN with four convolutional layers applied to CIFAR10, WideResNet-28-10 applied to CIFAR10 and CIFAR100, and the \"pre-activation\" version of ResNet-20. In this study, dropout and jumpout were applied to various popular DNN architectures and compared on six benchmark datasets of different scales. The architectures included a small CNN with four convolutional layers applied to CIFAR10, WideResNet-28-10 applied to CIFAR10 and CIFAR100, \"pre-activation\" version of ResNet-20 applied to Fashion-MNIST, WideResNet-16-8 applied to SVHN and STL10, and ResNet-18 applied to ImageNet. For Fashion-MNIST, CIFAR, and ImageNet datasets, standard settings and pre-trained models were used for experiments. Two copies of a pre-trained ResNet18 model were trained with dropout and jumpout for the same number of epochs on ImageNet. Starting from pre-trained weights was preferred due to the lack of overfitting issues in training DNNs on ImageNet. Details on datasets can be found in TAB3 at Appendix. The existing GitHub repository details the data preprocessing/augmentation and hyperparameters used. Two copies of a pre-trained ResNet18 model were trained with dropout and jumpout on ImageNet. Starting from pre-trained weights was chosen to avoid overfitting issues in DNN training on ImageNet. The experimental results show that jumpout consistently outperforms dropout on all datasets and DNNs tested, even bringing improvements to already high test accuracies on Fashion-MNIST and CIFAR10. Additionally, jumpout is effective on CIFAR100 and ImageNet, where many DNNs are trained. The experimental results in TAB1 demonstrate that jumpout consistently outperforms dropout on all datasets and DNNs tested, including Fashion-MNIST and CIFAR10 where test accuracy is already high. Jumpout also shows effectiveness on CIFAR100 and ImageNet, achieving improvements without the need to increase model size significantly. Jumpout's effectiveness is demonstrated on various datasets and DNNs, including ImageNet, where it achieves significant improvements without the need to increase model size. A thorough ablation study confirms the effectiveness of each modification, with the combination of all three modifications (jumpout) achieving the best performance. The study demonstrates the effectiveness of three proposed modifications, showing that each one improves vanilla dropout. Combining the modifications leads to further enhancements, with the best performance achieved by applying all three together (jumpout). Learning curves and convergence plots of dropout and jumpout equipped DNNs are provided, showing that jumpout outperforms dropout in early learning stages and reaches higher accuracy faster. In FIG5, learning curves and convergence plots of dropout and jumpout equipped DNNs are shown. Jumpout demonstrates advantages over dropout in early learning stages, reaching higher accuracy faster. Potential for a better learning rate schedule for jumpout is suggested for quicker final performance. Rescaling factors of -1, (1-p)-0.5, and (1-p)-0.75 are applied to y with dropout, using the CIFAR10(s) network. In the future, a better learning rate schedule method may be found for jumpout to achieve final performance earlier than dropout. Rescaling factors of -1, (1-p)-0.5, and (1-p)-0.75 are applied to y with dropout in the CIFAR10(s) network. The plots show that (1-p)-0.75 provides a good balance between mean and variance rescaling."
}