{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. The challenge lies in the instability of derivatives, especially for networks with piecewise linear activation functions. This paper introduces a new learning problem to promote stable derivatives over larger regions, focusing on provably stable linear approximations around points. In this paper, a new learning problem is proposed to encourage deep networks to have stable derivatives over larger regions, focusing on networks with piecewise linear activation functions. The algorithm includes an inference step to identify stable regions and an optimization step to expand them, with a novel relaxation to scale the algorithm to realistic models. The method is demonstrated with residual and recurrent networks on image and sequence datasets. The proposed algorithm aims to ensure stable derivatives over larger regions in deep networks with piecewise linear activation functions. A novel relaxation is introduced to scale the algorithm to realistic models, demonstrated with residual and recurrent networks on image and sequence datasets. The method involves identifying stable regions and expanding them through optimization steps. The derivatives of functions parameterized by deep learning models are not stable in general when considering inputs, rather than parameters. This poses a key challenge in sensitivity analysis and model explanation. Regularization of functional classes controlled by derivatives in deep learning models is a key challenge due to their instability, leading to unstable function values and derivatives. This instability affects the robustness of first-order approximations used for explanations. State-of-the-art deep learning models are often over-parametrized, resulting in unstable functions and derivatives. This instability affects the robustness of first-order approximations used for explanations. Gradient stability is different from adversarial examples, as stable gradients can vary in size within a local region. Adversarial examples are small perturbations of the input that change the predicted output. Robust estimation techniques focus on stable function values rather than stable gradients to protect against adversarial examples. Large local gradients, whether stable or not, can contribute to finding adversarial examples. Gradient stability is different from adversarial examples, as stable gradients can vary in size within a local region. In this paper, the focus is on deep networks with piecewise linear activations to address the issue of finding adversarial examples. The special structure of these networks allows for a more tractable approach to ensuring gradient stability. The paper focuses on deep networks with piecewise linear activations to ensure gradient stability. The special structure of these networks allows for lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. The special case of p = 2 is investigated, with an analytical solution for the lower bound, leading to a regularization problem formulation. The study focuses on deep networks with piecewise linear activations to ensure gradient stability. It investigates the lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable, specifically for p = 2. The resulting regularization problem is rigid and non-smooth, resembling support vector machines. The inference and learning tasks involve evaluating neuron gradients with respect to inputs, posing a computational challenge. The study focuses on deep networks with piecewise linear activations, investigating lower bounds on maximum radius of p-norm balls around a point for stable derivatives. The regularization problem is rigid and non-smooth, resembling support vector machines. Inference and learning tasks involve evaluating neuron gradients with respect to inputs, posing a computational challenge. A novel perturbation algorithm is proposed for collecting exact gradients in parallel without back-propagation. The study proposes a novel perturbation algorithm for piecewise linear networks to collect exact gradients in parallel without back-propagation. It addresses the challenge of GPU memory limitations by using unbiased approximations. Empirical evaluation is conducted on various network architectures with image and time-series datasets. Key contributions include inference algorithms for identifying input regions. The study introduces inference algorithms for identifying input regions in neural networks with piecewise linear activation functions. It also presents a novel learning criterion to expand regions of stable derivatives and perturbation algorithms for high-dimensional data. The algorithms are evaluated with fully-connected, residual, and recurrent networks on image and time-series datasets. The study introduces algorithms for identifying input regions in neural networks with piecewise linear activation functions, focusing on stability and scalability. It includes a novel learning criterion and perturbation algorithms for high-dimensional data, evaluated on various network types. The paper focuses on neural networks with piecewise linear activation functions like ReLU and its variants. These networks, including FC, CNN, RNN, and ResNet, are considered as piecewise linear networks. The proposed approach is based on mixed integer linear programming. The proposed approach is based on a mixed integer linear representation of piecewise linear networks, encoding the active linear piece of the activation function for each neuron. The feasible set corresponding to an activation pattern in the input space is a natural constraint. The proposed approach is based on a mixed integer linear representation of piecewise linear networks, encoding the active linear piece of the activation function for each neuron. The feasible set induced by an activation pattern forms a linear region in the input space. The feasible set induced by an activation pattern in the input space forms a linear region where derivatives are stable. This includes complete linear regions with the same end-to-end linear coefficients. Activation patterns have various applications, such as visualizing neurons and reachability of specific output values. Activation patterns in the input space form linear regions with stable derivatives, including complete linear regions with the same coefficients. These patterns have applications in visualizing neurons, reachability of specific output values, vector quantization, counting linear regions, and adversarial attacks or defense. The distinction is made between locally linear regions of functional mapping and decision regions defined by classes. In contrast to quantifying the number of linear regions as a measure of complexity, the focus is on local linear regions and expanding them via learning. The notion of stability considered differs from adversarial examples, with different methods used for finding exact adversarial examples. The focus is on local linear regions and expanding them via learning, with methods for finding exact adversarial examples being different. The defense methods are still intractable on ImageNet scale images, even with layer-wise relaxations of ReLU activations. Our inference algorithm certifies the exact 2 margin around a point subject to its activation pattern by forwarding O(D) samples in parallel, scaling to high-dimensional settings. The proposed learning algorithm is based on certifying a 2 margin around a point's activation pattern by forwarding O(D) samples in parallel, scaling to high-dimensional settings. It differs from SVM training in its purpose. The proposed learning algorithm maximizes the 2 margin of linear regions around each data point in an unsupervised manner, similar to transductive/semi-supervised SVM. Our approach maximizes the 2 margin of linear regions around each data point in an unsupervised manner, similar to transductive/semi-supervised SVM. It involves a smooth relaxation of the margin and novel perturbation algorithms for gradient stability in realistic networks. The idea of margin is extended to nonlinear classifiers with decision boundaries. A smooth relaxation of the margin and novel perturbation algorithms are developed for gradient stability in realistic networks, impacting the interpretability of complex models. Gradient-based explanation methods, such as gradient saliency map and its variants, are used for visualization. The gradient is a key component in explanation methods for deep models, including the gradient saliency map and its variants. The focus is on establishing robust derivatives for complex models, with a particular emphasis on gradient stability and the fundamental problem of interpretability. The approaches are developed under the notation of FC networks. The focus of this work is on establishing robust derivatives for complex models, particularly addressing the instability of gradient-based explanations. The approaches are developed under the notation of FC networks with ReLU activations, which can be generalized to other settings. The neural network considered has M hidden layers and N i neurons in each layer, with corresponding function f \u03b8 : R D \u2192 R L. The text discusses FC networks with ReLU activations, presenting inference and learning algorithms for a neural network with M hidden layers and N i neurons in each layer. It introduces notation and provides proofs in Appendix A. The network uses transformation matrices and biases to compute activated neurons in each layer. The text discusses the computation of activated neurons in each layer of a fully connected network with ReLU activations. It uses transformation matrices and biases to compute the neurons and outputs a linear transformation of the last hidden layer. The text discusses the computation of activated neurons in each layer of a fully connected network with ReLU activations using transformation matrices and biases. The output is a linear transformation of the last hidden layer, which can be further processed by a nonlinearity like softmax for classification. The focus is on the piecewise linear property of neural networks and a generic loss function is used to fold such nonlinear mechanisms. The text discusses the activation pattern in neural networks, focusing on the piecewise linear property and using a generic loss function for training data. The activation pattern is defined as a set of indicators for neurons that specify functional constraints. The activation pattern BID20 used in this paper is defined as a set of indicators for neurons that specify functional constraints. The sub-gradient found by back-propagation is denoted as \u2207 x z i j. The activation pattern BID20 is defined as a set of indicators for neurons specifying functional constraints. Each linear region of f \u03b8 is characterized as a convex polyhedron with linear constraints in the input space R D. The feasible set of the activation pattern is equivalent to a convex polyhedron. The activation pattern BID20 is defined as a set of indicators for neurons specifying functional constraints. Each linear region of f \u03b8 is characterized as a convex polyhedron with linear constraints in the input space R D. The feasible set of the activation pattern is equivalent to a convex polyhedron. Lemma 2 characterizes each linear region of f \u03b8 as the feasible set S(x) with linear constraints in the input space R D, making S(x) a convex polyhedron. The activation pattern, equipped with input space constraints, defines the p margin of x as\u02c6 x,p subject to its activation pattern. The definition of \u02c6 x,p, the p margin of x subject to its activation pattern, is derived from input space constraints. The convexity of S(x) is utilized to verify directional perturbations. If a point x, a feasible set S(x), and a unit vector \u2206x satisfy x + \u00af \u2206x \u2208 S(x) for some \u00af \u2265 0, then f \u03b8 is linear in that direction. The convexity of S(x) is used to verify directional perturbations. If x + \u00af \u2206x \u2208 S(x) for some \u00af \u2265 0, then f \u03b8 is linear in that direction. Feasibility can be determined by checking if x + \u00af \u2206x satisfies the activation pattern O in S(x). This method can be applied to 1-ball feasibility as well. If x + \u00af \u2206x \u2208 S(x), then f \u03b8 is linear in that direction. Feasibility can be determined by checking if x + \u00af \u2206x satisfies the activation pattern O in S(x). Proposition 5 can be applied to 1-ball feasibility, with the number of extreme points being linear to D. In high dimensions, the number of extreme points of an \u221e -ball is exponential to D, making it intractable. However, for a 1-ball, the number of extreme points is only linear to D. Feasibility for directional perturbations and 1-balls can be verified using binary searches. Certification for 1-balls is tractable due to the convexity of S(x) and efficient binary search. Further certification for 2-balls can be done by exploiting the polyhedron structure of S(x). To verify feasibility, binary searches can be used to find certificates for directional perturbations and 1-balls. Certification for 1-balls is tractable due to the convexity of S(x) and efficient binary search. Further certification for 2-balls can be done by exploiting the polyhedron structure of S(x). The minimum 2 distance between a point x and the union of hyperplanes can be computed analytically. The structure of S(x) can be certified analytically by finding the minimum 2 distance between a point x and the union of hyperplanes induced by neurons. This can be efficiently computed using forward passes, and the visualization of certificates on 2 margins can be seen in FIG3. The sizes of linear regions in the bounded input space are related to their overall number, which is intractable to count in f \u03b8. In \u00a74.1, it is shown that the computation of \u2207 x z i j can be efficiently done through parallel forward passes. The visualization of certificates on 2 margins can be seen in FIG3. Counting the number of linear regions in f \u03b8 is complex due to the combinatorial nature of activation patterns. Certifying the number of complete linear regions (#CLR) among data points D x is proposed as an efficient alternative. Certifying the number of complete linear regions (#CLR) of f \u03b8 among data points D x is proposed as an efficient alternative to counting the number of linear regions in f \u03b8, which is complex due to the combinatorial nature of activation patterns. The number of complete linear regions is upperbounded by the number of different activation patterns and lower-bounded by the number of data points with unique activation patterns. In this section, methods are focused on maximizing the 2 margin\u02c6 x,2 through a regularization problem to maximize the margin. The objective is rigid due to inner-minimization. In this section, methods aim to maximize the 2 margin\u02c6 x,2 through a regularization problem. The objective is rigid, hindering optimization, so a hinge-based relaxation is applied to the distance function. If Lemma 8 condition is not met, Eq. FORMULA12 remains a valid upper bound. To alleviate the rigid loss surface hindering optimization, a hinge-based relaxation is applied to the distance function, similar to SVM. If Lemma 8 condition is not met, Eq. FORMULA12 remains a valid upper bound due to a smaller feasible set. A smoother problem is solved by deriving a relaxation that includes a soft regularization approach. An upper bound of Eq. FORMULA12 remains valid even if Lemma 8 does not hold, due to a smaller feasible set. A relaxation is derived to solve a smoother problem by incorporating a soft regularization approach, involving a maximum aggregation of TSVM losses among all neurons. The relaxed regularization problem involves a hinge loss with a hyper-parameter C. It aims to maximize TSVM losses among neurons and compute the margin in a linear model scenario. The proposed methods are visualized using a 2D binary classification dataset and a 4-layer fully connected network trained with binary cross-entropy loss. The proposed methods aim to maximize the margin in a linear model scenario by using distance regularization and relaxed regularization. A 4-layer fully connected network is trained with different loss functions, resulting in piecewise linear regions and prediction heatmaps. The distance regularization enlarges the linear regions around each training point. The text discusses the use of distance regularization and relaxed regularization to maximize the margin in a linear model scenario. The distance regularization enlarges linear regions around training points, while the relaxed regularization generalizes this property to the whole space, resulting in a smoother prediction boundary. The relaxed regularization also includes a special central region where gradients are 0 to allow for smooth changes in direction. The relaxed regularization in the linear model scenario generalizes the property of enlarging linear regions around training points to the entire space. It creates a smoother prediction boundary with a special central region where gradients are 0, allowing for smooth changes in direction. The approach involves a set of neurons with top \u03b3 percent relaxed loss to address scalability issues in large networks. The relaxed regularization in the linear model scenario generalizes the property of enlarging linear regions around training points to the entire space. It involves a set of neurons with top \u03b3 percent relaxed loss to address scalability issues in large networks. The generalized loss for learning RObust Local Linearity (ROLL) is defined as a set of neurons with high losses to a given point. This final objective is written as a special case when \u03b3 = 100, where the nonlinear sorting step disappears, stabilizing the training process. The final objective for learning RObust Local Linearity (ROLL) involves a special case where \u03b3 = 100, eliminating the nonlinear sorting step and stabilizing the training process. This simple additive structure allows for parallelized computation and an approximate learning algorithm.\u03b3 = 100 induces a strong synergy effect by correlating all gradient norms between layers. The nonlinear sorting step stabilizes training, is easy to parallelize, and enables an approximate learning algorithm. Setting \u03b3 = 100 creates a strong synergy effect by correlating gradient norms between layers. A parallel algorithm is developed without back-propagation, utilizing the functional structure of f \u03b8. Each hidden neuron is a linear function of x in the activation pattern. The parallel algorithm developed avoids back-propagation by exploiting the functional structure of f \u03b8. Hidden neurons are linear functions of x in the activation pattern, allowing for the construction of a linear network g \u03b8 with fixed linear activation functions mimicking f \u03b8 behavior in S(x). This approach enables the computation of derivatives for all neurons to an input axis by forwarding two samples. The proposed approach avoids back-propagation by constructing a linear network g \u03b8 with fixed linear activation functions to mimic the behavior of f \u03b8 in S(x). Derivatives of all neurons to an input axis can be computed by forwarding two samples, allowing for parallelized analysis of complexity. The proposed approach avoids back-propagation by constructing a linear network to mimic the behavior of f \u03b8 in S(x). Derivatives of all neurons to an input axis can be computed by forwarding two samples, allowing for parallelized analysis of complexity. The perturbation algorithm takes 2M operations to compute gradients for a batch of inputs, while back-propagation takes DISPLAYFORM0. Despite the parallelizable computation, computing the loss for large networks in a high dimension setting remains challenging. The perturbation algorithm takes 2M operations to compute gradients for a batch of inputs, proposing an unbiased estimator for the loss in a high dimension setting. The computation of the loss for large networks remains challenging despite parallelizable computation. In a high-dimensional setting, the computation of loss for large networks is challenging due to memory constraints. An unbiased estimator for the ROLL loss is proposed, allowing for efficient computation of gradient norms using parallel passes. Sampling input axes provides an approximation to the equation, requiring minimal memory usage. In high-dimensional settings, computing loss for large networks is challenging due to memory constraints. An unbiased estimator for the ROLL loss allows for efficient gradient norm computation using parallel passes. Sampling input axes provides a low-memory approximation to the equation. The proposed algorithms work on deep learning models with affine transformations and piecewise linear activation functions. The proposed algorithms for deep learning models with affine transformations and piecewise linear activation functions require D + 1 times memory than a typical forward pass for x. They enumerate neurons with ReLU-like activation functions as z i j and suggest using average-pooling or convolution with large strides instead of maxpooling for better performance. In this section, the comparison is made between the 'ROLL' approach and a baseline model ('vanilla') in various scenarios. The experiments were conducted on a single GPU with 12G memory, and evaluation measures include accuracy (ACC) and number of complete. In this section, the 'ROLL' approach is compared with a baseline model ('vanilla') in different scenarios. Experiments were conducted on a single GPU with 12G memory, and evaluation measures include accuracy, number of complete linear regions, and p margins of linear regions. The margin\u02c6 x,p is computed for each testing point x with p \u2208 {1, 2}, and evaluated on different percentiles among the testing data. The Parameter analysis on the MNIST dataset is shown in Figure 2. Parameter analysis on MNIST dataset was conducted using accuracy, number of complete linear regions, and p margins of linear regions. The margin\u02c6 x,p was computed for each testing point x with p \u2208 {1, 2}, and evaluated on different percentiles among the testing data. Experiments were done on a 4-layer FC model with ReLU activations using a 55,000/5,000/10,000 split of the MNIST dataset for training/validation/testing. The median of\u02c6 x,2 in the testing data is reported for a 4-layer FC model with ReLU activations on a split of the MNIST dataset. Two models with the largest median\u02c6 x,2 are compared to the baseline model with 1% less validation accuracy. Results show tuned models with different parameters achieving tight upper and lower bounds for certifying #CLR. The ROLL loss achieves significantly larger margins for most cases. The results in TAB1 show tuned models with different parameters achieving tight upper and lower bounds for certifying #CLR. The ROLL loss achieves significantly larger margins for most cases, with a tradeoff of 1% accuracy for about 30 times larger margins. The Spearman's rank correlation between\u02c6 x,1 and\u02c6 x,2 among testing data is at least 0.98 for all cases. The lower #CLR in our approach compared to the baseline model indicates the presence of larger linear regions. The Spearman's rank correlation between\u02c6 x,1 and\u02c6 x,2 among testing data is at least 0.98 for all cases. The lower #CLR in our approach than the baseline model reflects the existence of certain larger linear regions that span across different testing points. Points inside the same linear region in the ROLL model with ACC= 98% have the same label, while visually similar digits are in the same linear region in the other ROLL model. In the ROLL model, points within the same linear region with ACC= 98% share the same label. A parameter analysis in Figure 2 shows the impact of different hyper-parameters on accuracy and margin. Higher \u03b3 values indicate less sensitivity to hyper-parameters C and \u03bb. The efficiency of the method is validated by measuring the running time for mini-batch gradient descent. The efficiency of the proposed method is validated by measuring the running time for mini-batch gradient descent. Increased C and \u03bb result in decreased accuracy with a larger margin. Higher \u03b3 values show less sensitivity to hyper-parameters C and \u03bb. Different loss computations are compared to assess efficiency. The efficiency of the proposed method is validated by measuring the running time for mini-batch gradient descent. Comparisons are made between different loss computations, including vanilla loss, full ROLL loss, approximate ROLL loss, and perturbation algorithm. Results show that the approximate ROLL loss is about 9 times faster than the full loss, with our perturbation algorithm achieving about 12 times empirical speed-up compared to back-propagation. The accuracy and margins of the approximate ROLL loss are comparable to the full loss. The computational overhead of the method is minimal, achieved by the perturbation algorithm and approximate loss. The computational overhead of the method is minimal, achieved by the perturbation algorithm and approximate loss. The network is trained for speaker identification on a Japanese Vowel dataset with variable sequence length and channels. The RNNs achieve about 12 times speed-up, implementing the network with scaled Cayley orthogonal RNN. We train RNNs for speaker identification on a Japanese Vowel dataset with variable sequence length and channels using the state-of-the-art scaled Cayley orthogonal RNN. The results are reported in TAB3. The state-of-the-art scaled Cayley orthogonal RNN (scoRNN) is used for speaker identification on a Japanese Vowel dataset. Results are reported in TAB3, showing a model with larger margins compared to vanilla loss. Spearman's rank correlation between variables is high at 0.98. Sensitivity analysis on derivatives is also conducted. Our approach in the speaker identification task using the scoRNN model results in larger margins on testing data compared to the vanilla loss. The Spearman's rank correlation between variables is high at 0.98. Sensitivity analysis on derivatives identifies stability bounds for stable derivatives. The ROLL regularization shows consistently larger stability bounds compared to the vanilla model. The stability bounds for stable derivatives are identified using the ROLL regularization, which consistently outperforms the vanilla model. Experiments are conducted on Caltech-256 dataset using a ResNet model with 98% accuracy. The ROLL regularization outperforms the vanilla model in experiments on the Caltech-256 dataset using a ResNet model. Images are downsized to 299 \u00d7 299 \u00d7 3, and the ROLL loss is applied with 120 random samples per channel. Evaluation measures are challenging due to high input dimensionality. The implementation details involve using 120 random samples per channel, with 5 and 15 samples selected for validation and testing sets. The remaining data is used for training. Due to high input dimensionality, computing the certificate is computationally challenging without a cluster of GPUs. A sample-based approach is used to evaluate gradient stability for the ground-truth label in different linear regions. The stability of gradients f \u03b8 (x) y for the ground-truth label in local regions is evaluated using a sample-based approach due to computational challenges. The gradient distortion is assessed in terms of expected 1 distortion and maximum 1 distortion within an intersection of an \u221e -ball and the image domain X. The stability of gradients f \u03b8 (x) y for the ground-truth label in local regions is evaluated using a sample-based approach due to computational challenges. The gradient distortion is defined and assessed in terms of expected 1 distortion and maximum 1 distortion within an intersection of an \u221e -ball and the image domain X. The adversarial gradient is found by maximizing the distortion over the \u221e -norm ball with radius 8/256. The adversarial gradient \u2207 x f \u03b8 (x ) y is found by maximizing the distortion \u2206(x, x , y) over an \u221e -norm ball with radius 8/256. Gradient-based optimization is not applicable due to the Hessian \u2207 2 x f \u03b8 (x ) y being either 0 or ill-defined. A genetic algorithm BID33 is used for black-box optimization. The maximum 1 distortion is computed using a genetic algorithm BID33 for black-box optimization. Implementation details are in Appendix J, with 8000 samples used to approximate the expected distortion. Only 1024 random images in the testing set are evaluated for both maximum and expected distortions. The \u221e -ball radius is set to 8/256. The genetic algorithm BID33 is used for black-box optimization, with 8000 samples to approximate the expected distortion. Only 1024 random images are evaluated for maximum and expected 1 gradient distortions. Results show ROLL loss yields more stable gradients with slightly better precisions compared to vanilla loss. Only 40 and 42 gradient-distorted images change prediction labels in ROLL and vanilla models, respectively. Examples are visualized in Figure 4. The ROLL loss provides more stable gradients and slightly better precisions compared to the vanilla loss. Only 40 and 42 gradient-distorted images change prediction labels in the ROLL and vanilla models, respectively. Examples are visualized in Figure 4, showing the stable shapes and intensities of gradients with the ROLL loss. This paper introduces a new learning problem to create locally transparent neural networks with stable gradients. It focuses on constructing piecewise linear networks using a margin principle similar to SVM. The proposed ROLL loss expands regions and yields stable shapes and intensities of gradients. More examples with integrated gradient attributions are provided in Appendix K. The paper introduces a new learning problem to create locally transparent neural networks with stable gradients, focusing on constructing piecewise linear networks using a margin principle similar to SVM. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions, ensuring directional feasibility in neural networks. The proof of directional feasibility in neural networks involves linear constraints and convex sets, ensuring stability and feasibility in the network's operations. Feasibility in neural networks involves linear constraints and convex sets, ensuring stability and feasibility in the network's operations. Given a point x, a feasible set S(x), and a unit vector \u2206x, if x +\u00af \u2206x \u2208 S(x), then f \u03b8 is linear in {x + \u2206x : 0 \u2264 \u2264\u00af }. This is proven by the convexity of S(x) and the inclusion of {x + \u2206x : 0 \u2264 \u2264\u00af } in S(x). Given a point x, a feasible set S(x), and a unit vector \u2206x, if x +\u00af \u2206x \u2208 S(x), then f \u03b8 is linear in {x + \u2206x : 0 \u2264 \u2264\u00af }. Feasibility in neural networks involves linear constraints and convex sets, ensuring stability and feasibility in the network's operations. S(x) is a convex set, and x is a convex combination of x 1 , . . . , x 2D , implying x \u2208 S(x). The minimum 2 distance between x and the union of hyperplanes is discussed, with the minimizing 2 distance being the maximizing distance that satisfies B ,2 (x) \u2286 S(x). The proof is based on constructing a neural network. The proof involves constructing a neural network feasible in Eq. (5) with the same loss as the optimal model in Eq. (4), utilizing parallel computation of gradients by linearity. The network g \u03b8 is constructed with corresponding neurons z i j and a given x, highlighting its functional relationship with respect to a new input x. The proof involves constructing a neural network feasible in Eq. (5) with the same loss as the optimal model in Eq. (4), utilizing parallel computation of gradients by linearity. The network g \u03b8 is constructed with corresponding neurons z i j and a given x, highlighting its functional relationship with respect to a new input x. The network g \u03b8 is constructed with the same weights and biases as f \u03b8 but with a linear activation function o i j = max(0, o i j ) \u2208 {0, 1}. Each layer in g \u03b8 is represented as: DISPLAYFORM4. The network g \u03b8 is constructed with the same weights and biases as f \u03b8 but with a well-crafted linear activation function. Each layer in g \u03b8 is represented as: DISPLAYFORM4. To collect partial derivatives with respect to an input axis k, a zero vector and a unit vector on the axis are fed to g \u03b8. The proposed approach involves using a network g \u03b8 with a fixed linear activation function to compute partial derivatives with respect to an input axis k. By feeding a zero vector and a unit vector on the axis to g \u03b8, the derivatives of neurons with respect to x k can be computed efficiently. This method allows for the computation of all neuron gradients with 2 forward passes, simplifying the complexity analysis. The derivative of each neuron with respect to an input dimension can be efficiently computed using a fixed linear activation function. This method simplifies complexity analysis by requiring only 2 forward passes to compute all neuron gradients. The proposed approach assumes no overhead for parallel computation and considers a unit operation for batch matrix multiplication. A forward pass up to the last hidden layer requires M operations. The perturbation algorithm computes gradients by first obtaining activation patterns and then calculating gradients with perturbations, totaling 2M operations. In contrast, back-propagation cannot parallelize neuron gradient computations. The proposed approach involves forward pass for activation patterns and perturbation for gradients, totaling 2M operations. Back-propagation requires sequential computation for each neuron, totaling Mi=1 2iN operations. The backpropagation process for computing neuron gradients involves sequential computation for each neuron, with a total of Mi=1 2iN operations. The chain-rule of Jacobian can be used for dynamic programming to compute all neuron gradients efficiently. The backpropagation process involves dynamic programming for computing all gradients efficiently using the chain rule of Jacobian. The approach is efficient for fully connected networks but inefficient for convolutional layers. The dynamic programming approach efficiently computes gradients using the chain rule of Jacobian for fully connected networks. However, it is inefficient for convolutional layers due to the expensive linear transformation representation. An introductory guide is provided for deriving methods for maxout/max-pooling nonlinearity in piecewise linear networks. The text provides an introductory guide for deriving methods for maxout/max-pooling nonlinearity in piecewise linear networks. It suggests using convolution with large strides or average-pooling instead of max-pooling to avoid new linear constraints. The goal is to show the feasibility of deriving inference and learning methods for a network with max-pooling nonlinearity. The text suggests using convolution with large strides or average-pooling instead of max-pooling to avoid new linear constraints. It assumes a target network with a single nonlinearity mapping N neurons to 1 output. Activation patterns induce a feasible set in the input space where derivatives are stable. The activation pattern induces a feasible set in the input space where derivatives are stable, but may lead to a degenerate case where two patterns yield the same linear coefficients. The feasible set of a feasible activation pattern can be derived with linear constraints, forming a convex set. The feasible set of a feasible activation pattern can be derived with linear constraints, forming a convex set. For each max-pooling neuron with N inputs, it will induce N \u2212 1 linear constraints. The FC model consists of 4 fully-connected hidden layers, each with 100 neurons. The input dimension is 2. The feasible set is a convex polyhedron, allowing for the application of inference and learning algorithms with linear constraints. Each max-pooling neuron with N inputs induces N \u2212 1 linear constraints. The FC model has 4 hidden layers with 100 neurons each, input dimension D is 2, and output dimension L is 1. The model is trained for 5000 epochs with Adam optimizer, selecting based on training loss. C is fixed at 5, and \u03bb is increased for distance and relaxed regularization problems. The input dimension D is 2 and the output dimension L is 1. The loss function is sigmoid cross entropy. The model is trained for 5000 epochs with Adam optimizer, selecting based on training loss. C is fixed at 5, and \u03bb is increased for regularization problems until the classifier is not perfect. The tuned \u03bb in both cases is 1. Data is normalized with \u00b5 = 0.1307 and \u03c3 = 0.3081. The margin is computed in the normalized data and reported in the table. The FC model consists of 4 fully-connected hidden layers with 300 neurons each, using ReLU activation function. The loss function is cross-entropy with soft-max. The exact ROLL loss is computed during training without approximate learning. The margin is calculated in normalized data and reported in the table. The FC model has 4 hidden layers with 300 neurons each, using ReLU activation. The loss function is cross-entropy with soft-max. The exact ROLL loss is computed during training without approximation. Tuning involves grid search on \u03bb, C, \u03b3. The model has 20 epochs, uses stochastic gradient descent with Nesterov momentum, and has a learning rate of 0.01 and momentum of 0.5. Tuning involves grid search on \u03bb, C, \u03b3 with specific ranges. The data is not normalized, and the exact ROLL loss is computed during training. The model uses a single layer scoRNN with LeakyReLU activation functions and hidden neurons set to 512. The loss function is cross-entropy, and the data is not normalized. The exact ROLL loss is computed during training. The representation is learned using a single layer scoRNN with LeakyReLU activation functions and 512 hidden neurons. The loss function is cross-entropy with soft-max, optimized using AMSGrad with a learning rate of 0.001 and batch size of 32 sequences. Tuning involves a grid search on \u03bb and C parameters, with \u03b3 set to 100. Models achieve testing accuracy comparable to the baseline. The loss function used is cross-entropy with soft-max, optimized with AMSGrad. Tuning involves a grid search on \u03bb and C parameters, with \u03b3 set to 100. Models achieve testing accuracy comparable to the baseline, with a bijective mapping applied to compute distances in the original space. The model achieves 1% less testing accuracy compared to the baseline. A bijective mapping is established between normalized and original distances. The ResNet-18 model is modified by replacing max-pooling with average-pooling after the first convolutional layer. We modify the pre-trained ResNet-18 model by replacing max-pooling with average-pooling after the first convolutional layer and enlarging the receptive field of the last pooling layer to output 512 dimensions. We modify the pre-trained ResNet-18 model by replacing max-pooling with average-pooling after the first convolutional layer and enlarging the receptive field of the last pooling layer to output 512 dimensions. The model is trained with stochastic gradient descent with Nesterov momentum for 20 epochs, starting with an initial learning rate of 0.005 and adjusting to 0.0005 after 10 epochs. The batch size is 32, and the best validation loss model is selected among the 20 epochs. The model is trained with stochastic gradient descent using Nesterov momentum for 20 epochs. The initial learning rate is 0.005, adjusted to 0.0005 after 10 epochs, with a momentum of 0.5 and a batch size of 32. Tuning involves fixing C = 8, using 18 samples for learning, and tuning \u03bb until a significant drop in validation accuracy is observed.\u03bb is then fixed at 0.001, and C is found to be optimal at 8. To improve model accuracy, C is fixed at 8 and 18 samples are used for learning. \u03bb is tuned until a drop in validation accuracy is observed. C is then set to 8 and a genetic algorithm with 4800 populations and 30 epochs is implemented for further training. The genetic algorithm (GA) BID33 is implemented with 4800 populations and 30 epochs. Initially, 4800 samples are uniformly sampled in the domain for the population. Samples are sorted based on distance evaluation, with the top 25% kept in the population. The remaining 75% are replaced with a random linear combination of pairs from the top samples. The genetic algorithm (GA) BID33 is implemented with 4800 populations and 30 epochs. Samples are sorted based on distance evaluation, with the top 25% kept in the population. The remaining 75% are replaced with a random linear combination of pairs from the top samples. In each epoch, the algorithm evaluates the gradient distance of the samples and performs crossover and projection operations to ensure feasibility. Mutation is not implemented due to computational reasons. The genetic algorithm (GA) BID33 is implemented with 4800 populations and 30 epochs. Samples are sorted based on distance evaluation, with the top 25% kept in the population. The remaining 75% are replaced with a random linear combination of pairs from the top samples. In each epoch, the algorithm evaluates the gradient distance of the samples and performs crossover and projection operations to ensure feasibility. Mutation is not implemented due to computational reasons. The updated samples in P undergo an \u221e -projection to the domain B ,\u221e (x) \u2229 X to ensure feasibility, with the sample achieving the maximum 1 distance returned. The crossover operator in GA is analogous to a gradient step, with direction determined by other samples and step size determined randomly. Visualizations include the original image, original gradient, adversarial gradient, and image of adv. gradient. The crossover operator in GA is similar to a gradient step, with direction determined by other samples and step size determined randomly. Visualizations include the original image, original gradient, adversarial gradient, image of adv. gradient, original int. gradient, and adversarial int. gradient. No optimization was performed to find the image yielding the maximum distorted integrated gradient. The integrated gradient attribution method visualizes gradients by aggregating derivatives in each channel, taking the absolute value, normalizing, and clipping values above 1. The integrated gradient method visualizes gradients by aggregating derivatives in each channel, taking the absolute value, normalizing, and clipping values above 1, resulting in a gray-scaled image. The integrated gradient method aggregates derivatives in each channel, normalizes, and clips values above 1 to create a gray-scaled image. Visualizing the integrated gradient highlights differences in settings, with examples from the Caltech-256 dataset showing gradient distortions on the ROLL model. The integrated gradient method aggregates derivatives in each channel, normalizes, and clips values above 1 to create a gray-scaled image. Visualizing the examples in Caltech-256 dataset on the ROLL model shows gradient distortions at different percentiles of maximum 1 gradient distortions among the testing data. The visualization in Figure 5 and 6 shows examples from the Caltech-256 dataset that yield different percentiles of maximum 1 gradient distortions on the ROLL model. The maximum 1 gradient distortion for the vanilla model is 893.3 for the 'Projector' category. Visualization of examples in Caltech-256 dataset showing P 25, P 50, P 75, and P 100 of maximum 1 gradient distortions on ROLL model. Maximum distortion for vanilla model: 893.3 for 'Projector' and 1547.1 for 'Bear'. ROLL model distortion: 1367.9 for 'Bear' and 5473.5 for 'Rainbow'. The ROLL model in the Caltech-256 dataset has maximum 1 gradient distortions of 1367.9 for 'Bear' and 3882.8 for 'Rainbow', while the vanilla model has distortions of 1547.1 for 'Bear' and 5473.5 for 'Rainbow'."
}