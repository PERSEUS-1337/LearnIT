{
    "title": "rJfW5oA5KQ",
    "content": "Recent works have shown that Generative Adversarial Networks (GANs) suffer from lack of diversity or mode collapse, as powerful discriminators cause overfitting while weak discriminators cannot detect mode collapse (Arora et al., 2017a). GANs can learn distributions in Wasserstein distance or KL-divergence with polynomial sample complexity by designing discriminators with strong distinguishing power against specific generator classes. Discriminators, often neural nets, can approximate the Wasserstein distance and/or KL-divergence using Integral Probability Metric (IPM). By designing discriminators to approximate the Wasserstein distance and/or KL-divergence using Integral Probability Metric (IPM), GANs can learn distributions with polynomial sample complexity. Preliminary experiments suggest that the lack of diversity in GANs may be caused by the design of discriminators against specific generator classes. Our preliminary experiments suggest that the lack of diversity in GANs may be due to sub-optimal optimization rather than statistical inefficiency. The success of Generative Adversarial Networks (GANs) in generating high-quality samples has been observed in various domains. Various ideas have been proposed to enhance the quality of learned distributions. The lack of diversity in GANs may be attributed to sub-optimal optimization rather than statistical inefficiency. Despite the success of GANs in generating high-quality samples, there is still a need to understand if they truly learn the target distribution. Various approaches have been proposed to improve distribution quality and training stability. Recent work has highlighted concerns about the lack of diversity in distributions learned by GANs, leading to mode collapse. This raises questions about whether GANs actually learn the target distribution effectively. Various approaches have been proposed to address these issues and improve distribution quality and training stability. Recent work has raised concerns about GANs suffering from mode collapse, where the learned distribution lacks diversity. The paper suggests that designing discriminators with strong distinguishing power can alleviate mode collapse, focusing on the Wasserstein GAN formulation. The paper addresses mode collapse in GANs by proposing discriminators with strong distinguishing power, focusing on Wasserstein GAN formulation. It introduces the F-Integral Probability Metric and sets up a family of generators and discriminators to learn data distribution. The paper focuses on Wasserstein GAN formulation to address mode collapse in GANs. It introduces the F-Integral Probability Metric and sets up families of generators and discriminators to learn data distribution using parametric functions like neural networks. The paper introduces the F-Integral Probability Metric for Wasserstein GANs to combat mode collapse. It utilizes parametric functions like neural networks to learn data distribution. The main concern with GANs is the issue of mode collapse, where the learned distribution may lack diversity. The paper addresses mode collapse in GANs by using Lipschitz functions and gradient-based algorithms to optimize the objective. The issue arises from the weakness of IPM compared to W 1, leading to low-diversity examples in the learned distribution. The problem of mode collapse in GANs is attributed to the weakness of IPM compared to W 1, resulting in high-quality but low-diversity examples in the learned distribution. This issue arises from the fact that the mode-dropped distribution can deceive IPM, leading to a solution involving the strength of the discriminator. The weakness of IPM compared to W 1 leads to mode collapse in GANs. Increasing the strength of the discriminator to larger families like all 1-Lipschitz functions is a proposed solution. However, the Wasserstein-1 distance lacks good generalization properties according to Arora et al. (2017a). Arora et al. (2017a) highlighted that increasing the discriminator to larger families like all 1-Lipschitz functions may not be effective due to the poor generalization properties of Wasserstein-1 distance. This is evident even when the distributions are exactly equal, as the empirical Wasserstein distance used in optimization can differ significantly from the population distance. Theoretical challenges in establishing GANs arise from the dilemma between powerful discriminators causing overfitting and weak discriminators leading to diversity issues in approximating the Wasserstein distance. Empirical observations show that even when distributions are equal, the empirical Wasserstein distance may not reflect the true distance. This paper addresses the challenge in GANs of balancing powerful discriminators causing overfitting and weak discriminators leading to diversity issues in approximating the Wasserstein distance. It proposes a solution by designing a strong discriminator class F against a specific generator class G, with restricted approximability towards the data distribution. This paper proposes a solution to the challenge in GANs by designing a discriminator class F that is strong against a specific generator class G, with restricted approximability towards the data distribution. The paper focuses on designing discriminators F in GANs that can approximate the Wasserstein distance W1 for data distribution p and any q \u2208 G, with restricted approximability towards G. The goal is to have F-IPM approximate W1 using specific functions \u03b3L(t) and \u03b3U(t). The paper aims to design discriminators in GANs to approximate the Wasserstein distance W1 for data distribution p and any q \u2208 G, with restricted approximability towards G. The focus is on the realizable case where p \u2208 G, but the framework allows for non-realizable cases as well. A discriminator class F with restricted approximability in GANs resolves mode collapse by ensuring that if the IPM between p and q is small, then p and q are also close in Wasserstein distance, preventing significant mode-dropping. This allows for population-level guarantees to be achieved. The discriminator class F with restricted approximability in GANs prevents mode collapse by ensuring that small IPM between p and q results in close Wasserstein distance, avoiding significant mode-dropping. This allows for transitioning from population-level guarantees to empirical-level guarantees, expanding on the statistical properties of Wasserstein GANs. Classical capacity bounds like Rademacher complexity relate W F (p, q) to W F (p n ,q m ). By bounding the capacity, we can understand the statistical properties of Wasserstein GANs. The inequalities in eq. (5) address diversity, generalization, and the reverse guarantee of the distance. This provides insights into the relationship between training success and the distance between p and q in Wasserstein GANs. The first theoretical framework for GANs with polynomial samples addresses diversity, generalization, and the reverse guarantee of the distance in Wasserstein GANs. Techniques are developed for designing discriminator class F with restricted approximability for various generator classes. This theoretical framework is the first to address the statistical theory of GANs with polynomial samples. Techniques are developed for designing discriminator class F with restricted approximability for different generator classes, providing diversity guarantees. The paper focuses on designing F for simple classes like mixtures of Gaussians and more complex classes like distributions generated by invertible neural networks. In the next subsection, we will show that properly chosen F provides diversity guarantees such as inequalities eq. (5). We start with relatively simple families of distributions G such as Gaussian distributions and exponential families, where we can directly design F to distinguish pairs of distribution in G. For Gaussians, one-layer neural networks with ReLU activations suffice as discriminators, and for exponential families, linear combinations of the sufficient statistics are used. In Section 4, the family of distributions generated by invertible neural networks is studied. In Section 4, a special type of neural network discriminators with one additional layer than the generator has restricted approximability. This discriminator class guarantees certain properties, including hiding polynomial dependencies on relevant parameters. Neural networks with one additional layer than the generator have restricted approximability. This discriminator class guarantees certain properties, including hiding polynomial dependencies on relevant parameters. The networks can produce an exponentially large number of modes due to non-linearities, and if W F (p, q) is small, most of these modes will show up in the learned distribution q. The invertibility assumption only produces distributions supported on the entire space, which may not align with the distribution of natural images. The invertibility assumption in neural networks can lead to distributions supported on the entire space, which may not align with the distribution of natural images residing on a low-dimensional manifold. The KL-divergence is crucial in the invertible case, but becomes infinite if the support of the estimated distribution does not coincide with the support of p. The KL-divergence is not the proper measurement of statistical distance for cases where distributions have low-dimensional supports. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE approach in learning distributions with low-dimensional supports. The main proof technique involves developing tools for approximating the log-density of a smoothed neural network generator. The paper demonstrates the advantage of GANs over MLE in learning distributions with low-dimensional supports by approximating Wasserstein distance using IPMs. The main proof technique involves developing tools for approximating the log-density of a smoothed neural network generator. The IPM correlates with Wasserstein distance for low-dimensional distributions and with KL-divergence for invertible generator families. The IPM correlates with the Wasserstein distance for low-dimensional distributions and with KL-divergence for invertible generator families. It could serve as an alternative for measuring diversity and quality in more complex settings where KL-divergence or Wasserstein distance is not measurable. In complex settings where KL-divergence or Wasserstein distance is not measurable, the test IPM can be used to measure diversity and quality of learned distributions. Optimizers often balance learning generators and discriminators on real datasets, leading to discrepancies between reported training loss and test IPM. GANs may produce distributions distinguishable from data distribution, indicating suboptimal IPM optimization. The lack of diversity in GAN experiments may be due to suboptimal optimization of the discriminator, leading to discrepancies between reported training loss and test IPM. Various tests for diversity, memorization, and generalization have been developed to address this issue. Various tests have been developed to address the lack of diversity in GAN experiments, indicating that optimization sub-optimality may be the cause. Tests include interpolation between images, semantic combination of images, and classification tests. The lack of diversity in GANs has been addressed through various tests such as interpolation between images, semantic combination of images, and classification tests. Mode collapse has been formalized as a potential theoretical issue, with proposed solutions including different architectures and algorithms. Arora et al. (2017a; b) identified mode collapse as a problem stemming from a weak discriminator in GANs. Various architectures and algorithms have been proposed to address this issue, with some success. Feizi et al. (2017) demonstrated guarantees when training GANs with quadratic discriminators and Gaussian generators. However, a general solution to mode collapse remains elusive. Zhang et al. (2017) highlighted the importance of the IPM in addressing this challenge. In the context of addressing mode collapse in GANs, Feizi et al. (2017) provided guarantees for training GANs with quadratic discriminators and Gaussian generators. However, a general solution to this issue remains unresolved. Zhang et al. (2017) emphasized the significance of the IPM as a proper metric in overcoming mode collapse. The IPM is a proper metric with statistical guarantees in Wasserstein distance for distributions like injective neural network generators. Liang (2017) discusses GANs in a non-parametric setup, highlighting improvements in sample complexity for learning GANs. The strength of our work lies in developing statistical guarantees in Wasserstein distance for distributions like injective neural network generators. Liang (2017) discusses GANs in a non-parametric setup, emphasizing that the sample complexity for learning GANs improves with the smoothness of the generator family. However, the derived rate is non-parametric - exponential in dimension - unless the Fourier spectrum of the target family decays extremely fast, which may be unrealistic in practical instances. The invertible generator structure was utilized in Flow-GAN (Grover et al., 2018). The sample complexity for learning GANs improves with the smoothness of the generator family. The rate of improvement is non-parametric and exponential in dimension unless the Fourier spectrum of the target family decays extremely fast. The invertible generator structure in Flow-GAN (Grover et al., 2018) addresses the issue of KL divergence in GAN training on real datasets. Successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. Our theoretical result and experiments show that successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. This suggests that real data cannot be generated by an invertible neural network. Additionally, if the data can be generated by an injective neural network, we can bound the closeness between the learned distribution and the true distribution in Wasserstein distance. The theory suggests that real data cannot be generated by an invertible neural network. If data can be generated by an injective neural network, the closeness between the learned distribution and the true distribution can be bounded in Wasserstein distance. The F-IPM, which includes statistical distances like TV and Wasserstein-1 distance, is referred to as the neural net IPM when F is a class of neural networks. The IPM includes statistical distances like TV and Wasserstein-1 distance. When F is a class of neural networks, it is referred to as the neural net IPM. Other distances of interest are the KL divergence and Wasserstein-2 distance. The curr_chunk discusses various distances between distributions, including the KL divergence and Wasserstein-2 distance. It also introduces the concept of Rademacher complexity and the training IPM loss for the Wasserstein GAN. The Rademacher complexity of a function class is defined, and the training IPM loss for the Wasserstein GAN is discussed. Generalization of the IPM is governed by the Rademacher complexity over the distribution space G. The Rademacher complexity of a function class is discussed in relation to the training IPM loss for the Wasserstein GAN. Generalization is governed by the quantity R n (F, G), with additional notation for Gaussian distributions and discriminators with restricted approximability. One-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees. This includes Gaussian distributions with bounded mean and well-conditioned covariance. One-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees, including those with bounded mean and well-conditioned covariance. The discriminators induce restricted approximability with respect to the Gaussian distributions. The IPM WF induced by discriminators has restricted approximability w.r.t. Gaussian distributions in G. The set of one-layer neural networks has restricted approximability with bounds differing by a factor of 1/ \u221a d. The 1/ \u221a d factor is not improvable without using more sophisticated functions than Lipschitz functions of one-dimensional projections of x. The IPM WF induced by discriminators has restricted approximability w.r.t. Gaussian distributions in G. The 1/ \u221a d factor is not improvable without using more sophisticated functions than Lipschitz functions of one-dimensional projections of x. Extension to mixture of Gaussians is also possible with a discriminator family F. The proof of the maximum Wasserstein distance between one-dimensional projections of p, q is deferred. Extension to mixture of Gaussians with a discriminator family F is possible. Linear combinations of sufficient statistics in exponential families can serve as discriminators with restricted approximability. Linear combinations of sufficient statistics in exponential families can serve as discriminators with restricted approximability, where the log partition function must satisfy a specific condition. Linear combinations of sufficient statistics in exponential families can serve as discriminators with restricted approximability. The log partition function must satisfy specific conditions related to curvature and bounds. Linear combinations of sufficient statistics in exponential families can serve as discriminators with restricted approximability. The log partition function must satisfy specific conditions related to curvature and bounds. Additionally, assuming X has diameter D and T(x) is L-Lipschitz in X, F has a Rademacher complexity bound. The log partition function is always convex, with requirements on the Fisher information matrix. Geometric assumptions on the sufficient statistics are necessary for the bound eq. (8) due to the Wasserstein distance's dependence on the underlying geometry of x in exponential families. The proof of eq. FORMULA15 follows standard theory. In this section, discriminators with restricted approximability for neural net generators are designed for GANs using invertible neural networks generators. The proof of eq. (8) is deferred to Section B.2 and requires specific geometric assumptions on the sufficient statistics due to the Wasserstein distance's dependence on the underlying geometry of x in exponential families. In Section 4, discriminators with restricted approximability for neural net generators are designed for GANs. The focus is on invertible neural networks generators with proper densities in Section 4.1, and then extended to injective neural networks generators in Section 4.2, where latent variables can have lower dimensions than observable dimensions. In Section 4.2, the focus shifts to injective neural networks generators, allowing latent variables with lower dimensions than observable dimensions. The distributions no longer have densities, and the generators are parameterized by invertible neural networks. Generators parameterized by invertible neural networks allow for non-spherical variances, enabling different impacts on output distribution. The case of \u03b3 = [1 k , \u03b41 d\u2212k ] can model data around a \"k-dimensional manifold\" with noise level \u03b4. The focus is on the set of invertible neural networks G \u03b8, consisting of standard -layer feedforward nets x = G \u03b8 (z). The focus is on invertible neural networks G \u03b8 that allow different impacts on output distribution. The case of \u03b3 = [1 k , \u03b41 d\u2212k ] can model data around a \"k-dimensional manifold\" with noise level \u03b4. The family G consists of standard -layer feedforward nets x = G \u03b8 (z) with invertible parameters. The text discusses invertible neural networks G \u03b8 with parameters W i, b i, and activation function \u03c3. The networks are assumed to be invertible and the standard deviation of hidden factors satisfies a certain range. The inverse of the network is also a feedforward neural net with activation \u03c3 \u22121. The text discusses neural networks G \u03b8 with parameters W i, b i, and activation function \u03c3. The networks are assumed to be invertible, with the standard deviation of hidden factors satisfying a specific range. The inverse of the network is also a feedforward neural net with activation \u03c3 \u22121. It is important to impose assumptions on the generator networks to prevent them from implementing pseudo-random functions that cannot be distinguished from random functions. The text discusses neural networks G \u03b8 with parameters W i, b i, and activation function \u03c3. A smoothed version of Leaky ReLU satisfies conditions on activation functions. Assumptions are necessary on generator networks to prevent them from implementing pseudo-random functions. Lemma 4.1 states that log p \u03b8 can be computed by a neural network with specific parameters and activation functions. Family F of neural networks with specific activation functions contains all. Lemma 4.1 states that log p \u03b8 can be computed by a neural network with specific parameters and activation functions, leading to the family F of neural networks containing all functions log p \u2212 log q. The exact form of the parameterized family F may not be crucial in practice, as other neural net families could also provide good approximations of log p \u2212 log q. The proof for computing log p \u03b8 involves a change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian requires computing the determinant of the weight matrices. This computation may be challenging for a given G \u03b8. The proof builds on the change-of-variable formula for log p \u03b8 and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian involves computing the determinant of the weight matrices, which can be simplified by adding a bias on the final output layer. This eliminates the need for structural assumptions on the weight matrices, unlike in flow-GANs architectures. The proof of Theorem 4.2 shows that the discriminator class F has restricted approximability with respect to the set of invertible-generator distributions G. This is achieved by adding a bias on the final output layer, eliminating the need for structural assumptions on weight matrices. The discriminator class F has restricted approximability with respect to the set of invertible-generator distributions G, as shown in Theorem 4.2. The proof utilizes a lemma that relates KL divergence to IPM when log densities exist and belong to the family of discriminators. The proof of Theorem 4.2 outlines how the discriminator class F is restricted in approximating invertible-generator distributions G. It connects KL divergence to IPM when log densities are within the discriminator family. The proof involves bounding quantities by Wasserstein distance. The proof sketch of Theorem 4.2 connects KL divergence to IPM for invertible-generator distributions G by bounding quantities with Wasserstein distance. It relies on transportation inequalities to establish lower and upper bounds. Lemma D.3 proves that for any p, q \u2208 G, if X \u223c p (or q) and f is 1-Lipschitz, then f (X) is sub-Gaussian. In the case of an invertible generator, X = G \u03b8 (Z) where Z are independent Gaussians. As long as G \u03b8 is suitably Lipschitz, f (X) = f (G \u03b8 (Z)) is a sub-Gaussian random variable. The upper bound (2) would have been immediate if functions in F are Lipschitz globally, but we provide workarounds. The upper bound (2) would have been immediate if functions in F are Lipschitz globally, but we provide workarounds by either using a truncation argument to get a W 1 bound with some tail probability, or a W 2 bound which only requires the Lipschitz constant to grow at most linearly in x 2. This is a straightforward extension of the result in (Polyanskiy & Wu, 2016). By using either a truncation argument for a W 1 bound or a W 2 bound with linear growth in the Lipschitz constant, Theorem D.2 extends the result in (Polyanskiy & Wu, 2016). The combination of restricted approximability and generalization bound shows that successful training with small expected IPM leads to q being close to p in Wasserstein distance. Corollary 4.4 states that in the setting of Theorem 4.2, with high probability over the choice of training data, a close match between estimated distribution q and true distribution p is achieved. If training succeeds with small expected IPM, the estimated distribution q is close to the true distribution p in Wasserstein distance. The training error is measured by Eqm [W F (p n ,q m )], which can be estimated by drawing fresh samples from q. Designing efficient algorithms for achieving a small expected IPM is an important open question. In this section, we focus on injective neural network generators that produce distributions on a low-dimensional manifold, which is more realistic for modeling real images but technically challenging due to the KL divergence. Designing efficient algorithms to minimize training error based on this definition is a future research direction. In this section, injective neural network generators are discussed, focusing on generating distributions on a low-dimensional manifold for modeling real images. A novel divergence between distributions is designed, sandwiched by Wasserstein distance and optimized as IPM. The generator is invertible only on the image of G \u03b8, a k-dimensional manifold in R d. The text discusses a novel divergence between distributions, sandwiched by Wasserstein distance and optimized as IPM. It introduces a smoothed F-IPM between two distributions, which can be optimized as W F with an additional variable \u03b2. The text introduces a variant of the IPM to approximate the Wasserstein distance between distributions generated by neural nets in G. It defines a smoothed F-IPM between two distributions, optimized with an additional variable \u03b2. Theorem 4.5 states the existence of a discriminator class F that approximates the Wasserstein distance for any pair of distributions p, q \u2208 G. The text introduces a variant of the IPM to approximate the Wasserstein distance between distributions generated by neural nets in G. It defines a smoothed F-IPM with an additional variable \u03b2. Theorem 4.5 states the existence of a discriminator class F that approximates the Wasserstein distance for any pair of distributions p, q \u2208 G. The theorem guarantees that if the distance between distributions is small for a polynomial number of samples, mode collapse will not occur. The results suggest that mode collapse can be avoided as long as the discriminator family has limited approximability compared to the generator family. Theoretical results show that mode collapse can be prevented if the discriminator family has restricted approximability compared to the generator family. The IPM W F (p, q) is bounded by the Wasserstein distance W 1 (p, q) under this condition. Specific discriminator classes are designed to ensure this, with synthetic experiments confirming the theory's consistency in practice. In practice, the Wasserstein distance W 1 (p, q) bounds the IPM W F (p, q) when the discriminator family has restricted approximability. Synthetic experiments confirm this theory, showing correlation between IPM and Wasserstein / KL divergence. In synthetic experiments, GANs were trained with either a theoretically proposed discriminator class with restricted approximability or vanilla neural network discriminators. The results showed a correlation between IPM and Wasserstein / KL divergence, suggesting that optimization difficulty, rather than statistical inefficiency, may be the main challenge in GAN training. The experiments conducted with synthetic datasets showed a correlation between IPM and Wasserstein distance, indicating that GAN training challenges may stem from optimization difficulty rather than statistical inefficiency. The experiments involved training GANs on synthetic 2D datasets using neural net generators and discriminators. The IPM was found to be correlated with the Wasserstein distance. Additionally, invertible neural net generators with discriminators of restricted approximability were used to show a correlation between IPM and KL divergence. Synthetic experiments with WGANs were performed to train models on various curves in two dimensions, including the unit circle. In synthetic experiments with WGANs, it was shown that the IPM is well-correlated with the KL divergence. Training was done on GANs learning curves in two dimensions, such as the unit circle and a \"swiss roll\" curve, demonstrating the ability of WGANs to learn distributions effectively. In synthetic experiments, WGANs were trained to learn the unit circle and a \"swiss roll\" curve, showing strong correlation between IPM and Wasserstein distance. The ground truth distributions were not covered in Theorems 4.2 and 4.5, but evidence suggests restricted approximability holds. In synthetic experiments, WGANs were trained to learn the unit circle and a \"swiss roll\" curve using standard two-hidden-layer ReLU nets for generators and discriminators. The RMSProp optimizer was used with learning rates set at 10^-4 for both the generator and discriminator. The study utilized standard two-hidden-layer ReLU nets for generators and discriminators to train WGANs on learning the unit circle and a \"swiss roll\" curve. The RMSProp optimizer with learning rates of 10^-4 was employed for both the generator and discriminator. The architecture for the generator was 2-50-50-2, and for the discriminator was 2-50-50-1. The training process involved comparing two metrics between the ground truth distribution and the learned distribution. The study compared metrics between the ground truth distribution and the learned distribution using neural net IPM W F (p, q) and Wasserstein distance W 1 (p, q). The empirical Wasserstein distance W 1 (p, q) was computed on fresh batches from p, q and is a good proxy for the true Wasserstein distance. The Wasserstein distance W 1 (p, q) was computed on fresh batches from p, q using the POT package. It is a good proxy for the true Wasserstein distance and is not affected by the curse of dimensionality. The learned generator closely matched the ground truth distribution at iteration 10000 in both the Swiss roll and unit circle experiments. The neural net IPM and Wasserstein distance showed a strong correlation. At iteration 500, the generators had not fully learned the true distributions yet. The learned generator closely matched the ground truth distribution at iteration 10000 for the Swiss roll and unit circle experiments. The neural net IPM and Wasserstein distance are well correlated. At iteration 500, the generators had not fully learned the true distributions yet. Sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance are presented. The first polynomial-in-dimension sample complexity bounds for learning distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented. The analysis technique involves designing discriminators tailored to the generator class for better generalization and mode collapse avoidance. The hope is to extend these techniques to other distribution families in the future. The analysis technique involves designing discriminators with restricted approximability tailored to the generator class for better generalization and mode collapse avoidance. The goal is to extend these techniques to other distribution families in the future by exploring and generalizing approximation theory results in the context of GANs. The text discusses extending techniques to other distribution families by designing discriminators with better approximability bounds and exploring approximation theory results in the context of GANs. The goal is to avoid mode collapse and improve generalization. In the vanilla functional approximation settings, the upper bound W F (p 1 , p 2 ) \u2264 W 1 (p 1 , p 2 ) is established for the discriminator family. The lower bound is then recovered using a linear discriminator and mean distance. The upper bound W F (p 1 , p 2 ) \u2264 W 1 (p 1 , p 2 ) is established for the discriminator family in vanilla functional approximation settings. The lower bound is recovered using a linear discriminator and mean distance, where a simple fact is used to show the relationship between the terms. The linear discriminator is the sum of two ReLU discriminators, represented as t = \u03c3(t) \u2212 \u03c3(\u2212t). The neuron distance between two Gaussians is computed using the function R(a) = E[max {W + a, 0}] for W \u223c N(0, 1). The function R is strictly increasing and equals 1/ \u221a 2\u03c0 at 0. By manipulating v, the covariance distance can be determined without changing \u03a3. The function R(a) = E[max {W + a, 0}] for W \u223c N(0, 1) is strictly increasing and equals 1/ \u221a 2\u03c0 at 0. By manipulating v, the covariance distance can be determined without changing \u03a3. The quantity in the supremum can be further bounded using the perturbation bound. The function R(a) = E[max {W + a, 0}] for W \u223c N(0, 1) is strictly increasing and equals 1/ \u221a 2\u03c0 at 0. By manipulating v, the covariance distance can be determined without changing \u03a3. The quantity in the supremum can be further bounded using the perturbation bound. Using the W 2 distance, the lower bound with c = 1/(2 \u221a 2\u03c0) is obtained, bridging the KL and the F-distance. The lower bound with c = 1/(2 \u221a 2\u03c0) is obtained by using the W 2 distance to bridge the KL and the F-distance for two Gaussian distributions. The growth of \u2207 log p 1 (x) 2 is bounded to upper bound the term involving log p 1 (X). The text discusses bounding the growth of \u2207 log p 1 (x) 2 for two Gaussian distributions with parameters. By using the Rademacher contraction inequality, the bound for log p 1 and log p 2 can be obtained, leading to a KL bound. The text discusses bounding the growth of \u2207 log p 1 (x) 2 for two Gaussian distributions with parameters. By using Theorem D.2(c) and the Rademacher contraction inequality, a KL bound can be obtained. The text presents proofs for KL bounds and Wasserstein bounds for exponential families, along with generalization results using Rademacher complexity. The proof for Wasserstein bounds is completed by combining equations. Additionally, a one-hidden-layer neural network is used to implement log p \u03b8. The family F is suitable for learning a mixture of k Gaussians. The family F is suitable for learning a mixture of k Gaussians using a one-hidden-layer neural network. The Gaussian concentration result will be used in later proofs. The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) is utilized for convenience in later proofs. To establish the upper bound, it is sufficient to show that each function is D-Lipschitz. This implies that every discriminator in F is at most 2D-Lipschitz. The lower bound is established by considering the regularity properties of the distributions in the Bobkov-Gotze sense. The discriminator functions in F are at most 2D-Lipschitz, as each function is D-Lipschitz. By considering the regularity properties of the distributions in the Bobkov-Gotze sense, the lower bound is established. The Bobkov-Gotze distributions are considered, leading to the establishment of lower bounds. The mixture components are Gaussian with 1-sub-Gaussian properties. The Rademacher complexity of f \u03b8 is bounded, showing Lipschitz properties in the \u03c1 metric. The Rademacher complexity of f \u03b8 is bounded, showing Lipschitz properties in the \u03c1 metric. By reparametrizing the one-hidden-layer neural net, we can bound the expected supremum over a covering set using a one-step discretization bound. The Rademacher complexity of f \u03b8 is bounded, showing Lipschitz properties in the \u03c1 metric. By reparametrizing the one-hidden-layer neural net, we can bound the expected supremum over a covering set using a one-step discretization bound. For any \u03b5 > 0, there exists a constant C > 0 such that the expected supremum can be bounded. The Rademacher complexity of f \u03b8 is bounded, showing Lipschitz properties in the \u03c1 metric. Reparametrizing the one-hidden-layer neural net allows us to bound the expected supremum over a covering set using a one-step discretization bound. For any \u03b5 > 0, there exists a constant C > 0 such that the expected supremum can be bounded. The term \u03b5 i log is 1)-subGaussian, leading to sub-Gaussian maxima bounds. By choosing \u03b5 = c/n for sufficiently small c, we can obtain Theorem D.2 (Upper bounding f -contrast by Wasserstein). The Rademacher complexity of f \u03b8 is bounded, showing Lipschitz properties in the \u03c1 metric. Reparametrizing the one-hidden-layer neural net allows us to bound the expected supremum over a covering set using a one-step discretization bound. The term \u03b5 i log is 1)-subGaussian, leading to sub-Gaussian maxima bounds. By choosing \u03b5 = c/n for sufficiently small c, we can obtain Theorem D.2 (Upper bounding f -contrast by Wasserstein). The text discusses the Wasserstein distance bounds for two distributions on R d with positive densities. It involves a truncation argument and the Lipschitzness of a function f. The proof involves a dual formulation of W 1 and a coupling with specific inequalities. The proof involves a truncation argument and the Lipschitzness of a function f. By definition of the Wasserstein distance, there exists a coupling. Inequality (i) used the Lipschitzness of f in the D-ball, and (ii) used Cauchy-Schwarz. Putting terms I and II together, we get a straightforward extension of a previous proposition. The proof involves a truncation argument and the Lipschitzness of a function f. Inequality (i) utilized the Lipschitzness of f in the D-ball and Cauchy-Schwarz. By combining terms I and II, we extend a previous proposition. The W 2 distance is defined by a coupling (X, Y) \u223c \u03c0 where X \u223c P, Y \u223c Q. The inverse of x = G \u03b8 (z) can be computed straightforwardly. The problem involves representing log p \u03b8 (x) by a neural network using a -layer feedforward net with activation \u03c3 \u22121. The inverse network G \u22121 \u03b8 can be computed straightforwardly with d2 + d parameters. The log density of Z \u223c N(0, diag(\u03b3 2 )) has a specific formula. The problem involves representing log p \u03b8 (x) by a neural network using a -layer feedforward net with activation \u03c3 \u22121. The inverse network G \u22121 \u03b8 can be computed with d2 + d parameters. Additionally, the log density of Z \u223c N(0, diag(\u03b3 2 )) has a specific formula that can be used to compute the log determinant. The text discusses adding layers to a neural network with specific parameters and activation functions to compute the log determinant of the Jacobian. It also mentions using branches to compute the log determinant and describes the process recursively. The text discusses computing the log determinant of the Jacobian using specific parameters and activation functions in a neural network. It describes adding layers and using branches recursively to achieve this computation efficiently. The text discusses computing log p \u03b8 (x) efficiently using a neural network with specific parameters and activations. It proves a restricted approximability bound in terms of the W 2 distance and shows the Gozlan condition holds for p \u03b8. The theorem proves a lower bound for p \u03b8 and shows that the Gozlan condition is satisfied for any \u03b8 \u2208 \u0398. The network G \u03b8 is also shown to be L-Lipschitz, and the random variable is proven to be L 2 -sub-Gaussian. The network G \u03b8 is L-Lipschitz, and the random variable is L 2 -sub-Gaussian. The Gozlan condition is satisfied for any \u03b8 \u2208 \u0398, and the Wasserstein distances are used to upper bound W F through Theorem D.2. The network G \u03b8 is L-Lipschitz and the random variable is L 2 -sub-Gaussian. The Gozlan condition is satisfied for any \u03b8 \u2208 \u0398. By applying Theorem D.1(b), we obtain a bound on log p \u03b81 \u2212 log p \u03b82. The Wasserstein distances are used to upper bound W F through Theorem D.2. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 is upper bounded using the W 2 bound. The network G \u03b8 is L-Lipschitz and the random variable is L 2 -sub-Gaussian. The Gozlan condition is satisfied for any \u03b8 \u2208 \u0398. The Wasserstein distances are used to upper bound W F through Theorem D.2. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 is upper bounded using the W 2 bound. Each layer h k is C(R W , R b , k)-Lipschitz in x. The bound eq. (22) implies a tail bound for X 2. The bound | log p \u03b8 (x)| is used to derive bounds under p \u03b81 or p \u03b82. The W 1 bound is used to derive tail bounds for X 2. Reparameterization is done for log-density neural network F \u03b8 (x) = log p \u03b8 (x), with (W i , b i ) representing weights and biases. This reparametrized \u03b8 = (W i , b i ) belongs to the network. Reparameterization is applied to the log-density neural network F \u03b8 (x) = log p \u03b8 (x), with (W i , b i ) as weights and biases. The reparametrized \u03b8 = (W i , b i ) belongs to the network, and the Rademacher complexity of F is at most two times a certain quantity. Additional re-parametrization is also performed, with the log-density network having a specific form. The Rademacher complexity of the log-density network F is bounded by a certain quantity, with additional re-parametrization involving a specific form for the network. A parameter K is introduced for this term, and a metric is defined for any reparametrized \u03b8. The Rademacher complexity of the log-density network F is bounded by a certain quantity, with additional re-parametrization involving a specific form for the network. A parameter K is introduced for this term, and a metric is defined for any reparametrized \u03b8. The one-step discretization bound is discussed in the following lemmas, dealing with discretization error and other terms separately. The Rademacher process is denoted by Y \u03b8 = 1 n n i=1 \u03b5 i F \u03b8 (X i ). Two lemmas address discretization error and expected max over a finite set. Substituting these Lemmas into the bound yields a final bound for all \u03b5 \u2264 min {R W , R b } and \u03bb \u2264 \u03bb 0 \u03b4 2 n, valid if n \u2265 d. Lemma D.7 states that for \u03b5 \u2264 min {R W , R b } and \u03bb \u2264 \u03bb 0 \u03b4 2 n, the generalization error is dominated by a specific term. Choosing \u03bb = n log n/\u03b4 4 is valid if n/ log n \u2265 \u03b4. This term is crucial in understanding the generalization error. The generalization error is dominated by the term \u03bb = n log n/\u03b4 4 when n/ log n \u2265 \u03b4. Fixing parameters such that \u03c1(\u03b8, \u03b8 ) \u2264 \u03b5, the Lipschitzness of hidden layers is claimed for all k, leading to specific bounds when \u03c1(\u03b8, \u03b8 ) \u2264 \u03b5. Induction is used to show these bounds. The k-th hidden layer is denoted as h_k(x) in G^-1\u03b8(x). It is claimed that all hidden layers are Lipschitz, leading to specific bounds when \u03c1(\u03b8, \u03b8) \u2264 \u03b5. Induction is used to prove these bounds. The k-th hidden layer is proven to be Lipschitz, with specific bounds when \u03c1(\u03b8, \u03b8) \u2264 \u03b5. Induction is used to verify these bounds, showing the Lipschitzness and tail decay at a single \u03b8. The k-th hidden layer is proven to be Lipschitz with specific bounds when \u03c1(\u03b8, \u03b8) \u2264 \u03b5. The quadratic term is shown to be sub-Gaussian with mean and sub-Gaussianity parameter O(Cd). The random variable is proven to be suitably sub-exponential by looking at a single x and using rules for independent sums. The curr_chunk discusses the sub-Gaussian and sub-exponential properties of a quadratic function of a random vector. The mean and sub-Gaussianity parameter are bounded, and the function is shown to be Cd-sub-Gaussian. The sub-exponential parameter is also determined to be Cd/\u03b4^2. The curr_chunk discusses the sub-Gaussian and sub-exponential properties of a quadratic function of a random vector, showing that it is Cd-sub-Gaussian and has a sub-exponential parameter of Cd/\u03b4^2. The curr_chunk discusses the properties of a quadratic function of a random vector, showing it is Cd-sub-Gaussian and has a sub-exponential parameter of Cd/\u03b4^2. Jensen's inequality is used to bound the expected maximum and the covering number of \u0398 is bounded by the product of independent covering numbers. The curr_chunk discusses bounding the expected maximum using Jensen's inequality and the covering number of \u0398. It introduces a truncated version of the convolution of p and a Gaussian distribution for the generator class. The curr_chunk introduces a truncated region in the latent space and defines a distribution with added Gaussian noise for the generator class. The curr_chunk discusses the convolution of a distribution with added Gaussian noise and truncation to a high-probability region in both latent and observable domains. It introduces regularity in the context of the generator class. The curr_chunk introduces regularity conditions for the family of generators G, including bounds on partial derivatives of f and the introduction of t(z) and T as maximum values. The curr_chunk introduces regularity conditions for the family of generators G, including bounds on partial derivatives of f and the introduction of t(z) and T as maximum values. The regularity conditions involve bounds on partial derivatives of f, maximum values of t(z) and T, and Lipschitz property of the inverse activation function. The main theorem states that for certain F,d, F approximates the Wasserstein distance. Theorem E.1 states that for every p, q \u2208 G, when n is polynomial in d, we have R n (F, G) polynomial in log n. The main theorem states that for certain F, d, F approximates the Wasserstein distance. Theorem E.1 states that for every p, q \u2208 G, when n is polynomial in d, we have R n (F, G) polynomial in log n. The proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. Theorem E.2 states that for \u03b2 = O(poly(1/d)), there exists a family of neural networks F of size poly(1/\u03b2). The proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. Theorem E.2 states that for \u03b2 = O(poly(1/d)), there exists a family of neural networks F of size poly(1/\u03b2) that satisfies certain conditions. A family of neural networks F of size poly(1/\u03b2, d) can approximate log p for typical x and serve as a lower bound of p globally. The approach involves approximating p \u03b2(x) using Laplace's method of integration. The approach involves approximating p \u03b2(x) using Laplace's method of integration, with a greedy \"inversion\" procedure for typical x and a lower bound for atypical x. Theorem E.1 is proven assuming the correctness of Theorem E.2. Theorem E.1 is proven by approximating p \u03b2(x) using Laplace's method of integration, with neural networks N1 and N2 in F approximating log p \u03b2 and log q \u03b2 respectively. This is done by following bullet FORMULA0 - FORMULA2 in Theorem E.2, leading to equations DISPLAYFORM54 and DISPLAYFORM55. By setting f = N1(x) \u2212 N2(x), the proof is obtained. Theorem E.2 involves neural networks N1 and N2 in F approximating log p \u03b2 and log q \u03b2, leading to equations DISPLAYFORM54 and DISPLAYFORM55. By setting f = N1(x) \u2212 N2(x), the lower bound is proven. For the upper bound, setting \u03b2 = W 1/6 gives the necessary bound. The optimal coupling C of p, q is considered for the claim. By setting f = N1(x) \u2212 N2(x) and \u03b2 = W 1/6, the lower and upper bounds are proven respectively. The optimal coupling C of p, q is considered for the claim, leading to the generalization claim. The induced coupling C z on the latent variable z in p, q is considered, along with the coupling C of p \u03b2 , q \u03b2. The proof of Theorem E.2 is discussed, with helper lemmas provided. The generalization claim follows analogously to Lemma D.5, using Lipschitzness bound of the generators in Theorem E.2. The proof of Theorem E.2 is discussed, with helper lemmas provided, leading to the generalization claim following analogously to Lemma D.5. The proof of Theorem E.2 is discussed, with helper lemmas provided, leading to the generalization claim following analogously to Lemma D.5. Published as a conference paper at ICLR 2019, the claim trivially holds for i = 0, so we proceed to the induction. Suppose the claim holds for i. Then, DISPLAYFORM2 recovers a\u1e91, s. N will iteratively produce estimates \u0125 i , s.t. We will prove by induction that. DISPLAYFORM6 holds by the inductive hypothesis, and the next-to-last one due to Lipschitzness of \u03c3 \u22121. This implies that, which in turn means, completing the claim. Turning to the size/Lipschitz constant of the neural network: all we need. The induction is carried out to prove the claim for i, with the last inequality holding by the inductive hypothesis and the next-to-last one due to Lipschitzness of \u03c3 \u22121. This implies DISPLAYFORM9, which means DISPLAYFORM10, completing the claim. The size/Lipschitz constant of the neural network is discussed, noting that \u0125 i = \u03c3 \u22121 (W. The integral on the right is the (unnormalized) cdf of a Gaussian with covariance DISPLAYFORM11, with the smallest eigenvalue bounded by DISPLAYFORM12. The claim DISPLAYFORM13 follows from the bound on r and Cauchy-Schwartz. The algorithm approximates the integral and can be implemented by a small, Lipschitz network. The algorithm approximates the integral and can be implemented by a small, Lipschitz network. Parameters are set for the discriminator family with restricted approximability for a degenerate manifold. The algorithm approximates the integral using a small, Lipschitz network. Parameters are set for the discriminator family with restricted approximability for a degenerate manifold. The algorithm involves calculating gradients and finding the nearest matrix in a set to achieve separation of eigenvalues. The algorithm approximates the integral using a small, Lipschitz network with parameters set for the discriminator family. It involves calculating gradients and finding the nearest matrix in a set to achieve separation of eigenvalues. The circuit implied in Lemma E.7 is used for this purpose. The algorithm approximates the integral using a small, Lipschitz network with parameters set for the discriminator family. It involves calculating gradients and finding the nearest matrix in a set to achieve separation of eigenvalues. In Lemma E.6, Algorithm 1 approximates the integral by using an approximate version of Lemma E.8 with a different division of the integral. The \"invertor\" circuit from Lemma E.4 is utilized, and matrices E i are chosen from a set S to ensure eigenvalues are separated by at least \u2126(\u03b2). The algorithm approximates the integral using a small, Lipschitz network with parameters set for the discriminator family. Matrices E i are chosen from a set S to ensure eigenvalues are separated by at least \u2126(\u03b2). There exist matrices E 1 , E 2 , . . . , E r , s.t. if M \u2208 S, at least one of the matrices M + E i has eigenvalues that are \u2126(\u03b2)-separated. The algorithm approximates the integral using a small, Lipschitz network with parameters set for the discriminator family. Matrices E i are chosen from a set S to ensure eigenvalues are separated by at least \u2126(\u03b2). The eigenvalues of i \u2208 [r] are \u2126(\u03b2)-separated and E i 2 \u2264 up to a multiplicative factor of 1 \u2212 O(\u03b2 log(1/\u03b2)). Synthetic WGAN experiments with invertible neural net generators and discriminators designed with restricted approximability are performed. The algorithm approximates the integral using a small, Lipschitz network with parameters set for the discriminator family. Matrices E i are chosen from a set S to ensure eigenvalues are separated by at least \u2126(\u03b2). Synthetic WGAN experiments with invertible neural net generators and discriminators designed with restricted approximability are performed, demonstrating the correlation between empirical IPM W F (p, q) and the KL-divergence between p and q on synthetic data. The empirical IPM W F (p, q) is well correlated with the KL-divergence between p and q on synthetic data generated from a ground-truth invertible neural net generator. The data is generated using a layer-wise invertible feedforward net with Leaky ReLU activation function. The data is generated from a ground-truth invertible neural net generator using Leaky ReLU activation function. The weight matrices are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen for restricted approximability guarantee. The piecewise constant function log \u03c3 \u22121 is modeled as trainable. The weight matrices of the layers are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen based on restricted approximability guarantee. The piecewise constant function log \u03c3 \u22121 is modeled as a trainable one-hidden-layer neural network. Training involves generating stochastic batches from both the ground-truth generator and the trained generator with batch size 64. The piecewise constant function log \u03c3 \u22121 is modeled as a trainable one-hidden-layer neural network. Training involves generating stochastic batches from both the ground-truth generator and the trained generator with batch size 64. Constraints are added on all parameters according to Assumption 1. The generator and discriminator networks are trained using the Wasserstein GAN formulation with 10 updates of the discriminator between each generator step. RMSProp optimizer is used as the update rule. Evaluation metrics are used to compare the true and learned models. The trained generator in the Wasserstein GAN formulation undergoes 10 updates of the discriminator between each generator step. Various regularization methods are used for discriminator training. The RMSProp optimizer is employed as the update rule. Evaluation metrics include computing the KL divergence between the true and learned generator densities. The KL divergence and training loss metrics are used to compare the true and learned generator in the Wasserstein GAN formulation. The KL divergence is computed analytically, while the training loss is the unregularized GAN loss during training. The balance of steps for discriminator and generator training can lead to a potentially large difference between the training IPM and the true W F. The KL divergence is considered a strong criterion for distributional closeness in the Wasserstein GAN formulation. The training loss, also known as the unregularized GAN loss, is carefully balanced between discriminator and generator steps during training. Additionally, a separately optimized WGAN loss is reported occasionally, where the generator is fixed, and the discriminator is trained from scratch to optimality. The neural net IPM (W F eval) is used to approximate W F by optimizing the discriminator separately from the generator. This approach aims to find the generator that maximizes contrast, with no regularization on the discriminator. The theory shows that WGAN can learn the true generator in KL divergence. In the experiment, a two-layer neural net is used as the generator in 10 dimensions, with a focus on maximizing contrast without regularization. The theory suggests that WGAN can learn the true generator in KL divergence. In the experiment, a two-layer neural net is used as the generator in 10 dimensions to maximize contrast without regularization. The F-IPM in evaluation should reflect the KL divergence. The discriminator is trained using Vanilla WGAN or WGAN-GP with a gradient penalty. Results from 6 random initializations are shown in FIG5, revealing that WGAN training is effective. The experiment used a two-layer neural net as the generator in 10 dimensions without regularization. The discriminator was trained with Vanilla WGAN or WGAN-GP with a gradient penalty, showing effectiveness in training. Results from 6 random initializations are presented in FIG5, indicating that WGAN training with a discriminator design of restricted approximability can learn the true distribution in KL divergence. The KL divergence starts at around 10 -30 and the best run achieves a KL lower than 1, suggesting that GANs are finding the true distribution without mode collapse. The main findings show that WGAN training with a discriminator design of restricted approximability can learn the true distribution in KL divergence. The KL divergence starts at around 10-30 and the best run achieves a KL lower than 1, indicating that GANs are finding the true distribution without mode collapse. Additionally, the W F (eval) and KL divergence are highly correlated, with adding gradient penalty improving optimization significantly. The W F (eval) and KL divergence are highly correlated, with adding gradient penalty improving optimization significantly. The quantity W F can serve as a good metric for monitoring convergence, better than the training loss curve. Testing with vanilla discriminators also shows good correlation with IPM. The quantity W F can serve as a good metric for monitoring convergence, correlating well with the KL-divergence. Testing with vanilla discriminators also shows good correlation with IPM. The vanilla discriminators correlate well with the KL-divergence. The left-most figure shows the KL-divergence between the true distribution p and learned distribution q at different training steps, the middle shows the estimated IPM between p and q, and the right shows the training loss. The estimated IPM in evaluation correlates well with the KL-divergence. The inferior performance of the WGAN-Vanilla algorithm in KL-divergence does not stem from the statistical properties of GANs but rather from the convergence of the IPM during training. This phenomenon is likely to occur in training GANs with real-life data as well. In this synthetic case, the inferior performance of the WGAN-Vanilla algorithm in KL divergence is not due to the statistical properties of GANs but rather the convergence of the IPM during training. The correlation between perturbations and p is tested by comparing KL divergence and neural net IPM on pairs of perturbed generators. Each pair consists of generators G and G' with the same architecture, where G' is a perturbation of G with small Gaussian noise. In testing the correlation between perturbations and p, the KL divergence and neural net IPM were compared on pairs of perturbed generators. The neural net IPM was computed by optimizing the discriminator from 5 random initializations to denoise the unstable training process. A clear positive correlation between the KL divergence and neural net IPM was observed, as shown in FIG6. The KL divergence and neural net IPM were computed between generators G and G. To stabilize the training process for neural net IPM, the discriminator was optimized from 5 random initializations. A positive correlation between KL divergence and neural net IPM was observed, with most points falling around the line W F = 100D kl. Outliers with large KL were attributed to perturbations causing poorly conditioned weight matrices. Results from experiments using a three-layer fully-connected discriminator net with hidden dimensions 50-10 are plotted in FIG7. The majority of points fall around the line W F = 100D kl, indicating a linear scaling in KL divergence for the neural net distance. Outliers with large KL were due to perturbations causing poorly conditioned weight matrices. In experiments with a three-layer fully-connected discriminator net, results show that generators converge well in KL divergence. However, correlation is slightly weaker compared to the setting with restricted approximability. This suggests that vanilla discriminator structures may be satisfactory for achieving a good generator. Results are plotted in FIG7, showing that the generators converge well in KL divergence. The correlation is slightly weaker compared to the setting with restricted approximability. This suggests that vanilla discriminator structures may be satisfactory for achieving a good generator, although specific designs could improve the quality of the distance W F. The left-most figure displays the KL divergence between the true distribution p and learned distribution q during training steps, the middle shows the estimated IPM between p and q, and the right figure represents the training loss. The correlation between KL divergence and neural net IPM is roughly the same for vanilla fully-connected discriminators and those with restricted approximability, with values of 0.7489 and 0.7315 respectively."
}