{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning is crucial for evaluating and improving policies using historical data from a logging policy. The main challenge is to develop counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over conventional baseline algorithms. Our end-to-end training algorithms using variational divergence minimization showed significant improvement over conventional baseline algorithms in off-policy learning. This approach eliminates the need for sample variance regularization and focuses on minimizing distribution divergence between logging policy and new policy. In real-world scenarios, off-policy evaluation is crucial for improving policies without costly on-policy evaluations. Utilizing historic data can enable safe exploration of policy hypotheses before deployment, reducing risks and costs associated with human trials or A/B testing in various domains. Off-policy evaluation is essential for improving policies without costly on-policy evaluations. Various methods such as Q learning, doubly robust estimator, and self-normalized approaches have been studied in the context of reinforcement learning and contextual bandits. A new direction involves using logged interaction data with bandit feedback for off-policy learning. In the context of off-policy learning, various methods like Q learning, doubly robust estimator, and self-normalized approaches have been studied. A new direction involves using logged interaction data with bandit feedback, where only limited feedback is observed in the form of a scalar reward or loss for each action. This setting lacks information on alternative actions, potential rewards, and the best action to take. In the context of off-policy learning, methods like Q learning and doubly robust estimator have been studied. A new approach involves using logged interaction data with bandit feedback, where only limited feedback is observed in the form of a scalar reward or loss for each action. This setting lacks information on alternative actions and potential rewards. The new counterfactual risk minimization framework addresses the challenge of distribution mismatch between logging policy and new policy in off-policy learning with bandit feedback. The counterfactual risk minimization framework tackles the distribution mismatch issue in off-policy learning with bandit feedback by adding sample variance as a regularization term. However, the linear stochastic model parametrization limits representation power, and computing sample variance regularization is computationally intensive. Our contribution in this paper is proposing a new learning principle for off-policy learning with bandit feedback, drawing a connection to the generalization error bound of importance sampling. The framework adds sample variance as a regularization term to tackle distribution mismatch in off-policy learning, but the parametrization of policies as linear stochastic models limits representation power and requires computationally intensive iterations through all training samples. The paper proposes a new learning principle for off-policy learning with bandit feedback by minimizing distribution divergence between the new policy and the logging policy. The policy is parametrized as a neural network for end-to-end training. The proposed learning principle for off-policy learning with bandit feedback involves regularizing the generalization error of the new policy by minimizing distribution divergence from the logging policy. The policy is parametrized as a neural network for end-to-end training and the learning objective balances empirical risk and sample variance. Experimental evaluation demonstrates significant performance improvements over conventional baselines. The policy, implemented as a neural network, addresses divergence minimization using recent techniques. Experimental results on benchmark datasets show improved performance compared to traditional methods. The framework of off-policy learning with logged bandit feedback is reviewed, where a policy maps inputs to structured outputs. The framework of off-policy learning with logged bandit feedback is based on a policy mapping inputs to structured outputs, utilizing stochastic policies parametrized by \u03b8 to define posterior distributions over the output space. The curr_chunk discusses using stochastic policies to define posterior distributions over the output space, allowing actions to be taken by sampling from the distribution. The feedback observed in online systems is denoted as \u03b4(x, y; y*). In online systems, actions are taken by sampling from a distribution h(Y|x), with feedback observed as \u03b4(x, y; y*). The expected risk of a policy h(Y|x) is defined as DISPLAYFORM1. In off-policy learning, the goal is to find a policy with minimum expected risk on test data by improving upon a logging policy h0(Y|x). The data used includes feedbacks \u03b4(x, y; y*) for actions sampled from h(Y|x), aiming for lower expected risks R(h) < R(h0). Off-policy learning aims to improve upon a logging policy h0(Y|x) by finding a policy with lower expected risks R(h) < R(h0). The data used includes feedbacks \u03b4 i and p i, where N is the number of training samples. Challenges arise when the distribution of the logging policy is skewed and lacks support everywhere. Off-policy learning involves improving a logging policy h0(Y|x) by finding a policy with lower expected risks R(h) < R(h0). The main challenges include skewed distribution of the logging policy and the need for empirical estimation due to finite samples, leading to generalization error and requiring additional regularization. The vanilla approach to address this is using propensity scoring. Off-policy learning involves improving a logging policy h0(Y|x) by finding a policy with lower expected risks R(h) < R(h0). The main challenges include skewed distribution of the logging policy and the need for empirical estimation using finite samples, leading to generalization error and requiring additional regularization. The vanilla approach to address this is using propensity scoring with importance sampling to account for distribution mismatch. The propensity scoring approach using importance sampling addresses distribution mismatch between h and h0 by reweighting the expected risk. Counterfactual risk minimization introduces a regularization term to control variance and improve the objective function. The authors proposed a regularization term for sample variance to address flaws in the vanilla approach for counterfactual risk minimization. The modified objective function includes a term for sample variance derived from empirical Bernstein bounds. Stochastic training is challenging due to the variance term's dependency on the entire dataset, so the authors approximated it using a first-order Taylor expansion for stochastic optimization. The authors approximated the regularization term for sample variance using a first-order Taylor expansion to enable stochastic optimization. This approach neglects non-linear terms and introduces approximation errors while reducing sample variance. Empirical estimation of variance from samples is avoided to allow for direct stochastic training. The authors used a first-order Taylor expansion to approximate the regularization term for sample variance in stochastic optimization. They avoided empirical estimation of variance from samples to enable direct stochastic training. The authors derived a variance bound directly from the parametrized distribution using importance sampling weights. The importance sampling weight w(z) = p(z) / p0(z) where p and p0 are probability density functions. The authors derived a variance bound using importance sampling weights based on the probability density functions p and p0. The importance sampling weight w(z) = p(z) / p0(z) leads to an identity involving the R\u00e9nyi divergence with \u03b1 = 2. This identity allows for an upper bound on the second moment of the weighted loss. The importance sampling weight w(z) = p(z) / p0(z) leads to an identity involving the R\u00e9nyi divergence with \u03b1 = 2, allowing for an upper bound on the second moment of the weighted loss. This bound is derived for two sampling distributions of y, h(y|x) and h0(y|x), with a joint distribution over x, y. The text discusses the bound between expected risk and empirical risk using distribution divergence functions. The bound is derived for two sampling distributions of y with a joint distribution over x, y. Detailed proofs can be found in Appendix 1. The text presents a generalization bound between expected risk and empirical risk using distribution divergence functions. The bound is derived for two sampling distributions of y with a joint distribution over x, y. Detailed proofs can be found in Appendix 1. The theorem highlights bias-variance trade-offs in empirical risk minimization problems. The text discusses a theorem on risk divergence bounded by DISPLAYFORM4, with a probability guarantee of at least 1 \u2212 \u03b7. The proof utilizes Bernstein inequality and the second moment bound, emphasizing bias-variance trade-offs in empirical risk minimization. This motivates minimizing variance regularized objectives in bandit learning settings. In bandit learning settings, instead of directly optimizing the reweighed loss and facing high variance in testing, minimizing variance regularized objectives is proposed. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance, but setting \u03bb empirically and optimizing the objective remain challenging. This approach is influenced by the success of distributionally robust learning. In bandit learning settings, minimizing variance regularized objectives is proposed with the model hyper-parameter \u03bb controlling the trade-off between empirical risk and model variance. Setting \u03bb empirically and optimizing the objective remain challenging. To address this, a new constrained optimization formulation is explored, inspired by the success of distributionally robust learning. In bandit learning, a new constrained optimization formulation is explored inspired by the success of distributionally robust learning. The formulation involves a regularization hyper-parameter \u03c1 and a policy h, with the robust objective serving as a good surrogate for the true risk. The new constrained optimization formulation involves a regularization hyper-parameter \u03c1 and a policy h, with the robust objective serving as a good surrogate for the true risk. The objective function eliminates the need to compute sample variance but estimating the divergence function remains challenging with a parametrized distribution of h(y|x) and finite samples. The regularization hyper-parameter \u03c1 approaches 0 as N \u2192 \u221e. The new objective function simplifies computation by removing the need to calculate sample variance. However, estimating the divergence function with a parametrized distribution of h(y|x) and finite samples is still challenging. Recent f-gan networks and Gumbel soft-max sampling techniques can aid in solving this task. The bounds also highlight the importance of the stochasticity of the logging policy, emphasizing the difficulty with a deterministic logging policy. Recent f-gan networks and Gumbel soft-max sampling techniques can aid in solving the task of estimating the divergence function with a parametrized distribution of h(y|x) and finite samples. The importance of the stochasticity of the logging policy is emphasized, as learning becomes difficult with a deterministic logging policy. The theory reflects that learning will be difficult with a logging policy that has peaked masses and zeros in certain regions. This leads to an unbounded generalization bound, making counterfactual learning impossible in this scenario. The derived variance regularized objective requires a stochastic logging policy for successful learning. The deterministic policy with a non-zero measure region of h 0 (Y|x) and probability density of h 0 (y|x) = 0 results in an unbounded generalization bound, making counterfactual learning impossible. The derived variance regularized objective involves minimizing the square root of the condi- dy, with a convex function f (t) = t 2 \u2212 1 in the domain {t : t \u2265 0}. The variance regularized objective involves minimizing the square root of the condition dy, with a convex function f(t) = t^2 - 1. By connecting the divergence to the f-divergence measure, a lower bound of the objective can be reached using the f-GAN for variational divergence minimization method. The operator provides a minimization objective of D f (h||h 0 ; P(X)). By following the f-GAN method for variational divergence minimization, a lower bound of the objective can be achieved using convex duality. The bound is tight when T 0 (x) = f (h/h 0 ). The dual formulation of the minimization objective is obtained by applying Fenchel convex duality to the convex function f. The bound is tight when T 0 (x) = f (h/h 0 ), and the third inequality arises from restricting T to a specific family of functions. Neural networks can approximate continuous functions on a compact set with any desired precision, allowing for the choice of T to be flexible. The universal approximation theorem of neural networks states that they can approximate continuous functions on a compact set with any desired precision. By choosing the family of neural networks as T, the second equality condition can be satisfied theoretically. The final objective is a saddle point of a function that maps input pairs to a scalar value, with the policy to learn acting as a sampling distribution. By choosing neural networks as the family of T, the final objective is a saddle point of a function mapping input pairs to a scalar value. The policy to learn acts as a sampling distribution, with the saddle point trained using mini-batch estimation as a consistent estimator of the true divergence. The true divergence is denoted by Df = sup T T dhdx - f*(T)dh0dx, with \u0125 and h0 as the empirical distribution obtained by sampling from the two distributions. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence, denoted by Df = sup T T dhdx - f*(T)dh0dx. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation, obtained by sampling from the two distributions respectively. The first term of error comes from restricting the parametric family of T to neural networks, while the second term involves the approximation error of empirical mean estimation to the true distribution. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. The first term arises from restricting the parametric family of T to neural networks, while the second term involves the approximation error of empirical mean estimation to the true distribution. By applying the strong law of large numbers and optimality conditions, the error terms can be minimized. The strong law of large numbers (SLLN) applies to the optimization of T function, represented as a discriminator network. By using generative-adversarial approach, the policy distribution h(y|x) is parametrized as a generator neural network. This allows for minimizing error terms related to neural networks and empirical mean estimation. The T function is optimized using Theorem 5 and a generative-adversarial approach. The policy distribution h(y|x) is parametrized as a generator neural network, allowing for error minimization in neural networks and empirical mean estimation. Gumbel soft-max sampling is used for differential sampling from the distribution h(y|x). The Gumbel soft-max sampling method is used for differential sampling from the distribution h(y|x) in structured output problems. The training procedure involves sampling from a logging policy, setting a threshold, initializing generator and discriminator functions, and optimizing the generator distribution to minimize divergence. The training procedure involves sampling 'real' and 'fake' samples, updating for variance regularization, and backpropagating the gradient for optimization. The training procedure involves sampling 'real' and 'fake' samples from D, updating for variance regularization, and backpropagating the gradient for optimization. The algorithm presented solves the robust regularized formulation and includes training for the original ERM formulation. The algorithm presented in the full treatment involves sampling from h0, regularization with hyper-parameter \u03c1, and minimizing divergence steps. The goal is to optimize the generator h*\u03b8(y|x) to approximate the minimizer of R(w) through reweighted loss minimization. The algorithm involves sampling from h0, regularization with hyper-parameter \u03c1, and minimizing divergence steps to optimize the generator h*\u03b8(y|x) for R(w) minimization. The algorithm involves two separate training steps: 1) updating policy parameters to minimize reweighted loss, and 2) updating generator and discriminator parameters to regularize variance for improved generalization. Exploiting historic data is crucial in multi-armed bandit problems. The algorithm involves two separate training steps: 1) updating policy parameters to minimize reweighted loss, and 2) updating generator and discriminator parameters to regularize variance for improved generalization. Exploiting historic data is crucial in multi-armed bandit problems and its variants. Approaches like doubly robust estimators have been proposed, and recent theoretical studies have explored the finite-time minimax risk lower bound of the problem. Bandits problems, including contextual bandit, have wide applications. Doubly robust estimators have been proposed for these problems, and recent theoretical studies have explored the finite-time minimax risk lower bound. Bandits problems can be seen as single-state reinforcement learning problems, with techniques like Q function learning and temporal difference learning used for off-policy learning in RL. Recent works in deep RL have addressed off-policy updates using methods such as multi-step bootstrapping and off-policy training of Q functions. Learning from logs traces, with the application of propensity scores, has also been explored in the theoretical analysis of bandits problems. Off-policy learning in RL involves methods like multi-step bootstrapping and off-policy training of Q functions. Learning from log traces with propensity scores evaluates candidate policies. In statistics, treatment effect estimation focuses on estimating intervention effects from observational studies. Unbiased counterfactual estimators have been derived for computational advertising. In statistics, treatment effect estimation focuses on estimating intervention effects from observational studies using propensity scores. Unbiased counterfactual estimators have been derived for computational advertising, with techniques reducing bandit learning to supervised learning problems but showing poor generalization performance. Our variance regularization aims at off-policy learning with bandit feedback, supported by the study of generalization bounds in importance. The text discusses variance regularization for off-policy learning in computational advertising, with a focus on generalization bounds in importance sampling problems. It also touches on the connection to supervised learning with a convex objective function. The study focuses on generalization bounds in importance sampling problems and variance regularization for supervised learning with a convex objective function. It explores how divergence minimization techniques can be applied to address distribution mismatch in supervised learning and domain adaptation problems. The objective function has connections to distributionally robust optimization problems. The divergence minimization technique can be applied to supervised learning and domain adaptation as an alternative to address distribution match issues. Regularization for the objective function is closely connected to distributionally robust optimization techniques. The Wasserstein distance between empirical distribution and test distribution is a well-studied constraint that achieves robustness. The objective function is closely connected to distributionally robust optimization techniques. The Wasserstein distance between empirical and test distributions is a well-studied constraint for achieving robust generalization performance. The proposed algorithms are empirically evaluated using a conversion from supervised learning to bandit feedback method. The proposed algorithms are evaluated using a conversion from supervised learning to bandit feedback method. A logging policy is constructed for a given dataset, and feedback is collected for each sample. The conditional random field policy is used as the logging policy for benchmarks, with hamming loss as the loss function. The proposed algorithms are evaluated using a conversion from supervised learning to bandit feedback method. A logging policy is constructed for a given dataset, and feedback is collected for each sample. Bandit feedback datasets are created with sampled actions, loss values, and propensity scores. Two evaluation metrics are used for the probabilistic policy h(Y|x), including expected loss. The bandit feedback datasets are created by passing samples through a logging policy four times. Evaluation metrics for the probabilistic policy include expected loss and average hamming loss of maximum a posteriori probability prediction. The expected loss (EXP) R(h) measures generalization performance of the learned policy, while the average hamming loss of maximum a posteriori probability prediction (MAP) considers diversity of predictions. A model with high MAP but low EXP performance may be overfitting. In practice, MAP predictions do not consider prediction diversity, leading to potential overfitting. Baselines like IPS and POEM are compared for optimization. The study compares different algorithms for optimization, including IPS and POEM, to address potential overfitting in low EXP performance. Neural network policies without divergence regularization are also examined as baselines. The study compares optimization algorithms for addressing overfitting in low EXP performance. Neural network policies without divergence regularization are examined as baselines. Four multi-label classification datasets are used, and statistics are reported in the Appendix. For benchmark comparison, a three-layer feed-forward neural network is used for policy distribution, and a two or three layer feed-forward neural network is used for divergence minimization. The networks are trained with Adam for faster convergence and better performance. The policy distribution is done using a three-layer feed-forward neural network, while a two or three layer feed-forward neural network is used for divergence minimization. The networks are trained with Adam for faster convergence and better performance. PyTorch is used for implementation and Nvidia K80 GPU cards are used for training. Results from 10 experiment runs are averaged for analysis. The neural networks were trained using Adam with different learning rates. PyTorch was used for implementation and Nvidia K80 GPU cards for training. Results from 10 experiment runs were averaged, showing improved test performance with neural network policies. Gumbel-softmax sampling schemes were utilized for regularization. The introduction of neural network policies with Gumbel-softmax sampling schemes led to significant improvements in test performance compared to baseline CRF policies. Additional variance regularization further enhanced testing and MAP prediction losses. The introduction of neural network policies with Gumbel-softmax sampling schemes improved test performance compared to baseline CRF policies. Additional variance regularization further enhanced testing and MAP prediction losses, with no significant difference observed between the two sampling schemes. The effectiveness of variance regularization is studied quantitatively by varying the maximum number of iterations in each divergence minimization sub loop. Different Gumbel soft-max sampling schemes show no significant difference in testing and MAP prediction losses. The expected loss in test sets is plotted against epochs average over 10 runs using the yeast dataset, with models without regularization shown in gray lines. By setting the maximum number of iterations to 10 and adjusting divergence thresholds, the study compared models with and without regularization using Gumbel soft-max sampling. Results showed that regularization led to lower test loss and faster convergence rates, indicating better generalization to test sets. By adding regularization, models show lower test loss, faster convergence, and improved generalization to test sets. Increasing divergence minimization steps enhances test performance and helps the algorithm converge faster. Theoretical bounds suggest better generalization with more training samples. The regularization improves test performance and helps the algorithm converge faster. Theoretical bounds indicate better generalization with more training samples. Varying the number of passes of training data in the bandit dataset shows increasing test performance in expected loss for models with and without regularization. When the number of training samples in the bandit dataset increases, models with and without regularization show increasing test performance in expected loss. Regularized policies demonstrate better generalization performance compared to models without regularization, as stronger regularization leads to improved generalization ability. Regularized policies have better generalization performance compared to models without regularization. Stronger regularization improves generalization ability. However, MAP prediction performance decreases when the number of training samples exceeds 24, indicating potential overfitting. Experiments compare two training schemes: cotraining in Alg. 3 and an easier version in Alg. The experiments compare two training schemes: cotraining in Alg. 3 and an easier version in Alg. 2. Different Gumbel-softmax sampling schemes are also compared. Blending weighted loss and distribution divergence slightly improves performance, but training becomes more challenging. In this section, the comparison of Gumbel-softmax sampling schemes is discussed, showing no significant performance difference between the two. The effect of logging policies on stochasticity and quality is also examined. The training difficulty in balancing the gradient of the objective function is discussed, along with the comparison of Gumbel-softmax sampling schemes showing no significant performance difference. The impact of logging policies on learning performance is explored by modifying the stochasticity with a temperature multiplier. Additional visualizations of metrics can be found in Appendix 7. The impact of stochasticity on learning performance is explored by modifying the logging policy with a temperature multiplier \u03b1. Varying \u03b1 in the range of 2 [-1,1,...,8] results in a more peaked distribution of h0, ultimately leading to a deterministic policy with \u03b1 \u2192 \u221e. Additional visualizations of metrics can be found in Appendix 7. The impact of stochasticity on learning performance is explored by modifying the logging policy with a temperature multiplier \u03b1. Varying \u03b1 in the range of 2 [-1,1,...,8] results in a more peaked distribution of h0, ultimately leading to a deterministic policy with \u03b1 \u2192 \u221e. NN policies outperform logging policies when h0's stochasticity is sufficient, but become more challenging when the temperature parameter exceeds 2/3. The average ratio of expected test loss to the logging policy loss of algorithms is compared, showing NN policies perform better when h0's stochasticity is sufficient. However, it becomes harder to learn improved NN policies when the temperature parameter exceeds 2/3. Stochasticity does not affect expected loss values, and the drop in ratios is mainly due to decreased loss of the logging policy. Policies with stronger regularization show slightly better performance within NN policies. When the temperature parameter exceeds 2/3, it becomes harder to learn improved NN policies. Stronger regularization within NN policies shows slightly better performance. The decreasing stochasticity of h0 makes it harder to obtain an improved NN policy, but regularization can help achieve better generalization performance. Regularization improves performance against weaker models, demonstrating the robustness of the learning principle. As the quality of h0 improves, models consistently outperform baselines, but the difficulty also increases. The impact of logging policies on learned improved policies is discussed, with a better policy leading to bandit datasets with more correct predictions. Visualizations of other metrics can be found in the appendix. The difficulty increases as the quality of h0 improves, with models consistently outperforming baselines. The impact of logging policies on learned improved policies is discussed, highlighting the trade-off between policy accuracy and sampling biases. Varying the proportion of training data points used to train the logging policy from 0.05 to 1 is explored. The study explores the impact of logging policies on learned improved policies by varying the proportion of training data points. As the logging policy improves, both NN and NN-Reg policies outperform it, addressing sampling biases. The study compares the performance of improved policies trained with logging policies ranging from 0.05 to 1. Both NN and NN-Reg policies outperform the logging policy, indicating they can address sampling biases. The increasing ratios of test expected loss to h 0 performance show relative policy improvement. The paper proposes a new training principle inspired by learning bounds for importance. The paper introduces a new training principle to improve generalization performance in off-policy learning for logged bandit datasets by regularizing variance. This training objective combines importance reweighted loss with a regularization term measuring distribution divergence between logging and learned policies. Variational divergence minimization and Gumbel soft-max sampling are applied for implementation. The paper introduces a new training principle for off-policy learning in bandit datasets by combining importance reweighted loss with a regularization term measuring distribution divergence. Variational divergence minimization and Gumbel soft-max sampling techniques are used to train neural network policies effectively. Evaluations on benchmark datasets confirm the effectiveness of the approach, with limitations in implementation highlighted. By utilizing variational divergence minimization and Gumbel soft-max sampling techniques, neural network policies are trained end-to-end to minimize variance. Evaluations on benchmark datasets demonstrate the effectiveness of the learning principle and training algorithm. The main limitation is the requirement for propensity scores, which may not always be available. Learning to estimate propensity scores and incorporating them into the training framework will enhance the applicability of the algorithms. The work emphasizes the importance of propensity scores for training algorithms, suggesting the use of importance weights as an extension. The techniques and theorems can be applied to general supervised learning and reinforcement learning. The importance weights have theoretical guarantees and can be extended to general supervised and reinforcement learning. Applying Lemma 1 to z, importance sampling weight function w(z) = p(z)/p 0 (z) = h(y|x)/h 0 (y|x), and loss l(z)/L, we can bound the variance using Reni divergence. The text discusses the application of Lemma 1 to importance sampling weight function w(z) in bandit learning, bounding the variance using Reni divergence and Bernstein's concentration bounds. The goal is to optimize the importance sampling for bandit learning with theoretical guarantees. The text discusses the optimization of importance sampling in bandit learning using Bernstein's concentration bounds and Reni divergence. It involves updating the generator and discriminator iteratively to obtain an optimized generator. The text discusses optimizing importance sampling in bandit learning using Bernstein's concentration bounds and Reni divergence. It involves updating the generator and discriminator iteratively to obtain an optimized generator with hyper-parameter \u03bb. The text discusses minimizing variance regularized risk in co-training by updating the generator and discriminator iteratively. It also analyzes the effect of stochasticity on test loss with MAP predictions. The text discusses the effect of stochasticity on test loss with MAP predictions. NN policies show improvement over h0 in expected loss, but not in MAP prediction performance. Further investigation is needed to understand this phenomenon. The text discusses the impact of stochasticity on test loss with MAP predictions. While NN policies show improvement over h0 in expected loss, they struggle to outperform baselines in MAP prediction performance. Further investigation is warranted to explore this phenomenon."
}