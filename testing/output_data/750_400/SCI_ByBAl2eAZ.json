{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods can use noise injection in the action space for exploratory behavior. Alternatively, adding noise directly to the agent's parameters can lead to more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods, as shown in experimental comparisons of DQN, DDPG, and TRPO on various environments. Efficient exploration is crucial in deep reinforcement learning to prevent premature convergence to local optima. Parameter noise combined with traditional RL methods benefits both off- and on-policy approaches, as demonstrated in comparisons of DQN, DDPG, and TRPO on different environments. Efficient exploration is essential in deep reinforcement learning to avoid premature convergence to local optima. Various methods have been proposed to address this challenge in high-dimensional and continuous-action Markov decision processes, often relying on complex additional structures. Various methods have been proposed to address exploration challenges in high-dimensional and continuous-action Markov decision processes. One approach is the addition of temporally-correlated noise or parameter noise to increase exploratory nature and obtain a policy with a larger variety of behaviors. The addition of temporally-correlated noise or parameter noise can enhance exploration in algorithms for Markov decision processes. These methods aim to increase the variety of behaviors exhibited by the policy. This paper explores combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to improve exploratory behavior. Experiments demonstrate its effectiveness in both high-dimensional discrete and continuous environments. This paper investigates the effectiveness of combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to enhance exploratory behavior in high-dimensional discrete and continuous environments. Results show that parameter noise outperforms traditional action space noise-based methods, especially in tasks with sparse reward signals. Parameter noise is found to be more effective than traditional action space noise in enhancing exploratory behavior in discrete and continuous control tasks with sparse reward signals. The standard RL framework involves an agent interacting with a fully observable environment modeled as a Markov decision process (MDP). The environment is modeled as a Markov decision process (MDP) with states, actions, initial state distribution, reward function, transition probabilities, time horizon, and discount factor. The agent aims to maximize expected discounted return by following a policy parametrized by \u03b8, which can be deterministic or stochastic. The agent's goal is to maximize the expected discounted return by following a policy parametrized by \u03b8. Experimental evaluation is based on the undiscounted return. Off-policy RL methods allow learning based on data captured by arbitrary policies. This paper considers Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG) as off-policy algorithms. Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG) are popular off-policy algorithms. DQN uses a deep neural network to estimate the optimal Q-value function, while DDPG focuses on policy gradients. Deep Q-Networks (DQN) uses a deep neural network to estimate the optimal Q-value function, with policies derived from the Q-value function to encourage exploration. DDPG is an actor-critic algorithm for continuous action spaces. The DDPG algorithm is an actor-critic approach for continuous action spaces, where the actor is trained to maximize the critic's estimated Q-values by back-propagating through both networks. Exploration in DDPG is achieved through a stochastic policy that adds noise to the action space. The Q-value function is estimated using off-policy data and the Bellman equation, with the actor trained to maximize Q-values by back-propagating through both networks. DDPG uses a stochastic policy for exploration, incorporating action space noise. In contrast to off-policy methods, on-policy approaches like TRPO update function approximators based on the current policy. Trust Region Policy Optimization (TRPO) is an on-policy method that improves upon traditional policy gradient methods by computing an ascent direction that ensures a small change in the policy distribution. It solves a constrained optimization problem to update function approximators based on the current policy. Trust Region Policy Optimization (TRPO) improves upon traditional policy gradient methods by computing an ascent direction that ensures a small change in the policy distribution. It solves a constrained optimization problem to update function approximators based on the current policy. The work considers policies represented as parameterized functions, such as neural networks, and achieves structured exploration by sampling from a set of policies with additive Gaussian noise applied to the parameter vector. The technique involves representing policies as parameterized functions, like neural networks, and achieving structured exploration by sampling from a set of policies with additive Gaussian noise. The perturbed policy is sampled at the beginning of each episode and kept fixed for the entire rollout. State-dependent exploration is emphasized for effective action space noise management. The current policy is updated with noise and kept fixed for the entire rollout. State-dependent exploration is crucial for managing action space noise effectively. In the continuous action space case, Gaussian action noise leads to different actions being sampled for the same state in each rollout. Parameter space noise, on the other hand, perturbs policy parameters at the beginning of each episode, resulting in the same action being taken for the same state in each rollout. When perturbing policy parameters at the beginning of each episode, the same action will be taken every time the same state is sampled in the rollout, ensuring consistency in actions and introducing a dependence between the state and the exploratory action taken. This perturbation method may not be immediately obvious for deep neural networks with millions of parameters and complex interactions. Perturbing deep neural networks involves reparameterizing the network to introduce meaningful perturbations using spherical Gaussian noise. Layer normalization is used between perturbed layers to ensure consistent perturbation scale across all layers. The reparameterization of deep neural networks allows for meaningful perturbations using spherical Gaussian noise. Layer normalization is utilized between perturbed layers to maintain a consistent perturbation scale across all layers, addressing the challenge of varying sensitivities to noise. Adaptive noise scaling in parameter space requires selecting a suitable scale \u03c3, which can be challenging due to the dependence on network architecture and changing sensitivities over time. Adaptive noise scaling in parameter space involves selecting a suitable scale \u03c3 that can vary over time due to changing sensitivities in the network architecture. This adaptation resolves limitations by adjusting the scale of parameter space noise over time. The proposed solution involves adapting the scale of parameter space noise over time based on the variance in action space it induces. This is achieved by defining a distance measure between perturbed and non-perturbed policy in action space and adjusting the parameter space noise accordingly. The approach involves adjusting parameter space noise based on the variance in action space it causes. A distance measure is defined between perturbed and non-perturbed policies in action space, with the parameter space noise being scaled up or down depending on the threshold. In off-policy methods, parameter space noise can be directly applied due to the nature of the collected data. The realization of distance measures for DQN, DDPG, and TRPO depends on the algorithm. Parameter space noise can be used in off-policy methods by perturbing the policy for exploration. On-policy methods can incorporate parameter noise using an adapted policy gradient. Off-policy data collection allows for the use of parameter space noise in exploration. On-policy methods can incorporate parameter noise through an adapted policy gradient, optimizing stochastic policies for expected returns. Policy gradient methods optimize stochastic policies by incorporating parameter space noise. The expected return is expanded using likelihood ratios and the re-parametrization trick for N samples. The value of \u03a3 is fixed to \u03c3^2I and scaled adaptively. This section explores the benefits of incorporating parameter space noise in state-of-the-art RL algorithms for exploring sparse reward environments effectively. The section discusses the benefits of incorporating parameter space noise in state-of-the-art RL algorithms and compares it to evolution strategies for deep policies in terms of sample efficiency. Reference implementations of DQN and DDPG with adaptive parameter space noise are available online. The added value of parameter space noise over action space noise is compared in high-dimensional discrete-action environments and continuous control tasks using DQN, DDPG, and TRPO. For discrete environments, the Arcade Learning Environment benchmark is used with a standard DQN implementation. In high-dimensional discrete-action environments and continuous control tasks, comparisons are made using DQN for discrete environments and DDPG and TRPO for continuous tasks. The Arcade Learning Environment benchmark is used for discrete environments with a standard DQN implementation. The baseline DQN agent is compared with a version using parameter noise, with a linear annealing process from 1.0 to 0.1 over the first 1 million timesteps. In comparing a baseline DQN agent with -greedy action noise to a version with parameter noise, a linear annealing process is used over the first 1 million timesteps. The scale for parameter noise is adjusted based on KL divergence between perturbed and non-perturbed policy. This ensures a fair comparison between action space noise and parameter space noise without introducing additional hyperparameters. By using parameter perturbation, the network is reparametrized to represent the greedy policy \u03c0 implied by the Q-values, avoiding direct perturbation of the Q-function. This involves adding a fully connected layer after the convolutional part of the network to predict the policy \u03c0(a|s) with a softmax output layer. This approach ensures a fair comparison between action space noise and parameter space noise without the need for an additional hyperparameter. By reparametrizing the network to represent the policy \u03c0 implied by Q-values, parameter perturbation is used instead of directly perturbing the Q-function. A fully connected layer is added to predict the policy \u03c0(a|s) with a softmax output layer, allowing for meaningful changes in the behavioral policy. The Q-network is trained following standard DQN practices, while the policy \u03c0 is trained to maximize the probability of outputting the greedy action. The policy \u03c0 is trained by maximizing the probability of outputting the greedy action based on the current Q-network. Comparisons are made against regular DQN and two-headed DQN with -greedy exploration to evaluate the effectiveness of the parameter space noise approach. The policy is trained to output the greedy action based on the current Q-network. Parameter space noise is compared against regular DQN and two-headed DQN with -greedy exploration. Random actions are sampled for the first 50 thousand timesteps to fill the replay buffer before training. Parameter space noise performs better when combined with a bit of action space noise. The study compares parameter space noise with regular DQN and two-headed DQN using -greedy exploration. Random actions are sampled initially to fill the replay buffer before training. Parameter space noise performs better when combined with a bit of action space noise. The experiments were conducted on 21 games of varying complexity, with each agent trained for 40 million frames. Performance was evaluated by running each configuration three times. The study conducted experiments on 21 games of varying complexity, training each agent for 40 million frames. Results show that parameter space noise often outperforms action space noise, especially on games that require exploratory policy evaluation. The study compared the performance of parameter space noise and action space noise on 21 games, showing that parameter space noise often outperforms action space noise, especially on games that require consistency. The results also indicate that learning progress starts sooner when using parameter space noise. Additionally, a comparison against a double-headed version of DQN with -greedy exploration was conducted to ensure architecture changes were not responsible for improvements. Parameter space noise outperforms action space noise, especially on games requiring consistency. Learning progress starts sooner with parameter space noise. Results confirm architecture changes are not responsible for improved exploration. However, parameter space noise struggles in extremely challenging games like Montezuma's Revenge. More sophisticated exploration methods may be necessary for these games. The results confirm that parameter space noise is not responsible for improved exploration. More sophisticated exploration methods like BID4 may be necessary for challenging games like Montezuma's Revenge. It would be interesting to evaluate the effect of combining parameter space noise with exploration methods. Proposed improvements to DQN include double DQN, prioritized experience replay, and dueling networks. Parameter space noise is being compared with action noise in continuous control environments using DDPG as the RL algorithm. Proposed improvements to DQN are orthogonal to this comparison and may further enhance results. Experimental validation of this theory is left for future work. The comparison between parameter noise and action noise in continuous control environments using DDPG as the RL algorithm is conducted. Proposed improvements to DQN are considered orthogonal to this comparison and may enhance results further. Experimental validation of this theory is deferred for future work. DDPG BID18 is used as the RL algorithm with layer normalization applied after each layer. Performance is compared with different noise configurations: (a) no noise, (b) uncorrelated additive Gaussian action space noise, (c) correlated additive Gaussian action space noise, and (d) adaptive parameter space noise. Scale is adapted for parameter space noise to ensure comparable changes in action space. The study evaluates DDPG BID18 performance with various noise configurations: no noise, uncorrelated Gaussian action space noise, correlated Gaussian action space noise, and adaptive parameter space noise. Results are shown for continuous control tasks with each agent trained for 1 million timesteps. The study evaluates DDPG BID18 performance with different noise configurations on continuous control tasks. Results show that parameter space noise outperforms other configurations, leading to higher returns on HalfCheetah environment. Parameter space noise achieves significantly higher returns on HalfCheetah compared to other exploration schemes, breaking out of sub-optimal behavior and outperforming correlated action space noise. Parameter space noise outperforms correlated action space noise on HalfCheetah, breaking out of sub-optimal behavior and achieving higher returns. In other environments, parameter space noise performs similarly to other exploration strategies, but DDPG can still learn good policies even without noise. In the Walker2D environment, adding parameter noise reduces performance variance between seeds, suggesting it helps in escaping sub-optimal behavior. The results for TRPO in the Walker2D environment show that adding parameter noise decreases performance variance between seeds, indicating it aids in escaping local optima and enables learning on environments with sparse rewards. Parameter noise helps in escaping local optima and enables learning on environments with sparse rewards. A toy example is used to evaluate parameter noise, where the agent moves in a chain of states with different rewards. The study evaluates parameter noise on a toy problem with a chain of states and varying rewards. Different DQN methods are compared with varying chain lengths and seeds for evaluation. The study compares adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN on a toy problem with varying rewards in a chain of states. The chain length N is varied, and the performance of each method is evaluated with different seeds. The problem is considered solved if one hundred subsequent rollouts achieve the optimal return. The study evaluates adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN on a toy problem with varying rewards in a chain of states. Three different seeds are trained and evaluated, with the problem considered solved if one hundred subsequent rollouts achieve the optimal return. The median number of episodes before the problem is considered solved is plotted, with green indicating success and blue indicating failure within 2 thousand episodes. Less number of episodes before solved is better. The study compares adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN on a toy problem with varying rewards. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN. The environment is simple, with the optimal strategy being to always go right. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN in a simple environment where the optimal strategy is always to go right. However, in more complex environments where the optimal action depends on the current state, parameter space noise may not work as well. In a case where the agent needs to select a different optimal action depending on the current state, parameter space noise may not work well as weight randomization of the policy is less likely to yield the desired behavior. Results only highlight the difference in exploration behavior compared to action space noise in this specific case. In challenging continuous control environments, sparse rewards are used to incentivize significant progress towards a goal. Examples include SparseCartpoleSwingup, SparseDoublePendulum, and SparseHalfCheetah, where rewards are only given for specific achievements. Sparse rewards are used in challenging continuous control environments to incentivize progress towards specific goals. Environments like SparseCartpoleSwingup, SparseDoublePendulum, and SparseHalfCheetah only yield rewards for achieving certain milestones. Other environments like SparseMountainCar and SwimmerGather also have specific reward conditions. Tasks have a time horizon of T = 500 steps before resetting, and DDPG and TRPO are used to solve these environments. Sparse rewards are used in challenging continuous control environments to incentivize progress towards specific goals. Tasks have a time horizon of T = 500 steps before resetting, and DDPG and TRPO are used to solve these environments. The performance of DDPG in various tasks is shown in FIG6, while the results for TRPO are in Appendix F. The overall performance is estimated by running each configuration with five different random seeds, plotting the median return (line) and interquartile range (shaded area). Sparse rewards are utilized in challenging continuous control environments to encourage progress towards specific goals. DDPG's performance in solving tasks is illustrated in FIG6, with TRPO results found in Appendix F. The overall performance is evaluated by running each configuration with five random seeds, displaying the median return (line) and interquartile range (shaded area). SparseDoublePendulum is relatively easy to solve with DDPG, while SparseCartpoleSwingup and SparseMountainCar require parameter space noise for successful policy learning. SparseDoublePendulum is easy to solve with DDPG, while SparseCartpoleSwingup and SparseMountainCar require parameter space noise for successful policy learning. SparseHalfCheetah shows some progress with DDPG but fails to learn a successful policy. SwimmerGather tasks prove challenging for all DDPG configurations. Parameter space noise can enhance exploration behavior in these algorithms. Parameter space noise can improve exploration behavior in off-the-shelf algorithms like DDPG, but its effectiveness varies depending on the task. It is important to evaluate the potential benefits of using parameter space noise on a case-by-case basis. Evolution strategies (ES) also utilize noise in the parameter space for exploration, which can lead to improvements in learning. Parameter space noise can enhance exploration in off-the-shelf algorithms, but its effectiveness varies by task. Evolution strategies (ES) use noise in the parameter space for exploration, leading to improved learning. ES lacks temporal information and relies on black-box optimization, while combining parameter space noise with traditional RL algorithms allows for temporal information integration and optimization through back-propagation gradients. By combining parameter space noise with traditional RL algorithms, temporal information can be integrated for optimization using back-propagation gradients. This approach aims to improve exploration behavior while still benefiting from the advantages of both methods. Comparing ES and traditional RL with parameter space noise directly on 21 ALE games shows DQN outperforming ES on 15 out of 21 Atari games, despite being exposed to significantly less data. The study compares ES and DQN with parameter space noise on 21 Atari games, showing DQN outperforming ES on 15 games despite less data exposure. ES results were obtained after training on 1,000 M frames, while DQN trained on 40 M frames. Parameter space noise combines ES exploration properties with traditional RL sample efficiency. Parameter space noise outperforms ES on 15 out of 21 Atari games, demonstrating its combination of exploration properties with sample efficiency in reinforcement learning. Various algorithms have been proposed to address exploration in RL, but real-world problems often involve continuous and high-dimensional state and action spaces, posing challenges for traditional algorithms. In real-world reinforcement learning problems with continuous and high-dimensional state and action spaces, traditional algorithms become impractical. Various techniques in deep reinforcement learning aim to improve exploration but are computationally expensive. R\u00fcckstie\u00df et al. proposed perturbing policy parameters as a solution. In the context of deep reinforcement learning, techniques have been proposed to improve exploration, but they are computationally expensive. R\u00fcckstie\u00df et al. suggested perturbing policy parameters as a solution, which outperforms random exploration. The authors proposed a policy perturbation method that outperforms random exploration in policy gradient methods. Their approach was evaluated with specific algorithms on low-dimensional policies and state spaces. In contrast, our method is applied to both on and off-policy settings, uses high-dimensional policies, and is related to evolution strategies. Our method is applied and evaluated for both on and off-policy settings, using high-dimensional policies and environments with large state spaces. It is closely related to evolution strategies and policy optimization methods. Evolution strategies (ES) are closely related to policy optimization methods. ES has shown success in high-dimensional environments like Atari and OpenAI Gym, but it lacks efficiency in handling temporal structures and sample inefficiency. Bootstrapped DQN offers more directed exploration, while our approach perturbs network parameters directly for similar results. Parameter space noise is proposed as a simpler and sometimes superior method for exploration compared to Bootstrapped DQN BID20. This approach perturbs network parameters directly, offering an effective alternative to traditional action space noise. Parameter space noise is suggested as a more effective method for exploration compared to traditional action space noise like -greedy and additive Gaussian noise. It perturbs network parameters directly, showing improved performance in deep RL algorithms such as DQN, DDPG, and TRPO. Parameter space noise is proposed as a superior alternative to traditional action space noise in deep RL algorithms like DQN, DDPG, and TRPO. It has shown improved performance, especially in environments with sparse rewards where action noise fails. This suggests that parameter space noise could be a viable alternative to the current standard of action space noise in reinforcement learning applications. Parameter noise is suggested as a better option than action noise in deep RL algorithms like DQN, DDPG, and TRPO, especially in environments with sparse rewards. The network architecture for ALE BID3 includes 3 convolutional layers and 1 hidden layer with ReLUs and layer normalization. In ALE BID3 network architecture, 3 convolutional layers followed by a hidden layer with ReLUs and layer normalization are used. A second head for parameter space noise is included with a policy network having a softmax output layer. Target networks are updated every 10 K timesteps. The Q-value network in the ALE architecture uses layer normalization in the fully connected part, with a second head for parameter space noise determining a policy network. Target networks are updated every 10 K timesteps, and the Q-value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32. The replay buffer can hold 1 M state transitions, and for the -greedy baseline, noise is scaled adaptively in action space. The Q-value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32. The replay buffer can hold 1 M state transitions. Parameter space noise is scaled adaptively in action space, ensuring maximum KL divergence between perturbed and non-perturbed \u03c0 is softly enforced. The policy is perturbed at the beginning of each episode, with the standard deviation adapted every 50 timesteps. The policy is perturbed at the beginning of each episode, with the standard deviation adapted every 50 timesteps. Layer normalization is included in the fully connected part of the network to avoid getting stuck. -greedy action selection with = 0.01 is used to collect initial data for the replay buffer before training. The network includes layer normalization in the fully connected part to prevent getting stuck. -greedy action selection with = 0.01 is used for initial data collection. Initial data collection involves 50 K random actions before training. Observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale, with a concatenation of 4 subsequent frames sent to the network. Before training starts, initial data for the replay buffer is set with specific parameters such as \u03b3 = 0.99, reward clipping, gradient clipping, down-sampling observations to 84 \u00d7 84 pixels, grayscale conversion, concatenation of 4 frames, and using up to 30 noop actions at the beginning of each episode. The network architecture for DDPG includes 2 hidden layers with 64 ReLU units each for both the actor and critic, with actions included in the second hidden layer for the critic. Layer normalization is also applied. For DDPG, the network architecture includes 2 hidden layers with 64 ReLU units each for both the actor and critic. Actions are included in the second hidden layer for the critic. Layer normalization is applied to all layers. The target networks are soft-updated with \u03c4 = 0.001. The critic is trained with a learning rate of 10 \u22123 while the actor uses a learning rate of 10 \u22124. Both actor and critic are updated using the Adam optimizer with batch sizes of 128. The critic is regularized using an L2 penalty with 10 \u22122. The actor and critic in DDPG are updated using the Adam optimizer with batch sizes of 128. The critic is regularized with an L2 penalty of 10 \u22122. The replay buffer holds 100 K state transitions and \u03b3 = 0.99 is used. Each observation dimension is normalized by an online estimate of the mean and variance. Parameter space noise is scaled to match action space noise. For dense environments, action space noise with \u03c3 = 0.2 is used. TRPO uses a step size of \u03b4 KL = 0.01, a policy network of 2 hidden layers with 32 tanh units for nonlocomotion tasks, and 2 hidden layers of 64 tanh units for locomotion tasks. The study uses action space noise with different \u03c3 values for different environments. TRPO utilizes specific parameters for step size, policy network architecture, Hessian calculation, and batch size. OpenAI Gym and rllab environments are employed for the experiments. The study uses specific parameters for TRPO, including action space noise with different \u03c3 values. OpenAI Gym and rllab environments are utilized for experiments, with tasks involving Hessian calculation, subsampling, and batch size adjustments. The study utilizes various environments from rllab, including DISPLAYFORM2, DISPLAYFORM3, DISPLAYFORM4, SparseDoublePendulum, and DISPLAYFORM5 with specific reward conditions. The state encoding follows BID20's proposal, and DQN is used with a simple network to approximate the Q-value function. The study uses different environments from rllab with specific reward conditions. DQN is employed with a simple network to approximate the Q-value function, using layer normalization for hidden layers. Each agent is trained for up to 2K episodes, varying the chain length N and evaluating performance after each episode. The study uses DQN with a simple network to approximate the Q-value function, using layer normalization for hidden layers. Each agent is trained for up to 2K episodes, varying the chain length N and evaluating performance after each episode. The problem is considered solved if one hundred subsequent trajectories achieve the optimal episode return in a simple and scalable environment. The current policy is evaluated by sampling a trajectory with noise disabled. The problem is considered solved if one hundred subsequent trajectories achieve the optimal episode return. Different DQN variations are compared in a simple and scalable environment. In testing for exploratory behavior, various DQN variations are compared, including adaptive parameter space noise DQN, bootstrapped DQN, and -greedy DQN. Parameters such as the number of heads, masking probability, and annealing schedule are specified. Key details include the use of a single head for adaptive parameter space noise, scaling of noise, replay buffer size, learning start time, target network update frequency, optimizer choice, learning rate, and batch size. The study compares various DQN variations for exploratory behavior, including adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN. Key details include the use of a single head for adaptive parameter space noise, scaling of noise, replay buffer size, learning start time, target network update frequency, optimizer choice, learning rate, and batch size. Parameter space noise is adaptively scaled with \u03b4 \u2248 0.05, \u03b3 = 0.999, and training details such as updating the target network every 100 timesteps and using the Adam optimizer with a learning rate of 10^-3 and batch size of 32 are specified. The stochastic policy \u03c0\u03b8(a|s) with \u03b8 \u223c N(\u03c6, \u03a3) is used, and the expected return is expanded using likelihood ratios and the reparametrization trick for N samples i \u223c N(0, I) and \u03c4i \u223c (\u03c0. This approach allows for subtracting a variance-reducing baseline, with \u03a3 set as \u03c3^2I. The expected return is expanded using likelihood ratios and the reparametrization trick for N samples i \u223c N(0, I) and \u03c4i \u223c (\u03c0. This allows for subtracting a variance-reducing baseline, with \u03a3 set as \u03c3^2I and using an adaption method to re-scale as needed. Parameter space noise requires selecting a suitable scale \u03c3, which can vary over time and depend on the network architecture. In our case, we set \u03a3 as \u03c3^2I and adapt the scale of parameter space noise over time using a time-varying scale \u03c3k to address limitations and variations in network architecture. The proposed solution involves adapting the scale of parameter space noise over time using a time-varying scale \u03c3k, which is related to action space variance and updated every K timesteps. This approach aims to address limitations and variations in network architecture. The proposed solution involves updating the time-varying scale \u03c3k every K timesteps based on a heuristic related to action space variance. This approach is inspired by the Levenberg-Marquardt heuristic and involves rescaling \u03c3k using a distance measure d(\u00b7, \u00b7), \u03b1 \u2208 R >0, and \u03b4 \u2208 R >0. The choice of distance measure and \u03b4 depends on the policy representation, with different methods like DDPG, TRPO, and DQN using different approaches. In the proposed solution, the time-varying scale \u03c3k is updated every K timesteps based on a heuristic related to action space variance. The choice of distance measure and \u03b4 depends on the policy representation, with different methods like DDPG, TRPO, and DQN using different approaches. In our experiments, \u03b1 is always set to 1.01. For DQN, the policy is defined implicitly by the Q-value function, leading to pitfalls in measuring distance between Q-values. In our experiments, \u03b1 is always set to 1.01. For DQN, the policy is defined implicitly by the Q-value function, leading to pitfalls in measuring distance between Q-values. A probabilistic formulation is used for both non-perturbed and perturbed policies. In experiments, \u03b1 is set to 1.01. For DQN, policy is defined implicitly by Q-value function, leading to pitfalls in measuring distance between Q-values. Probabilistic formulation used for non-perturbed and perturbed policies. Q-values are compared using a probabilistic formulation of policies. The policies are defined using softmax function over predicted Q values. The distance in action space is measured using Kullback-Leibler divergence. This formulation normalizes Q-values and avoids the need for an additional hyperparameter \u03b4. The distance measure is related to -greedy action space noise for fair comparison. The distance measure in action space is related to -greedy policy, allowing for fair comparison between different approaches without the need for an additional hyperparameter \u03b4. The KL divergence between a greedy policy and an -greedy policy can be expressed as D KL (\u03c0 \u03c0) = \u2212 log (1 \u2212 + |A| ), where |A| denotes the number of actions. The KL divergence between a greedy policy and an \u03b5-greedy policy can be used to relate action space noise and parameter space noise in DDPG. This allows for fair comparison without the need for an additional hyperparameter \u03b4. The KL divergence between a greedy policy and an \u03b5-greedy policy can be used to relate action space noise and parameter space noise in DDPG. This involves adaptively scaling \u03c3 to match the KL divergence between policies, setting \u03b4 := \u2212 log (1 \u2212 + |A| ). The distance measure between non-perturbed and perturbed policy is used to compare noise induced by parameter space perturbations and additive Gaussian noise in DDPG. In DDPG, noise induced by parameter space perturbations is related to additive Gaussian noise using a distance measure between non-perturbed and perturbed policies. The adaptive parameter space threshold \u03b4 is set to \u03c3, resulting in effective action space noise with the same standard deviation as regular Gaussian noise. For TRPO, noise vectors are scaled by computing a natural step H \u22121 \u03c3 to create a trust region. The adaptive parameter space threshold \u03b4 is set to \u03c3 for effective action space noise in TRPO. Noise vectors are scaled by computing a natural step H \u22121 \u03c3 to ensure perturbed policies remain close to the non-perturbed version. Trust region is created using the conjugate gradient algorithm. The trust region around the noise direction is computed to keep the perturbed policy close to the original using the conjugate gradient algorithm and line search. Learning curves for 21 Atari games are shown in FIG8, while TAB2 compares ES and DQN performance after a certain number of frames. Performance is evaluated by running 10 episodes with exploration disabled. The learning curves for 21 Atari games are shown in FIG8, with comparisons between ES and DQN performance after a specific number of frames in TAB2. Performance is evaluated by running 10 episodes with exploration disabled. In Appendix C of Schulman et al. (2015b), the results for InvertedPendulum and InvertedDoublePendulum are noted to be noisy due to small policy changes affecting performance significantly. Adaptive parameter space noise shows promising results. The results for InvertedPendulum and InvertedDoublePendulum are noisy due to small policy changes affecting performance significantly. Adaptive parameter space noise achieves stable performance on InvertedDoublePendulum. Performance is comparable to other exploration approaches, indicating that these environments with DDPG are not ideal for testing exploration. TRPO with noise scaled accordingly also shows promising results. Adding parameter space noise improves learning consistency on challenging sparse environments like InvertedDoublePendulum. TRPO with noise scaled according to parameter curvature shows promising results, indicating the effectiveness of this approach in aiding learning. The performance of TRPO with noise scaled according to parameter curvature is shown in FIG10. TRPO baseline uses action noise with a policy network outputting the mean of a Gaussian distribution. Results indicate that adding parameter space noise helps in learning more consistently on challenging sparse environments."
}