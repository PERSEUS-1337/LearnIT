{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer fully-connected neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function exhibits strong convexity and smoothness in a local neighborhood of the ground truth, allowing gradient descent to converge linearly to a critical point close to the ground truth without needing fresh samples at each iteration. Neural networks with one-hidden-layer and sigmoid activations show strong convexity and smoothness in a local neighborhood of the ground truth. Gradient descent converges linearly to a critical point close to the ground truth without requiring new samples at each iteration, achieving global convergence for empirical risk minimization using cross entropy. This is a significant advancement in learning one-hidden-layer neural networks with optimal sample and computational complexity. Neural networks with one-hidden-layer and sigmoid activations have shown strong convexity and smoothness in a local neighborhood of the ground truth. Gradient descent achieves global convergence for empirical risk minimization using cross entropy without requiring new samples at each iteration. This is a significant advancement in learning one-hidden-layer neural networks with optimal sample and computational complexity. Theoretical underpinnings behind the success of deep neural networks in practical domains like computer vision and artificial intelligence remain mysterious, prompting efforts to understand which classes of functions are involved. Neural networks have attracted research interest due to the success of deep neural networks in computer vision and artificial intelligence. Efforts are being made to understand the theoretical underpinnings behind this success, including which classes of functions can be represented by deep neural networks and why these networks generalize well. Model-recovery setup is a key area of research attracting attention. One important line of research focuses on model-recovery setup, aiming to recover the underlying model parameter W from training samples generated from a neural network model. This is crucial for the network to generalize well. The goal is to recover the underlying model parameter W from training samples generated from a neural network model. Previous studies have focused on two types of data generations, including a regression problem where each sample y is generated using a weight vector w and input x. The curr_chunk discusses two types of data generations: regression problems and classification problems. Regression problems involve generating samples using weight vectors and Gaussian inputs, while classification problems involve drawing labels under conditional distributions. Various studies have focused on different models under these two types of data generations. The curr_chunk discusses neural network models under ReLU activation for regression and classification problems. Previous studies focused on single-neuron and multi-neuron network models, as well as gradient descent for parameter recovery. The curr_chunk discusses the recovery of neural network parameters using gradient descent for regression and classification with Gaussian input. Previous studies focused on single-neuron models and provided statistical guarantees for model recovery using squared loss. The curr_chunk discusses statistical guarantees for model recovery using gradient descent with squared loss. Previous studies showed positive definite Hessian in the local neighborhood of the ground truth, requiring fresh samples at each iteration for convergence. The curr_chunk discusses developing a strong statistical guarantee for gradient descent without per-iteration resampling, focusing on the loss function. This contrasts with previous studies that required fresh samples at each iteration for convergence to the ground truth. In this paper, the focus is on developing a strong statistical guarantee for gradient descent without per-iteration resampling, specifically for the loss function in eq. (2). The study provides the first performance guarantee for recovering one-hidden-layer neural networks using the cross entropy loss function, which is more practical than the squared loss for classification problems. This study offers a strong statistical guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function for classification problems. If the input is Gaussian, the empirical risk function based on the cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth. For multi-neuron classification with sigmoid activations, if input is Gaussian, the empirical risk function is uniformly strongly convex near the ground truth. Gradient descent converges linearly to a critical point with sample complexity near-optimal. Gradient descent converges linearly to a critical point with a sample complexity of O(dK 5 log 2 d) for multi-neuron classification with sigmoid activations. The convergence to a critical point W n is shown to exist, with a near-optimal sample complexity. The recovery of W is up to a certain statistical accuracy, converging to W at a rate of O(dK 9/2 log n/n) in the Frobenius norm. The convergence guarantee does not require a fresh set of samples at each iteration due to uniform strong convexity in the local neighborhood. The text discusses the convergence of W in a quantized label setting, with a rate of O(dK 9/2 log n/n) in the Frobenius norm. The convergence guarantee does not need fresh samples at each iteration due to uniform strong convexity. The computational complexity for achieving -accuracy is O(ndK 2 log(1/ )). The tensor method from BID38 provides an initialization near the ground truth, replacing the homogeneous assumption on activation functions with a condition on the curvature. The text discusses the convergence of W in a quantized label setting with a rate of O(dK 9/2 log n/n) in the Frobenius norm. The computational complexity for achieving accuracy is O(ndK 2 log(1/ )). The tensor method from BID38 provides an initialization near the ground truth by considering the curvature of activation functions. The proof develops new techniques to analyze the cross-entropy loss function using statistical information of geometric curvatures. The proof introduces new techniques to analyze the challenging cross-entropy loss function by leveraging statistical information of geometric curvatures, including gradient and Hessian of the empirical risk, to ensure uniform concentrations. Performance guarantees for classification using squared loss are also discussed, but omitted due to space constraints. Our technique focuses on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization. The parameter recovery viewpoint is crucial for success in signal processing problems such as matrix completion, phase retrieval, blind deconvolution, and tensor decomposition. The parameter recovery viewpoint is important for non-convex learning in signal processing problems like matrix completion, phase retrieval, blind deconvolution, and tensor decomposition. Statistical models for data generation remove worst-case instances, focusing on average-case performance with benign geometric properties enabling global convergence of local search algorithms. Studies on one-hidden-layer network models are categorized into landscape analysis and model. The statistical model for data generation effectively removes worst-case instances, allowing focus on average-case performance with benign geometric properties for global convergence of local search algorithms. In the landscape analysis of one-hidden-layer network models, it is known that with a large network size compared to data input, there are no spurious local minima, and all local minima are global. In landscape analysis, for large network sizes compared to data input, there are no spurious local minima, and all local minima are global. However, in the under-parameterized setting with multiple neurons, spurious bad local minima can exist even at the population level. Zhong et. al. provided characterizations for the local Hessian in regression problems with various activation functions. Tian BID33 studied the landscape of the population squared loss surface with ReLU activations, revealing spurious bad local minima even at the population level. Zhong et. al. BID38 provided key characterizations for the local Hessian in regression problems with various activation functions. BID28 showed that gradient descent converges linearly with ReLU activation and Gaussian input when the number of neurons is smaller than the input dimension. In the model recovery problem, BID28 demonstrated linear convergence of gradient descent with ReLU activation and Gaussian input when the number of neurons is smaller than the input dimension. BID21 showed that with bounded derivatives, there is only one global minimum for the regression problem. Our study focuses on analyzing the cross entropy loss function, which differs from previous work. We investigate model recovery in the context of gradient descent convergence with ReLU activation and Gaussian input, showing unique global minimums for regression problems with bounded derivatives. Our study analyzes the cross entropy loss function for the classification problem with sub-Gaussian inputs, focusing on model recovery under the multi-neuron case. This differs from previous work on neural networks with different structures and input types. The paper discusses the model recovery classification problem under the multi-neuron case, which is a new area of study. Previous research has focused on one-hidden-layer or two-layer neural networks with different structures under Gaussian input. The results presented in the paper are not directly comparable to existing studies due to differences in networks and loss functions. The paper introduces a new model recovery classification problem under the multi-neuron case, which differs from previous studies on one or two-layer neural networks. The main results on local geometry and convergence of gradient descent are discussed, along with an initialization method. Numerical examples are provided, and conclusions are drawn in the final section. Boldface letters are used to denote vectors and matrices throughout the paper. Section 3 discusses local geometry and local linear convergence of gradient descent, while Section 4 covers the initialization method. Numerical examples are demonstrated in Section 5, and conclusions are drawn in Section 6. Boldface letters are used to denote vectors and matrices throughout the paper. The transpose of W is denoted by W, W F denotes the spectral norm, and the Frobenius norm. For a positive semidefinite matrix A, we write A 0. The identity matrix is denoted by I. The gradient and Hessian of a function f(W) is denoted by \u2207f(W) and \u22072f(W), respectively. \u03c3i(W) denotes the i-th singular value of W. Denote \u00b7 \u03c81 as the sub-exponential norm of a random variable. Constants c, C, C1 are used, and O(g(x)) and \u2126(g(x)) are defined for nonnegative functions. The generative model for training data and the gradient descent algorithm for learning network weights are described. Training samples {(x_i, y_i)} are drawn i.i.d., with x \u223c N(0, I) and sigmoid activation function \u03c6(z) = 1/(1 + e^(-z)). The one-hidden layer neural network model is used for classification, with the goal of estimating W by minimizing the empirical risk function. The one-hidden layer neural network model is utilized for classification, aiming to estimate W by minimizing the empirical risk function through cross entropy loss. The function is nonconvex, making vanilla gradient descent with arbitrary initialization prone to getting stuck at local minima. The one-hidden layer neural network model aims to estimate W by minimizing the nonconvex empirical risk function using cross entropy loss. To avoid local minima, a well-designed initialization scheme is implemented in the gradient descent algorithm with an update rule given by DISPLAYFORM0. The gradient descent algorithm is implemented with a well-designed initialization scheme to avoid getting stuck at local minima. The update rule is given by DISPLAYFORM0 with a step size \u03b7. The algorithm uses the same set of training samples throughout, unlike other methods that resample at every iteration. Before presenting main results, an important quantity regarding \u03c6(z) is introduced. The algorithm uses the same training samples consistently, unlike other methods that resample at each iteration. An important quantity related to \u03c6(z) is introduced to capture the geometric properties of the loss function. An important quantity related to \u03c6(z) is introduced to capture the geometric properties of the loss function, depicted in Figure 1 for sigmoid activation. The definition differs from previous works but is consistent with removing unnecessary terms. The local strong convexity of f n (W) near the ground truth W is characterized. The local strong convexity of the empirical risk function f n (W) near the ground truth W is guaranteed to have a positive definite Hessian in a neighborhood of W. The condition number \u03ba and \u03bb are defined in relation to the singular values of W. Theorem 1 guarantees the positive definiteness of the Hessian of the empirical risk function in the local neighborhood of the ground truth W for a classification model with sigmoid activation function. Theorem 1 ensures that the Hessian of the empirical cross-entropy loss function is positive definite in a neighborhood of the ground truth W for a classification model with sigmoid activation function, as long as certain conditions are met. Theorem 1 guarantees that the Hessian of the empirical cross-entropy loss function is positive definite in a neighborhood of the ground truth W for a classification model with sigmoid activation function, as long as certain conditions are met. The proof is outlined in Appendix A and applies to all permutation matrices of W. The bounds in Theorem 1 depend on network dimension parameters, activation function, and ground truth values. The Hessian of the empirical cross-entropy loss function is guaranteed to be positive definite in a neighborhood of the ground truth for a classification model with sigmoid activation, under certain conditions. The sample complexity is near-optimal in terms of the number of unknown parameters. The sample complexity for the classification problem is order-wise near-optimal in terms of the number of unknown parameters. The strong convexity of the empirical risk function ensures the existence of a unique local minimizer close to the ground truth, where gradient descent converges. The strong convexity of the empirical risk function guarantees a unique local minimizer near the ground truth, where gradient descent converges linearly. For the classification model with sigmoid activation function, a critical point Wn exists in the local neighborhood of W with high probability. Theorem 2 states that for the classification model with sigmoid activation function, if the sample size is sufficiently large, there exists a unique critical point Wn in the local neighborhood of W where gradient descent converges linearly to Wn. This result holds for all column permutations of W. Theorem 2 guarantees linear convergence of gradient descent to a critical point Wn in the local neighborhood of W for the classification model with sigmoid activation function. This result holds for all column permutations of W and ensures consistent recovery of W as n approaches infinity. The computational complexity for achieving -accuracy is O(ndK^2 log(1/\u03b5)). The tensor method proposed in BID38 is used for initialization, ensuring consistent recovery of W as n approaches infinity. Gradient descent converges linearly to Wn at a linear rate with computational complexity of O(ndK^2 log(1/\u03b5)). The tensor method proposed in BID38 is utilized for initialization, guaranteeing consistent recovery of W as n approaches infinity. The method has a linear computational complexity of O(ndK^2 log(1/\u03b5). It defines vectors and matrices P2 and P3 based on randomly picked vectors, with specific calculations outlined. The tensor method proposed in BID38 is used for initialization, ensuring consistent recovery of W as n approaches infinity. It defines vectors and matrices P2 and P3 based on randomly picked vectors, with specific calculations outlined in Algorithm 2. The tensor method proposed in BID38 is used for initialization, ensuring consistent recovery of W as n approaches infinity. It defines vectors and matrices P2 and P3 based on randomly picked vectors. The initialization algorithm in Algorithm 2 estimates the direction of each column of W and approximates the magnitude and sign of w i for the classification problem. The tensor method proposed in BID38 is used for initialization, ensuring consistent recovery of W as n approaches infinity. It defines vectors and matrices P2 and P3 based on randomly picked vectors. The algorithm estimates the direction of each column of W and approximates the magnitude and sign of w i for the classification problem by applying non-orthogonal tensor decomposition. The activation function \u03c6(z) must satisfy certain conditions, including at least one of M3 and M4 being non-zero. Unlike previous assumptions, this method does not require a homogeneous assumption but focuses on the curvature of the activation function around the ground truth. The activation function must meet specific conditions, such as having at least one non-zero M3 or M4. This method does not rely on a homogeneous assumption but instead focuses on the curvature of the activation function around the ground truth. The performance guarantee for the initialization algorithm is presented in Theorem 3 under certain assumptions. The activation function \u03c6 (\u00b7) must meet specific conditions, such as being strictly monotone over intervals. The performance guarantee for the initialization algorithm is provided in Theorem 3, stating that the output W 0 of Algorithm 2 satisfies DISPLAYFORM1 with high probability. The proof involves accurate estimation of the direction and norm of W. The proof of Theorem 3 involves showing accurate estimation of the direction and norm of W. The argument for the norm approximation differs from previous work, relaxing the homogeneous assumption on activation functions. More details can be found in the supplementary materials. In this section, gradient descent is used to verify the strong convexity of the empirical risk function around W. Multiple random initializations in the local region are expected to converge to the same critical point Wn with the same training samples. Variance of the output of gradient descent is calculated after multiple random initializations. The empirical risk function is strongly convex around W, leading to convergence to the same critical point Wn with the same training samples through multiple random initializations. The variance of the output of gradient descent is calculated after L = 20 random initializations, quantifying the standard deviation of the estimator Wn. An experiment is considered successful if SDn \u2264 10\u22122. The variance of the output of gradient descent is quantified after 20 random initializations, with a successful experiment defined as SDn \u2264 10\u22122. Gradient descent converges to the same local minima with high probability when the sample complexity is large enough. The successful rate of gradient descent is shown in Figure 2, with averaging over 50 sets of training samples for each pair of n and d. The statistical accuracy of the local minimizer is discussed when initialized close to the ground truth, with an average estimation error calculated through Monte Carlo simulations. The statistical accuracy of the local minimizer for gradient descent is demonstrated when initialized near the ground truth. Average estimation error is calculated through Monte Carlo simulations, showing a decrease as sample size increases. The average estimation error decreases gracefully with increasing sample size in gradient descent algorithm. Cross entropy loss outperforms squared loss in classification problems. The performance of gradient descent algorithm is compared for cross entropy loss and squared loss in a classification problem. Cross entropy loss achieves lower error than squared loss, indicating its preference in classification tasks. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem, characterizing sample complexity for local strong convexity around the ground truth. In this paper, the model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem is studied. The sample complexity for guaranteeing local strong convexity around the ground truth is characterized, ensuring that gradient descent converges linearly to the ground truth with high probability. Future work aims to extend the analysis to include more general activation functions like ReLU and network structures such as convolutional neural networks. The paper discusses model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. It aims to extend the analysis to include more general activation functions like ReLU and network structures such as convolutional neural networks. The population loss function is denoted as DISPLAYFORM0, and the proof of Theorem 1 involves showing the smoothness of the Hessian \u2207 2 f (W) with respect to \u2207 2 f (W). The proof of Theorem 1 involves showing the smoothness of the Hessian \u2207 2 f (W) of the population loss function with respect to \u2207 2 f (W). It includes demonstrating local strong convexity and smoothness in a neighborhood of W, B(W, r), and the closeness of the empirical loss function's Hessian \u2207 2 f n (W) to its population counterpart \u2207 2 f (W) uniformly in B(W, r). Lemma 1 shows that the Hessian of the population risk function is smooth around W for sigmoid activations when W F \u2264 1. Lemmas 2 and 3 demonstrate the local strong convexity and smoothness of \u2207 2 f (W) in a neighborhood B(W, r), and the closeness of \u2207 2 f n (W) to \u2207 2 f (W) in the same region. Lemma 1 establishes the smoothness of the Hessian of the population risk function around W for sigmoid activations. This smoothness is crucial for bounding the Hessian in a neighborhood around the ground truth, as shown in Lemma 2. Lemma 1 establishes smoothness of the Hessian of the population risk function for sigmoid activations. This smoothness allows bounding the Hessian in a neighborhood around the ground truth, as shown in Lemma 2. For W F \u2264 1, DISPLAYFORM1 holds for a large enough constant C when W \u2212 W F \u2264 0.7. Lemma 3 aims to show the closeness of the Hessian of the empirical loss function to the population loss function in a uniform sense. The Hessian of the empirical loss function is shown to be close to the Hessian of the population loss function in a uniform sense for sigmoid activations. Lemma 3 establishes this closeness with a constant C, as long as n \u2265 C \u00b7 dK log dK. Combining Lemma 3 and Lemma 1 leads to Theorem 1. Lemma 3 states that for sigmoid activations, with a sample size n \u2265 C \u00b7 dK log dK, the Hessian of the empirical loss function is close to the population loss function. Combining Lemma 3 and Lemma 1 leads to Theorem 1, showing the strong convexity of f n (W) in B(W, r) with at most one critical point. The proof of Theorem 2 follows similar steps. The proof of Theorem 2 involves showing that the gradient \u2207f n (W) concentrates around \u2207f (W) in B(W, r) and guaranteeing the existence of a critical point W n in B(W, r). Additionally, it is shown that W n is close to W and gradient descent converges linearly to W n with a properly chosen step size. The proof of Theorem 2 shows that the gradient \u2207f n (W) concentrates around \u2207f (W) in B(W, r) and guarantees the existence of a critical point W n in B(W, r). Additionally, it demonstrates that W n is close to W and gradient descent converges linearly to W n with a properly chosen step size. The lemma establishes that \u2207f n (W) concentrates around \u2207f (W) for sigmoid activation function. With a properly chosen step size, there exists one critical point W n in B(W*, r) that is close to W. The lemma shows that \u2207f n (W) concentrates around \u2207f (W) for sigmoid activation function. There exists one critical point W n in B(W*, r) close to W, guaranteed by (Mei et al., 2016, Theorem 2) and Corollary 1. The intermediate value theorem guarantees the existence of a critical point in B(W, r). Local linear convergence of gradient descent is established by the update rule. The gradient of f n (W t ) can be expressed as \u2207f n (W t ) = \u2207f (W t ) + O(\u2016W t - W n \u2016). Local linear convergence of gradient descent is proven by showing that the estimation of the direction of W is accurate. The proof involves setting \u03b7 < a certain value for convergence to the local minimizer W n. Theorem 1 states that gradient descent converges linearly to the local minimizer W n by setting \u03b7 < a certain value. The proof is divided into two parts: (a) accurate estimation of the direction of W, and (b) proof based on a mild condition in Assumption 2. Part (b) of the proof is based on a mild condition in Assumption 2. A tensor operation is defined for matrices A, B, and C. BID38 shows that for the regression problem, if the sample size is large enough, a certain condition holds with high probability. The tensor T (A, B, C) is defined for matrices A, B, and C. For the regression problem, if the sample size is sufficient, a certain condition holds with high probability. The proof involves bounding the estimation error of P 2 and R 3 using Bernstein inequality. The proof involves bounding the estimation error of P 2 and R 3 using Bernstein inequality for the classification problem, where the label y i is naturally bounded. The classification problem involves applying Bernstein inequality to all neurons together, with bounded label y i. The regression model requires upper bounding of output y i via activation function conditions. Different proof for estimating w i without homogeneous activation function condition is provided. The proof for estimating w i without homogeneous activation function conditions is provided, using a relaxed condition in Assumption 2. Quantity Q 1 is defined to contain information of w i, which can be estimated through an optimization problem. In Assumption 2, a quantity Q1 is defined to contain information of wi, which can be estimated through an optimization problem. This estimation process involves substituting various values and solving equations to obtain an estimate of wi. In the initialization, Q1 and Vu i are substituted into an equation to estimate \u03b2. The estimate ai of wi can be obtained from another equation. The sign of \u03b2i can correctly estimate si. In the initialization, Q1 and Vu i are used to estimate \u03b2, while the sign of \u03b2i can correctly estimate si. The inverse function of m3,1 has an upper-bounded derivative in a specific interval, allowing for close estimates of Q1 and Vu i. The inverse function of m3,1 has an upper-bounded derivative in the interval (\u03b21,i \u2212 \u03b4, \u03b21,i + \u03b4), allowing for close estimates of Q1 and Vu i. If the sample size n \u2265 dpoly(K, \u03ba, t, log d), then Q1 and Vu i can be arbitrarily close, leading to the desired result. Useful definitions and results, such as the sub-gaussian norm of a random variable X, are introduced for the proofs. The sub-gaussian and sub-exponential norms of a random variable X are defined as X \u03c82 and X \u03c81 respectively. If X \u03c82 is upper bounded, then X is a sub-gaussian random variable. Calculations of the gradient and Hessian of E [ (W ; are provided next. The sub-exponential norm of a random variable X is denoted as X \u03c81. Calculations of the gradient and Hessian of E [ (W ; are then provided. The gradient and Hessian of E are evaluated, with calculations for \u2206 j,l and upper bounding E T 2 j,l,k. The sub-exponential norm of a random variable X is denoted as X \u03c81. The text discusses the calculation of \u2206 j,l and upper bounding E T 2 j,l,k using Cauchy-Schwarz inequality and sub-exponential norms. The text discusses upper bounding E T 2 j,l,k using Cauchy-Schwarz inequality and sub-exponential norms, with a focus on the sigmoid activation function \u03c6(x). The goal is to leave only the denominator in the equation, with the inequality holding for a large constant C. The text presents upper and lower bounds of the Hessian of the population risk at ground truth, and applies Lemma 1 to obtain a uniform bound in the neighborhood of W. The text provides upper and lower bounds of the Hessian of the population risk at ground truth, applying Lemma 1 to obtain a uniform bound in the neighborhood of W. It concludes that if DISPLAYFORM17 holds for some constant C, then \u2207 2 f (W) can be upper bounded as DISPLAYFORM3. The text discusses upper bounding the Hessian of the population risk at ground truth in a neighborhood of W, concluding that if a certain condition holds, the Hessian can be upper bounded as DISPLAYFORM3. In Lemma 1, it is shown that when W \u2212 W F \u2264 0.7 and W \u2212 W F \u2264 min \u03ba 2 \u03bb , 0.7, a certain inequality holds. The proof of Lemma 3 involves adapting analysis from a previous setting, with the -covering number of the Euclidean ball B (W , r) being a key concept. In our setting, we adapt the analysis from BID21. Let N be the -covering number of the Euclidean ball B (W , r), with log N \u2264 dK log (3r/ ) BID35. We consider the -cover set W = {W 1 , \u00b7 \u00b7 \u00b7 , W N }. For any W \u2208 B (W , r), let j (W ) = argmin j\u2208[N ] W \u2212 W j(W ) F \u2264 for all W \u2208 B (W , r). The events A t , B t, and C t are defined as DISPLAYFORM13 DISPLAYFORM14 DISPLAYFORM15. We will bound the terms P (A t ), P (B t ), and P (C t ) separately. Upper bounding P (B t ) involves a technical lemma from BID21. In the current section, we focus on bounding the terms P(A_t), P(B_t), and P(C_t) separately. To upper bound P(B_t), a technical lemma from BID21 is utilized, stating the existence of a constant C such that G_i\u03c81 is upper bounded."
}