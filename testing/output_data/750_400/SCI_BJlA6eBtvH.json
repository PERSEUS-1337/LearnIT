{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a major challenge for neural networks in real-world scenarios with non-stationary data distribution. A Differentiable Hebbian Consolidation model addresses this issue by adding a rapid learning plastic component to the fixed parameters of the softmax output layer. The Differentiable Hebbian Consolidation model addresses catastrophic forgetting in neural networks by adding a rapid learning plastic component to the fixed parameters of the softmax output layer. It enables learned representations to be retained for a longer timescale and integrates task-specific synaptic consolidation methods. The approach is evaluated on various benchmarks including Permuted MNIST, Split MNIST, and Vision Datasets Mixture. The proposed model, Differentiable Hebbian Consolidation, addresses catastrophic forgetting in neural networks by adding a rapid learning plastic component to the softmax output layer. It integrates task-specific synaptic consolidation methods and outperforms comparable baselines on benchmarks like Permuted MNIST and Split MNIST. The model requires no additional hyperparameters and reduces forgetting, showcasing adaptability in dynamic learning environments. Our proposed model, Differentiable Hebbian Consolidation, tackles catastrophic forgetting in neural networks by incorporating a rapid learning plastic component. It outperforms comparable baselines on benchmarks like Permuted MNIST and Split MNIST, requiring no extra hyperparameters and reducing forgetting. The ability to adapt and learn in dynamic environments is a key aspect of human intelligence, challenging to embed in artificial intelligence despite recent advances in machine learning. Recent advances in machine learning have shown improvements in solving complex tasks through extensive training on large datasets. However, ML models used in real-world deployment face non-stationarity, leading to performance degradation when trained with new data after learning is complete. After extensive training on large datasets, machine learning models used in real-world deployment often face non-stationarity, causing performance degradation when trained with new data. This phenomenon, known as catastrophic forgetting, presents a crucial problem for deep neural networks tasked with continual learning. In continual learning, deep neural networks face catastrophic forgetting, hindering their ability to adapt to new tasks without losing performance on previously learned tasks. This poses a significant challenge for models aiming for scalability and efficiency over extended periods. In continual learning, deep neural networks struggle with catastrophic forgetting, hindering their ability to adapt to new tasks without losing performance on previously learned tasks. This challenge arises in real-world applications where the iid assumption is easily violated due to concept drift, imbalanced class distributions, and incomplete initial data representation. In real-world applications, ML systems face continual learning challenges due to concept drift, imbalanced class distributions, and incomplete data representation. This leads to the \"stability-plasticity dilemma\" where the model must balance its plasticity to integrate new information. In real-world applications, ML systems encounter continual learning challenges like concept drift and imbalanced data. The \"stability-plasticity dilemma\" arises, requiring a balance between integrating new knowledge and preserving existing knowledge in neural networks. Synaptic plasticity is crucial for learning and memory in biological networks, with theories explaining human continual learning abilities. In biological neural networks, synaptic plasticity is essential for learning and memory. Two theories explain human continual learning abilities, balancing plasticity and stability. The first theory involves synaptic consolidation in the neocortex, preserving important synaptic parameters for previously learned tasks. The first theory for continual learning is inspired by synaptic consolidation in the mammalian neocortex, preserving important synaptic parameters for previously learned tasks. The second theory, the complementary learning system (CLS) theory, suggests that humans extract high-level structural information and store it in different brain areas while retaining episodic memories. Recent work on differentiable plasticity has shown that neural networks with \"fast weights\" can be trained end-to-end through backpropagation and stochastic gradient descent to optimize synaptic connections and plasticity. Recent work on differentiable plasticity has demonstrated that neural networks can be trained with \"fast weights\" through backpropagation and stochastic gradient descent to optimize synaptic connections and plasticity, while retaining episodic memories. The fast weights change quickly based on input representations, in contrast to the slow weights used in training vanilla neural networks for long-term memory. Recent work on differentiable plasticity has shown that neural networks can be trained with \"fast weights\" that change quickly based on input representations, while slow weights are updated slowly for long-term memory. Miconi et al. (2018) demonstrated that networks with learned plasticity outperform those with uniform plasticity on various tasks. Differentiable plasticity involves training neural networks with fast weights that adapt quickly to input representations, while slow weights are updated gradually for long-term memory. Research has shown that networks with learned plasticity outperform those with uniform plasticity on different tasks. Various approaches have been proposed to address the issue of catastrophic forgetting in fixed-capacity models by adjusting the plasticity of each synapse based on its importance for retaining past memories. This work extends the concept of differentiable plasticity to task-incremental continual learning settings. Differentiable plasticity involves training neural networks with fast weights that adapt quickly to input representations, while slow weights are updated gradually for long-term memory. This work extends the concept to task-incremental continual learning settings by developing a Differentiable Hebbian Consolidation model that selectively adjusts the plasticity of synapses to adapt quickly to changing environments and consolidate previous knowledge. Differentiable Hebbian Consolidation model is developed for task-incremental continual learning settings, adapting quickly to changing environments and consolidating previous knowledge by adjusting synapse plasticity. The traditional softmax layer is modified by augmenting slow weights in the final fully-connected layer with plastic weights using Differentiable Hebbian Plasticity. This model shows flexibility by combining with task-specific synaptic consolidation approaches to overcome catastrophic forgetting. The model proposed combines Differentiable Hebbian Plasticity with task-specific synaptic consolidation approaches to address catastrophic forgetting. It unifies core concepts from Hebbian plasticity, synaptic consolidation, and CLS theory for rapid adaptation to new data while consolidating synapses. Our model integrates Differentiable Hebbian Plasticity with task-specific synaptic consolidation methods to tackle catastrophic forgetting. It combines key concepts from Hebbian plasticity, synaptic consolidation, and CLS theory to adapt quickly to new data, consolidate synapses, and utilize compressed episodic memories in the softmax layer to prevent catastrophic forgetting. Testing on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture validates the effectiveness of our approach. Neural Networks with Non-Uniform Plasticity: The study introduces a method that leverages compressed episodic memories in the softmax layer to prevent catastrophic forgetting. Testing on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture validates the effectiveness of the approach. Additionally, the Imbalanced Permuted MNIST problem demonstrates that plastic networks with task-specific synaptic consolidation methods outperform those with uniform plasticity. The study introduces non-uniform plasticity in neural networks, leveraging compressed episodic memories in the softmax layer to prevent forgetting. It shows that plastic networks with task-specific synaptic consolidation methods outperform those with uniform plasticity on benchmarks like Imbalanced Permuted MNIST. Hebbian learning theory suggests that memory and learning are linked to synaptic plasticity, where the strength of connections between neurons is modified based on correlated activation. After learning, synaptic strength increases while plasticity decreases to retain knowledge. Recent meta-learning approaches have demonstrated the effectiveness of this theory. The Hebbian learning theory states that correlated activation of pre-and post-synaptic neurons strengthens their connection. Recent meta-learning approaches incorporate fast weights in neural networks for one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights in FC layers to bind labels to representations. Recent studies in meta-learning have demonstrated the use of fast weights in neural networks for one-shot and few-shot learning. Munkhdalai & Trischler (2018) introduced a model with fast weights in FC layers to associate labels with representations, implemented using non-trainable Hebbian learning-based associative memory. Rae et al. (2018) proposed a Hebbian Softmax layer to enhance learning of rare classes by combining Hebbian learning and SGD updates. Miconi et al. (2018) suggested differentiable plasticity, utilizing SGD for optimization. The fast weights were implemented with non-trainable Hebbian learning-based associative memory to improve learning of rare classes. Differentiable plasticity proposed by Miconi et al. (2018) uses SGD to optimize synaptic plasticity in addition to fixed weights. This approach offers a powerful new method for training neural networks. Our work introduces a method that augments the slow weights in the FC layer with a set of plastic weights, optimizing synaptic plasticity using SGD. This approach has been demonstrated on recurrent neural networks for pattern memorization and maze exploration tasks, but not on the challenge of catastrophic forgetting. Our work introduces a method to overcome catastrophic forgetting by leveraging task-specific synaptic consolidation and updating only the parameters of the softmax output layer for fast learning and knowledge preservation. This work introduces a method to overcome catastrophic forgetting by leveraging task-specific synaptic consolidation and updating only the parameters of the softmax output layer for fast learning and knowledge preservation. It augments the slow weights in the FC layer with plastic (fast) weights using DHP, focusing on preserving knowledge over time. The strategies employed include Task-specific Synaptic Consolidation to protect previously learned knowledge and CLS Theory, which involves a dual memory system with the neocortex gradually learning structured representations and the hippocampus facilitating rapid learning. Task-specific Synaptic Consolidation and CLS Theory are utilized to protect and retain previously learned knowledge, inspired by works on overcoming catastrophic forgetting. These strategies involve adjusting synaptic strengths and utilizing a dual memory system for structured representation extraction and rapid learning. Task-specific synaptic consolidation involves rapid learning and individuated storage to memorize new instances or experiences. Inspired by works on overcoming catastrophic forgetting, regularization strategies estimate the importance of each parameter or synapse for continual learning. Least plastic synapses retain memories for long timescales, while more plastic synapses are considered less important. Parameters and network parameters are updated either online or after learning. Regularization approaches estimate parameter importance for continual learning, adjusting plasticity to prevent changes to important parameters of previously learned tasks. Regularization approaches in continual learning involve adding a regularizer to the loss function to adjust plasticity and prevent changes to important parameters of previously learned tasks. The main difference lies in how the importance of each parameter is computed, with Elastic Weight Consolidation (EWC) using values from the diagonal of a matrix. In Elastic Weight Consolidation (EWC), the importance of each parameter is computed using values from the diagonal of an approximated Fisher information matrix. An online variant of EWC was proposed to improve scalability by controlling the computational cost of the regularization term. In 2017, an online variant of Elastic Weight Consolidation (EWC) was proposed by Schwarz et al. to improve scalability by controlling the computational cost of the regularization term. Zenke et al. (2017b) introduced Synaptic Intelligence (SI) for computing parameter importance, while Aljundi et al. (2018) developed Memory Aware Synapses (MAS) as an online method to measure parameter sensitivity. Our work is inspired by CLS theory, a computational framework for representing memories with a dual memory system. Various approaches have been developed based on CLS principles, such as Synaptic Intelligence (SI) and Memory Aware Synapses (MAS) for measuring parameter importance and sensitivity. Our work draws inspiration from CLS theory, a computational framework for representing memories with a dual memory system. Various approaches based on CLS principles involve pseudo-rehearsal, exact or episodic replay, and generative replay methods. In our work, we focus on neuroplasticity techniques inspired by CLS theory. Previous approaches include pseudo-rehearsal, exact or episodic replay, and generative replay methods. iCaRL utilizes rehearsal and regularization with an external memory for storing exemplar patterns from old task data. In our work, we focus on neuroplasticity techniques inspired by CLS theory. Previous approaches include generative replay methods like iCaRL, which utilizes rehearsal and regularization with an external memory for storing exemplar patterns from old task data. Our interest lies in alleviating catastrophic forgetting through techniques inspired by CLS theory. Earlier work by Hinton & Plaut (1987) and Gardner-Medwin (1989) demonstrated the use of slow and fast weights in synaptic connections for long-term knowledge storage and temporary associative memory. Recent research has focused on neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting during continual learning. This includes incorporating fast weights in RNNs, Hebbian Softmax layer, and augmenting slow weights in the FC layer with a fast weights matrix. Recent research has explored various techniques inspired by CLS theory to tackle catastrophic forgetting in continual learning, such as incorporating fast weights in RNNs, Hebbian Softmax layer, and augmenting slow weights in the FC layer with a fast weights matrix. Recent research has explored techniques like fast weights matrix, differentiable plasticity, and neuromodulated differentiable plasticity to address catastrophic forgetting in continual learning. These methods focus on rapid learning on simple tasks or meta-learning over a distribution of tasks, enabling one-shot and few-shot learning. The methods for continual learning benchmark problems focus on rapid learning on simple tasks or meta-learning over a distribution of tasks, enabling one-shot and few-shot learning. The Hebbian Softmax layer adjusts its parameters through annealing between Hebbian and SGD updates, achieving fast binding for rarer classes but switching to SGD updates with frequent observations from the same class. Our work focuses on metalearning a local learning rule for fast weights in continual learning setups, using fixed weights and an SGD optimizer. The Hebbian Softmax layer switches between Hebbian and SGD updates based on a scheduling scheme, achieving fast binding for rarer classes but transitioning to SGD with frequent observations from the same class. Our work aims to metalearn a local learning rule for fast weights in continual learning setups, utilizing fixed weights and an SGD optimizer. The model incorporates slow weights and a Hebbian plastic component in the softmax layer, adjusting between Hebbian and SGD updates based on a scheduling scheme. In the model, each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component. The slow weights are represented by \u03b8 \u2208 R m\u00d7d, while the Hebbian component consists of a plasticity coefficient \u03b1 and Hebbian trace Hebb. The \u03b1 parameter scales the Hebbian trace, which accumulates mean hidden activations of the final hidden layer for each target label in a mini-batch. The Hebbian trace, Hebb, accumulates mean hidden activations of the final hidden layer for each target label in a mini-batch. The post-synaptic activations of neurons are computed using Eq. 2 to obtain unnormalized log probabilities, z, which are then passed through a softmax function to get predicted probabilities, \u0177. The parameter \u03b7 in Eq. 3 dynamically learns how quickly to acquire new experiences. The post-synaptic activations of neurons j are computed using Eq. 2 to obtain unnormalized log probabilities z, which are then passed through a softmax function to get predicted probabilities \u0177. The parameter \u03b7 in Eq. 3 dynamically learns how quickly to acquire new experiences and acts as a learning rate for the plastic connections. The \u03b7 parameter also serves as a decay term for the Hebb to prevent instability from positive feedback loops. The network parameters \u03b1 i,j , \u03b7, and \u03b8 i,j are optimized by gradient descent during sequential training. The \u03b7 parameter in the model dynamically learns how quickly to acquire new experiences and serves as a learning rate for plastic connections. It also acts as a decay term for Hebb to prevent instability from positive feedback loops. Network parameters are optimized by gradient descent during sequential training. The model is trained sequentially on different tasks in the continual learning setup, with fixed weights in standard neural networks. The Hebbian weight update is used, along with hidden activations and target classes. The \u03b7 parameter dynamically learns how quickly to acquire new experiences and serves as a learning rate for plastic connections, preventing instability from positive feedback loops. Network parameters are optimized by gradient descent during sequential training. In a continual learning setup, the model uses Hebbian weight updates, hidden activations, and target classes to update weights. The Hebbian traces are only updated during training, and at test time, the most recent traces are used for predictions. The model accumulates hidden activations directly into the softmax output layer weights when a class has been seen. Our model utilizes Hebbian traces during training to learn tasks continually. At test time, the most recent Hebb traces are used for predictions. Hidden activations are accumulated into the softmax output layer weights when a class is encountered, leading to better initial representations and improved test accuracy. This approach allows for fast learning and retention of deep representations over a longer timescale. The model utilizes Hebbian traces during training to learn tasks continually, leading to better initial representations and improved test accuracy. Fast learning is enabled by a highly plastic weight component, with selective consolidation into a stable component to protect old memories. This allows the model to learn to remember by modeling plasticity over a range of timescales. The model utilizes Hebbian traces during training to learn tasks continually, leading to better initial representations and improved test accuracy. DHP Softmax improves test accuracy for a given task by modeling plasticity over a range of timescales. It allows the model to learn to remember and scale easily with increasing number of tasks. DHP Softmax simplifies implementation without extra space or computation, allowing easy scaling with more tasks. Hebbian updates quickly store memory by averaging hidden activations for each class. The final hidden layer, h, averages multiple hidden activations for class c = 6 to form a compressed episodic memory in Hebbian traces. This allows rapid learning and sparse parameter updates to store memory traces for recent experiences without interference. The final hidden layer averages hidden activations for class c = 6 to create a compressed episodic memory in Hebbian traces, improving learning of rare classes and speeding up binding of class labels without additional hyperparameters. A sample implementation of the DHP Softmax using PyTorch is provided in Appendix B. Hebbian Synaptic Consolidation improves learning of rare classes and speeds up binding of class labels without additional hyperparameters. It regularizes the loss and updates synaptic importance parameters in an online manner. A sample implementation using PyTorch is provided in Appendix B. Hebbian Synaptic Consolidation regularizes the loss and updates synaptic importance parameters in an online manner, adapting existing task-specific consolidation approaches without computing the synaptic importance parameters. The updated quadratic loss for Hebbian Synaptic Consolidation involves network parameters as weights of connections between neurons. Task-specific consolidation approaches are adapted without computing synaptic importance parameters on the plastic component, only regularizing slow weights. During training the first task, the synaptic importance parameter was set to 0 for all methods except for SI. During training the first task, the synaptic importance parameter was set to 0 for all methods except for SI, which estimates \u2126 i,j while training. The plastic component of the softmax layer helps prevent catastrophic forgetting by optimizing the plasticity of connections. Our model estimates \u2126 i,j while training, using a plastic component in the softmax layer to prevent catastrophic forgetting. Comparisons are made with Online EWC, SI, and MAS in experiments, where our approach increases DNN capacity by adding plastic weights. An extra set of slow weights is added to the softmax output layer to match capacity. In experiments, the approach increases DNN capacity by adding plastic weights. An extra set of slow weights is added to the softmax output layer to match capacity for fair evaluation. Tested on various benchmarks including Permuted MNIST and Split MNIST. Introduced the Imbalanced Permuted MNIST problem. The study added plastic weights to increase DNN capacity for fair evaluation in sequential task learning. Tested on various benchmarks including Permuted MNIST and Split MNIST, as well as introduced the Imbalanced Permuted MNIST problem. Model evaluation based on average classification accuracy on all tasks trained so far, focusing on memory retention and flexibility. The study introduced the Imbalanced Permuted MNIST problem and evaluated the model's performance on all tasks trained so far. Memory retention, flexibility, and forgetting were measured using test performance on the first and most recent tasks, as well as the backward transfer metric. The study evaluated model performance on all tasks by measuring memory retention, flexibility, and forgetting using test performance on the first and most recent tasks, as well as the backward transfer metric (BWT). Training neural networks with Online EWC, SI, and MAS sequentially established a baseline for comparison of task-specific consolidation methods. The study evaluated model performance on all tasks by measuring memory retention, flexibility, and forgetting using test performance on the first and most recent tasks, as well as the backward transfer metric (BWT). Training neural networks with Online EWC, SI, and MAS sequentially established a baseline for comparison of task-specific consolidation methods. The hyperparameters of the consolidation methods (EWC, SI, MAS) were kept consistent with and without DHP Softmax, and the plastic components were not regularized. MNIST pixels were permuted differently for each task with a fixed random permutation in this benchmark. In the Permuted MNIST and Imbalanced Permuted MNIST benchmarks, a multi-layered perceptron (MLP) network with two hidden layers is used. The input distribution changes between tasks, leading to concept drift. The plastic components are not regularized, and hyperparameters can be found in Appendix A. In the Permuted MNIST and Imbalanced Permuted MNIST benchmarks, a multi-layered perceptron (MLP) network with two hidden layers is utilized. The input distribution varies between tasks, causing concept drift. The plastic component's \u03b7 value is set at 0.001 without extensive tuning. Performance is compared between the network with DHP Softmax and a fine-tuned vanilla MLP network named Finetune. The plastic component's \u03b7 value was set at 0.001 without much tuning effort. Comparing the network with DHP Softmax to a fine-tuned vanilla MLP network named Finetune, DHP Softmax showed improvement in preventing catastrophic forgetting across all tasks. Performance was also compared with and without DHP Softmax using task-specific consolidation methods, showing an increase in average test accuracy as new tasks are learned. The network with DHP Softmax alone improved in preventing catastrophic forgetting across all tasks compared to the baseline network. Performance was compared with and without DHP Softmax using task-specific consolidation methods, showing higher test accuracy with DHP Softmax. Ablation study examined network structural parameters and Hebb traces for interpretability. During sequential training of tasks, DHP Softmax with consolidation maintains higher test accuracy compared to without DHP Softmax. An ablation study analyzes network structural parameters and Hebb traces for interpretability, showing increased plasticity in synaptic connections during task learning. The behavior of \u03b7 during training on 10 tasks in the Permuted MNIST benchmark is analyzed. Initially, \u03b7 increases rapidly in task T1, indicating increased plasticity in synaptic connections. However, after the 3rd task, \u03b7 decays to reduce plasticity and prevent interference between learned representations. Within each task from T4 to T10, \u03b7 shows a similar pattern of initial increase followed by decay. The Frobenius Norm of the Hebb trace suggests controlled growth without runaway positive feedback. After initially increasing in task T1, \u03b7 decays post 3rd task to reduce plasticity and prevent interference. Within tasks T4 to T10, \u03b7 shows a pattern of initial increase followed by decay. The Frobenius Norm of the Hebb trace indicates controlled growth without runaway positive feedback, while the Frobenius Norm of \u03b1 suggests plasticity coefficients grow within each task. The plasticity coefficients in Hebb grow within each task, leveraging the structure in the plastic component. Gradient descent and backpropagation are used for meta-learning to tune the structural parameters. The Imbalanced Permuted MNIST problem introduces imbalanced distributions in each task. The Imbalanced Permuted MNIST problem introduces imbalanced distributions in each task, where training samples in each class are artificially removed based on random probability. This benchmark addresses the challenges posed by class imbalance and concept drift, which can hinder predictive performance. DHP Softmax achieves the best average test accuracy for the best hyperparameters of each method. The benchmark involved addressing challenges of class imbalance and concept drift by removing samples based on random probability. DHP Softmax achieved 80.85% accuracy after learning 10 tasks with imbalanced class distributions, showing a 4.41% improvement over the standard neural network baseline. The significance of compressed episodic memory in Hebbian traces was highlighted in this benchmark. DHP Softmax with MAS achieves 88.80% test accuracy, a 1.48% improvement over MAS alone, outperforming all other methods. The plastic component in Hebbian traces allows rare classes to be remembered longer, resulting in a 0.04 decrease in BWT. The plastic component in DHP Softmax with MAS allows rare classes to be remembered longer, resulting in a 0.04 decrease in BWT. The model achieves an average test accuracy of 88.80% and a 1.48% improvement over MAS alone, outperforming all other methods across all tasks. The MNIST dataset is split into 5 binary classification tasks with disjoint output spaces. The network architecture is similar to Zenke et al. (2017b), using an MLP network with two hidden layers of 256 ReLU nonlinearities each. The MNIST dataset is divided into 5 binary classification tasks with disjoint output spaces. An MLP network with two hidden layers of 256 ReLU nonlinearities is used, along with a cross-entropy loss. The initial \u03b7 value is set to 0.001. DHP Softmax alone achieves 98.23% test performance, providing a 7.80% improvement compared to a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreases BWT, leading to a higher average test accuracy across all tasks, especially the most recent one, T 5. Performance of DHP Softmax combined with task-specific consolidation consistently improves average test accuracy across all tasks, especially the most recent one, T5, in a continual learning setup with 5 vision datasets. The datasets include MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10, with adjustments made for image size and channels. The study focused on training a CNN architecture on 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The datasets were adjusted for image size and channels. The network was trained with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. The initial \u03b7 parameter value was set to 0.0001. The CNN architecture was trained on 5 vision datasets with mini-batches of size 32 using plain SGD. DHP Softmax plus MAS decreased BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax achieved an average test performance of 81.75% and BWT of -0.04 after learning all five tasks. In the study, DHP Softmax combined with MAS reduced BWT by 0.04, leading to a 2.14% increase in average test accuracy compared to MAS alone. SI with DHP Softmax performed the best among competitive methods, achieving an average test performance of 81.75% and a BWT of -0.04 after learning all five tasks. The results show that catastrophic forgetting in continual learning environments can be mitigated by incorporating compressed techniques. The study demonstrated that adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. This allows new information to be learned without interference, improving average test accuracy and reducing BWT. Adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. The \u03b1 parameter in the plastic component automatically scales the magnitude of the plastic connections, allowing the model to protect old knowledge or acquire new knowledge effectively. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically scales the plastic connections, allowing the model to protect old knowledge or acquire new information quickly. DHP Softmax outperformed traditional softmax in benchmarks without introducing additional hyperparameters. The neural network with DHP Softmax showed significant improvement compared to traditional softmax, without introducing extra hyperparameters. The model demonstrated flexibility by incorporating Hebbian Synaptic. The plastic part parameters \u03b1 and \u03b7 were learned with minimal tuning effort. The model's flexibility was demonstrated by incorporating Hebbian Synaptic Consolidation using EWC, SI, or MAS to alleviate catastrophic forgetting after learning multiple tasks. DHP Softmax combined with SI outperformed other consolidation methods on Split MNIST and 5-Vision Datasets Mixture. Hebbian Synaptic Consolidation techniques like EWC, SI, and MAS are used to improve model performance in alleviating catastrophic forgetting after learning multiple tasks. DHP Softmax combined with SI shows superior results on Split MNIST and 5-Vision Datasets, while combining DHP Softmax with MAS leads to overall better performance on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. MAS computes synaptic importance parameters based on Hebb's rule layer by layer. Combining DHP Softmax with MAS leads to superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The local variant of MAS computes synaptic importance parameters based on Hebb's rule layer by layer, prioritizing highly correlated connections. The model consistently shows lower negative BWT and higher average test accuracy with DHP, indicating strong Hebbian influence. Hebbian plasticity, through DHP, prioritizes highly correlated synaptic connections, leading to lower negative BWT and higher test accuracy. This enables neural networks to learn continually, remember distant memories, and reduce catastrophic forgetting when learning from sequential datasets in dynamic environments. Additionally, continual synaptic plasticity aids in learning from limited labelled data and scaling at long timescales. Hebbian plasticity, specifically through DHP, allows neural networks to learn continually and remember distant memories, reducing catastrophic forgetting when learning from sequential datasets in dynamic environments. Continual synaptic plasticity plays a key role in learning from limited labelled data and adapting at long timescales, opening new investigations into gradient descent optimized Hebbian consolidation for learning and memory in DNNs. Our work aims to enable continual learning in neural networks by training on a sequence of tasks, unlike conventional supervised learning. The model receives sequential chunks of tasks to be learned, each with associated training data. Continual learning trains a model on sequential tasks with associated training data. Each task has its own loss function to prevent forgetting. The model learns an approximated mapping to the true underlying function. The model is trained on sequential tasks with specific loss functions to prevent forgetting. After training, the model learns to map new inputs to target outputs for all tasks learned so far. The set of classes in each task can vary, as shown in SplitMNIST and Vision Datasets Mixture benchmarks. Experiments were conducted on Nvidia Titan V or Nvidia RTX 2080. The network is trained on a sequence of tasks with different classes in each task. Experiments were conducted on Nvidia Titan V or Nvidia RTX 2080. Training is done with mini-batches of size 64 using plain SGD with a learning rate of 0.01, and early stopping is applied if validation error does not improve for 5 epochs. For training on a sequence of tasks T n=1:10, the network is trained on either a Nvidia Titan V or a Nvidia RTX 2080 Ti with mini-batches of size 64 using plain SGD with a learning rate of 0.01. Training continues for at least 10 epochs with early stopping if validation error does not improve for 5 epochs. If validation error increases for more than 5 epochs, training on the current task is terminated, network weights and Hebbian traces are reset to values with the lowest test error, and the next task is proceeded to. Hyperparameters for Permuted MNIST experiments include a regularization hyperparameter \u03bb set to 100 for Online EWC. After training on a sequence of tasks, the network weights and Hebbian traces are reset to values with the lowest test error if validation error increases for more than 5 epochs. Hyperparameters for Permuted MNIST experiments include regularization hyperparameters set to specific values for different consolidation methods. For Online EWC, SI, and MAS synaptic consolidation methods, hyperparameters \u03bb were set to specific values. A grid search was conducted to find the best hyperparameter combination for each method using task sequences determined by a single seed.\u03bb values were tested within specific ranges for each method. The damping parameter, \u03be, was set to 0.1. A grid search was performed to determine the optimal hyperparameter combination for Online EWC, SI, and MAS synaptic consolidation methods. Different removal probabilities were applied to each class in the Imbalanced Permuted MNIST problem. For the Imbalanced Permuted MNIST problem, training samples were artificially removed from each class in the original MNIST dataset based on random probabilities. Different removal probabilities were applied to each class for tasks 1 to 10, as shown in Table 2. The distribution of classes in each imbalanced dataset for tasks 1 to 10 is shown in Table 2. For the Imbalanced Permuted MNIST experiments in Figure 5, the regularization hyperparameter \u03bb for each task is crucial. For the Imbalanced Permuted MNIST experiments in Figure 5, the regularization hyperparameter \u03bb for each task-specific consolidation method is crucial. The hyperparameters for Online EWC, SI, and MAS are \u03bb = 400, \u03bb = 1.0, and \u03bb = 0.1 respectively. In SI, the damping parameter \u03be is set to 0.1. The best hyperparameter combination for each synaptic consolidation method is determined similarly to the Permuted MNIST benchmark. The regularization hyperparameters for task-specific consolidation methods are \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. In SI, the damping parameter \u03be is set to 0.1. Hyperparameter combinations were found through grid search with specific values for each method. For synaptic consolidation methods, a grid search was conducted with specific values for Online EWC, SI, and MAS. The regularization hyperparameters for task-specific methods were \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. The regularization hyperparameters for task-specific consolidation methods were determined through a grid search. For Online EWC, \u03bb = 400, for SI \u03bb = 1.0, and for MAS \u03bb = 1.5. Additionally, a damping parameter of \u03be = 0.001 was set for SI. In a grid search, hyperparameters were optimized for synaptic consolidation methods using a 5-task binary classification sequence. Different values were tested for Online EWC, SI, and MAS. The network was trained on 5 tasks with mini-batches of size 64 using plain SGD with a fixed learning rate of 0.01 for 10 epochs. The Vision Datasets Mixture benchmark involves image classification tasks. The Vision Datasets Mixture benchmark includes 5 tasks: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The notMNIST dataset contains font glyphs for letters 'A' to 'J', with 60,000 images used for training and 10,000 for testing. The Vision Datasets Mixture benchmark consists of MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets. Each dataset has specific characteristics and image sizes for training and testing. FashionMNIST, SVHN, and CIFAR-10 datasets contain 60,000 and 10,000 grayscale images sized 28\u00d728, 73,257 and 26,032 color images of size 32\u00d732, and 50,000 and 10,000 color images of size 32\u00d732, respectively. The CNN architecture includes 2 convolutional layers with 20 and 50 channels and LeakyReLU nonlinearities. The CNN architecture for CIFAR-10 dataset includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations. A multi-headed approach was used due to different class definitions between datasets. The CNN model for the CIFAR-10 dataset consists of 2 convolutional layers with LeakyReLU nonlinearities and max-pooling. A multi-headed approach was used to account for different class definitions. The model has trainable \u03b7 values for each connection in the final output layer, improving optimization stability. In the benchmark problems, a trainable \u03b7 value is used for each connection in the final output layer, improving optimization stability and convergence to optimal test performance. This approach allows each plastic connection to adjust its own rate of plasticity when learning new experiences. Using separate \u03b7 parameters for each connection was found to be more effective than using a single \u03b7 value across all connections. The stability of optimization and convergence to optimal test performance is improved by using a trainable \u03b7 value for each connection in the final output layer. This approach allows plastic connections to adjust their own rate of plasticity when learning new experiences. Different hyperparameters are used for task-specific consolidation methods in the experiments. The regularization hyperparameter \u03bb for task-specific consolidation methods is set differently: \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. A random search was conducted to find the best hyperparameter combination for each method, testing various values of \u03bb. Sensitivity analysis was performed on the Hebb decay term \u03b7. Sensitivity analysis was conducted on the Hebb decay term \u03b7 to observe its impact on the average test performance in continual learning. Different values of \u03bb were tested for Online EWC, SI, and MAS methods. The effect of initial \u03b7 values on test performance for tasks T n=1:10 in Permuted MNIST and Imbalanced Permuted MNIST benchmarks was illustrated in Figure 4. Sensitivity analysis was performed on the Hebb decay term \u03b7 to assess its impact on test performance in continual learning. Varying \u03b7 values were tested, with lower values proving most effective in reducing catastrophic forgetting. Additionally, a sensitivity analysis on the \u03b7 parameter was also conducted for the Split MNIST problem. Setting low values for the Hebb decay term \u03b7 proved to be most effective in reducing catastrophic forgetting in continual learning tasks, as shown in sensitivity analyses for various benchmarks like MNIST and Split MNIST. The text chunk discusses sensitivity analysis results for MNIST-variant benchmarks, focusing on test accuracy and learning rate values for plastic connections. It also mentions updating Hebbian traces for the next iteration in the context of continual learning tasks. The curr_chunk discusses initializing weights and learning rates for plastic connections in a neural network model. It includes setting He initialization for weights and updating the learning rate parameter. The curr_chunk provides a PyTorch implementation of the DHP Softmax model, emphasizing the simplicity of implementation using popular ML frameworks. It adds a compressed episodic memory to the final output layer of a neural network through plastic connections. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to the final output layer of a neural network through plastic connections. It was trained on the full CIFAR-10 dataset and sequentially on 5 additional tasks from the CIFAR-100 dataset. Test accuracies for CIFAR-10 and CIFAR-100 splits are reported after learning the final task in the sequence. The DHP Softmax model, trained on CIFAR-10 and CIFAR-100 datasets, outperforms Finetune in class-incremental learning. Test accuracies show DHP Softmax performing as well as training from scratch on some tasks. Test accuracies of Finetune and SI were referenced from a previous study."
}