{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the focus is on generating online explanations during execution to reduce human mental workload. The challenge lies in the interdependence of different parts of an explanation, requiring a general formulation for online explanation generation. Three implementations satisfying various online properties are presented, based on a model reconciliation setting. Evaluation is done with human subjects in a standard planning context. The study focuses on generating online explanations to reduce human mental workload. Three different implementations are presented based on a model reconciliation setting. Evaluation is conducted with human subjects in a standard planning competition domain and in simulation with different problems. As intelligent robots become more prevalent, human-AI interaction is essential. Explanations from AI agents help maintain trust and shared awareness. Prior work on explanation generation often overlooks the recipient's need to understand. The importance of generating explanations from the recipient's perspective to maintain trust and shared awareness in human-AI interaction. The agent should consider discrepancies between the human and its own model when generating explanations. In human-AI interaction, generating explanations from the recipient's perspective is crucial for maintaining trust and shared awareness. The agent should address discrepancies between the human and its own model by reconciling the two models through explanations. The robot's model (M R) and the human's model of expectation (M H) are used to generate explanations when their plans differ. The model reconciliation setting involves the robot adjusting its behavior to align with the human's expectations. Model reconciliation occurs when there are differences between the robot's model (M R) and the human's model of expectation (M H), leading to the robot explaining these differences to maintain coherence. In the model reconciliation setting, the robot adjusts its behavior to align with human expectations by reconciling model differences. One issue is the lack of consideration for the mental workload required for humans to understand explanations. In the context of model reconciliation, the importance of considering the mental workload for humans to understand explanations is highlighted. The argument is made for providing complex explanations online to reduce mental workload by intertwining them with plan execution, despite the challenge of interdependence between explanation parts. In the context of model reconciliation, it is argued that complex explanations should be provided online to reduce mental workload by intertwining them with plan execution. The challenge lies in the interdependence between explanation parts, which must be considered during online explanation generation to ensure smooth communication of information. The online explanation generation process involves spreading out information to be communicated smoothly, considering the interdependence between explanation parts. This is illustrated through a scenario where two friends, Mark and Emma, plan to study together for an exam. Mark and Emma have different study session preferences. Mark likes to break the session into two parts with lunch in between, while Emma prefers one continuous session. Mark knows Emma would suggest ordering takeout for lunch if he had explained his plan. Mark and Emma have different study session preferences. Mark prefers to break the session into two parts with lunch in between, while Emma likes one continuous session. Mark knows Emma would suggest ordering takeout for lunch if he had explained his plan. Without revealing his plan, Mark goes to the library with Emma, studies for 60 minutes, then suggests going to lunch for energy before continuing. He refrains from mentioning he also needs a walk until after lunch to avoid Emma suggesting he take a walk alone. Mark reveals his plan to Emma in the library, studying for 60 minutes before suggesting lunch for energy. He withholds the need for a walk until after lunch to avoid Emma suggesting he go alone. This demonstrates the importance of explaining decisions online to ensure understanding and acceptance. Mark explains his plan to Emma in the library, studying for 60 minutes before suggesting lunch for energy. He delays mentioning a walk until after lunch to prevent Emma from suggesting he go alone. This highlights the significance of providing explanations online to ensure comprehension and approval, even when individuals have different values. The key is to explain minimally and only when necessary, spreading out information throughout the plan execution to reduce mental workload. In this paper, a new method for explanation generation called online explanation is developed, intertwining explanation with plan execution to reduce mental workload. The focus is on conveying information minimally and only when necessary, ensuring a more straightforward interaction between individuals with different values. The new method for explanation generation, called online explanation, aims to reduce mental workload by breaking explanations into parts communicated at different times during plan execution. Three different approaches were implemented, focusing on matching the plan prefix and other \"online\" properties. The new method for online explanation generation aims to reduce mental workload by breaking explanations into parts communicated at different times during plan execution. Three different approaches were implemented, focusing on matching the plan prefix and other \"online\" properties. The model search method ensures that earlier information communicated does not affect later parts of the explanation, creating a desirable experience for the receiver. In the second approach, the focus is on making the next action understandable to the human teammate. The third approach matches the prefix of the robot's plan with any possible optimal human plan. A model search method ensures earlier information does not affect later parts of the explanation, reducing mental workload for the recipient. Recently, explainable AI paradigm has emerged as an essential component to enable AI agents to be self-explanatory in their behaviors, reducing the mental workload for the recipient and enhancing the agent's ability to operate as a teammate. Explainable AI is crucial for AI agents to operate effectively as teammates by providing transparency and improving human trust in the decision-making process. Explainable AI is essential for human-AI collaboration, improving trust and maintaining shared awareness. The effectiveness of explainable agency is based on accurately modeling human perception and other agents' expectations. This allows the AI agent to generate understandable motions and plans. An explainable AI agent must accurately model human perception and other agents' expectations to generate understandable motions, plans, and assistive actions. This model allows the agent to signal its intention before execution and consider both cost and explicability in decision-making. The model allows an AI agent to signal its intention before execution, explain its behavior through generated explanations, and consider cost and explicability in decision-making. The model enables an AI agent to signal its intention before execution and explain its behavior through generated explanations, focusing on generating the \"right\" explanations based on the recipient's perception model. The mental workload required for understanding an explanation is often overlooked in this research direction. In prior work, the focus was on generating explanations based on the recipient's perception model. The mental workload required for understanding an explanation was often overlooked. This study explores how the ordering of information in an explanation can influence perception, especially for complex explanations that require a large amount of information to be conveyed. In this work, the focus is on online explanation generation for complex explanations that intertwine with plan execution. The goal is to provide a minimal amount of information to explain part of the plan currently of interest. This approach is based on the model reconciliation setting from prior research. The problem definition in this work is closely related to planning problems, defined as a tuple (F, A, I, G) using PDDL, similar to STRIPS. In our prior work BID6, we define the setting and relevant concepts before introducing our closely related planning problem. A planning problem is represented as a tuple (F, A, I, G) using PDDL, similar to STRIPS. F denotes predicates for the world state, A represents actions to change the state, and I, G are initial and goal states. The robot's plan, DISPLAYFORM0 and \u03c0 * I,G, is explained with costs calculated using M R. The planning problem involves defining the state of the world with actions to change it. The robot's plan, \u03c0 * I,G, is optimized based on cost under model reconciliation (M R) and human model (M H) expectations. The cost of the optimal plan under model reconciliation (M R) is determined based on the initial and goal state pair. The model reconciliation setting considers the human's model (M H) to align the robot's behavior with human expectations. Reconciliation occurs when the robot's behavior matches the human's expectations. Explanation generation in this setting aims to bring the human and robot models closer together. Explanation generation in a model reconciliation setting involves updating the human model (M H) to align with the robot's behavior (\u03c0 * I,G). A mapping function converts planning problems into a feature space to facilitate this reconciliation process. The explanation generation problem involves updating the human model (M H) to align with the robot's behavior (\u03c0 * I,G) by defining a mapping function to convert planning problems into a feature space. The explanation generation problem involves converting planning problems into a feature space through a mapping function. An explanation reconciles human and robot models by minimizing cost differences in expected plans. A complete explanation satisfies the cost criteria for the robot's optimal plan. The explanation generation problem involves converting planning problems into a feature space through a mapping function. An explanation reconciles human and robot models by minimizing cost differences in expected plans. A complete explanation satisfies the cost criteria for the robot's optimal plan, making the cost difference between the human's expected plan and the robot's plan smaller after model updates. The explanation generation problem involves converting planning problems into a feature space through a mapping function. An explanation reconciles human and robot models by minimizing cost differences in expected plans. The robot's plan must be optimal in the human's model after a complete explanation. Online explanation generation is introduced to address the mental workload requirement of the human for understanding the explanation. The key is to provide a minimal amount of information during plan execution to explain the part of the plan that is of interest and not explainable. Online explanation generation is introduced to address the mental workload requirement of the human for understanding the explanation. It involves providing a minimal amount of information during plan execution to explain the part of the plan that is of interest and not explainable. Online explanation generation involves providing a set of sub-explanations during plan execution, where each sub-explanation represents a set of unit features to be made at a specific step in the plan. This approach allows the robot to split the explanation into multiple parts, aligning with the human's expectation of actions in the plan. Three different approaches of online explanation generation are provided, each focusing on a different aspect of explanation. The robot can generate sub-explanations online during plan execution by splitting the explanation into multiple parts. Three approaches are discussed: Plan Prefix matching, Next Action matching, and any prefix matching. The planning process must consider the sequence of model changes to generate sub-explanations effectively. Approaches for explanation generation during plan execution focus on different matching techniques such as Plan Prefix, Next Action, and any prefix matching. The planning process must consider how model changes affect human expectations for generating sub-explanations online. The challenge lies in converting the problem of explanation generation into model search within possible models, where model changes may not be independent. The planning process must consider how model changes affect human expectations for generating sub-explanations online. The challenge is to ensure that model changes do not render a mismatch in previously reconciled plan prefixes. This can be achieved by searching for the largest set of model changes from M R to M H. To address the challenge of model changes affecting plan prefixes, a search from M R to M H is conducted to find the largest set of changes that would not alter the plan prefixes. This process ensures that future sub-explanations do not disrupt the previously reconciled plan prefixes. The search process from M R to M H aims to identify the largest set of model changes that maintain plan prefixes intact for future sub-explanations. This recursive process ensures that plan prefixes remain consistent with the human model M H. The optimal plan \u03c0* is created from M H after providing sub-explanations e1 to ek. The search process starts from the robot model and stops when the plan prefixes for the updated human model and the robot model match. Our approach in finding sub-explanations starts from the robot model and stops when the plan prefixes for the updated human model and the robot model match. This differs from the previous approach which starts from the human model. Our method requires running the process multiple times, but it allows us to outperform other models in terms of computation. Our research process focuses on matching prefixes in the robot model, running the process multiple times to outperform other models in terms of computation. The maximum state space model modification reconciles the two models up to the current plan execution. The maximum state space model modification reconciles the two models up to the current plan execution by finding the largest set of model changes to M R such that the prefix of a plan using the corresponding model matches with that of \u03c0 * I,G up to step t 2 \u2212 1. The human model is updated by finding the largest set of changes to M R that match the prefix of a plan using the corresponding model. The complement set of changes will be E1 for the next recursive step, starting from action t 1 with the human model M H E1. Compatibility with the prefix is ensured for future steps to maintain the optimal plan. The recursive search algorithm for model space OEG is presented in Algorithm 1 to find e k given E k\u22121. A recursive model reconciliation procedure is used to search for e k by finding the difference between models M DISPLAYFORM1 and M R, and modifying M R with respect to M H. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. A model reconciliation procedure is used to find the difference between models M DISPLAYFORM1 and M R, and modify M R with respect to M H to match the human's plan with the robot's plan. The goal of explanation generation is to ensure that explanations modify M H. The algorithm aims to find the largest set of model changes to match the human's plan with the robot's plan. Explanation generation ensures both plans have the same prefix during execution, relaxing the condition to reconcile only the very next action. Explanation generation aims to align the robot and human plans by reconciling only the very next action, relaxing the need for a matching prefix. This approach considers the human's limited cognitive memory span and focuses on explaining the immediate actions. The approach in explanation generation aligns robot and human plans by reconciling the very next action, regardless of earlier actions in the plan prefix. This is motivated by the human's limited cognitive memory span and focuses on explaining immediate actions. The search is performed from M H \\M H for computational efficiency. The agent focuses on explaining the immediate next action that differs between the most recent human plan and the robot plan. The search is conducted from M H \\M H for computational efficiency, and only the next action that does not match between the two plans is explained. The search process in OEG is computationally faster as it does not require identical plan prefixes. It focuses on explaining the immediate next action that differs between human and robot plans, similar to MME in BID6 but executed multiple times in an online fashion. The OEG search process focuses on explaining actions that do not match in human and robot plans, maintaining prefixes. It combines search from M H and M R for better performance, assuming the robot has only one plan. The OEG-PP approach combines search from M H and M R for better performance, assuming the robot has only one plan. It aims to reconcile human and robot plans by searching for optimal plans without the need for explanations. The OEG-PP approach aims to reconcile human and robot plans by searching for optimal plans without the need for explanations. In this setting, the robot's goal is to match its plan with a human optimal plan, using a compilation approach to avoid computationally expensive solutions. The OEG-PP approach reconciles human and robot plans by searching for optimal plans without explanations. To avoid computational expense, a compilation approach is used to match the robot's plan with the human optimal plan. The OEG-PP approach reconciles human and robot plans by compiling the human's model into a new problem to ensure that a plan prefix in the robot's plan matches the human's plan. If the cost of the human's optimal plan remains the same after compilation, an optimal plan in the human's model exists that matches the prefix. Otherwise, an explanation is needed to reconcile the discrepancy. The key is to ensure that the plan prefix is always satisfied in the compiled model. The OEG-PP approach reconciles human and robot plans by compiling the human's model to ensure that a plan prefix in the robot's plan matches the human's plan. To achieve this, predicates are added as effects to actions in the compiled model, ensuring that the plan prefix is always satisfied. The search for e k involves a recursive model reconciliation process on the model space. The compilation process involves adding predicates to actions in the model to ensure plan prefixes match. A recursive model reconciliation process is used to search for e k. The agent checks for a human optimal plan with the same plan prefix as the robot's plan. The agent uses a model reconciliation process to find differences between models. It checks for a human optimal plan matching the robot's plan and identifies new sub-explanations through model space search until a matching plan is found. The agent uses a model reconciliation process to find differences between models and identify new sub-explanations through model space search until a matching plan is found. The approach for online explanation generation was evaluated with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. The goal is to understand how online explanation differs from MCE in terms of the information needed. The approach for online explanation generation was evaluated with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. Evaluation was done on ten different problems across the rover and barman domains, with differences between models made by randomly removing preconditions. Our approach for online explanation generation was evaluated on ten problems in the rover and barman domains, comparing it with the Minimally Complete Explanation (MCE) approach. Differences between models were made by randomly removing preconditions. The human subject study aimed to confirm the benefits of online explanation generation, hypothesizing that it would reduce mental workload and improve task performance. The study evaluated online explanation generation in a rover domain to reduce mental workload and improve task performance. The rover's goal was to explore Mars, take rock and soil samples, calibrate its camera, and communicate results to the base station. The rover on Mars explores space, takes rock and soil samples, calibrates its camera, and communicates results to the base station. It can only store one sample at a time and must drop the current sample to take another. The rover on Mars explores space, takes rock and soil samples, and communicates results to the base station. It can only store one sample at a time and must drop the current sample to take another. In the Rover domain, the robot acts as a barman serving drinks using drink dispensers, glasses, and a shaker with specific constraints. The robot acts as a barman serving drinks with specific constraints such as grabbing one object at a time and ensuring glasses are empty and clean before filling them. Simulation results compare different approaches in the rover and barman domains. Results comparing minimally complete explanations (MCE) with OEG-PP, OEG-NA, and OEG-AP approaches for 5 problems in the rover domain and 5 problems in the barman domain show differences in the number of shared model features and total features in explanations. In some cases, OEG approaches have more model features than MCE, as they focus on generating minimal information at each instance. In some cases, OEG-PP and OEG-NA explanations have more model features than MCE, due to focusing on minimal information at each time step. The reason for sharing more information in OEG-PP and OEG-NA lies in the dependence between features and planner behavior. The OEG-AP approach shows an improvement in the amount of information in an explanation by considering all optimal plans. However, there is still a distance between the robot's plan and the human's plan in terms of plan action distance in OEG-NA. The OEG-AP approach considers all optimal plans, showing an improvement in explanation information. However, there is still a distance between the robot's plan and the human's plan in terms of plan action distance in OEG-NA. The OEG-AP approach considers all optimal plans, improving explanation information. The plan distance gradually decreases between the robot's plan and the human's plan in OEG approaches, leading to a smoother adjustment for the human during execution. This is expected to reduce the human's mental workload. In our implementation, model updates are sorted based on feature size, with backtracking performed if consistency check fails. This search process aims for smoother adjustment for human mental workload during execution. In our implementation, model updates are sorted based on feature size, with backtracking performed if consistency check fails. The search process takes advantage of the fact that latter information often does not affect previous sub-explanations. A human study was designed to compare three approaches for online explanation generation. The search process takes advantage of the fact that latter information often does not affect previous sub-explanations. A human study was designed to compare three approaches for online explanation generation, including minimally complete explanation (MCE) and randomly breaking MCE during plan execution (MCE-R). The experiment was conducted using Amazon Mechanical Turk with 3D simulation in the rover domain. In an experiment using Amazon Mechanical Turk with 3D simulation, subjects were given a rover task with a 30-minute time limit. Explanations were in plain English, and rover actions were shown with GIF images. The human subject acted as the rover's commander in a 3D simulated scenario. In an experiment using Amazon Mechanical Turk, subjects acted as rover commanders in a 3D simulated scenario on Mars. They had a 30-minute time limit to determine the rover's actions' validity with explanations provided. The experiment involved subjects acting as rover commanders on Mars, determining the validity of the rover's actions with explanations provided. Additional spatial puzzles were included to increase cognitive demand. Certain information was deliberately removed to test subjects' ability to create correct plans without explanations. In an experiment involving rover commanders on Mars, subjects were tested on their ability to create correct plans without certain information provided. This hidden information introduced differences between model reconciliation settings, resulting in scenarios where explanations were necessary. In an experiment involving rover commanders on Mars, subjects were tested on their ability to create correct plans without certain information provided. The hidden information introduced differences between model reconciliation settings, leading to scenarios where explanations were necessary, such as calibrating the camera without being informed about limited storage and memory. In the experiment, subjects were tested on creating plans without certain information, leading to scenarios where explanations were needed. The robot shared all information at the beginning in one setting, while in another, information was broken up. Different approaches of online explanation generation were used, with missing information provided at different steps. The robot in the experiment used different approaches of online explanation generation, intertwining explanation communication with plan execution. Subjects were asked to determine the sense of the robot's actions at different steps. Minimally complete explanations were based on BID6, and online explanations were generated using various approaches. Subjects evaluated the efficiency of different explanation approaches using the NASA Task Load Index (TLX) BID23 questionnaire. The study evaluated different explanation approaches for a robot's actions using the NASA Task Load Index (TLX) questionnaire. NASA TLX is a tool to assess workload in human-machine interface systems. The NASA TLX is used to assess workload in human-machine interface systems. It measures mental workload through variables like mental demand, physical demand, temporal demand, performance, effort, and frustration. Physical demand questions were excluded in the experiment. The NASA TLX assesses mental workload using variables like mental demand, physical demand, temporal demand, performance, effort, and frustration. The experiment excluded physical demand questions and recruited 150 human subjects on MTurk for an academic survey. The academic survey recruited 150 human subjects on MTurk, with specific criteria for worker acceptance rates. After filtering out invalid responses, 94 valid responses were obtained. The subjects' age range was between 18 and 70, with 29.8% being female. The study examined how well subjects understood the robot's plan based on different explanations. The study recruited 150 human subjects on MTurk, with 94 valid responses obtained. Subjects' age ranged from 18 to 70, with 29.8% being female. The study analyzed how well subjects understood the robot's plan with different explanations, comparing distances across five settings based on the ratio of questionable actions to total actions in a plan. The study analyzed subjects' understanding of the robot's plan with various explanations, comparing distances across five settings. The lower the distance value, the closer the human's plan aligns with the robot's plan, indicating better understanding. Overall, OEG approaches were found to reduce human mental workload more effectively than MCE approaches. The study found that OEG approaches were more effective in reducing human mental workload compared to MCE approaches, as shown by subjective questions and NASA TLX measures. OEG approaches also created more temporal demand during the explanation process and plan execution. FIG6 presents objective performance measures and subjective results from the human study. The OEG approaches resulted in better performance in NASA TLX measures, creating more temporal demand during the explanation process and plan execution. The number of questionable actions was significantly lower in OEG cases, indicating higher trust towards robots. Accuracy in identifying correct actions was also higher in OEG approaches. The human study showed that OEG approaches had fewer questionable actions and higher accuracy in identifying correct actions compared to MCEs. OEG-AP had the least questionable actions and highest accuracy. There was a significant difference in mental workload between OEG approaches and MCEs. The study compared OEG approaches to MCEs, showing OEG-AP had the least questionable actions and highest accuracy. There was a significant difference in mental workload between the two groups. Time analysis revealed OEG-NA had the shortest task completion time, followed by OEG-AP, MCE-R, MCE, and OEG-PP. In a pairwise comparison, OEGs and MCEs showed a significant difference in mental workload with an overall p-value of 0.0068. Time analysis revealed OEG-NA had the shortest task completion time, followed by OEG-AP, MCE-R, MCE, and OEG-PP. The accuracy of the secondary task did not show significant differences between the approaches. This study introduced a novel approach for explanation generation to reduce mental workload in human-robot interactions. In this paper, a novel approach for explanation generation in human-robot interactions was introduced to reduce mental workload. The key idea is to break down complex explanations into smaller parts and convey them in an online fashion during plan execution. The focus is on providing correct and easily understandable explanations. In a novel approach for explanation generation in human-robot interactions, complex explanations are broken down into smaller parts and conveyed online during plan execution. Three different approaches were provided, focusing on generating easily understandable explanations intertwined with plan execution. Evaluation using simulation and human subjects showed improved task performance and reduced mental workload, a step towards achieving explainable AI."
}