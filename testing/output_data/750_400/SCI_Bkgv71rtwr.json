{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where source and target domains may not share the same categories. This poses a challenge when the target domain includes classes not seen in the source domain. To address this, the paper proposes augmenting Self-Ensembling, a domain adaptation technique, with category-agnostic clusters in the target domain. The paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), a novel approach for domain adaptation in open-set scenarios. By incorporating category-agnostic clusters specific to the target domain, SE-CC enhances the generalization of Self-Ensembling for both closed-set and open-set situations. SE-CC is a novel architecture for domain adaptation that utilizes category-agnostic clusters specific to the target domain to enhance generalization in both closed-set and open-set scenarios. Clustering is used to reveal the underlying data space structure in the target domain, with a clustering branch ensuring that the learned representation preserves this structure. SE-CC is a novel architecture for domain adaptation that utilizes category-agnostic clusters specific to the target domain to enhance generalization in both closed-set and open-set scenarios. Clustering is used to reveal the underlying data space structure in the target domain, with a clustering branch ensuring that the learned representation preserves this structure. Additionally, SE-CC enhances the representation with mutual information maximization, leading to superior results in experiments on Office and VisDA datasets for domain adaptation. SE-CC is a novel architecture for domain adaptation that matches assignment distribution to cluster distribution for target samples. It enhances representation with mutual information maximization and outperforms state-of-the-art approaches on Office and VisDA datasets for domain adaptation. Convolutional Neural Networks (CNNs) have advanced vision technologies to new state-of-the-arts, relying on large amounts of annotated data for training. However, manual labeling is costly and labor-intensive. To address the issue of domain shift when transferring knowledge/models to new domains, unsupervised domain adaptation can be utilized. Unsupervised domain adaptation leverages labeled source samples and unlabeled target samples to generalize a target model, addressing the issue of domain shift when transferring knowledge/models to new domains. Existing models often only align data distributions between source and target domains, limiting their applicability to closed-set scenarios. One critical limitation of existing domain adaptation models is their focus on aligning data distributions between source and target domains, restricting their use to closed-set scenarios. This hinders their ability to generalize in open-set scenarios where distinguishing target samples of unknown classes is challenging. The difficulty of open-set domain adaptation lies in distinguishing unknown target samples from known ones and learning a hybrid network for both closed-set and open-set scenarios. One approach to address this is by using an additional binary classifier to assign known/unknown labels. The main challenges in open-set domain adaptation are distinguishing unknown target samples from known ones and learning a hybrid network for closed-set and open-set scenarios. One method to tackle this is by employing a binary classifier to assign known/unknown labels to target samples, discarding unknown samples during adaptation. However, this approach may not fully exploit the data structure as unknown samples are grouped together as a single generic class. In open-set domain adaptation, distinguishing unknown target samples from known ones is a challenge. One approach is to use a binary classifier to assign labels, but this may not fully exploit the data structure. Instead, clustering all unlabeled target samples can help model the diverse semantics of both known and unknown classes in the target domain. In open-set domain adaptation, clustering all unlabeled target samples helps model diverse semantics of known and unknown classes in the target domain. By decomposing target samples into clusters, category-agnostic clusters convey discriminative knowledge of unknown and known classes, leading to domain-invariant representations for known classes. In open-set domain adaptation, clustering target samples helps model diverse semantics of known and unknown classes. Category-agnostic clusters convey discriminative knowledge, leading to domain-invariant representations for known classes. To refine representations, a clustering branch estimates assignment distribution over all clusters for each target sample. In open-set domain adaptation, clustering target samples helps model diverse semantics of known and unknown classes. Category-agnostic clusters convey discriminative knowledge, leading to domain-invariant representations for known classes. To address the issue of unknown classes, a new Self-Ensembling with Category-agnostic Clusters (SE-CC) is introduced, incorporating a clustering branch to estimate assignment distribution over all clusters for each target sample, refining representations to preserve the inherent structure of the target domain. In open-set domain adaptation, clustering target samples helps model diverse semantics of known and unknown classes. A new Self-Ensembling with Category-agnostic Clusters (SE-CC) is introduced, incorporating a clustering branch to estimate assignment distribution over all clusters for each target sample, refining representations to preserve the inherent structure of the target domain. The Self-Ensembling with Category-agnostic Clusters (SE-CC) model integrates a clustering branch to predict cluster assignment distribution for target samples in open-set domain adaptation. The KL-divergence is used to minimize the mismatch between estimated and inherent cluster distributions, preserving the data structure in the target domain. Additionally, mutual information among input intermediate features is maximized. The SE-CC model uses KL-divergence to model mismatch in cluster assignment distribution for target samples in open-set domain adaptation. It enforces feature preservation and maximizes mutual information among input features to enhance representation. The framework is jointly optimized for unsupervised domain adaptation. The SE-CC framework enhances feature representation by optimizing mutual information among input features, output classification distribution, and cluster assignment distribution in student. Unsupervised domain adaptation involves minimizing domain discrepancy through Maximum Mean Discrepancy in CNNs. In unsupervised domain adaptation, CNNs learn transferrable features by minimizing domain discrepancy through Maximum Mean Discrepancy (MMD). Early works integrated MMD into CNNs for domain invariant representation, while another approach involves encouraging domain confusion via a domain discriminator to predict the domain of input samples. In unsupervised domain adaptation, CNNs use Maximum Mean Discrepancy (MMD) to learn transferrable features. Another approach involves domain confusion through a domain discriminator to enforce domain invariance. Ganin & Lempitsky (2015) utilize a gradient reversal algorithm for optimizing the domain discriminator. Open-Set Domain Adaptation extends traditional domain adaptation. In open-set domain adaptation, the target domain includes new and unknown classes not present in the source domain. Panareda Busto & Gall (2017) address this scenario by exploiting target sample assignments as known/unknown classes. In open-set domain adaptation, Panareda Busto & Gall (2017) and later Saito et al. (2018b) and Baktashmotlagh et al. (2019) have proposed methods to tackle the scenario where the target domain includes new and unknown classes not present in the source domain. They utilize target sample assignments and adversarial training to learn feature representations that separate unknown classes from known target samples. In open-set domain adaptation, methods have been proposed to handle scenarios where the target domain includes unknown classes. Adversarial training is used to learn feature representations that separate unknown classes from known target samples. Additionally, source and target data are factorized into shared and private subspaces to model known and unknown target samples separately. In open-set domain adaptation, methods involve separating unknown classes from known target samples using adversarial training. Source and target data are divided into shared and private subspaces to model known and unknown target samples separately. The target samples from unknown classes are modeled with a private subspace, tailored to the target domain. Conditional entropy and self-ensembling loss are applied to align classification predictions between teacher and student models. Clustering is used to decompose unlabeled target samples into category-agnostic clusters for further analysis. In open-set domain adaptation, methods involve separating unknown classes from known target samples using adversarial training. Self-ensembling loss is applied to align classification predictions between teacher and student models. Clustering decomposes unlabeled target samples into category-agnostic clusters for further analysis and incorporation into Self-Ensembling for closed-set and open-set scenarios. An additional clustering branch is integrated into the student model to infer cluster assignment distribution for each target sample. An additional clustering branch is integrated into the student model to infer the assignment distribution over all clusters for each target sample. The feature representation is enforced to preserve the underlying data structure in the target domain by aligning the estimated cluster assignment distribution with the original cluster distribution. The feature representation of the student is further enhanced by maximizing mutual information among its feature map, classification, and cluster assignment distributions. SE-CC utilizes unlabeled target samples to learn task-specific classifiers in the open-set scenario, leveraging category-agnostic clusters for feature representation enhancement. The approach aligns cluster assignment distribution with original clusters and maximizes mutual information among feature map, classification, and cluster assignment distributions. SE-CC leverages category-agnostic clusters for representation learning, preserving target data structure during domain adaption for effective alignment of sample distributions within known and unknown classes. This enables discrimination of samples between known and unknown classes, enhancing feature representation. The feature representation learning in SE-CC is driven by preserving the target data structure during domain adaption, enabling effective alignment of sample distributions within known and unknown classes. This structure preservation also allows discrimination of samples between known and unknown classes, enhancing representation learning by maximizing mutual information among input features, clusters, and class probability distributions. The utilization of category-agnostic clusters for open-set domain adaptation is a novel approach that has not been fully explored in previous studies. In this paper, the SE-CC model integrates category-agnostic clusters into domain adaptation to enhance representation learning. This approach maximizes mutual information among input features, clusters, and class probability distributions for both closed-set and open-set scenarios. The model aims to preserve the target data structure and discriminate between known and unknown classes in open-set domain adaptation. The SE-CC model integrates category-agnostic clusters into domain adaptation for open-set scenarios. It aims to discriminate between known and unknown classes in the target domain. The goal of open-set domain adaptation is to learn domain-invariant representations and classifiers for recognizing the known classes in the target domain while distinguishing unknown target samples. The Self-Ensembling method is briefly recalled, which builds upon the Mean Teacher. The goal of open-set domain adaptation is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown target samples. Self-Ensembling method, based on Mean Teacher, encourages consistent classification predictions between teacher and student models under small perturbations of input images. Self-Ensembling builds upon the Mean Teacher method for semi-supervised learning, aiming to ensure consistent classification predictions between teacher and student models under small perturbations of input images. The self-ensembling loss penalizes the difference between classification predictions of student and teacher models, ensuring consistent predictions under small perturbations of input images. The teacher model's weights are updated as an exponential moving average of the student weights during training. The Self-Ensembling approach involves penalizing the difference in classification predictions between the student and teacher models. The teacher's weights are updated as an exponential moving average of the student's weights during training. Additionally, an unsupervised conditional entropy loss is used to train the student's classification branch, aiming to move decision boundaries away from high-density regions in the target domain. The Self-Ensembling approach involves using unsupervised conditional entropy loss to train the student's classification branch, aiming to move decision boundaries away from high-density regions in the target domain. The overall training loss includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. Open-set domain adaptation is challenging as it requires classifying both inliers and outliers into known and unknown classes. Open-set domain adaptation is more challenging than closed-set domain adaptation as it involves classifying inliers and outliers into known and unknown classes. A typical approach is to use a binary classifier to identify target samples as known or unknown classes, but this oversimplifies the problem by assuming all unknown samples belong to one class. This approach may not fully utilize the inherent data distribution among unknown samples, raising questions about its robustness. To address the limitations of binary classification in open-set domain adaptation, clustering is used to model diverse semantics in the target domain. These category-agnostic clusters are integrated into Self-Ensembling for guiding domain adaptation. To improve open-set domain adaptation, clustering is used to model diverse semantics in the target domain. This approach involves integrating category-agnostic clusters into Self-Ensembling to guide domain adaptation by aligning cluster assignment distributions and enforcing domain-invariant feature representations for known classes. In open-set domain adaptation, clustering is integrated into Self-Ensembling to align cluster assignment distributions and enforce domain-invariant feature representations for known classes. K-means is used to decompose unlabeled target samples into category-agnostic clusters. In open-set domain adaptation, clustering with k-means is used to group unlabeled target samples into category-agnostic clusters, revealing underlying structure tailored to the target domain. The clusters are discriminative for both unknown and known classes in the target domain. In open-set domain adaptation, clustering with k-means groups unlabeled target samples into category-agnostic clusters, revealing underlying structure tailored to the target domain. Target samples with similar semantics stay closer with local discrimination. Target samples are represented as output features of pre-trained CNNs for clustering. Periodic cluster refresh did not significantly impact results. The underlying structure of each target sample is encoded as joint relations between features. We represent target samples as output features of pre-trained CNNs for clustering. The underlying structure of each sample is encoded as joint relations with category-agnostic clusters through cosine similarities. Periodic cluster refresh did not have a significant impact on the results. The target samples are represented as joint relations with category-agnostic clusters through cosine similarities. Each target sample's inherent cluster distribution is measured using a softmax function over cosine similarities with cluster centroids. The clustering branch in the student model focuses on defining centroids as the average of samples in each cluster. The clustering branch in the student model predicts the distribution over category-agnostic clusters for cluster assignment of target samples based on cosine similarities with centroids. The centroids are defined as the average of samples in each cluster. The clustering branch in the student model uses a modified softmax layer to predict the cluster assignment distribution for target samples based on their features. The clustering branch in the student model utilizes a modified softmax layer to predict cluster assignments for target samples. It uses a KL-divergence loss to measure the mismatch between estimated and inherent cluster distributions, aiming to minimize this loss during training. The clustering branch in the student model uses a KL-divergence loss to measure the mismatch between estimated and inherent cluster distributions, enforcing the learnt representation to preserve the data structure of the target domain. Inter-cluster relationships are also incorporated into the loss as a constraint to maintain inherent relations among cluster assignment parameter matrices. The KL-divergence loss in the student model enforces the learnt representation to preserve the data structure of the target domain by minimizing the loss and incorporating inter-cluster relationships as a constraint. The goal is to make the representation more discriminative for both unknown and known classes, ensuring similarity between semantically similar clusters. The SE-CC model utilizes KL-divergence loss with inter-cluster relationships to ensure similarity between semantically similar clusters. It also incorporates Mutual Information Maximization to strengthen target features in an unsupervised manner. The SE-CC model uses Mutual Information Maximization (MIM) in the student to enhance target features in an unsupervised way by maximizing mutual information among input features and output distributions. This helps tune the features for downstream tasks by estimating and maximizing local and global mutual information. The MIM module in the student model aims to estimate and maximize the local and global mutual information between input features, output distributions, and cluster assignments. The MIM module in the student model aims to estimate and maximize the local and global mutual information among input feature map, output classification distribution, and cluster assignment distribution. Global Mutual Information is encoded from the output feature map of the last convolutional layer in the student model, followed by concatenation with conditioning classification distribution and cluster assignment distribution. The feature map is encoded into a global feature vector using a convolutional layer and average pooling. This global feature vector is concatenated with classification and cluster assignment distributions before being fed into a Mutual information discriminator for alignment assessment. The discriminator consists of three fully-connected networks. The global Mutual information discriminator is used to assess alignment between the global feature vector and classification/cluster assignment distributions. It consists of three fully-connected networks and estimates Mutual Information via Jensen-Shannon MI estimator. The global Mutual Information discriminator uses three stacked fully-connected networks with nonlinear activation to estimate the probability of discriminating real input features. It also calculates the global Mutual Information via Jensen-Shannon MI estimator, incorporating a softplus function and global feature of a different target image. Additionally, local Mutual Information is exploited among local input features and output distributions at each spatial location. The MI estimator by Nowozin et al. (2016) utilizes the softplus function and global feature of a different target image. Local Mutual Information is also considered among input features and output distributions at each spatial location, using replicated distributions to construct feature maps for discrimination. The local Mutual information discriminator utilizes replicated distributions to construct feature maps for discriminating input local features based on given classification and cluster assignment distributions. It consists of three stacked convolutional layers with nonlinear activation, producing a final output score map indicating the probability of matching. The local Mutual information discriminator uses convolutional layers to match input local features with classification and cluster distributions, estimating Mutual Information for the MIM module objective. The local Mutual Information discriminator uses convolutional layers to estimate Mutual Information for the MIM module objective, balancing local and global Mutual Information estimations with a tradeoff parameter \u03b1. The training objective of the SE-CC model integrates various losses including cross entropy, self-ensembling, conditional entropy, and KL-divergence. Appendix A illustrates the process of mutual information estimation. The SE-CC model integrates cross entropy, self-ensembling, conditional entropy, and KL-divergence losses, along with local and global Mutual Information estimation using tradeoff parameters. Experimental validation was done on the Office Saenko et al. VisDA dataset for synthetic-real image transfer. The SE-CC model integrates various losses and Mutual Information estimation with tradeoff parameters. Experimental validation was conducted on the VisDA dataset, consisting of synthetic images from 3D CAD models and real images from COCO and YTBB. The synthetic images from 3D CAD models are used for training, while real images from COCO and video frames from YTBB are used for validation and testing. The ground truth for the testing set is not publicly available, so synthetic images are used as the source and COCO images as the target for evaluation. Open-set adaptation is followed with 12 known classes for source and target domains, 33 background classes as unknown in source, and 69 COCO categories as unknown in target. For open-set adaptation evaluation, source images are from synthetic 3D CAD models, and target images are from COCO. Known classes consist of 12 classes in both domains, with 33 background classes as unknown in the source and 69 COCO categories as unknown in the target. The known-to-unknown ratio in the target domain is 1:10. Three metrics - Knwn, Mean, and Overall - are used for evaluation, measuring accuracy over known classes, known & unknown classes, and all target samples, respectively. For open-set adaptation evaluation, source images are from synthetic 3D CAD models, and target images are from COCO. The known-to-unknown ratio in the target domain is 1:10. Three metrics - Knwn, Mean, and Overall - are used for evaluation. ResNet152 is utilized as the backbone for clustering and adaptation in both closed-set and open-set scenarios. For open-set adaptation, ResNet152 is used as the backbone for clustering and adaptation in both closed-set and open-set scenarios. AODA adopts a different open-set setting without unknown source samples. SE-CC \u2666 is a variant that learns a classifier without unknown source samples for fair comparison with AODA. The results of different models on Office for open-set adaptation are compared in Table 1. SE-CC \u2666 is a variant that learns a classifier without unknown source samples for fair comparison with AODA. SE-CC outperforms other state-of-the-art models in terms of performance. The SE-CC classifier can only recognize N-1 known classes, with target samples identified as unknown if the predicted probability is below a certain threshold. Results show SE-CC outperforms other closed-set and open-set adaptation models on most transfer directions, especially on harder transfers like D \u2192 A and W \u2192 A. Our SE-CC model outperforms other adaptation models on most transfer directions, particularly on challenging transfers like D \u2192 A and W \u2192 A. It leverages category-agnostic clusters to create domain-invariant features for known classes while effectively separating target samples from known and unknown classes. The results emphasize the advantage of utilizing category-agnostic clusters for open-set domain adaptation, making the feature representation domain-invariant for known classes while distinguishing target samples from known and unknown classes. RTN and RevGrad outperform Source-only by aligning data distributions between source and target domains. Open-set adaptation techniques reject unknown target samples as outliers and align data distributions for inliers. SE-CC outperforms AODA, ATI-\u03bb, and FRODA by injecting category-agnostic clusters for domain adaptation, effectively excluding unknown target samples in an open-set scenario. SE-CC surpasses AODA, ATI-\u03bb, and FRODA in open-set adaptation by injecting category-agnostic clusters for domain adaptation. The study demonstrates the effectiveness of SE-CC in closed-set domain adaptation on Office and VisDA datasets, outperforming other state-of-the-art techniques. SE-CC outperforms other closed-set adaptation techniques on Office and VisDA datasets, showcasing the advantage of exploiting target domain data structure through category-agnostic clusters. Ablation study examines the impact of each design element on overall performance, with Conditional Entropy incorporating unsupervised conditional entropy loss into SE for driving classifier decision boundaries. In target domain, SE-CC utilizes category-agnostic clusters for domain adaptation without diverse unknown samples. Ablation study investigates the influence of design elements like Conditional Entropy, KL-divergence Loss, and Mutual Information Maximization on overall performance. These elements drive classifier decision boundaries, align cluster assignment distribution, and enhance feature refinement in the student model. The SE-CC model utilizes category-agnostic clusters for domain adaptation in the target domain. Ablation study examines the impact of design elements such as Conditional Entropy, KL-divergence Loss, and Mutual Information Maximization on performance improvements in VisDA for open-set domain adaptation. These elements drive classifier decision boundaries and enhance feature refinement in the student model. Maximization (MIM) enhances feature suitability for downstream tasks by maximizing mutual information among input features, output classification, and cluster assignment distributions. Performance improvements on VisDA for open-set domain adaptation in SE-CC are detailed, with CE improving Mean accuracy from 65.2% to 66.3%. KL and MIM designs in SE-CC contribute 3.0% and 1.2% performance gain in Mean metric. In our SE-CC, Cross-Entropy (CE) improves Mean accuracy from 65.2% to 66.3%, demonstrating its effectiveness. KL and MIM designs contribute 3.0% and 1.2% performance gain in Mean metric, leading to a total performance boost of 4.2%. The results validate the use of category-agnostic clusters and mutual information maximization for open-set adaptation in SE-CC. The SE-CC approach leads to a significant 4.2% performance boost in Mean metric by exploiting category-agnostic clusters for domain adaptation in open-set scenarios. The method focuses on separating unknown target samples from known ones and integrating category-agnostic clusters into Self-Ensembling. The study focuses on utilizing category-agnostic clusters for domain adaptation in open-set scenarios. It involves decomposing target samples into clusters and integrating a clustering branch into the student model to align cluster assignments. This approach enforces the learned features to preserve the data structure in the target domain. The study utilizes category-agnostic clusters for domain adaptation by decomposing target samples into clusters and integrating a clustering branch into the student model to align cluster assignments. This enforces learned features to preserve the data structure in the target domain. Additionally, mutual information among input features, classification outputs, and clustering branches is used to enhance the learned features. Experimental results on Office and VisDA datasets show performance improvements compared to state-of-the-art techniques. The implementation of SE-CC in PyTorch utilizes mutual information among input features, classification outputs, and clustering branches to enhance learned features. Experiments on Office and VisDA datasets show performance improvements compared to state-of-the-art techniques. The detailed frameworks for global and local mutual information estimation in SE-CC are implemented using PyTorch. The network weights are optimized with SGD, with specific learning rate, mini-batch size, and training iterations. The settings for cluster number, tradeoff parameters, and dimension of global feature are specified for open-set and closed-set adaptation tasks on different datasets. The settings for cluster number, tradeoff parameters, and dimension of global feature are specified for open-set and closed-set adaptation tasks on different datasets. The number of clusters is determined using the Gap statistics method, with \u03bb1 fixed at 10 for all experiments. The other parameters are tuned within specific ranges for each transfer. The number of clusters is determined using the Gap statistics method, with \u03bb1 fixed at 10 for all experiments. Other parameters are tuned within specific ranges for each transfer, evaluating the use of KL-divergence in the proposed SE-CC compared to L1 and L2 distance. In the evaluation of the clustering branch, the use of KL-divergence in SE-CC outperforms L1 and L2 distance measures. Mutual information maximization is also assessed using different MIM module variants in SE-CC. In evaluating the clustering branch, KL-divergence in SE-CC performs better than L1 and L2 distance measures. Different variants of the MIM module in SE-CC are also assessed by estimating mutual information between input features and various outputs. In SE-CC, different variants of the MIM module (CLS, CLU, CLS+CLU) estimate mutual information between input features and classification or clustering branch outputs. CLS and CLU slightly improve performance by exploiting mutual information between input features and individual branch outputs. CLS+CLU shows the largest performance boost by combining outputs from both branches for mutual information estimation. In SE-CC, CLS and CLU slightly improve performance by exploiting mutual information between input features and individual branch outputs. CLS+CLU shows the largest performance boost by combining outputs from both branches for mutual information estimation. The results demonstrate the merit of exploiting mutual information among input features and combined outputs of downstream tasks in the MIM module. The MIM module explores mutual information between input features and combined outputs of downstream tasks. SE brings source and target distributions closer, but struggles to recognize unknown target samples. SE-CC preserves target data structure, separating unknown from known target samples. SE-CC separates unknown target samples from known target samples by preserving the underlying target data structure, making them distinguishable while keeping known samples indistinguishable in both domains."
}