{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a key feature in intelligence, making the brain flexible and powerful. Crossbar-based neuromorphic computing chips, which use analog circuits, can mimic the brain but current deep neural networks do not consider their uncertainty. A proposed uncertainty adaptation training scheme (UATS) aims to improve performance on neuromorphic computing chips by incorporating uncertainty in the training process. In this work, a proposed uncertainty adaptation training scheme (UATS) addresses the uncertainty of neuromorphic computing chips, improving neural network performance on these chips compared to original platforms. The experimental results demonstrate that neural networks achieve comparable inference performances on uncertain neuromorphic chips, emphasizing the importance of uncertainty reasoning in intelligence. The neural networks achieve comparable inference performances on uncertain neuromorphic chips with the proposed uncertainty adaptation training scheme, showing the importance of uncertainty reasoning in intelligence. Fuzziness and stochasticity are two types of uncertainties in intelligent systems, helping the brain efficiently process information in the real world. The fuzziness and stochasticity in intelligent systems help the brain process information efficiently and creatively, enabling adaptability in unfamiliar situations. These characteristics are lacking in most current artificial intelligence systems. The brain's ability to process information efficiently and creatively in unfamiliar situations is due to its stochasticity. This characteristic is lacking in most current artificial intelligence systems, such as deep neural networks using 32-bit or 64-bit floating numbers for weights and activations. Some researchers suggest that 8-bit integers are sufficient for many applications. Existing artificial intelligence (AI) systems, like deep neural networks, use 32-bit or 64-bit floating numbers for weights and activations. Some researchers propose using 8-bit integers for many applications. Methods like network quantization and Bayesian networks address issues, while neuromorphic computing chips offer a hardware solution. Nanotechnology is also emerging in this field. In recent years, nanotechnology and crossbar structure based neuromorphic computing chips have advanced significantly, offering efficient solutions for vector-matrix multiplication. Emerging nonvolatile memory devices at cross points enhance the hardware approach to address uncertainties in deep neural networks. Emerging nanotechnology devices and crossbar structures in neuromorphic computing chips have advanced significantly, utilizing Ohm's law and Kirchhoff's law for efficient vector-matrix multiplication. Nonvolatile memory devices at cross points provide additional storage capacity, enabling computing in memory architecture to alleviate memory bottlenecks in traditional von Neumann architecture. The crossbar in neuromorphic computing chips utilizes nonvolatile memory devices for additional storage capacity and computing functions. This computing in memory architecture helps alleviate memory bottlenecks in traditional von Neumann architecture, making the chips more energy and area efficient for AI applications with high memory requirements. The uncertainty is also an important feature of neuromorphic computing chips. Neuromorphic computing chips aim to be more energy and area efficient compared to traditional von Neumann architecture. They are promising for AI applications with high memory requirements. Uncertainty is a key feature of these chips, stemming from two aspects. The crossbar structure in these chips utilizes nonvolatile memory devices for storage and computing functions. The uncertainty in neuromorphic computing chips arises from analog to digital converters (ADCs) and NVM devices. The VMM result is a summarization of currents, requiring ADC conversion for digital voltages. The fuzziness in computing chips is caused by ADCs and the stochasticity by NVM devices. The VMM result summarizes currents, requiring ADC conversion to digital voltages. NVM device stochasticity is due to random particle movement, leading to varied conductance and different output currents. The ADC converts analog currents to digital voltages for data transfer, similar to activation quantization in networks. NVM device stochasticity is caused by random particle movement, leading to varied conductance and different output currents. A training scheme utilizing this stochasticity is proposed to improve neuromorphic computing chip performance. In this work, a training scheme is proposed to utilize the stochasticity of NVM devices to enhance the performance of neuromorphic computing chips. Various types of NVM devices exhibit different levels of stochasticity due to their intrinsic physical mechanisms. The different types of NVM devices, such as phase change memories, filamentary migrating oxide devices, and ferroelectric tunnel junction synapses, exhibit varying levels of stochasticity. A Gaussian distribution is used to model the device stochasticity, despite not fitting each type perfectly. The distribution represents a stable state, with lower probabilities further away from this state. The Gaussian distribution is used to model device stochasticity in various NVM devices. It represents a stable state, with lower probabilities further away from this state. The mean of the distribution corresponds to the conductance value of the stable state, and the variance is usually related to the mean. It is challenging to model the relations between the mean and variance of different devices, so for simplicity, it is assumed that they are correlated. The conductance values of NVM devices are modeled using a Gaussian distribution, with the mean representing the stable state conductance. The variance is typically related to the mean, and for simplicity, it is assumed that the standard deviation is linearly correlated to the mean. Conductances below a minimum value are cut off, as the real conductance of devices is always positive. The conductances of NVM devices follow a Gaussian distribution where the standard deviation is positively correlated to the mean. Conductances below a minimum value are cut off, with the stochastic conductance denoted as Gs sampled from N(\u00b5, \u03c3^2) where G0 is the stable state conductance and \u03b1 > 0 represents device variability. The model of devices' stochasticity involves conductance values sampled from a Gaussian distribution. Device fuzziness is a result of the writing process in neuromorphic computing chips for AI applications. Target conductance for each device is determined based on neural network weights before writing. The device fuzziness in neuromorphic computing chips for AI applications is a result of the writing process, with the level of stochasticity denoted by \u03b1 > 0. Writing the conductance of each device in the crossbar is crucial, determined by the target conductance based on neural network weights. The mapping process scales weights into the working range of device conductance [G low , G high], using the difference of two devices' conductances to express one weight w, which can be positive or negative for higher energy efficiency. The mapping process scales neural network weights into the device conductance range [G low , G high], using the difference of two devices' conductances to express a weight w, which can be positive or negative. To achieve higher energy efficiency, lower conductances are preferred to represent the same weight. The mapping algorithm aims to utilize the entire conductance working range by considering the absolute value of a singular weight, the maximum weight value, and target conductances G pos and G neg. However, accurate writing of conductance is challenging due to device fuzziness in neuromorphic computing chips. The conductance mapping algorithm aims to utilize the entire conductance working range by considering the absolute value of a weight, maximum weight value, and target conductances. However, accurate writing of conductance is challenging due to device stochasticity and measurement inaccuracies. The conductance mapping algorithm faces challenges in accurately writing conductance due to device stochasticity and measurement inaccuracies. A model using Gaussian distribution is used to describe the fuzziness in conductance values. The conductance mapping algorithm encounters challenges in accurately writing conductance due to device stochasticity and measurement inaccuracies. A model using Gaussian distribution is employed to describe the fuzziness in conductance values, with parameters such as target conductance, level of devices fuzziness, and writing strategy taken into account. The uncertainty in conductance values, denoted by G target and \u03b2, affects the performance of a DNN on a neuromorphic computing chip. The proposed uncertainty adaptation training scheme (UATS) can alleviate the decrease in classification accuracy caused by uncertainty, and in some cases, improve accuracy. The core idea of UATS is to address the uncertainty in conductance values. The uncertainty in conductance values impacts DNN performance on a neuromorphic chip. The proposed uncertainty adaptation training scheme (UATS) can mitigate accuracy decrease and even enhance it. UATS involves informing neural networks of uncertainty during training to help them adapt. Stochasticity model is used in feed forward processes to address uncertainty. The uncertainty adaptation training scheme (UATS) informs neural networks of uncertainty during training using a stochasticity model in feed forward processes. It introduces a fuzziness model to address uncertainty and adapt neural networks to deal with uncertain situations. The uncertainty adaptation training scheme (UATS) introduces a fuzziness model during training to address uncertainty in neural networks. The model replaces weights with random variables to obtain target conductances G ptarget and G ntarget. During the training process, weights are replaced by random variables to obtain target conductances. The loss function is calculated by the average output of multiple FF processes with the same input batch, teaching the network to evaluate performance in uncertain conditions. During training, weights are replaced by random variables to obtain target conductances. The loss function is calculated by the average output of multiple FF processes with the same input batch. The network is taught to evaluate performance in uncertain conditions by considering the uncertainty of G compared to G min. The effectiveness of this approach was evaluated on various models and datasets, including the MNIST dataset with MLP and CNN models. In the experiments, MLP and CNN models were trained on the MNIST dataset with 60,000 images. 50,000 images were used for training, 10,000 for validation, and 10,000 for testing. Different conductance values were tested, and the models were evaluated for performance. In the experiments, 60,000 images from the MNIST dataset were used for training MLP and CNN models. Conductance values of G min = 1\u00b5S, G low = 5\u00b5S, G high = 50\u00b5S were tested. The models were trained and tested with different uncertainty levels using fuzziness and stochasticity models. 20 trials were conducted for each model with every uncertainty level to report the average test error and standard deviation. Without using the UATS, uncertainty increases test errors for MLP and CNN models. Higher uncertainty levels result in higher test errors. The CNN model (LeNet-5) performs best without uncertainty but is most affected by it (t-test, p < 0.01). The study found that without using the UATS, uncertainty led to higher test errors for both MLP and CNN models. The CNN model (LeNet-5) performed the best without uncertainty but was most affected by it. The 'mlp2' model was more robust to uncertainty compared to 'mlp1'. The power of UATS was validated by tuning the weight of the pre-trained model and retraining the models. Fine-tuning experiments used k=5, n=5, and 25 epochs. The study validated the power of UATS by tuning the weight of the pre-trained model and retraining the models. UATS significantly improved accuracies with the same level of uncertainty in both retraining and fine-tuning experiments. Most retraining results were better than fine-tuning using UATS. The number of epochs is 25. UATS can significantly improve accuracies with the same uncertainty level in retraining and fine-tuning experiments. Results show that retraining with UATS is better than fine-tuning. UATS achieved comparable results to the ideal case with small uncertainty levels. The power of UATS was also validated on the CIFAR-10 dataset using a ResNet-44 DNN model. The UATS achieved comparable results to the ideal case with small uncertainty levels on the CIFAR-10 dataset using a ResNet-44 DNN model. It can even achieve a lower error rate than the ideal cases with proper hyper-parameters. UATS acts as a regularization method that makes training DNNs easier, especially with more layers, emphasizing the importance of uncertainty in intelligent systems. The results show that UATS can achieve a lower error rate than ideal cases with proper hyper-parameters. It acts as a regularization method for DNN training, especially with more layers, emphasizing the importance of uncertainty in intelligent systems. The Bayesian network is useful for building uncertain neural networks, but controlling the weight distribution is challenging, particularly with neuromorphic computing chips. The Bayesian network is a useful method for building uncertain neural networks, but controlling weight distribution is challenging, especially with neuromorphic computing chips. Various distributions have been tried to model device stochasticity, such as Laplacian, uniform, lognormal, and asymmetrical distributions. UATS is a convenient alternative to manipulate conductance distribution without requiring additional circuits. Different distributions have been explored to model device stochasticity, including Laplacian, uniform, lognormal, and asymmetrical distributions. While the behavior of the device varies based on the distribution used, the network performance remains similar when distributions have the same mean and variance. The VMM transforms individual device distributions into a summary of random parameters, reducing the need for a large number of random numbers in UATS computation. Methods to decrease random number requirements include sampling weights for each input. The VMM transforms individual device distributions into a summary of random parameters, reducing the need for a large number of random numbers in UATS computation. Methods to decrease random number requirements include sampling weights for each input or batch and using the uncertainty model of VMM results instead of weights to accelerate simulation speed and achieve similar results."
}