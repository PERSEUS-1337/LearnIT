{
    "title": "BkecJjCEuN",
    "content": "The aim of this work is to improve label efficiency of large neural networks on audio data using multitask and self-supervised learning. An end-to-end audio feature extractor based on WaveNet is trained, along with task-specific neural networks. Several self-supervised learning tasks are described for unlabeled audio data, showing significant improvements in scenarios with limited labeled training data. In this work, an end-to-end audio feature extractor based on WaveNet is trained for self-supervised learning on unlabeled data. The study demonstrates that incorporating self-supervised tasks can improve supervised classification tasks by up to 6% in scenarios with limited labeled training data. Additionally, data augmentation in a multitask setting further enhances performance of deep neural networks. Incorporating self-supervised tasks can boost supervised classification tasks by up to 6% with limited labeled data. Data augmentation in a multitask setting further improves deep neural network performance. Deep neural networks are crucial for modeling auditory data, but the lack of labeled datasets and high-dimensional input pose challenges. Unsupervised learning using unlabeled data is a promising approach to improve generalization in models. Incorporating self-supervised auditory tasks during model training can significantly improve performance by enabling models to generalize better. This approach is particularly beneficial when labeled datasets are scarce, and input is high-dimensional and noisy. Incorporating self-supervised auditory tasks during model training improves generalization by identifying appropriate tasks and training them jointly with supervised tasks. WaveNet can extract rich audio features from raw waveform data, enabling models to adapt efficiently to task variations. WaveNet can significantly improve performance by extracting rich audio features from raw waveform data. It can be used as a general feature extractor for various tasks, such as audio tagging, speaker identification, and speech command recognition. By learning multi-scale hierarchical representations, WaveNet-based models can adapt to subtle variations efficiently. Leveraging unlabeled data and self-supervised tasks can further enhance performance and pair well with data augmentation techniques. The framework explores supervised classification tasks like audio tagging, speaker identification, and speech command recognition. It shows that leveraging unlabeled data and self-supervised tasks can improve performance, especially when combined with data augmentation techniques. Additionally, the proposed self-supervised tasks can serve as a pre-training stage for performance enhancements through transfer learning. The authors propose self-supervised tasks for pre-training to enhance performance through transfer learning. Models trained on multiple tasks may uncover underlying structures, leading to better single-task performance with less data. The authors suggest using self-supervised tasks for pre-training to improve performance through transfer learning. Models trained on multiple tasks may reveal underlying structures, enhancing single-task performance with less data. The authors propose using self-supervised learning to address label scarcity and leverage unlabeled data for improved performance in multitask learning. This approach aims to learn a general-purpose representation by pooling data from different datasets. The problem of scarce labeled datasets in deep learning is being addressed using self-supervised learning to utilize unlabeled data. Self-supervised learning has shown success in the visual domain with tasks like image completion, colorization, and motion segmentation. However, there is limited previous work on applying self-supervision in the audio domain. An end-to-end audio processing network has been implemented to find a common embedding of the acoustic waveform within a \"trunk\" network. The implementation of an end-to-end audio processing network utilizes self-supervision to find a common embedding of the acoustic waveform within a \"trunk\" network modeled after the WaveNet architecture. The network is trained jointly with task-specific \"head\" networks for various experiments involving supervised and self-supervised tasks. The WaveNet trunk network consists of 3 blocks of 6 dilation stacks, each with a gate and filter module containing 64 convolutional units. The outputs are multiplied elementwise and summed with the input to the stack. The WaveNet trunk network consists of 3 blocks of 6 dilation stacks with 64 convolutional units per module. This results in an effective receptive field length of 190 samples or approximately 12 ms. The setup was tested on audio tagging, speaker identification, and speech command recognition tasks with up to three self-supervised auxiliary tasks. The WaveNet trunk network has an effective receptive field length of 190 samples or approximately 12 ms. It was tested on audio tagging, speaker identification, and speech command recognition tasks with up to three self-supervised auxiliary tasks. The audio tagging task was trained on the FSDKaggle2018 dataset. The audio tagging task is trained on the FSDKaggle2018 dataset, which contains 11,073 audio files. Each segment is cropped to 2 seconds and padded with zeros if needed before being fed to the network. The WaveNet trunk produces embeddings with a temporal structure, and the task averages the output across time to generate a single output vector for the entire audio sequence. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of monaural audio. Each audio segment is cropped to 2 seconds and padded with zeros if necessary. The WaveNet trunk produces embeddings with a temporal structure, and the output is averaged across time to create a single output vector for the entire audio sequence. This vector feeds into a fully-connected layer with 512 units and ReLU nonlinearity, followed by a softmax output layer for training by minimizing cross entropy with one-hot encoded labels. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Individual clips are sourced from interviews with celebrities in various settings, with one interview held-out for a test set. Each audio segment is cropped to 2 seconds before being fed into the network for training. The dataset BID12 contains 336 hours of data from 1251 speakers sourced from interviews with celebrities. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The task's head architecture includes a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, and ReLU nonlinearity. The speech command recognition task involves a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, ReLU nonlinearity, and softmax layer. It is trained on the Speech Commands dataset with 65,000 utterances of 30 short words in one-second WAVE format files. The speech command recognition task involves a stack of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The convolutions have widths of 100, 50, and 25, and strides of 16, 8, and 4, respectively. The dataset consists of 65,000 utterances of 30 short words in WAVE format files. The speech command recognition head consists of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The convolution layers have widths of 100, 50, and 25, and strides of 16, 8, and 4. Self-supervised tasks like next-step prediction, noise reduction, and upsampling were implemented alongside the main supervised tasks. The speech command recognition head includes three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. Self-supervised tasks such as next-step prediction, noise reduction, and upsampling were added to the main supervised tasks. These tasks were trained on data from the Librispeech dataset BID15. The auxiliary tasks in the multitask framework were trained on unlabeled data from the Librispeech dataset BID15. They share a common head architecture with convolutional layers and a regression-type loss function. The focus was on using waveform inputs instead of high-level feature representations like spectrograms. The primary goal was to develop a multitask framework for audio using waveform inputs instead of high-level feature representations like spectrograms. While convolutional architectures trained on spectral/cepstral representations can give better classification performance, they limit the range of audio processing tasks that can be performed. State-of-the-art baseline models for different tasks may vary widely in their network architectures. While convolutional architectures trained on spectral/cepstral representations of audio can improve classification performance, they restrict the range of audio processing tasks. State-of-the-art baseline models for different tasks vary in network architectures, limiting information gained from self-supervised tasks. To understand learning dynamics across tasks, focus on models with fewer assumptions about input representation for performance improvements. The focus is on models with fewer assumptions about input representation for performance improvements in multitask learning compared to single task baseline models. Joint training with three self-supervised tasks proved beneficial for supervised tasks. Closing the performance gap between spectral and waveform representations is left for future work. Multitask learning improved performance in audio tagging without increasing training data by including additional unsupervised tasks. Joint training with three self-supervised tasks was beneficial for supervised tasks. Future work will focus on closing the performance gap between spectral and waveform representations. Multitask training improved audio tagging performance by including unsupervised tasks with larger unlabeled datasets, resulting in significant performance gains. Incorporating Librispeech into training improved performance metrics, with a MAP@3 increase of up to .056 with an additional 500 hours of unlabeled data. Swapping tasks showed similar trends, with speech command classification improving from 93.05% to 93.78% with extra data. Speaker identification on VoxCeleb was more challenging. Multitask learning improved speech command classification and speaker identification tasks with additional unlabeled data. Speech command classification increased from 93.05% to 93.78%, while speaker identification performance peaked at 75.22% from the baseline of 73.81%. This shows the potential of multitask learning to enhance supervised tasks without extra labeled data. Multitask learning improved performance in speech command classification and speaker identification tasks without extra labeled data. Top-5 classification performance peaked at 75.22%, up from the baseline of 73.81%. Comparing with data augmentation techniques, pitch-shift augmentation produced an increase in MAP@3 of .066. Training a single task model on audio tagging with pitch shifting and additive noise data augmentation resulted in an increase in MAP@3 of .066 and .024 respectively. The performance gains from noisy data augmentation were similar to those obtained from training with a self-supervised noise-reduction task. Additionally, combining pitch-shift augmentation with self-supervised tasks showed promising results. Training with both pitch-shift augmentation and additional self-supervised tasks resulted in a MAP@3 increase of .089, the highest performance from any experiment. This suggests that both methods for improving label efficiency are complementary. In computer vision, transfer learning now includes knowledge transfer from self-supervised tasks trained on unlabeled data to supervised tasks. In a variant of transfer learning, self-supervised tasks are pre-trained on unlabeled data before fine-tuning with a smaller amount of labeled data for a supervised task. This approach led to a significant increase in performance, suggesting the complementary nature of methods for improving label efficiency. Transfer learning was explored as a method to improve label efficiency in training audio tasks with limited labeled data. Self-supervised tasks were pre-trained on unlabeled data before fine-tuning with a smaller amount of labeled data for a supervised task, resulting in improved performance. Transfer learning experiments were conducted on audio tasks with limited labeled data, favoring transfer learning over training all tasks simultaneously. The study showed that jointly training supervised tasks with self-supervised tasks using a WaveNet-based model on raw audio waveforms led to improved performance, which scaled with the quantity of unlabeled data and supplemented existing data augmentation schemes. Our approach involves jointly training supervised tasks with self-supervised tasks using a WaveNet-based model on raw audio waveforms, leading to improved performance that scales with the quantity of unlabeled data. This method can supplement existing data augmentation schemes and is expected to generalize to a broad range of supervised audio tasks. Our approach involves training supervised tasks with self-supervised tasks using a WaveNet-based model on raw audio waveforms, leading to improved performance that scales with the quantity of unlabeled data. This method can supplement existing data augmentation schemes and is expected to generalize to a broad range of supervised audio tasks. The multitasking model can benefit from multiple auxiliary tasks, forming a representation of the audio that can potentially be extracted. The multitasking model can benefit from auxiliary tasks to form a representation of audio, which may be extracted for broader auditory task handling. To handle a broader range of auditory tasks, a WaveNet model is chosen for its high temporal resolution processing of raw audio signals. This model is ideal for tasks requiring fine temporal resolutions in audio tag classification and other auditory tasks. Our model follows the WaveNet architecture, known for processing high temporal resolution raw audio signals efficiently. WaveNet models use causal dilated convolutions to process sequential inputs in parallel, making them faster to train compared to RNNs. WaveNet models use causal dilated convolutions to process sequential inputs in parallel, making them faster to train compared to RNNs. The architecture consists of a trunk with stacked dilated and causal convolutions, followed by task-specific heads. The WaveNet trunk is composed of N blocks, each with S dilated causal convolution layers. The WaveNet trunk follows the structure of WaveNet with stacked dilated and causal convolutions. It consists of N blocks, each with S dilated causal convolution layers. Each layer in the trunk involves a \"residual atom\" computation with \"Filter\" and \"Gate\" operations. The WaveNet trunk consists of N blocks with S dilated causal convolution layers. Each layer involves a \"residual atom\" computation with \"Filter\" and \"Gate\" operations, producing hidden state vector h and layer output x. The WaveNet trunk consists of N blocks with S dilated causal convolution layers. Each layer involves residual atom computation to produce hidden state vector h and layer output x using element-wise products and dilated convolutions. The total effective receptive field of the trunk is \u03c4 = 1+N(2S\u22121). The WaveNet trunk applies causal convolutions to raw audio waveforms to produce an output. The trunk consists of 3 blocks with 6 layers each, resulting in a total receptive field of 190, equivalent to 12 milliseconds of audio sampled at 16kHz. Each task-specific head is a simple neural network. The trunk in our experiment consists of 3 blocks with 6 layers each, resulting in a total receptive field of 190, equivalent to 12 milliseconds of audio sampled at 16kHz. Each task-specific head is a simple neural network with its own objective function and optimizer. In the experiment, each task-specific head in the neural network processes input independently with its own objective function and optimizer. Supervised tasks are designated as primary, while self-supervised tasks are auxiliary. The primary task is \"audio tagging\" with auxiliary tasks like \"next-step prediction\", \"noise reduction\", and \"upsampling\" using varying amounts of unlabeled data. In the experiment, supervised tasks are designated as primary, while self-supervised tasks are auxiliary. The primary task is \"audio tagging\" with auxiliary tasks like \"next-step prediction\", \"noise reduction\", and \"upsampling\" using varying amounts of unlabeled data. The head architectures were designed to be simple, using as few layers as necessary to solve the task. The next-step prediction task involves predicting the next value in a sequence of audio frames, allowing for large training datasets from unlabeled data. The head architectures are kept simple to facilitate learning representations for multiple audio tasks. The next-step prediction task involves predicting the next value in a sequence of audio frames using a 2-layer stack of 1 \u00d7 1 convolutional layers. This allows for large training datasets from unlabeled data. The next-step prediction head for audio data consists of a 2-layer stack of 1 \u00d7 1 convolutional layers. It takes \u03c4 frames of data from the trunk and predicts the next frame of audio in the sequence using mean squared error as a loss function. The next-step prediction head for audio data uses mean squared error as a loss function to predict the next frame of audio in the sequence, treating it as a regression problem. This approach was found to work better in multitask situations compared to the original WaveNet implementation, which treated it as a classification problem. In the noise reduction task, noise is treated as an additive random process on top of the clean audio signal. The model aims to denoise the audio waveform by removing this noise. In noise reduction tasks, noise is considered an additive random process on top of the clean audio signal. The model is trained to predict the clean sample given a window of noisy samples. The formulation of next-step prediction and denoising tasks are similar, leading to models with similar structures being effective for both tasks. The model for noise reduction tasks is trained to predict clean samples from noisy ones. The noise reduction head structure is similar to the next-step head, minimizing a smoothed L1 loss between clean and noisy waveform inputs. Our noise reduction head, similar in structure to the next-step head, is trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. The smooth L1 loss is preferred for denoising tasks over mean squared error due to its stable convergence. Additionally, an unsupervised upsampling task can be created by downsampling the audio source and using the original source as the target for upsampling. For the upsampling task, the original audio is downsampled to 4 kHz using the resample method in the librosa python package. The downsampled signal is used as input data, while the original source is the target. Time-points of the resampled signal are repeated 4 times to keep the network operating at the same time scale for all auxiliary tasks. The original audio is downsampled to 4 kHz using the resample method in the librosa python package. Time-points of the resampled signal are repeated 4 times to keep the network operating at the same time scale for all auxiliary tasks. The network's job is to infer the high frequency information lost during the transform. The original audio is downsampled to 4 kHz and then resampled 4 times to mimic the original signal's 16 kHz sample rate. The network's task is to infer the lost high frequency information. The model is trained using raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets, with code written in PyTorch framework. The model was trained using raw audio waveform inputs from the FSDKaggle2018 and Librispeech datasets. Audio samples were cropped to two seconds in duration, downsampled to 16 kHz, and normalized to lie in the interval [-1, 1]. Samples shorter than 2 seconds were zero padded for the noise-reduction task. For the noise-reduction task, noisy inputs were generated by adding noise from ChiME3 datasets at random SNR levels. Various noise types were included such as booth, bus, cafe, pedestrian area, and street junction. Hyperparameter search was conducted over the number of blocks in the trunk for the main task. The noise-reduction task involved adding noise from ChiME3 datasets at random SNR levels, including various noise types like booth, bus, cafe, pedestrian area, and street junction. A hyperparameter search was conducted over the number of blocks in the trunk, the number of layers per block, the number of layers and units in the main task head, and the learning rate. Different values were tried for the number of blocks in the trunk and the number of dilated convolution layers in each block to optimize performance and training characteristics of the network. The hyperparameter search involved exploring the number of blocks and layers in the trunk, dilated convolution layers, and learning rate. Performance was not significantly impacted by specific architecture choices. Additionally, the depth and width of auxiliary task heads were optimized by pairing each task with the main task. The network's training characteristics were not affected by architecture specifications, with the learning rate being crucial. Hyperparameters were chosen based on performance on both main and auxiliary tasks, favoring the main task. Tasks were jointly trained by computing loss and gradients for each task simultaneously. The final choice of hyper-parameters was made by selecting values that optimized performance on both the main and auxiliary tasks, with a focus on the main task. The model was trained on all tasks simultaneously using a uniform weighting strategy. The \"Adam\" optimizer was used with specific parameters, and the learning rate was decayed during training. In the experiments, gradients were calculated based on a weighted sum of losses for all tasks. A uniform weighting strategy was used, and advanced strategies did not benefit the tagging task. The \"Adam\" optimizer was utilized with specific parameters, and the learning rate was decayed every 5 epochs. A batch size of 48 was used due to computational constraints. Noise reduction and upsampling tasks required separate forward propagation of audio. The learning rate was decayed by a factor of .95 every 5 epochs to improve convergence. A batch size of 48 was used in all experiments. Noise reduction and upsampling tasks required separate forward propagation of audio. Important parameters can be found in TAB3."
}