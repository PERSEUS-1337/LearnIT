{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this, a new balanced face image dataset with 108,501 images representing 7 race groups was created. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups for evaluation on existing face attribute datasets. The study created a balanced face image dataset with 108,501 images representing 7 race groups. The dataset was labeled with race, gender, and age groups for evaluation on existing face attribute datasets. The model trained on this dataset showed higher accuracy on novel datasets and consistent performance across race and gender groups. Commercial computer vision APIs were also compared for accuracy across gender, race, and age groups. The study compared the accuracy of a model trained on a new face image dataset with existing datasets. It showed higher accuracy on novel datasets and consistent performance across race and gender groups. Commercial computer vision APIs were also evaluated for accuracy across gender, race, and age groups. Numerous large-scale face image datasets have been proposed in the past, fostering research in this area. Numerous large-scale face image datasets have fostered research and development for automated face detection, alignment, recognition, generation, and modification. Research and development in automated face detection, alignment, recognition, generation, modification, and attribute classification have been proposed and advanced through various studies. These systems have been applied in security, medicine, education, and social sciences. However, existing public face datasets are predominantly biased towards Caucasian faces. Existing public face datasets are biased towards Caucasian faces, with other races like Latino being underrepresented. Most large-scale face databases are skewed towards \"lighter skin\" faces, such as White, compared to \"darker\" faces like Black. This bias can lead to models that may not apply to all subpopulations and results that cannot be compared across different groups. The bias in existing large-scale face databases towards Caucasian faces, with underrepresentation of other races like Latino, can lead to models that may not be applicable to all subpopulations. This bias raises ethical concerns about fairness in automated systems and has become a critical topic in machine learning and AI literature. Biased data can lead to biased models, raising ethical concerns about fairness in automated systems. Recent studies have criticized commercial computer vision systems for their asymmetric accuracy across sub-demographics, with better performance on male and light faces. Several commercial computer vision systems have been criticized for their asymmetric accuracy across sub-demographics, performing better on male and light faces due to biases in training data. Unwanted biases in image datasets can easily occur from biased selection, capture, and negative sets. Public large scale face datasets are often collected from platforms frequented by or showing White people. To address biases in training data causing asymmetric accuracy in computer vision systems, a novel face dataset with balanced race composition containing 108,501 facial images has been proposed. The dataset was collected primarily from the YFCC-100M Flickr dataset and can be freely shared for research purposes. A novel face dataset with 108,501 facial images collected from various sources, emphasizing balanced race composition. The dataset includes 7 race groups and is well-balanced on these groups. The paper highlights biases in existing face attribute datasets and models. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. It is well-balanced on these groups. The paper shows that existing face attribute datasets and models do not generalize well to unseen data with more nonWhite faces. Their new dataset performs better on novel data, especially across racial groups. This dataset is the first large-scale face attribute dataset in the wild. The new dataset includes major racial groups like Latino and Middle Eastern, filling gaps in existing datasets. It performs better on novel data across racial groups, unlike previous datasets and models. This dataset is the first large-scale face attribute dataset in the wild. The new face attribute dataset includes Latino and Middle Eastern groups, differentiating East Asian and Southeast Asian. This dataset expands the applicability of computer vision methods to fields like economics and social sciences by analyzing various demographics. Face attribute recognition aims to classify human attributes from facial appearance. The new face attribute dataset includes diverse ethnic groups, expanding the applicability of computer vision methods to various fields. Face attribute recognition involves classifying human attributes from facial appearance, such as gender, race, age, emotions, and expressions. The statistics of large scale face attribute datasets, including a new dataset, show a dominance of the White race. Face attribute recognition is crucial for tasks like face verification and person re-identification, requiring equal performance across gender and race groups to maintain trust in machine learning systems. Face attribute recognition for tasks like face verification and person re-identification must perform equally well across different gender and race groups to maintain trust in machine learning systems. Incidents of racial bias, such as Google Photos mistaking African American faces for Gorillas and Nikon's cameras asking Asian users if they blinked, highlight the importance of ensuring fair performance across all demographics. Most notable incidents of racial bias in machine learning and computer vision research include Google Photos mistaking African American faces for Gorillas and Nikon's cameras asking Asian users if they blinked. These incidents often lead to termination of services or features, prompting commercial providers to stop offering race classifiers. Face attribute recognition is also used for demographic surveys in marketing and social science research. Most commercial service providers have stopped offering race classifiers due to incidents of racial bias in machine learning. Face attribute recognition is used for demographic surveys in marketing and social science research to understand human social behaviors and demographic backgrounds. Social scientists use images to infer demographic attributes and analyze behaviors. Social scientists use images to infer demographic attributes and analyze behaviors of individuals, with notable examples being demographic analyses of social media users. Unfair classification can have significant implications, over-or under-estimating specific sub-populations in their analysis. AI and machine learning communities are increasingly focusing on algorithmic fairness and dataset biases. Different definitions of fairness exist in the literature, with a focus on balanced accuracy in attribute classification accuracy. In this paper, the focus is on balanced accuracy in attribute classification, specifically whether classification accuracy is independent of race and gender. Research in fairness aims to ensure fair outcomes regardless of protected attributes like race or gender. Studies in algorithmic fairness either audit existing bias in datasets or focus on fair outcomes in systems. The attribute classification accuracy is independent of race and gender. Research in fairness focuses on fair outcomes regardless of protected attributes. Studies in algorithmic fairness audit bias in datasets or aim for fair outcomes in systems. The paper discusses bias in datasets and systems, the importance of creating better datasets, and designing improved algorithms for gender classification from facial images. Buolamwini & Gebru (2018) highlighted biases in gender classification systems, especially towards dark-skinned females, possibly due to skewed image origins. The paper focuses on biased gender classification from facial images, highlighting issues with accuracy on dark-skinned females. It discusses the challenges of balancing datasets and associations between scene and race in images. The contribution of the paper is to mitigate bias in gender classification systems. The paper aims to mitigate biases in gender classification systems by collecting more diverse face images from non-White race groups, improving generalization performance to novel datasets not dominated by the White race. The paper aims to mitigate biases in gender classification systems by collecting more diverse face images from non-White race groups, improving generalization performance to novel datasets not dominated by the White race. The dataset includes Southeast Asian and Middle Eastern races, defining 7 race groups in total. Our dataset is the first large-scale face image dataset including Southeast Asian and Middle Eastern races, defining 7 race groups. Race is based on physical traits while ethnicity is based on cultural similarities. The dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is defined by physical traits, while ethnicity is based on cultural similarities. The U.S. Census Bureau's race classification was initially adopted, with Latino considered a race based on facial appearance. In practice, race and ethnicity are often used interchangeably. The U.S. Census Bureau's race classification was adopted, including White, Black, Asian, Latino, Middle Eastern, East Asian, Southeast Asian, and Indian. Latino is considered a race based on facial appearance. Hawaiian and Pacific Islanders and Native Americans were excluded due to limited examples. The experiments in the paper were based on 7 race classifications. During data collection, subgroups like Middle Eastern, East Asian, Southeast Asian, and Indian were clearly distinguished. Hawaiian, Pacific Islanders, and Native Americans were excluded due to limited examples. The experiments in the paper focused on 7 race classifications. The measurement of dataset bias based on skin color or race is a key criterion, with some studies using skin color as a proxy for racial or ethnic grouping. The bias in dataset measurement can be based on skin color or race, with some studies using skin color as a proxy for racial or ethnic grouping. However, skin color has limitations due to variations caused by lighting conditions and within-group variations. The Pilot Parliaments Benchmark dataset used controlled lighting for profile photographs, making it non-in-the-wild. Skin color is affected by lighting conditions and within-group variations, making it non-in-the-wild. Race is multidimensional while skin color is one-dimensional, providing limited information to differentiate between groups. The text discusses how skin color can vary over time and is one-dimensional, while race is multidimensional. Skin color alone does not provide enough information to differentiate between groups like East Asian and White. Race is annotated by human judgment, but skin color is also considered using the ITA measurement. Many face datasets are sourced from public figures like politicians and celebrities, which may introduce bias in the selection of populations. Skin color alone is not sufficient to differentiate between groups like East Asian and White, so race is annotated by human judgment. Additionally, skin color is considered using the ITA measurement. Some face datasets are sourced from public figures like politicians and celebrities, leading to potential biases in age and attractiveness. Images are often taken by professional photographers, resulting in quality bias. Other datasets are collected through web searches using keywords, which may prioritize stereotypical or celebrity faces over diverse ones. The dataset used in this study aims to minimize selection bias by maximizing diversity and coverage. It started from the Yahoo YFCC100M dataset, detecting faces without preselection to avoid filtering based on stereotypes or celebrity status. The dataset used in this study aims to minimize selection bias by maximizing diversity and coverage. Faces were detected from the Yahoo YFCC100M dataset without preselection to avoid filtering based on stereotypes or celebrity status. The dataset is smaller but more balanced on race, with 7,125 faces randomly sampled and incrementally increased in size. The dataset used in this study aims to minimize selection bias by maximizing diversity and coverage. A recent work also used the same dataset to construct a huge unfiltered face dataset (Diversity in Faces, DiF). The dataset is smaller but more balanced on race, with faces incrementally increased in size and annotations obtained to estimate demographic compositions of each country. The dataset is adjusted to avoid dominance by the White race, excluding the U.S. and European countries in the later sampling. After adjusting the dataset to avoid dominance by the White race, the minimum size of a detected face was set to 50 by 50 pixels to make the classifiers more robust against noisy data. The U.S. and European countries were excluded in the later stage of data collection. The dataset included White faces from various countries, with a minimum face size of 50 by 50 pixels to enhance classifier robustness. Images with \"Attribution\" and \"Share Alike\" licenses were used, and race, gender, and age were annotated using Amazon Mechanical Turk with three workers per image for accuracy. The dataset included White faces from various countries, with a minimum face size of 50 by 50 pixels to enhance classifier robustness. Images with \"Attribution\" and \"Share Alike\" licenses were used, and race, gender, and age were annotated using Amazon Mechanical Turk with three workers per image for accuracy. Annotations were refined by training a model from initial ground truth annotations and applying it back to the dataset. After annotating race, gender, and age using Amazon Mechanical Turk with three workers per image, the annotations were refined by training a model from initial ground truth annotations and manually re-verifying annotations for images with differing predictions. Skewed datasets in terms of race composition were measured, with race labels annotated for 3,000 random samples from datasets without race annotations. After annotating race, gender, and age, annotations were refined by training a model and manually re-verifying annotations for images with differing predictions. Skewed datasets in terms of race composition were measured, showing bias towards the White race in existing face attribute datasets. Gender balance ranged from 40%-60% male ratio. Model performance across datasets was compared. Most face attribute datasets, especially those focusing on celebrities or politicians, show bias towards the White race. Gender balance in these datasets ranges from 40%-60% male ratio. Model performance was compared using the ResNet-34 architecture trained on each dataset with ADAM optimization and a learning rate of 0.0001. Face detection was done using dlib's CNN-based face detector, and the attribute classifier was run on each face in PyTorch. In the experiment, ResNet-34 model was trained on different datasets using ADAM optimization with a learning rate of 0.0001. Faces were detected using dlib's CNN-based face detector, and the attribute classifier was run on each face in PyTorch. The dataset was compared with UTKFace, LFWA+, and CelebA for gender and race annotations. The experiment compared the dataset with UTKFace, LFWA+, and CelebA for gender and race annotations. CelebA lacks race annotations, so it was only used for gender classification. FairFace defines 7 race categories but only 4 were used for comparison. Tables 2 and 3 display the dataset characteristics and results of cross-dataset classifications using models trained from various datasets. FairFace has 7 race categories but only 4 were used for comparison with UTKFace. CelebA, lacking race annotations, was included solely for gender classification. The study involved cross-dataset classifications using models trained on different datasets. FairFace, with 7 races, had its racial groups merged for compatibility. CelebA, without race annotations, was used for gender classification. Results showed that models performed best on the dataset they were trained on, with highest accuracy on LFWA+ dataset for some variables. The study involved cross-dataset classifications using models trained on different datasets. The accuracy of the model was highest on some variables on the LFWA+ dataset, which is the most biased, and very close to the leader in other cases. The model's performance was tested on three novel datasets collected from different sources than the training data. The study tested model generalization performance on three novel datasets collected from different sources than the training data, including geo-tagged Tweets from four countries. The study tested model generalization performance on diverse races using geo-tagged Tweets and media photographs from four countries (France, Iraq, Philippines, and Venezuela). The datasets included images from Twitter users with geo-tags and photographs from online professional media outlets. In this set, faces were randomly sampled from four countries (France, Iraq, Philippines, and Venezuela) and media photographs from online professional outlets. Additionally, faces were sampled from a dataset of tweet IDs posted by known media accounts. Another dataset used was collected for a recent protest activity. The dataset used includes media photographs from professional outlets and tweet IDs from known media accounts. Faces were randomly sampled from four countries and a public image dataset collected for a protest activity study. The dataset for a protest activity study was collected from Google Image search using keywords like \"Venezuela protest\" and \"football game\". It includes diverse race and gender groups from various countries, with 8,000 faces randomly sampled and annotated for gender, race, and age. Gender classification accuracy and model performance are reported in tables. The dataset is larger than LFWA+ and UTKFace datasets. The dataset for a protest activity study includes 8,000 faces randomly sampled and annotated for gender, race, and age. The FairFace model outperforms all other models for race, gender, and age classification on novel datasets. FairFace model outperforms other models for race, gender, and age classification on novel datasets, even with smaller training sets. The dataset size is not the only factor contributing to the model's improved accuracy. The FairFace model outperforms other datasets in race, gender, and age classification, even with smaller training sets. The dataset size is not the sole factor for the performance improvement. The model also shows more consistent results across different race groups compared to other datasets. The FairFace model demonstrates superior performance in race, gender, and age classification compared to other datasets, with consistent results across different race groups. The model evaluates fairness through measures like conditional use accuracy equality and equalized odds. The FairFace model shows superior performance in race, gender, and age classification compared to other datasets. It evaluates fairness through measures like conditional use accuracy equality and equalized odds. Gender classification accuracy is measured for different demographic groups, with the FairFace model achieving the lowest maximum accuracy disparity. The FairFace model demonstrates superior performance in race, gender, and age classification compared to other datasets. It achieves less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White for gender classification, showing the lowest maximum accuracy disparity among different models. The CelebA model shows bias towards the female category due to dataset imbalance. FairFace model achieves less than 1% accuracy difference between male \u2194 female and White \u2194 non-White for gender classification. Other models exhibit bias towards males, with lower accuracy on females and non-White groups. Gender performance gap is largest in LFWA+ dataset. Recent work also highlights gender biases in commercial computer vision services. The study found that there was a significant gender performance gap in the LFWA+ dataset, with much lower accuracy on the female group. The results suggest that this bias is likely due to unbalanced representation in the training data. Additionally, the researchers investigated data diversity using t-SNE visualization of facial embeddings based on ResNet-34 from dlib. The study found a gender performance gap in the LFWA+ dataset, likely due to unbalanced training data. Data diversity was investigated using t-SNE visualization of facial embeddings, showing FairFace has well-spread faces with loosely separated race groups. The embedding based on ResNet-34 from biased datasets like FaceScrub and VGG-Face shows that FairFace contains diverse faces with loosely separated race groups. The LFWA+ dataset has a gender performance gap due to unbalanced training data. The dataset LFWA+ was derived from LFW for face recognition, containing multiple images of the same individuals. UTKFace focuses more on local clusters compared to FairFace, with more tightly clustered faces. Pairwise distance distributions were examined to measure diversity, showing UTKFace had tightly clustered faces. To measure the diversity of faces in datasets, pairwise distances between faces were examined using 128-dimensional facial embeddings. UTKFace had tightly clustered faces, while LFWA+ showed diverse faces despite mostly white faces. This diversity may be due to the face embedding trained on a white-oriented dataset. The faces in the LFWA+ dataset were found to be diverse and far apart from each other, despite the majority being white faces. This diversity is attributed to the face embedding being trained on a white-oriented dataset, which effectively separates white faces. The FairFace dataset was used to test gender classification APIs like Microsoft Face API, Amazon Rekognition, IBM Watson Visual Recognition, and Face++. Compared to prior work with politicians' faces, this dataset is more diverse in terms of race and age. Inconsistent classification accuracies were found across different demographic groups. The FairFace dataset was used to test gender classification APIs, including Microsoft Face API, Amazon Rekognition, IBM Watson Visual Recognition, and Face++. This dataset is more diverse in terms of race, age, expressions, head orientation, and photographic conditions compared to prior work with politicians' faces. 7,476 random samples from FairFace were used, ensuring an equal number of faces from each race, gender, and age group, excluding children under 20 due to ambiguity in gender identification. The study used the FairFace dataset, which is more diverse in terms of race, age, expressions, head orientation, and photographic conditions. 7,476 random samples were used, ensuring an equal number of faces from each demographic group. Children under 20 were excluded due to gender ambiguity. The experiments were conducted on August 13th -16th, 2019. The experiments were conducted on August 13th -16th, 2019, using various facial recognition models such as Microsoft, Face++, IBM, and FairFace. The gender of individuals in the pictures was often ambiguous. Table 6 displays gender classification accuracies of tested APIs for face detection and gender classification. Amazon Rekognition detected all 7,476 faces, while other APIs had varying detection rates. Table 6 shows gender classification accuracies of tested APIs for face detection and gender classification. Not all faces were detected by the APIs except for Amazon Rekognition, which detected all faces. Two sets of accuracies are reported: one including mis-detections as mis-classifications and one excluding them. A model trained with their dataset is included for comparison. The gender classification accuracies of tested APIs for face detection were shown in Table 6. Amazon Rekognition detected all faces and two sets of accuracies were reported. The results indicated that male category was favored by all gender classifiers, with higher error rates for dark-skinned females. The results from prior work show that gender classifiers still favor males, with higher error rates for dark-skinned females. Skin color alone is not a sufficient guideline to study model bias, as some APIs classified Indians more accurately than Whites despite darker skin tones. Face detection can also introduce significant gender bias. This paper proposes a novel face image dataset balanced on race, gender, and age, highlighting the limitations of using skin color alone to study model bias. Microsoft's model showed gender bias in face detection by failing to detect many male faces. The paper proposes a new face image dataset balanced on race, gender, and age to address gender bias in face detection. The dataset achieves better classification performance for gender, race, and age on images collected from various sources. The model trained on this dataset shows balanced accuracy across race. Our dataset, derived from the Yahoo YFCC100m dataset, achieves better generalization classification performance for gender, race, and age on images collected from Twitter, online newspapers, and web search. The model trained on this dataset shows balanced accuracy across race groups. The dataset, derived from Yahoo YFCC100m, ensures balanced accuracy across race groups. It allows for training new models and evaluating existing classifiers for algorithmic fairness in AI systems. The novel dataset proposed in this paper aims to discover and mitigate race and gender bias in computer vision systems, improving their acceptance in society. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems for improved acceptance in society. The dataset includes an Individual Typology Angle (ITA) to measure skin color distribution among different races."
}