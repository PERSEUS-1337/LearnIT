{
    "title": "rJxlc0EtDr",
    "content": "Recent research on neural network architectures with external memory has focused on the bAbI question and answering dataset, which presents challenging tasks requiring reasoning. To further investigate the reasoning capacity of these architectures, a classic associative inference task from human neuroscience literature was utilized. This task aims to test the ability to understand distant relationships among elements distributed across multiple facts or memories. Surprisingly, current architectures struggle with reasoning over long distance associations. Similar difficulties were observed in a more complex task involving finding connections. The study focused on the limitations of current memory-augmented architectures in reasoning over long distance associations. A novel architecture, MEMO, was developed to address this issue by introducing components for reasoning over longer distances. MEMO is a novel architecture designed to reason over longer distances by introducing components for separating memories/facts in external memory and utilizing an adaptive retrieval mechanism. It can solve complex reasoning tasks and all 20 tasks in bAbI. MEMO is a novel architecture that utilizes an adaptive retrieval mechanism and external memory to solve complex reasoning tasks. It allows for connecting facts acquired at different times and points in experiences. Inferential reasoning involves combining separate experiences to infer relationships, supported by the hippocampus. Inferential reasoning combines separate experiences to infer relationships, supported by the hippocampus which stores memories independently through pattern separation. This minimizes interference between experiences, allowing for specific recall of 'episodic' memories. The hippocampus stores memories independently through pattern separation to minimize interference between experiences, allowing for specific recall of 'episodic' memories. Recent research sheds light on how separated memories can be chained together. Recent research shows that the integration of separated memories occurs at retrieval through a recurrent mechanism, allowing for interaction between multiple pattern separated codes to support inference. This sheds light on how memories can be chained together despite being stored independently in the hippocampus. The integration of separated experiences at retrieval through a recurrent mechanism supports inference. Neural networks like the Differential Neural Computer (DNC) and end-to-end memory networks (EMN) draw inspiration from neuroscience models to enhance inferential reasoning. Neural networks, such as the Differential Neural Computer (DNC) and end-to-end memory networks (EMN), use inspiration from neuroscience models to improve inferential reasoning. Attention mechanisms and context utilization have also enabled traditional neural networks to handle challenging tasks. Tasks like bAbI present opportunities for neural networks to exploit repetitions and commonalities between training and testing sets. To enhance inferential reasoning, new tasks like Paired Associative Inference (PAI) have been introduced, drawing from neuroscientific literature. This task aims to capture the essence of inferential reasoning by avoiding degenerate solutions that neural networks may exploit in tasks like bAbI. To address the limitations of neural networks exploiting degenerate solutions, a new task called Paired Associative Inference (PAI) was introduced. PAI is derived from neuroscientific literature and focuses on inferential reasoning by requiring networks to learn abstractions to solve unseen associations. This task is designed to encourage neural networks to appreciate distant relationships among elements distributed across multiple facts or memories. The Paired Associative Inference (PAI) task is procedurally generated to encourage neural networks to learn abstractions for inferential reasoning. Models like EMN have used fixed memory representations combining word embeddings with positional encoding for memory-based reasoning. MEMO is a new approach that retains all facts in memory and uses a linear projection with a recurrent attention mechanism for greater flexibility in memory usage. This contrasts with models like EMN that use fixed memory representations for memory-based reasoning. MEMO, a new approach in language modeling, retains all facts in memory and utilizes a linear projection with a recurrent attention mechanism for flexible memory usage. This differs from models like EMN that use fixed memory representations for reasoning. MEMO is a new approach in language modeling that retains all facts in memory and utilizes a linear projection with a recurrent attention mechanism for flexible memory usage. It is based on the external memory structure presented in EMN, but with new architectural components that allow for flexible weighting of individual elements in memory to support inferential reasoning. The problem of prohibitive computation time is addressed by adjusting the input size to reduce computation, either by padding with extra values or systematically dropping input values. In standard neural networks, computation grows with input size. Input can be padded with extra values or dropped to reduce computation. Values are usually hand-tuned, but we aim to adapt compute time to task complexity, inspired by REMERGE model of human associative memory. In standard neural networks, computation grows with input size, which can be padded or dropped. Values are typically hand-tuned, but we aim to adapt compute time to task complexity. Inspired by the REMERGE model of human associative memory, we recirculate content as a new query in a neural network to determine when it settles into a fixed point. In a neural network inspired by the REMERGE model of human associative memory, content is recirculated as a new query to determine when the network settles into a fixed point. The network outputs an action, known as the halting policy, to decide whether to continue computing or answer the task. This approach is influenced by techniques like adaptive computation time. In our neural network architecture, inspired by techniques like adaptive computation time, the network uses a halting policy to determine whether to continue computing or answer the task. Unlike ACT, we train the binary halting random variable using reinforcement learning, adjusting weights based on the optimal number of computation steps. The network learns the termination criteria of a fixed point operator using reinforcement learning to adjust weights based on the optimal number of computation steps. This approach differs from previous methods by adding an extra term to the REINFORCE loss to minimize the binary random variables. Our approach adds an extra term to the REINFORCE loss to minimize the expected number of computation steps, encouraging the network to prefer representations that require less computation. Our contributions include a new task emphasizing reasoning and the appreciation of distant relationships among elements. Our contributions involve a new task focusing on reasoning and distant relationships, an investigation into memory representations for inferential reasoning, extensions to memory architectures for improved results, and a REINFORCE loss component for learning optimal task-solving iterations. The text discusses relationships among elements in multiple facts, an investigation into memory representations for inferential reasoning, extensions to memory architectures for better results, and a REINFORCE loss component for learning optimal task-solving iterations. Empirical results on three tasks demonstrate the effectiveness of these contributions. The text discusses the effectiveness of End-to-End Memory Networks in solving tasks like paired associative inference, shortest path finding, and bAbI. It introduces notation and nomenclature, focusing on the multilayer, tied weight variant of EMN. The network predicts answers based on knowledge inputs and a query. The text discusses the effectiveness of End-to-End Memory Networks in solving tasks like paired associative inference, shortest path finding, and bAbI. It introduces notation and nomenclature, focusing on the multilayer, tied weight variant of EMN. The network predicts answers based on knowledge inputs and a query, with specific details on input sequences and word encoding. The text discusses the effectiveness of End-to-End Memory Networks in solving tasks like paired associative inference, shortest path finding, and bAbI. It introduces notation and nomenclature, focusing on the multilayer, tied weight variant of EMN. The network predicts answers based on knowledge inputs and a query, with specific details on input sequences and word encoding. In the model, x represents the length of the knowledge input sequence, S is the length of each input sentence, and O is the size of the vocabulary. EMN embeds each word and sums the resulting vectors using embedding matrices for key, values, and query. The End-to-End Memory Network (EMN) utilizes embedding matrices for key, values, and query to calculate weights over memory elements and produce outputs. It involves positional encoding, element-wise multiplication, and linear mappings to relate queries at different steps. The network predicts answers based on knowledge inputs and queries, with specific details on input sequences and word encoding. MEMO embeds the input differently by deriving a common embedding for each input matrix, utilizing a linear mapping to relate queries at different steps. It involves a cross entropy loss training on the final step for answer prediction. MEMO embeds input differently by deriving a common embedding for each input matrix and utilizing a linear mapping for queries at different steps. It involves cross entropy loss training on the final step for answer prediction. Unlike EMN, MEMO does not use hand-coded positional embeddings but combines words in each sentence with their one-hot encoding before passing through a linear projection and attention mechanism. MEMO uses multiple heads to attend to the memory, with each head having a different view of the common inputs. Each input matrix is embedded differently, and a linear mapping is used for queries at different steps. This approach allows for flexible capturing of any part of the input sentence. MEMO utilizes multiple heads to attend to the memory, allowing for flexible capturing of any part of the input sentence. Each head has a different view of the common inputs, with key, values, and query embedding matrices used for each head. This approach enables the learning of how to weight each item in memory. MEMO uses multiple heads to attend to memory, allowing for flexible capturing of input sentences. Each head has key, values, and query embedding matrices, enabling the learning of how to weight each item in memory. This differs from hand-coded positional embeddings used in previous models and is critical for recombining stored items. The attention mechanism in MEMO also differs from previous models. MEMO uses multi-head attention to weight each item in memory, contrasting with hand-coded positional embeddings in previous models. The attention mechanism in MEMO also differs by using multi-head attention, DropOut, and LayerNorm for improved generalization and learning dynamics. The attention mechanism in MEMO is adapted to use multi-head attention, DropOut, and LayerNorm for improved generalization and learning dynamics. Matrices are used for transforming logits, queries, and output MLP to produce the answer. The attention mechanism in MEMO utilizes matrices for transforming logits, queries, and output MLP to produce the answer. It differs from Vaswani et al. (2017) by preserving the query separated from the keys and values, leading to improved computational complexity. MEMO's attention mechanism differs from Vaswani et al. (2017) by preserving query separation from keys and values, improving computational complexity. MEMO is linear with input sentences, while self-attention methods have quadratic complexity. This section discusses learning the number of computational steps required for effective answers. MEMO's attention mechanism is linear with input sentences, unlike self-attention methods which have quadratic complexity. The process involves collecting information at each step, processing it with GRUs and an MLP to create an observation, and defining a binary policy and value function. MEMO's attention mechanism involves collecting information at each step, processing it with GRUs and an MLP to create an observation, and defining a binary policy and value function based on the Bhattacharyya distance between attention weights of current and previous time steps. The network input is based on the Bhattacharyya distance between attention weights of current and previous time steps, along with the number of steps taken so far as a one-hot vector. The network aims to avoid settling into a fixed point by adjusting parameters using REINFORCE during training. The network is trained using REINFORCE to adjust parameters and avoid settling into a fixed point. The objective function is to minimize L Hop, which follows from the fact that \u03c0 is a binary policy. The objective function of the network is to minimize L Hop, a term that encourages minimizing the expected number of hops in computation. This term directly promotes representations and computation that require less computation. The new term introduced in the loss function, L Hop, aims to minimize the expected number of hops in computation, promoting efficient representations and computation. It addresses the issue of high variance when training discrete random variables and specifically focuses on reducing the amount of required computation. The loss function L Hop minimizes expected computation hops, reducing variance in training discrete random variables. The reward structure is defined by the target answer a and prediction \u00e2 from the network. The final layer of MLP R is initialized with bias init to enhance probability production by \u03c0. The variance is bounded by 1/4 for successful learning. The reward structure is defined by the target answer a and prediction \u00e2. The final layer of MLP R is initialized with bias init to increase the probability of producing a probability of 1. A maximum number of hops N is set for the network, with no gradient sharing between the hop network and the main MEMO network. In recent years, there has been a growing interest in memory-augmented networks for abstract and relational reasoning tasks. The Differential Neural Computer (DNC) is another influential model in this field, separate from the main MEMO network with no gradient sharing. Model hyperparameters are detailed in the appendix. In recent years, there has been increasing interest in memory-augmented networks for abstract and relational reasoning tasks. The Differential Neural Computer (DNC) is a model that operates sequentially on inputs, learning to read and write to a memory store. It has shown capability in solving algorithmic problems but struggles with scalability to higher-dimensional domains. A recent extension of the DNC incorporated sparsity, improving its performance. The Neural Computer (DNC) operates sequentially on inputs, learning to read and write to memory. An extension with sparsity improved performance on larger-scale tasks. Alternative memory-augmented architectures have been developed since. Several alternative memory-augmented architectures have been developed since the Neural Computer (DNC) was introduced in 2016. These include the Dynamic Memory Network, Recurrent Entity Network, and Working Memory Network, each with unique features and capabilities. The Recurrent Entity Network and Working Memory Network are memory-augmented architectures that enable relational reasoning over memory contents. These models have shown good performance on various tasks, including the bAbI task suite. The Recurrent Entity Network and Working Memory Network incorporate a RelationNet for relational reasoning over memory contents. These models perform well on tasks like the bAbI task suite. Adaptive Computation Time (ACT) is a mechanism for adjusting computational budget based on task complexity by learning a scalar halting probability. Adaptive Computation Time (ACT) and Adaptive Early Exit Networks are approaches to adjust computational budget based on task complexity in machine learning algorithms. ACT learns a scalar halting probability to modulate computational steps, while Early Exit Networks allow premature exit if further computation is unnecessary. Conditional computation techniques in machine learning include using halting probability to adjust computational steps, Adaptive Early Exit Networks for premature exit, and REINFORCE for learning discrete latent variables to dynamically adjust computation steps. These methods have been applied to neural networks to optimize computational efficiency. Another approach to conditional computation is using REINFORCE to learn discrete latent variables that adjust the number of computation steps in neural networks. This technique has been applied to recurrent neural networks to decide on activating the next layer or reducing the total number of processed inputs. Additionally, the jump technique has been implemented in recurrent neural networks without the need for REINFORCE, as well as in neural networks augmented with external memory. The jump technique, applied to neural networks, reduces processed inputs and has been used in recurrent neural networks without REINFORCE. Another approach involves using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Our method introduces the use of attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Graph Neural Networks consist of an iterative message passing process that propagates node and edge embeddings throughout a graph for various learning tasks. Graph Neural Networks (GNNs) involve iterative message passing to propagate embeddings in a graph for learning tasks. Neural networks aggregate functions over graph components for various learning tasks, similar to attention mechanisms. The message passing process in our work implements computation similar to attention mechanisms and self-attention can be seen as a fully-connected GNN. Our method differs from GNNs by performing adaptive computation to modulate the number of message passing steps. Our method differs from GNNs by performing adaptive computation to modulate the number of message passing steps, eliminating the need for message passing between memories as input queries directly attend to memory slots. The model does not require message passing between memories; input queries attend directly to memory slots. One contribution of this paper is to introduce a task derived from neuroscience to probe the reasoning capacity of neural networks by appreciating distant relationships among elements distributed across multiple facts or memories. The paper introduces a task derived from neuroscience to probe the reasoning capacity of neural networks by appreciating distant relationships among elements distributed across multiple facts or memories, formalized in a paired associative inference (PAI) task. The paired associative inference (PAI) task involves randomly associating two images together to study the role of the hippocampus in generalization. For example, a woman (image A) and a girl (image B) are paired, and later image B is paired with a new image C. In the PAI task, an agent is presented with pairs of images to test memory and inference abilities. Direct queries test episodic memory, while indirect queries require inference across multiple episodes. During the PAI task, an agent is tested with direct and indirect queries to assess episodic memory and inference abilities. Direct queries rely on retrieving experienced episodes, while indirect queries require linking multiple episodes to answer correctly. The network is presented with a cue, image A, and two choices: the match, image C, originally paired with B, or a lure, another image C paired with B in a different triplet. The correct answer requires understanding the link between A and C based on their previous pairings. During the PAI task, the network is presented with a cue, image A, and two choices: the match, image C, originally paired with B, or a lure, another image C paired with B in a different triplet. The correct answer requires understanding the link between A and C based on their previous pairings. MEMO was compared with End to End Memory Networks (EMN) and DNC. Refer to appendix A.1 for specific details on batch creation. The study compared MEMO with other memory-augmented architectures like End to End Memory Networks (EMN), DNC, and Universal Transformer (UT). Table 1 shows the results of MEMO and the baselines on the hardest inference query. Refer to the appendix for more details on the baselines. MEMO was compared with other memory-augmented architectures like EMN, DNC, and UT. Table 1 displays the results of MEMO and the baselines on the toughest inference query. MEMO and DNC achieved the highest accuracy on the A-B-C set, while EMN and UT struggled to match their performance. MEMO outperformed other memory-augmented architectures like EMN, DNC, and UT on the toughest inference queries. While DNC required 10 pondering steps to achieve the same accuracy as MEMO on a length 3 PAI task, MEMO converged to 3 hops. MEMO outperformed other memory-augmented architectures on complex inference queries, requiring only 3 hops to achieve accuracy compared to DNC's 10 pondering steps on a length 3 PAI task. Further analysis on MEMO's approach involved studying attention weights for associating CUE with MATCH and avoiding interference of LURE. In a study analyzing MEMO's approach on inference queries, attention weights were examined to associate a CUE with a MATCH and avoid interference from a LURE. The network retrieved memory in slot 10 in the first hop, containing IDs 611 and 191, forming an A - B sequence. In the second hop, MEMO focused on slot 16 containing the B - C association (IDs 191 and 840), with some attention on slot 13 linked to the LURE (ID 943). In the second hop, MEMO assigned probability masses to slots 16 (IDs 191 and 840) and 13 (ID 943) to support correct inference decisions, following a sequence similar to computational models of the hippocampus. In the second hop, MEMO assigned probability masses to slots to support correct inference decisions, following a sequence similar to computational models of the hippocampus. The number of hops taken by the network influences the algorithm used to solve the inference problem. The number of hops taken by the network influences the algorithm used to solve the inference problem, as seen in neural data. This could be related to knowledge distillation in neural networks. A set of ablation experiments on MEMO was also conducted. The number of hops taken by the network influences the algorithm used to solve the inference problem in neural networks. A set of ablation experiments on MEMO confirmed that specific memory representations and the recurrent attention mechanism are crucial for successful inference. The combination of specific memory representations and the recurrent attention mechanism is essential for successful inference in neural networks, as confirmed by ablation experiments on MEMO. This conclusion applies to inference queries but not direct queries, which test episodic memory. Additionally, the adaptive computation mechanism was found to be more data efficient than ACT for this task. The conclusion was valid for inference queries, not direct queries. Our adaptive computation mechanism was more data efficient than ACT for the task. Figure 2 shows weights analysis of an inference query in the length 3 PAI task. Our method was more data efficient for inference queries, as shown in Figure 5 in Appendix A.3.3. The weights analysis of an inference query in the length 3 PAI task is illustrated in Figure 2. The memory content and related inference query are depicted, along with the weights associated with the 3 hops used by the network. The weights analysis of an inference query in the length 3 PAI task is shown in Figure 2. Table 2 displays the accuracy of models in finding the shortest path between nodes on a small graph. DNC, Universal Transformer, and MEMO had perfect accuracy in predicting the intermediate shortest path node. Experiments on randomly generated graphs showed that MEMO outperformed EMN in predicting the first node of the path by more than 50% on more complicated graphs. Additionally, MEMO performed better than DNC in graphs with a high degree of connectivity. MEMO outperformed EMN in predicting the first node of the path by more than 50% on complex graphs with a path length of 3. It also surpassed DNC in graphs with high connectivity, showing great scalability by considering more paths as the number of hops increases. Universal Transformer had varying performance in predicting the first and second nodes of the shortest path. MEMO showed great scalability by considering more paths as the number of hops increases. Universal Transformer had different performance in predicting the first and second nodes of the shortest path. Test results for the best 5 hyper-parameters for MEMO are reported. The study compared the performance of MEMO and Universal Transformer (UT) in computing operations that require direct reasoning. Results for the best hyper-parameters for MEMO were reported, along with accuracy on the bAbI question answering dataset. In the study, MEMO was able to solve all tasks on the bAbI question answering dataset with high accuracy, outperforming other baselines with lower error rates. MEMO achieved high accuracy in solving all tasks on the bAbI dataset, outperforming other baselines with lower error rates. Ablation experiments showed that memory representations and recurrent attention were critical for achieving state-of-the-art performance. The combination of memory representations and recurrent attention was crucial for achieving state-of-the-art performance on the bAbI task. The use of layernorm in the recurrent attention mechanism was also essential for stable training and improved performance. Test results for the best run on the bAbI task are provided, along with comparisons to DNC and Universal Transformer results. In this paper, an investigation was conducted on memory representations supporting inferential reasoning, introducing MEMO as an extension to existing architectures. MEMO demonstrated state-of-the-art results in reasoning tasks, outperforming DNC and Universal Transformer models. In this paper, an investigation was conducted on memory representations supporting inferential reasoning, introducing MEMO as an extension to existing architectures. MEMO showed state-of-the-art results in a new proposed task, paired associative inference, and also solved the 20 tasks of the bAbI dataset. MEMO demonstrated state-of-the-art performance in paired associative inference and solved challenging tasks like graph traversal. It also matched the best results on the bAbI dataset by flexibly weighting individual elements in memory with a recurrent attention mechanism. Our analysis supported the hypothesis that MEMO achieved state-of-the-art results on 20 tasks of the bAbI dataset by flexibly weighting individual elements in memory with a recurrent attention mechanism. The task was made challenging by starting from the ImageNet dataset and creating three distinct datasets with embedded images using a pre-trained ResNet. To challenge a neural network, datasets were created from the ImageNet dataset with embedded images using a pre-trained ResNet. Three sets were made with sequences of length three, four, and five items, each containing training, evaluation, and testing images. Sequences were randomly generated with no repetition within each dataset. Three distinct datasets were created with sequences of length three, four, and five items, each containing training, evaluation, and testing images. The batch was built by selecting N sequences from the pool, with N = 16, and creating memory content with all possible pairwise associations between the items in the sequence. The batch is composed of a memory, query, and target, with N sequences selected from a pool. Memory content includes pairwise associations between items in the sequence, resulting in 32 rows for sequences of length 3. Queries consist of a cue, match, and lure images, with the cue and match from the same sequence and the lure from a different sequence. Memory content includes pairwise associations between items in the sequence, resulting in 32 rows for sequences of length 3. Queries consist of a cue, match, and lure images, with the cue and match from the same sequence and the lure from a different sequence. Queries can be 'direct' or 'indirect', where 'direct' queries involve the cue and match in the same memory slot, eliminating the need for inference. The memory network involves two types of queries - 'direct' and 'indirect'. 'Direct' queries involve retrieving information from the same memory slot, while 'indirect' queries require inference across multiple episodes. The memory network involves two types of queries - 'direct' and 'indirect'. 'Direct' queries test episodic memory by retrieving a specific episode, while 'indirect' queries require inference across multiple episodes. The queries are presented as a concatenation of image embedding vectors, with the cue always in the first position. The position of the match and lure is randomized to avoid degenerate solutions. The queries in the memory network involve presenting image embedding vectors in a specific order to test episodic memory. The position of the match and lure is randomized to prevent degenerate solutions, requiring the correct connection between images to be appreciated without interference from other memory items. The memory network involves presenting image embedding vectors in a specific order to test episodic memory. The task requires appreciating the correct connection between images without interference from other memory items. The batch is balanced with direct and indirect queries, and longer sequences provide more direct queries for the network to predict the class of the matches. The batch generated all possible queries supported by the memory store, with half being direct and half indirect. Longer sequences result in more direct queries, but also multiple indirect queries requiring different levels of inference. The longer sequences produce both direct and indirect queries, with indirect queries requiring more inference steps. Different levels of inference are needed for trials with varying distances between cues and targets. Inputs are utilized differently for EMN, MEMO, and DNC architectures. The latter trial required more inference steps to appreciate overlapping images of the entire sequence. Inputs are used differently for EMN, MEMO, DNC, and UT architectures. Memory and query are presented in sequence to the model, followed by blank inputs as pondering steps for a final prediction. The output of the UT model is used as the final result. The model processes memory and query sequentially for MEMO and UT architectures. The output of the UT model is used as the final result for evaluation. Graph generation for shortest path experiments follows a similar approach to previous studies. Graph generation for shortest path experiments involves generating graphs by uniformly sampling two-dimensional points from a unit square to represent nodes. Each node has K nearest neighbors as outbound connections. The task is divided into a graph description, a query, and a target. The task involves generating graphs by sampling points from a unit square to represent nodes with K nearest neighbors as outbound connections. The graph description consists of tuples representing connections between nodes, the query is a tuple indicating the path to find, and the target is the sequence of node IDs for the path. The task involves generating graphs by sampling points from a unit square to represent nodes with K nearest neighbors as outbound connections. The graph description is presented as a sequence of tuples of integers representing connections between nodes. Queries are represented as a tuple of integers indicating the path to find, and targets are the sequence of node IDs that constitute the path between the source and destination of the query. During training, a mini-batch of 64 graphs, queries, and target paths are sampled. During training, a mini-batch of 64 graphs, queries, and target paths are sampled. Queries are represented as a matrix of size 64 \u00d7 2, targets are of size 64 \u00d7 (L \u2212 1), and graph descriptions are of size 64 \u00d7 M \u00d7 2. All networks were trained for 2e4 epochs, each one formed by 100 batch updates. The graph description is set for EMN and MEMO. During training, a mini-batch of 64 graphs, queries, and target paths are sampled. The graph description is set for EMN and MEMO, with the upper bound M determined by the maximum number of nodes multiplied by the out-degree of the nodes in the graph. The model predicts answers for nodes sequentially based on the contents of their memory, using the query as input and keeping keys fixed. During training, a mini-batch of 64 graphs, queries, and target paths are sampled. The model predicts answers for nodes sequentially based on the contents of their memory, using the query as input and keeping keys fixed. One key difference between MEMO and EMN is that for EMN, the ground truth answer of the first node is used as the query for the second node, while for MEMO, the predicted answer by the model for the first node is used as the query for the second node to enhance performance. The Universal Transformer also embeds the query and graph description like EMN and MEMO. The embeddings are concatenated and used in the encoder of the UT. The weights for each answer are not shared. The weights used for each answer are not shared in the Universal Transformer, which embeds the query and graph description similar to EMN and MEMO. The embeddings are concatenated and used in the encoder of the UT. The Universal Transformer architecture uses the encoder to process the graph description and answer, with specific details in Section H. The weights for each answer are not shared. For DNC, the query and graph description are embedded similarly to EMN and MEMO, with a sequential presentation of information. The output is trained using Adam optimization. The Universal Transformer architecture processes graph descriptions and answers using the encoder. The output of the models is trained using Adam optimization with a cross-entropy loss. Evaluation involves sampling a batch of 600 graph descriptions, queries, and targets to calculate mean accuracy over all nodes of the target path. The models are trained using Adam with cross-entropy loss. Evaluation involves sampling 600 graph descriptions, queries, and targets to calculate mean accuracy over all nodes of the target path. The best 5 hyperparameters are used, and DNC and UT have a 'global view' to provide an answer for the second node. The training regime involves DNC and UT having a 'global view' on the problem, allowing them to reason and work backwards from the end node to achieve better performance. In contrast, MEMO has a 'local view' where the answer to the second node depends on the answer about the first node. The training regime involves DNC and UT having a 'global view' on the problem, allowing them to reason and work backwards from the end node to achieve better performance. On the contrary, MEMO has a 'local view' where the answer to the second node depends on the answer about the first node. MEMO's performance depends on the answer to the first node, affecting its ability to outperform chance. Comparing MEMO and EMN, experiments were conducted using ground truth vs. predicted answers for the first node as queries for the second node. Results showed that providing MEMO with the ground truth for node 1 as a query for node 2 significantly improved performance compared to using the predicted answer (85.38% vs. 69.20%). The model's performance is influenced by the answer to the first node, impacting its ability to surpass chance levels. When comparing MEMO and EMN, using ground truth vs. predicted answers for the first node as queries for the second node showed a significant performance improvement for MEMO (85.38% vs. 69.20%). When using the same training regime as MEMO, EMN performs at chance level (22.30%) in the experiment with 20 nodes and 3 outbound edges using the English Question Answer dataset. Text is pre-processed by converting to lowercase, ignoring periods and interrogation marks, treating blank spaces as word separation tokens, and not ignoring commas in answers. The dataset by Weston et al. (2015) is pre-processed by converting all text to lowercase, ignoring periods and interrogation marks, treating blank spaces as word separation tokens, and not ignoring commas in answers. Each input consists of a query and stories, with questions stripped out and provided separately as queries to the system. In the path finding task, each answer has its own label. Questions are separated from the text and used as queries. During training, a mini-batch of 128 queries and corresponding stories are sampled. Queries are 128 \u00d7 11 tokens, while stories are 128 \u00d7 320 \u00d7 11 in size. Padding with zeros is done for queries and groups of stories. During the path finding task, a mini-batch of 128 queries and corresponding stories are sampled. Queries are 128 \u00d7 11 tokens, while stories are 128 \u00d7 320 \u00d7 11 in size. Padding with zeros is done for queries and groups of stories. Stories and queries are used as inputs in the architecture of EMN and MEMO, while for DNC, stories and queries are embedded in the same way as MEMO. During the path finding task, queries and stories are padded with zeros to reach the required sizes. EMN and MEMO use stories and queries as inputs, while DNC embeds them similarly to MEMO. UT also embeds stories and queries like MEMO, using its encoder for model output. Optimization is done using Adam after sampling a mini-batch for all models. For UT, stories and queries are embedded similarly to MEMO, using its encoder for model output. Optimization is done using Adam after sampling a mini-batch for all models. Tasks in bAbI require temporal context, which is accounted for in MEMO by adding a time encoding column vector to the memory store. In our experiments, we used Adam for all models with hyperparameters detailed in Appendix Section D.2. To address temporal context in bAbI tasks, we added a time encoding column vector to the memory store. Networks were trained for 2e4 epochs with 100 batch updates each. Evaluation involved sampling a batch of 10,000 elements and computing mean accuracy over examples, as well as accuracy per task for the 20 tasks in bAbI. The models were trained using Adam with hyperparameters from Vaswani et al. (2017) for 2e4 epochs and 100 batch updates. Evaluation included sampling 10,000 elements and computing mean accuracy over examples for the 20 tasks in bAbI. The networks used a polynomial learning rate decay and cross entropy loss for training. MEMO was trained using a cross entropy loss to predict class ID, node ID, and word ID in different tasks. The halting policy network parameters were updated using RMSProp. Temporal complexity of MEMO was discussed. In the paired associative inference task, MEMO had to predict class ID, node ID, and word ID. The halting policy network parameters were updated using RMSProp. MEMO's temporal complexity is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d). MEMO has a temporal complexity of O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various parameters are fixed constants. The hopping procedure is done for every answer, interacting with all memory slots for each hop. The complexity is linear with respect to the number of sentences in the input. MEMO has a spatial complexity that holds constant weights information, except for context details needed to answer queries. The spatial complexity is O(I \u00b7 S). The Universal Transformer has quadratic complexity compared to MEMO, which has a spatial complexity of O(I \u00b7 S \u00b7 d) due to its constant weight information. The halting unit h in MEMO is defined by a binary policy, different from the original ACT implementation. In our experiments, the spatial complexity is O(I \u00b7 S \u00b7 d) with a fixed memory size. We implement ACT following Graves (2016) and define the halting unit h differently from the original ACT, using a binary policy \u03c0 t. This change aims to enhance fairness in comparison. In our experiments, the halting unit in ACT is defined differently from the original, using a binary policy \u03c0 t. This change aims to increase fairness in comparison by enabling more powerful representations and evaluating the feasibility of the halting mechanism. The halting unit in ACT is defined using a binary policy \u03c0 t to enable more powerful representations and evaluate the feasibility of the halting mechanism. The halting probability is defined as T = min{t : where , with a fixed value of 0.01 in all experiments. The reminder R is defined, and the answer provided by MEMO+ACT is determined based on the answer provided by MEMO at each hop. The same architecture as described in Graves et al. (2016) is used, with hyperparameters optimized through a search. The architecture used in the experiments is based on Graves et al. (2016) with hyperparameters optimized through a search. The implementation and hyperparameters are from 'universal_transformer_small' available at https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py. The architecture used in the experiments is based on Graves et al. (2016) with hyperparameters optimized through a search. The hyperparameters used are described as 'universal_transformer_small' available at https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py. Additional hyperparameters were searched for training on specific tasks, with ranges reported."
}