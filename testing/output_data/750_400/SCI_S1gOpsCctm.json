{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are used for control policies in reinforcement and imitation learning. They are challenging to explain due to continuous-valued memory vectors. A new technique called Quantized Bottleneck Insertion is introduced to create finite representations of these vectors. This allows for better analysis of memory use and behavior. Results on synthetic environments and Atari games show surprisingly small finite representations. The new technique, Quantized Bottleneck Insertion, creates finite representations of memory vectors in RNNs for better analysis. Results on synthetic environments and Atari games show small finite representations, with as few as 3 memory states and 10 observations for a perfect Pong policy. This approach leads to improved interpretability in deep reinforcement learning and imitation learning. Finite policy representations with as few as 3 memory states and 10 observations have shown improved interpretability in deep reinforcement learning and imitation learning. This is particularly beneficial for policies represented as recurrent neural networks, which are challenging to understand and explain. In this paper, the focus is on understanding and explaining RNN policies by learning more compact memory representations. RNN policies use internal memory to encode features of the observation history, which are crucial for decision making but hard to interpret. In this paper, the focus is on comprehending and explaining RNN policies by learning more compact memory representations. RNN policies utilize internal memory to encode observation history features crucial for decision making but challenging to interpret. The challenge lies in explaining RNN memory due to high-dimensional continuous memory vectors updated through complex gating networks like LSTMs and GRUs. The hypothesis is that continuous memory may capture discrete concepts, which if revealed, could enhance explainability. The focus is on understanding RNN policies by creating more compact memory representations. RNN memory is challenging to explain due to high-dimensional continuous vectors updated through complex gating networks. The hypothesis is that quantizing the memory can capture discrete concepts, aiding explainability. This approach allows for manipulation and analysis of the quantized system to understand memory usage better. Our main contribution is introducing a method to transform an RNN policy with continuous memory and observations into a finite-state representation called a Moore Machine using Quantized Bottleneck Network (QBN) insertion. QBNs act as auto-encoders to capture powerful forms of memory usage in a more compact way. The approach introduces transforming an RNN policy with continuous memory and observations into a Moore Machine using Quantized Bottleneck Network (QBN) insertion. QBNs encode memory states and observation vectors encountered during RNN operation, replacing wires in the policy to create a combined RNN and QBN policy. QBNs are used to quantize the latent representation in auto-encoders. Trained RNNs are combined with QBNs to encode memory states and observation vectors, creating a Moore Machine Network (MMN) policy. The MMN can be used directly or fine-tuned for better accuracy. Training quantized networks is simplified with this approach. The combination of RNN and QBN results in a Moore Machine Network (MMN) with quantized memory and observations, nearly equivalent to the original RNN. The MMN can be used directly or fine-tuned to improve accuracy. Training quantized networks is simplified with effective gradient estimators. Experiments show accurate extraction of ground-truth MMNs in various domains. Our approach demonstrates the effectiveness of \"straight through\" gradient estimators in QBNs, showing accurate extraction of ground-truth MMNs in synthetic domains and benchmark grammar learning problems. Experiments on 6 Atari games using RNNs achieve state-of-the-art performance, revealing that MMNs can be surprisingly small and provide insights into memory usage not easily observable. Our experiments on 6 Atari games using RNNs show that it is possible to extract near-equivalent MMNs that are surprisingly small. These MMNs provide insights into memory usage not easily observable, revealing cases where RNNs do not use memory or observations in a meaningful way. Our work explores learning finite-memory representations of continuous RNN policies, which is a novel approach compared to prior research on understanding the internals of Recurrent Networks. Our work is related to prior research on learning finite-state representations of recurrent neural networks, specifically in extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. This involves discretizing the continuous memory space via gridding or clustering. Our work is related to prior research on learning finite-state representations of recurrent neural networks. There has been a significant history of work on extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. Typical approaches include discretizing the continuous memory space via gridding or clustering followed by minimization. A more recent approach is to use classic query-based learning algorithms to extract FSMs by asking membership and equivalence queries. However, none of these approaches directly apply to learning policies, which require extending to Moore Machines. Our approach involves directly inserting discrete elements into the RNN to preserve its behavior and allow for a finite state characterization. This method enables fine-tuning and visualization using standard learning frameworks. Our approach involves inserting discrete elements into the RNN to preserve its behavior and allow for a finite state characterization. This method enables fine-tuning and visualization using standard learning frameworks. The work most similar to ours focused on learning FSMs, but our approach extends beyond that by describing the behavior of a continuous RNN. Our work extends the approach of learning FSMs by introducing the method of QBN insertion to transform pre-trained recurrent policies into a finite representation. This allows for learning via guidance from a continuous RNN. Our work extends the approach of learning finite state machines (FSMs) by introducing QBN insertion to transform pre-trained recurrent policies into a finite representation. This allows for learning discrete representations of memory and observations while the rest of the network can use arbitrary activations and weights. Our work focuses on learning discrete representations of memory and observations in recurrent neural networks (RNNs) for interpretability, rather than efficiency like other approaches that use binary activation functions and weights. RNNs are commonly used in reinforcement learning to represent policies that require internal memory. Our work focuses on learning discrete representations of memory and observations in RNNs for interpretability, rather than efficiency. RNNs are commonly used in reinforcement learning to represent policies that require internal memory, maintaining a continuous-valued hidden state that influences action choice based on observation features extracted at each time step. During execution, an RNN maintains a continuous-valued hidden state that influences action choice based on observation features extracted at each time step. The RNN outputs actions according to a policy and transitions to a new state using a transition function implemented via gating networks like LSTMs or GRUs. The high-dimensional nature of the hidden state and observation features can make interpretation challenging. The text discusses the challenges of interpreting the continuous hidden state in RNNs during action selection and state transitions. It introduces the concept of extracting compact quantized representations of the hidden state and observation features using Moore Machines and deep network counterparts. The text introduces Moore Machines as a way to extract compact quantized representations of hidden states and observations in RNNs for investigating memory and observations in a finite system. Moore Machines are described as standard finite state machines labeled by output values corresponding to actions. A Moore Machine is a standard finite state machine where states are labeled by output values corresponding to actions. It is described by a finite set of hidden states, initial hidden state, observations, actions, transition function, and policy mapping hidden states to actions. A Moore Machine Network (MMN) is a Moore Machine where the transition function and policy are represented via deep networks, providing a mapping from continuous observations to a finite discrete set. In this work, quantized state and observation representations are considered in a Moore Machine Network (MMN), where deep networks represent the transition function and policy. The MMN provides a mapping from continuous observations to a finite discrete observation space, with discrete vectors describing raw observations. The quantization level is denoted as k, and the dimensions of the discrete vectors are represented by B h. The Moore Machine Network (MMN) uses deep networks to map continuous observations to a finite discrete space. Each observation is represented by a discrete vector, with a quantization level denoted as k. The dimensions of the vectors are represented by B h and B f. The MMN functions like a traditional RNN with memory composed of k-level activation units and environmental observations transformed to a k-level representation before being fed to the recurrent module. The Moore Machine Network (MMN) is a deep network that maps continuous observations to a finite discrete space with quantization level k. The memory is composed of k-level activation units, and environmental observations are transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for non-trivial problems, even though standard RNN learning algorithms can be used. Learning MMNs from scratch can be challenging for non-trivial problems, even though standard RNN learning algorithms can be used. A new approach for learning MMNs leverages the ability to learn RNNs by first training quantized bottleneck networks (QBNs) to embed continuous observation features and hidden states into a k-level quantized representation. The new approach for learning MMNs leverages the ability to learn RNNs by training quantized bottleneck networks (QBNs) to embed continuous observation features and hidden states into a k-level quantized representation. This involves inserting QBNs into the original recurrent net to create a network that consumes quantized features and maintains quantized state, effectively forming an MMN. The approach for learning MMNs involves training QBNs to embed continuous features and hidden states into a k-level quantized representation. QBNs are autoencoders with a constrained bottleneck composed of k-level activation units, different from traditional autoencoders used for dimensionality reduction in continuous space. A Quantized Bottleneck Network (QBN) is an autoencoder with a constrained bottleneck of k-level activation units, aimed at discretizing a continuous space. It involves a multilayer encoder E mapping inputs to a latent encoding E(x), and a multilayer decoder D. The QBN output is quantized to represent the activations of units in the encoding layer. The QBN aims to discretize a continuous space by quantizing the activations of units in the encoding layer. It uses a 3-level quantization scheme and the tanh activation function to map inputs to a latent encoding. The QBN uses a 3-level quantization scheme with the tanh activation function to map inputs to a latent encoding. To support 3-valued quantization, a flatter activation function \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x) is used. The quantize function makes b(x) non-differentiable, posing challenges for backpropagation. To support 3-valued quantization, a flatter activation function \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x) is used. The quantize function in the QBN results in b(x) being non-differentiable, posing challenges for backpropagation. The straight-through estimator is effective in handling this issue. The straight-through estimator is effective in handling the non-differentiability of the quantize function in the QBN during backpropagation. This allows the last layer of E to produce a k-level encoding in an autoencoder trained with standard L2 reconstruction error. The inclusion of the quantize function in the QBN allows the last layer of E to produce a k-level encoding. Training a QBN as an autoencoder involves using the L2 reconstruction error for a given input. Running a recurrent policy in the target environment generates training sequences of triples (o t , f t , h t ), with observation, observation feature, and hidden state at time t. Two QBNs, b f and b h, are trained as the first step in the approach. The approach involves training two QBNs, b f and b h, on sets of observed features and states respectively. If the QBNs achieve low reconstruction error, their latent \"bottlenecks\" can be seen as high-quality encodings of the original hidden states and features. These QBNs act as \"wires\" propagating input to output with some noise due to imperfect reconstruction. The QBNs b f and b h are trained on features F and states H respectively. If they achieve low reconstruction error, their latent \"bottlenecks\" act as high-quality encodings. These QBNs function as \"wires\" adding noise due to imperfect reconstruction when inserted into the original RNN. The QBNs b f and b h, trained on features F and states H, act as high-quality encodings if they achieve low reconstruction error. When inserted into the original RNN, they add noise due to imperfect reconstruction. The RNN can be seen as an MMN with the bottlenecks of b f and b h providing a quantized representation of features f t and states h t. The RNN can be viewed as an MMN with bottlenecks b f and b h providing a quantized representation of features f t and states h t. The QBNs may not achieve perfect reconstruction, leading to potential differences in behavior compared to the original RNN. Fine-tuning the MMN using the original RNN's data can help mitigate performance degradation. The MMN may not behave identically to the original RNN due to imperfect reconstruction. Fine-tuning the MMN using the original RNN's data can help mitigate performance degradation. Training the MMN to match the softmax distribution over actions produced by the RNN is more stable than simply outputting the same action. During fine-tuning, the goal is for the MMN to match the softmax distribution of actions from the RNN, which is more stable than outputting the same action. Visualization tools can be used to analyze the MMN's memory and feature bits for a semantic understanding. Full interpretation is not addressed in this work. One way to gain insight is to use the MMN to create a Moore Machine over discrete state and observation spaces. By running the MMN, a dataset of consecutive quantized states can be produced for analysis of different machine states and their relationships. To gain insight, a Moore Machine can be created over discrete state and observation spaces using the MMN. The machine analyzes different machine states and their relationships by producing a dataset of consecutive quantized states, quantized features, and selected actions. The state-space corresponds to distinct quantized states, while the observation-space consists of unique quantized feature vectors. The transition function of the machine is \u03b4. The Moore Machine is constructed using consecutive quantized states, features, and actions. The state-space corresponds to distinct quantized states, and the observation-space consists of unique quantized feature vectors. The transition function is derived from the data to create a transaction table capturing transitions. Minimization is applied to reduce the number of states in the resulting Moore Machine. The transition function of the machine is constructed from data to create a transaction table capturing transitions. Minimization techniques are applied to reduce the number of states in the resulting Moore Machine, leading to a minimal equivalent Moore Machine BID19. The experiments aim to extract MMNs from RNNs without significant loss in performance and determine the general magnitude of states and observations. In this section, standard Moore Machine minimization techniques are applied to reduce the number of states and observations in the minimal 2 equivalent Moore Machine BID19. The experiments aim to extract MMNs from RNNs without significant loss in performance and explore the interpretability of recurrent policies in two known domains. In this section, the general magnitude of states and observations in minimal machines is explored, focusing on complex domains like Atari. The study addresses questions about the interpretability of recurrent policies by analyzing two domains with known ground truth Moore Machines. The first domain is a parameterized synthetic environment called Mode Counter, which can capture various memory types. The second domain involves benchmark grammar learning problems, allowing for variations in memory requirements and types of memory usage. The Mode Counter environment captures different memory types and memory usage for policy learning. It is a type of Partially Observable Markov Decision Process with varying memory requirements and internal counters. The environment transitions between modes based on a distribution and actions are taken accordingly. The Mode Counter environment is a type of Partially Observable Markov Decision Process with varying memory requirements. It transitions between modes based on a distribution, and actions are taken accordingly to receive a reward at the end of each episode. The agent infers the mode through observations and memory use. The Mode Counter environment transitions between modes, and the agent must infer the mode using observations and memory. Different parameterizations test how memory is used to achieve optimal performance. Three MCE instances experiment with memory and observations in different ways, such as the Amnesia instance. In our experiments, we explore how memory is used to infer the mode and achieve optimal performance in three MCE instances: Amnesia and Blind. Amnesia does not require memory for optimal actions, while Blind necessitates memory to track a deterministic mode. In three MCE instances, memory is used differently: 1) Amnesia requires no memory for optimal actions, 2) Blind needs memory to track a deterministic mode, and 3) Tracker uses both observations and memory for optimal actions. The MCE is designed with a recurrent architecture using observations and memory to select optimal actions. Memory implements counters for key time steps where observations provide information about the mode. The architecture includes a feed-forward layer, a GRU layer, and a fully connected softmax layer for action distribution. The MCE uses a recurrent architecture with feed-forward and GRU layers to select optimal actions based on observations and memory. The trained RNNs achieve 100% accuracy on the imitation dataset, producing optimal policies. The observation and hidden-state QBN have the same architecture with quantized bottleneck units. In the MCEs, imitation learning is used for training the RNNs, which achieve 100% accuracy on the imitation dataset. The observation and hidden-state QBN have the same architecture with varied bottleneck units. The encoder and decoder structures are symmetric in both b f and b h. In experiments, the bottleneck units Bf and Bh are varied. Encoders have tanh nodes followed by quantized bottleneck nodes. Training in MCE environments is faster than RNN training. QBNs with sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8} are embedded into RNNs for performance measurement. Training of bottleneck units in MCE environments was fast compared to RNN training. QBNs with sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8} were embedded into RNNs for performance measurement. The average test score over 50 test episodes was recorded to evaluate the agent's performance. Fine tuning was not required in most cases due to low reconstruction error. After embedding bottleneck units into RNNs, fine-tuning was mostly unnecessary due to low reconstruction error. However, in one case, fine-tuning improved MMN performance to 98% accuracy. Inserting bottlenecks individually resulted in perfect performance, highlighting the impact of error accumulation from both bottlenecks. After embedding bottleneck units into RNNs, fine-tuning was mostly unnecessary due to low reconstruction error. However, in one exceptional case, fine-tuning improved MMN performance to 98% accuracy. Inserting bottlenecks individually resulted in perfect performance, showing the combined error accumulation of the two bottlenecks affecting performance. Moore Machine Extraction revealed the number of states and observations before and after minimization, indicating the impact of bottleneck nodes on MMNs. The Moore Machine Extraction process showed that bottleneck nodes in MMNs do not always learn minimal state and observation representations, but accurately describe the RNN. After minimization, there were typically fewer states and observations, indicating improved efficiency. The MMN learning process does not always result in minimal state and observation representations, but accurately describes the RNN. After minimization, exact minimal machines were obtained for most MCE domains, showing that MMNs learned via QBN insertions were optimal in most cases. The exception occurred when the MMN did not achieve perfect accuracy. The ground truth minimal machines found for most MCE domains were equivalent to the MMNs learned via QBN insertions, showing their optimality. The exception was when the MMN did not achieve perfect accuracy, allowing for an understanding of memory use based on the machines examined. The MMN did not achieve perfect accuracy, revealing insights into memory use. For example, the machine for Blind has only one observation symbol, limiting transitions. In contrast, the machine for Amnesia shows that each observation symbol leads to the same state, determining the action choice solely based on the current observation. The evaluation was done over 7 Tomita Grammars, treating them as environments for policy learning problems. The evaluation was done over 7 Tomita Grammars, treating them as environments for policy learning problems. The agent's action choice is solely based on the current observation, with a reward of 1 for choosing the correct action on the last symbol of a string. The RNN for each grammar consists of a one-layer GRU with 10 hidden units and a fully connected softmax layer with 2 nodes. Imitation learning is used to train the RNNs with an Adam optimizer and learning rate of 0.001. Training data includes equal numbers of accept/reject strings with lengths in the range [1, 50]. The RNNs were trained using imitation learning with a GRU having 10 hidden units and a softmax layer with 2 nodes for accept/reject classification. The test results showed high accuracy, except for grammar #6. No bottleneck encoder was needed for discretizing observations due to the finite alphabet used in the problem. The trained RNNs were highly accurate, except for grammar #6. Bottlenecks were added to create MMNs, with performance results before and after fine-tuning shown in TAB1. The study added bottlenecks to create Moore Machine Networks (MMNs) based on trained RNNs, with performance results before and after fine-tuning presented in TAB1. The MMNs were able to maintain RNN performance without fine-tuning, with only minor improvements observed in some cases. Considerable reduction in MM's state-space was achieved through MM extraction and minimization. Fine-tuning results in minor improvements in performance for MMNs compared to RNNs. MM extraction and minimization lead to a significant reduction in state-space while maintaining performance. The study focuses on Tomita grammars and their solutions, showing equivalence between MMNs and minimal machines. After fine-tuning, MMNs show slight performance improvements compared to RNNs. MM extraction and minimization result in a significant reduction in state-space while preserving performance. The study examines Tomita grammars and their solutions, demonstrating equivalence between MMNs and minimal machines. Applying the technique to RNNs learned for Atari games reveals similarities to known minimal machines. In this section, the technique is applied to RNNs learned for six Atari games using the OpenAI gym. Unlike previous experiments with known ground truth minimal machines, for Atari, there was no preconception of what the minimal machines might look like. The complexity of input observations for Atari makes it unclear if similar results can be expected. Other recent efforts have been made to understand Atari agents, but no other work is known. The input observations for Atari are more complex than previous experiments, making it unclear if similar results can be expected. While other efforts have been made to understand Atari agents, this work aims to extract finite state representations for Atari policies using RNN training with a recurrent architecture and specific preprocessing steps for the input observation. The Atari agents have a recurrent architecture with specific preprocessing steps for input observations. The network includes 4 convolutional layers, a GRU layer, and a fully connected layer with n+1 units. Softmax is applied to the first n neurons. The network architecture for the Atari agents includes 4 convolutional layers, a GRU layer, and a fully connected layer with n+1 units. Softmax is applied to the first n neurons for policy determination and the last neuron predicts the value function. The A3C RL algorithm with specific parameters was used for training and performance was reported on six games. We applied a softmax to the first n neurons for policy determination and used the last neuron to predict the value function. The A3C RL algorithm was used for training with specific parameters, and the performance was reported on six games. The RNN architecture for the QBN was adjusted to match the dimension of continuous observation features. The architecture for the QBN in the second column of TAB2 .MMN Training was adjusted for the Atari domains using noisy rollouts to increase training data diversity. The encoder for b h has 3 feed-forward layers with varying node sizes, and the decoder is symmetric to the encoder. Training episodes were generated by executing the learned RNN for a random number of steps and then executing an \u03b5-greedy version of the RNN policy. The training data for bottlenecks in the Atari domains was generated using noisy rollouts to increase diversity. Bottlenecks were trained for B h \u2208 {64, 128} and B f \u2208 {100, 400}, with larger values due to the complexity of Atari. Training bottlenecks for Atari games with larger values for B h and B f to learn QBNs more robustly. Each bottleneck was trained to saturation of training loss and inserted into the RNN to create an MMN for each game. Performance of MMNs before and after finetuning varied for different combinations of B h and B f. The bottlenecks were trained with larger values for B h and B f to learn QBNs more robustly for Atari games. The MMNs achieved identical or very close scores to the RNN after fine-tuning for games like Pong, Freeway, Bowling, and Boxing, demonstrating the ability to learn a discrete representation of input and memory without impacting performance. After fine-tuning, MMNs achieved similar scores to RNNs in complex games like Pong, Freeway, Bowling, and Boxing, showing the ability to learn input representation and memory without affecting performance. However, in Breakout and Space Invaders, MMNs scored lower than RNNs even after fine-tuning. After fine-tuning, MMNs achieved similar scores to RNNs in complex games like Pong, Freeway, Bowling, and Boxing. However, in Breakout and Space Invaders, MMNs scored lower than RNNs due to poor reconstruction in certain game scenarios. For example, in Breakout, the MMN failed to press the fire-button after clearing the first board, resulting in a lower score. This highlights the need for more intelligent approaches in training MMNs. After fine-tuning, MMNs achieved similar scores to RNNs in complex games like Pong, Freeway, Bowling, and Boxing. However, in Breakout, the drop in performance was due to poor reconstruction on rare game parts, such as failing to press the fire-button after clearing the first board. This calls for more intelligent training approaches for MMNs to capture critical information in such important states. The investigation focuses on training more intelligent MMNs to capture critical information in rare but important states. Before minimization, MMNs have large numbers of states and observations, but after minimization, these numbers reduce significantly, sometimes to just one state or observation. This makes the MMNs easier to analyze by hand, although the analysis may still be challenging. After minimizing MMNs, the number of states and observations drastically decreases, sometimes to just one state or observation. This simplifies analysis, although understanding complex policies may still be challenging. In Atari, similar memory use patterns were observed, with Pong having only three states and 10 discrete actions. The analysis of memory use in Atari games reveals that Pong has just three states and 10 discrete observation symbols. Each observation transitions to the same state regardless of the current state, making memory unnecessary for mapping observations to actions. This simplifies analysis but may still be challenging for complex policies. The Pong game has three states and 10 observation symbols, with each observation leading to the same state/action. This eliminates the need for memory in mapping observations to actions. In contrast, Bowling and Freeway have only one observation symbol in their minimal Markov models, ignoring input images and relying on open-loop controllers for actions. The Pong policy is similar to the Amnesia MCE, while Bowling and Freeway have minimal Markov models that ignore input images and rely on open-loop controllers for actions. Freeway's policy always takes the Up action, and Bowling has a more interesting policy. The Freeway policy always takes the Up action, while Bowling has a more complex open-loop policy structure. The MM extraction approach provides additional insight into these policies. In Figure 2b, Bowling exhibits a unique open-loop policy structure with an initial action sequence followed by a repeating loop. The MM extraction approach offers valuable insights into the policies of Breakout, Space Invaders, and Boxing, which utilize memory and observations. Further analysis of Atari policies will require visualization tools and is a key focus for future research. The approach introduced involves extracting finite state Moore Machines from Atari policies by training Quantized Bottleneck Networks to produce binary encodings of continuous RNN memory and input. This method aims to better understand memory use in RNN policies and requires additional visualization and interaction tools for full semantic analysis. Our approach involves extracting finite state Moore Machines from RNN policies by training Quantized Bottleneck Networks to produce binary encodings of memory and input features. This allows for better understanding of memory use in RNN policies and enables the transformation of the extracted Moore machine into a minimal machine for analysis and usage. Our results demonstrate the effectiveness of this approach in environments where ground truth machines are known. Our approach involves extracting finite state Moore Machines from RNN policies by training Quantized Bottleneck Networks to produce binary encodings of memory and input features. This allows for better understanding of memory use in RNN policies and enables the transformation of the extracted Moore machine into a minimal machine for analysis and usage. Our results on known environments show accurate extraction of ground truth machines, while experiments on Atari games demonstrate similar performance of learned MMNs to original RNN policies. The extracted machines also provide insight into memory usage of the policies. In experiments on six Atari games, our approach accurately extracts ground truth machines and shows that learned MMNs maintain similar performance to original RNN policies. The extracted machines reveal insights into memory usage, with some policies requiring minimal memory states and observations, while others rely heavily on memory or ignore observations entirely. This work provides novel insights into memory utilization in policies. The study provides insights into memory usage in policies for Atari games, revealing that some policies require minimal memory states and observations while others heavily rely on memory or ignore observations entirely. Future work includes developing tools for attaching meaning to observations and states for additional insights into policies. The study highlights the importance of developing tools and visualizations to attach meaning to observations and states in policies for complex domains. Future work includes analyzing finite-state machine structure and formal properties of policies using tools. An MCE is parameterized by mode number, transition function, life span mapping, and count set, with the hidden state represented as a tuple. The MCE is defined by mode number, transition function, life span mapping, and count set. The hidden state is represented as a tuple, with the mode changing based on the lifespan and transition distribution. The MCE hidden state is a tuple (m t , c t ), where m t is the current mode and c t is the count of time-steps in that mode. The mode changes based on the lifespan and transition distribution, with the agent receiving continuous-valued observations o t at each step. The transition distribution specifies initial modes and the agent receives continuous-valued observations at each step. Observations determine the mode when the mode count is in C, otherwise, they are uninformative. The agent must remember the current mode and use memory mode to keep track of how long the mode has been active. The agent must remember the current mode and use memory to keep track of how long the mode has been active in order to determine when it needs to \"pay attention\" to the current observation. Experiments are conducted with different instances of MCE, including one with amnesia where an optimal policy will not use memory to track. Experiments are conducted with different instances of MCE, including one with amnesia where an optimal policy will not use memory to track information from the past. Experiments test the use of memory in different MCE instances: 1) Reactive policy - no memory used, 2) Blind policy - memory required for optimal performance, 3) Tracker policy - memory used to ignore observations. The experiments test memory usage in various MCE instances: Reactive policy uses no memory, Blind policy requires memory for optimal performance, and Tracker policy uses memory to ignore observations. The observations provide no information about the mode, requiring memory to track the deterministic mode sequence. The most general instance, Tracker, allows for larger \u2206(m) values and requires paying attention to observations when c t = 0. The experiments test memory usage in various MCE instances, with the most general instance being Tracker, which allows for larger \u2206(m) values and requires paying attention to observations when c t = 0. This instance can result in difficult problems as the number of modes and their life-spans grow."
}