{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively in a bottom-up manner. This approach preserves semantic content better through explicit factorization of semantics and syntax. The new image captioning paradigm involves two stages: extracting explicit semantic representation from the image and constructing the caption recursively in a bottom-up manner. This approach better preserves semantic content through explicit factorization of semantics and syntax, requiring less effort in the caption construction process. Image captioning involves generating short descriptions for images and has gained attention recently. State-of-the-art models use an encoder-decoder paradigm, encoding the image with a convolutional network and decoding it into a caption with a recurrent network. Despite its effectiveness, the sequential model has a fundamental problem. The sequential model in image captioning lacks the ability to reflect hierarchical structures of natural languages, despite its effectiveness on various benchmarks. Sequential models in image captioning struggle to capture hierarchical structures of natural languages, leading to reliance on n-gram statistics and favoring frequent n-grams in training, resulting in captions that may be syntactically correct but lack semantic meaning. Sequential models in image captioning have drawbacks such as relying on n-gram statistics, favoring frequent n-grams in training, and obscuring the dependency structure between syntax and semantics. To address these issues, a new paradigm for image captioning is proposed to extract semantics and construct captions effectively. The proposed paradigm for image captioning aims to address issues with sequential models by decomposing the extraction of semantics and construction of syntactically correct captions into two stages. It involves deriving explicit representations of semantic content from images, such as noun-phrases like \"a white cat\" or \"two men\", before constructing captions recursively. The proposed paradigm for image captioning involves decomposing the process into two stages: extracting semantic content from images, such as noun-phrases like \"a white cat\" or \"two men\", and constructing captions recursively by combining sub-phrases. The process of constructing captions involves recursive composition to form higher-level phrases by joining sub-phrases. The compositional procedure is not a hand-crafted algorithm. The proposed paradigm for generating captions involves two parametric modular nets for phrase composition and completeness evaluation. It offers advantages over conventional models by factorizing semantics and syntax to better preserve semantic content. The proposed paradigm for caption generation consists of two parametric modular nets for phrase composition and completeness evaluation. It offers advantages over conventional models by preserving semantic content and capturing hierarchical dependencies in natural language. The proposed paradigm for caption generation preserves semantic content and captures hierarchical dependencies in natural language, increasing caption diversity and generalizing well to new data. The literature in image captioning has seen a surge in interest in the neural network era. The proposed paradigm for caption generation increases caption diversity and generalizes well to new data, preserving semantic content and capturing hierarchical dependencies in natural language. The literature in image captioning has grown significantly in the neural network era, with early approaches being bottom-up and detection based. In the neural network era, early approaches to image captioning were bottom-up and detection based, extracting visual concepts from images and assembling them into captions. Recent works use convolutional neural networks for image representation and recurrent neural networks for caption generation. Recent works on image captioning use convolutional neural networks for image representation and recurrent neural networks for caption generation. Vinyals et al proposed a neural image captioner that uses LSTM conditioned on a feature vector to generate words. Xu et al extended this work by using multiple feature vectors and applying an attention mechanism to extract relevant image information. Recent works on image captioning have explored different approaches to represent input images and generate captions. Xu et al extended the use of feature vectors and applied an attention mechanism to extract relevant image information. Lu et al adjusted the attention computation to also consider the generated text, while Anderson et al added an additional LSTM for better control. Dai et al reformulated latent states as 2D maps to capture semantic information more effectively. Some recent approaches in image captioning focus on extracting key information from images. Lu et al adjusted attention computation to consider generated text, while Anderson et al added an LSTM for better control. Dai et al reformulated latent states as 2D maps to capture semantic information effectively. Other methods involve extracting phrases or semantic words directly from images. Recent approaches in image captioning have focused on extracting key information from images. Some methods involve directly extracting phrases or semantic words from the input image. For example, Yao et al predicted frequent training words occurrences, Tan et al treated noun-phrases as hyper-words, and authors in BID21 proposed a hierarchical approach with two LSTM levels. Despite improvements in model architectures, these approaches still generate captions sequentially, favoring frequent n-grams. Our proposed paradigm in image captioning focuses on a bottom-up approach, representing the input image with noun-phrases and constructing captions through a recursive composition procedure. This method aims to address issues such as incorrect semantic coverage and lack of diversity seen in sequential caption generation approaches. Our proposed paradigm in image captioning takes a bottom-up approach by representing the input image with noun-phrases and constructing captions through a recursive composition procedure. This method aims to address issues such as incorrect semantic coverage and lack of diversity in caption generation. The proposed compositional paradigm in image captioning uses a recursive composition procedure to preserve semantics effectively, require less data, and generate diverse captions. This approach contrasts with non-recursive methods that can only generate captions with a single object. The proposed compositional paradigm in image captioning uses a dynamic programming approach to generate captions with multiple objects, contrasting with non-recursive methods that are limited to single-object captions. The proposed compositional paradigm in image captioning uses a dynamic programming approach to generate captions with multiple objects. It involves recursively composing phrases from an initial pool until a complete caption is obtained, utilizing neural networks to learn plausible compositions following a hierarchical structure. The proposed two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. This approach follows a hierarchical structure and is different from mainstream captioning models. The two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. Unlike mainstream models, CompCap considers nonsequential dependencies among words and phrases in a sentence. In a two-stage framework for image captioning, CompCap considers nonsequential dependencies among words and phrases, representing image semantics explicitly with noun-phrases like \"a black cat\" and \"two boys\". In a framework for image captioning, image semantics are represented explicitly with noun-phrases like \"a black cat\" and \"two boys\". The extraction of these noun-phrases from the input image is essential for visual understanding tasks. Sophisticated techniques like object detection and attribute recognition can be applied, but the approach presented here completes the paradigm. In image captioning, noun-phrases like \"a black cat\" and \"two boys\" are extracted from the input image for visual understanding tasks. Despite more advanced techniques like object detection and attribute recognition, the approach presented here focuses on completing the paradigm. The dataset MS-COCO contains 120K images with only about 3K distinct noun-phrases in the associated captions, leading to the formalization of noun-phrase extraction as a multi-label classification problem. In image captioning, noun-phrases are extracted from images for visual understanding tasks. The dataset MS-COCO has 120K images with only about 3K distinct noun-phrases in the captions, leading to noun-phrase extraction being formalized as a multi-label classification problem. In image captioning, noun-phrases are extracted from images for visual understanding tasks. The dataset MS-COCO has 120K images with only about 3K distinct noun-phrases in the captions, leading to noun-phrase extraction being formalized as a multi-label classification problem. Specifically, a list of distinct noun-phrases is derived from the training captions by parsing and selecting those that occur more than 50 times. Each selected noun-phrase is treated as a class for binary classification using visual features extracted via a Convolutional Neural Network. In image captioning, noun-phrases are extracted from images using a Convolutional Neural Network. The input image is encoded and classified for each noun-phrase using weight vectors and the sigmoid function. The top scoring noun-phrases are selected and pruned through Semantic Non-Maximum Suppression to construct the caption. The input image is represented using top-scoring noun-phrases, pruned through Semantic Non-Maximum Suppression. A recursive compositional procedure called CompCap constructs the caption by maintaining a phrase pool and applying a Connecting Module to generate a sequence. CompCap constructs captions by maintaining a phrase pool and using a Connecting Module to connect phrases in a plausible way, generating longer phrases with maximum connecting scores. The C-Module generates longer phrases by connecting two phrases with a sequence of words. It computes a score for the new phrase and selects the one with the maximum connecting score as the resulting caption. The Evaluation Module assesses if the new phrase is complete before finalizing it as the caption. The Connecting Module (C-Module) selects phrases with the maximum connecting score to form a new phrase P new. An Evaluation Module (E-Module) determines if P new is a complete caption. If not, the pool P is updated and the process repeats until a complete caption is obtained or only one phrase remains in P. The Connecting Module (C-Module) selects connecting phrases based on left and right phrases, evaluating the connecting score. Using an LSTM to decode intermediate words was found to be ineffective. The C-Module selects connecting phrases based on left and right phrases, evaluating the connecting score. Using an LSTM to decode intermediate words was ineffective, leading to the adoption of a classification approach for generating connecting phrases. In this work, an alternative strategy is adopted to generate connecting phrases as a classification problem due to the limited number of distinct connecting phrases in the proposed paradigm. For example, in MS-COCO BID4, there are over 1 million samples collected for the connecting module, containing only about 1,000 distinct connecting phrases. The proposed paradigm limits the number of distinct connecting phrases by excluding semantic words like nouns and adjectives. Over 1 million samples in MS-COCO BID4 for the connecting module only contain around 1,000 distinct connecting phrases. Distinct connecting sequences are mined from training captions and treated as different classes for the connecting module classifier. The proposed paradigm limits the number of distinct connecting phrases to about 1,000 by excluding semantic words. Connecting sequences are mined from training captions and treated as different classes for the classifier. The connecting module uses a two-level LSTM model to encode phrases and outputs a normalized score. The proposed paradigm limits connecting phrases to 1,000 by excluding semantic words. A two-level LSTM model encodes phrases and outputs a normalized score. The model includes a low-level LSTM for attention with visual features and a high-level LSTM for evolution of the encoded state. The encoders for phrases share the same structure but have different parameters based on their position in the ordered pair. The proposed paradigm limits connecting phrases to 1,000 by excluding semantic words. A two-level LSTM model encodes phrases and outputs a normalized score. The model includes a low-level LSTM for attention with visual features and a high-level LSTM for evolution of the encoded state. The encoders for phrases share the same structure but have different parameters based on their position in the ordered pair. The values of the softmax output are used as connecting scores to determine the connecting phrase between two phrases. The proposed paradigm limits connecting phrases to 1,000 by excluding semantic words. Phrases are encoded by a two-level LSTM model, with softmax output used as connecting scores to determine the connecting phrase between two phrases. A virtual neg is added as a negative class, and scores for phrases are computed based on a C-Module. The Evaluation Module (E-Module) determines if a phrase is a complete caption by encoding it into a vector using a two-level LSTM model and evaluating the probability. The Evaluation Module (E-Module) computes the score of longer phrases produced by the C-Module to determine if they are complete captions. It encodes phrases into a vector using a two-level LSTM model and evaluates the probability of them being complete captions. Additionally, the E-Module can check other properties such as caption quality using a caption evaluator. Extensions to the framework allow for generating diverse phrases without following a greedy search strategy. The Evaluation Module (E-Module) evaluates the completeness and quality of captions generated by the C-Module. It can also check other properties besides completeness, such as caption quality using a caption evaluator. Extensions to the framework allow for generating diverse captions through beam search or probabilistic sampling. Multiple beams can be formed to avoid local minima and generate diverse captions. The framework can be extended to generate diverse captions for images using beam search or probabilistic sampling. This allows for retaining multiple ordered pairs and connecting sequences, forming multiple beams to avoid local minima. User preferences or conditions can also be incorporated into the framework. The framework can be extended to generate diverse captions by sampling ordered pairs or connecting sequences based on normalized scores. User preferences can be incorporated by filtering noun phrases or modulating their scores. This control is easier to implement on an explicit representation than on an encoded feature vector. The experiments were conducted on MS-COCO BID4 and Flickr30k BID5 datasets, each containing a large number of images with ground-truth captions. The vocabulary was standardized by converting words to lowercase and removing certain words. In the experiments on MS-COCO BID4 and Flickr30k BID5 datasets, there are a total of 123,287 images and 31,783 images respectively, with 5 ground-truth captions for each image. The vocabulary is standardized by converting words to lowercase, removing non-alphabet characters, and words appearing less than 5 times. This results in a vocabulary size of 9,487 for MS-COCO and 7,000 for Flickr30k. Training captions are truncated to have a maximum of 18 words. The vocabulary sizes for MS-COCO and Flickr30k datasets are 9,487 and 7,000 respectively, achieved by converting words to lowercase, removing non-alphabet characters, and words appearing less than 5 times. Training captions are limited to 18 words, and ground-truth captions are parsed into trees using NLP toolkit BID31 for training the connecting and evaluation modules separately. In experiments, ground-truth captions are parsed into trees using NLP toolkit BID31 for training connecting and evaluation modules separately. The recursive compositional procedure is modularized for better generalization. Each step of the procedure during testing requires two forward passes. CompCap is compared with other methods like Neural Image Captioner (NIC) BID2. CompCap is compared with other methods like Neural Image Captioner (NIC) BID2, AdapAtt, TopDown BID1, and LSTM-A5 BID19, all of which encode images as semantical feature vectors. Each step of the procedure during testing requires two forward passes to obtain a complete caption in 2 or 3 steps. CompCap is compared with other methods like AdapAtt and TopDown BID1, which apply the attention mechanism for state-of-the-art performances. LSTM-A5 BID19 predicts semantical concepts as additional visual features. All methods are re-implemented and trained with the same hyperparameters using ResNet-152 BID16 pretrained on ImageNet. In addition to extracting noun-phrases for CompCap, predictions from noun-phrase classifiers are used as extra features for LSTM-A5. ResNet-152 BID16 pretrained on ImageNet is utilized to extract image features, with fixed ResNet-152 during training and a learning rate of 0.0001 for all methods. Best parameters are selected for testing across all methods. During training, ResNet-152 BID16 pretrained on ImageNet is used to extract image features without finetuning. The learning rate is set to 0.0001 for all methods. Best parameters are selected for testing, with beam-search of size 3 for baselines. CompCap selects 7 noun-phrases with top scores to represent the input image, balancing semantics and syntax. Pair selection uses beam-search of size 3. CompCap selects 7 noun-phrases with top scores to represent the input image, balancing semantics and syntax. Beam-search of size 3 is used for pair selection, while no beam-search is used for connecting phrase selection. Quality of generated captions compared on MS-COCO and Flickr30k test sets using SPICE, CIDEr, BLEU-4, ROUGE, and METEOR metrics. CompCap performs well in comparison. CompCap selects 7 noun-phrases with top scores for image representation. No beam-search is used for connecting phrase selection. Quality of generated captions compared on MS-COCO and Flickr30k test sets using SPICE, CIDEr, BLEU-4, ROUGE, and METEOR metrics. CompCap with predicted noun-phrases performs best in SPICE metric but lags behind baselines in other metrics. Among all methods, CompCap with predicted noun-phrases performs best in the SPICE metric, showing higher correlation with human judgements but inferior results in CIDEr, BLEU-4, ROUGE, and METEOR. These results reflect the differences between sequential and compositional caption generation methods, with SPICE focusing on semantical analysis while other metrics favor frequent training n-grams. The study compares SPICE with other metrics like CIDEr, BLEU-4, ROUGE, and METEOR, which favor frequent training n-grams. The compositional generation procedure preserves semantic content effectively but may contain more unseen n-grams. An ablation study on the proposed compositional paradigm shows a significant boost in metrics when using groundtruth noun-phrases from associated captions. CompCap effectively preserves semantic content by using groundtruth noun-phrases from associated captions, leading to better caption generation. CompCap generates better captions by preserving semantic content and integrating ground-truth noun-phrases. This approach improves metrics, except for SPICE, as it follows a composing order for better generalization. CompCap boosts metrics by selecting connecting phrases, disentangling semantics and syntax. It excels in handling out-of-domain content and requires less data to learn. The proposed compositional paradigm, CompCap, disentangles semantics and syntax into two stages. It excels at composing semantics into a syntactically correct caption, handling out-of-domain content effectively, and requiring less data to learn. Two studies were conducted to verify these claims, showing promising results in terms of SPICE and CIDEr metrics. The study controlled data ratios for training CompCap, showing improved results in SPICE and CIDEr metrics. Baselines saw drops in performance when tested on different datasets, while CompCap performed competitively, highlighting the benefits of disentangling semantics and syntax. Classifiers trained with in-domain data showed significant drops in performance for baselines in terms of SPICE and CIDEr metrics. In contrast, CompCap, trained with both in-domain and out-of-domain data, achieved competitive results, indicating the advantage of separating semantics and syntax. The ability of CompCap to generate diverse captions was analyzed through five metrics evaluating caption diversity. The distribution of semantics varies across datasets, while syntax remains stable. CompCap can generate diverse captions by varying noun-phrases and order. Diversity analysis was conducted using five metrics to evaluate caption diversity, including novel and unique captions. The diversity of captions generated by CompCap was evaluated using five metrics, including novel and unique captions, vocabulary usage, and pair-wise editing distances. The diversity of captions generated by CompCap was evaluated using metrics such as vocabulary usage and pair-wise editing distances to quantify the diversity of captions. The diversity of captions generated by CompCap was evaluated using metrics such as vocabulary usage and pair-wise editing distances to quantify the diversity of captions. The average distance over captions of different images is defined as diversity at the dataset level, while the average distance over captions of the same image represents diversity at the image level. CompCap achieved the best results in all metrics, indicating diverse and novel captions. CompCap generated diverse and novel captions, as shown by metrics evaluating caption diversity. The best results were obtained in all metrics, indicating a variety of captions at both image and dataset levels. Additionally, qualitative samples demonstrated diverse captions with different composing orders and noun-phrases. Error analysis revealed distinct causes for failures, despite similarities with previous results. The captions generated by CompCap are diverse and novel, with qualitative samples showing different composing orders and noun-phrases. Error analysis revealed that errors mainly stem from a misunderstanding of the visual content, which could be improved with more sophisticated techniques in noun-phrase extraction. Sequential models tend to favor frequent n-grams, but with a perfect understanding of the visual content, they may perform better. In this paper, a novel paradigm for image captioning is proposed. Unlike typical approaches that encode images with feature vectors and generate captions sequentially, this method generates captions in a compositional manner by factorizing the visual content. The proposed method for image captioning involves a compositional approach, where the captioning procedure is divided into two stages. The first stage extracts an explicit representation of the input image, while the second stage assembles noun-phrases into a caption using a recursive compositional procedure. This hierarchical structure in caption generation aligns with the novel paradigm presented in the paper. Our approach for image captioning involves a two-stage process: extracting noun-phrases from the input image in the first stage, and assembling them into a caption using a recursive compositional procedure in the second stage. This method aims to preserve semantics effectively, require less training data, generalize better across datasets, and produce diverse captions by finding semantically similar noun-phrases. The proposed compositional procedure for image captioning involves a hierarchical structure that preserves semantics effectively, requires less training data, generalizes better across datasets, and yields diverse captions by finding semantically similar noun-phrases. The key for suppression is to identify semantically similar noun-phrases based on the central nouns they contain. The proposed compositional procedure for image captioning involves identifying semantically similar noun-phrases based on central nouns, using encoders in the C-Module to compare and suppress them if they are semantically similar conditioned on the input image. The C-Module uses encoders to compare and suppress semantically similar noun-phrases based on central nouns in image captioning. It computes normalized euclidean distances for two encodings of each noun-phrase and sums them to determine similarity. If the sum of distances is small, the noun-phrases are considered semantically similar. The C-Module uses normalized euclidean distances to compare encodings of noun-phrases for semantic similarity. If the sum of distances is less than a threshold, the noun-phrases are considered similar. The module contains two independent encoders for each phrase in an ordered pair. The C-Module uses normalized euclidean distances to compare encodings of noun-phrases for semantic similarity. It contains two independent encoders for each phrase in an ordered pair, with parameters tuned for optimal performance. The C-Module uses normalized euclidean distances to compare encodings of noun-phrases for semantic similarity, with independent encoders for each phrase in an ordered pair. Comparing C-Modules with shared vs. independent parameters showed better performance with independent parameters. Additional hyperparameters for CompCap include beam search sizes for pair and phrase selection, with experiments showing adjustments in FIG6. In experiments for CompCap, additional hyperparameters like beam search sizes for pair and phrase selection were adjusted. The curves in FIG6 show that these hyperparameters have minor influence on CompCap's performance."
}