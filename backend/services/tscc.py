import os
import json
import time

from dotenv import load_dotenv
from utils.config import DEFAULT_CHUNK_OVERLAP, DEFAULT_CHUNK_SIZE, LLM_TEMP, LLMS, PROMPT_2_1, PROMPT_2_2, PROMPT_MAIN
from models import Extracted

from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain


from utils import log_error, write_to_file


load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")


def add_context_to_chunk(text_chunks: list):
    """Adds the previous chunk to the current chunk as part of its object properites

    Args:
        - text_chunks (list): _description_
    """
    prev_chunk = ""
    for i, chunk in enumerate(text_chunks):
        chunk.prev = prev_chunk
        prev_chunk = chunk.curr
        print(f"{i}. {chunk}\n")


def tscc_process(
    json_data,
    size=DEFAULT_CHUNK_SIZE,
    overlap=DEFAULT_CHUNK_OVERLAP,
    chosen_model=LLMS["dev"],
):
    """Summarizes the document using the TSCC method and returns a single string of the ciombined segmented summaries for further processing

    Args:
        - json_data (json): contains the json formatted text extracted from the json files, that is to be processed by TSCC

    Returns:
        - str: Combined summaries generated by the TSCC from the json_data
    """
    start_time = time.time()  # Record start time
    print(f"> [PROCESS]\t{json_data['title']} - TSCC():\n", end="", flush=True)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=size, chunk_overlap=overlap
    )
    text_list = text_splitter.split_text(json_data["content"])

    processed_chunks = []
    prev_chunk = ""

    for curr_chunk in text_list:
        result = llm_process(curr_chunk, prev_chunk, chosen_model)
        prev_chunk = result
        processed_chunks.append(result)

        # Update progress bar
        progress = len(processed_chunks) / len(text_list) * 100
        print(
            f"\t[{'#' * int(progress/10)}{'-' * (10 - int(progress/10))}] {int(progress)}%",
            end="\r",
            flush=True,
        )

    combined_str = " ".join(processed_chunks)
    print("> DONE!\t")

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time
    print(f"> [INFO]\tElapsed time: {elapsed_time:.2f}s")
    print(
        f"> [INFO]\tTime completed: {time.strftime('%m-%d %H:%M:%S', time.localtime())}"
    )

    return combined_str


def llm_process(curr_chunk, prev_chunk, chosen_model, tscc_type=0) -> str:
    """_summary_

    Args:
        curr_chunk (str): The text that is to be condensed by TSCC
        prev_chunk (str): The text that provides context to aid the TSCC
        chosen_model (str, optional): Can choose between 3.5-turbo or 4-turbo-preview
        tscc_type (int, optional): For testing purposes, choosing between the type of TSCC for varied outputs. Defaults to 0.

    Returns:
        str: The chunk of text that has been condensed by the TSCC
    """

    turbo_llm = ChatOpenAI(temperature=LLM_TEMP, model_name=chosen_model)

    if tscc_type == 0:
        prompt = PromptTemplate.from_template(template=PROMPT_MAIN)
        llm_chain = LLMChain(prompt=prompt, llm=turbo_llm)
        response = llm_chain.invoke(
            {"curr_chunk": curr_chunk, "prev_chunk": prev_chunk}
        )
        return response["text"]

    if tscc_type == 1:
        prompt = PromptTemplate.from_template(template=PROMPT_2_1)

        llm_chain = LLMChain(prompt=prompt, llm=turbo_llm)
        return llm_chain.run({"curr_chunk": curr_chunk})

    if tscc_type == 2:
        prompt_1 = PromptTemplate.from_template(template=PROMPT_2_1)

        llm_chain = LLMChain(prompt=prompt_1, llm=turbo_llm)
        response = llm_chain.run({"curr_chunk": curr_chunk})

        prompt_2 = PromptTemplate.from_template(template=PROMPT_2_2)
        llm_chain = LLMChain(prompt=prompt_2, llm=turbo_llm)

        return llm_chain.run({"curr_chunk": response, "prev_chunk": prev_chunk})


def summarize_document():
    error_log_file = os.path.join(LOGS_PATH, "error_logs.txt")
    file_name = "SCI_Tu1NiBXxf0.json"

    file_path = os.path.join("./extracted_data/test", file_name)
    print(f"> Processing {file_name}")

    with open(file_path, "r", encoding="utf-8") as file:
        try:
            json_data = json.load(file)
            # tscc_process(json_data)
            processed_content = tscc_process(json_data)
            extracted = Extracted(json_data["title"], processed_content)
            write_to_file(extracted, file_name, OUTPUT_PATH)

        except Exception as e:
            # Log the error to the error log file
            log_error(file_name, str(e), error_log_file)

    print("> Processing complete.")


def main():
    """Main Menu for testing purposes"""
    while True:
        print("\nText Segmentation and Contextual Condensing\n===========")
        print("[1] Test 1 document on main prompt choice")
        print("[2] Test 1 document on 3 prompt choices")
        choice = input(">Enter your choice: ")
        if choice == "0":
            break
        elif choice == "1":
            # file = input(">Enter filename: ")
            summarize_document()
        elif choice == "2":
            test_types()
        else:
            print("Invalid choice")


if __name__ == "__main__":
    main()
